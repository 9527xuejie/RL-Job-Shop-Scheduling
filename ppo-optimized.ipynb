{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO\n",
    "\n",
    "## Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gym\n",
    "import random\n",
    "import statistics \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from multiprocessing_env import SubprocVecEnv\n",
    "from max_step_wrapper import MaxStepWrapper\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "learning_rate = 1e-4\n",
    "n_steps = 128\n",
    "tau = 0.98\n",
    "gamma = 0.999\n",
    "number_episodes = 10000\n",
    "max_steps_per_episode = 2000 # None if we don't want to limit the number of steps\n",
    "value_coefficient = 1.0\n",
    "entropy_regularization = 1e-3\n",
    "nb_actors = 16\n",
    "env_name = 'LunarLander-v2'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # We train on a GPU if available\n",
    "ppo_epoch = 10\n",
    "clipping_param = 0.2\n",
    "clipping_param_vf = 1 # None to avoid clipping the value estimation\n",
    "minibatch_size = 32\n",
    "STEP_BATCH = nb_actors * n_steps\n",
    "min_reward = -10\n",
    "max_reward = 10\n",
    "gradient_norm_clipping = 0.5\n",
    "max_kl_div = None\n",
    "assert minibatch_size <= STEP_BATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear_1 = nn.Linear(state_size, 64)\n",
    "        self.linear_2 = nn.Linear(64, action_size)\n",
    "        torch.nn.init.xavier_uniform_(self.linear_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear_2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.linear_1(x))\n",
    "        x = F.softmax(self.linear_2(x), dim=-1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear_1 = nn.Linear(state_size, 64)\n",
    "        self.linear_2 = nn.Linear(64, 64)\n",
    "        self.linear_3 = nn.Linear(64, 1)\n",
    "        torch.nn.init.xavier_uniform_(self.linear_1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear_2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear_3.weight, gain=0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.linear_1(x))\n",
    "        x = torch.tanh(self.linear_2(x))\n",
    "        x = self.linear_3(x)\n",
    "        return x\n",
    "    \n",
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = Actor(state_size, action_size)\n",
    "        self.critic = Critic(state_size, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.actor(x), self.critic(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_seeded_env(i):\n",
    "    def _anon():\n",
    "        if max_steps_per_episode is None:\n",
    "            env = gym.make(env_name)\n",
    "        else:\n",
    "            env = MaxStepWrapper(gym.make(env_name), max_steps_per_episode)\n",
    "        env.seed(seed + i)\n",
    "        return env\n",
    "\n",
    "    return _anon\n",
    "\n",
    "envs = [make_seeded_env(i) for i in range(nb_actors)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "env_infos = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_infos.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActorCritic(env_infos.observation_space.shape[0], env_infos.action_space.n).to(device)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True, betas=(0.5, 0.999), eps=1e-05)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10004\tMean current return: 3.71"
     ]
    }
   ],
   "source": [
    "# vectorized environments are automatically reset during step\n",
    "# see https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html\n",
    "states = envs.reset()\n",
    "\n",
    "episode_nb = 0\n",
    "\n",
    "while episode_nb < number_episodes:\n",
    "    \n",
    "    states_reps= []\n",
    "    state_rewards = []\n",
    "    state_dones = []\n",
    "    log_probabilities = []\n",
    "    states_values = []\n",
    "    actions_used = []\n",
    "    \n",
    "    for step in range(n_steps):\n",
    "        # we compute the state value and the probability distribution\n",
    "        with torch.no_grad():\n",
    "            states_tensor = torch.FloatTensor(states).to(device)\n",
    "            prob_state, value_state = model(states_tensor)\n",
    "            categorical  = Categorical(prob_state)\n",
    "            actions_sampled = categorical.sample()\n",
    "            log_prob = categorical.log_prob(actions_sampled)\n",
    "            # gym env needs a numpy object\n",
    "            actions = actions_sampled.cpu().numpy()\n",
    "        \n",
    "        # we act in the environments\n",
    "        states, rewards, dones, _ = envs.step(actions)\n",
    "        rewards = np.array([max(min(max_reward, reward), min_reward) for reward in rewards])\n",
    "        '''\n",
    "        reward_scaler.extend(rewards)\n",
    "        if len(reward_scaler) > 2:\n",
    "            rewards = rewards / statistics.stdev(reward_scaler)\n",
    "        '''\n",
    "        # we store the datas\n",
    "        states_reps.append(states_tensor)\n",
    "        actions_used.append(actions_sampled)\n",
    "        states_values.append(value_state)\n",
    "        state_rewards.append(torch.FloatTensor(rewards).unsqueeze(1))\n",
    "        state_dones.append(torch.IntTensor(dones).unsqueeze(1))\n",
    "        log_probabilities.append(log_prob)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # we also compute the next_state value\n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        _, next_value = model(states_tensor)\n",
    "        states_values.append(next_value)\n",
    "        gae = 0\n",
    "        for step in reversed(range(n_steps)):\n",
    "            current_step = (state_rewards[step] + gamma * states_values[step + 1] * (1 - state_dones[step])) - states_values[step]\n",
    "            gae = current_step + (gamma * tau * (1 - state_dones[step]) * gae)\n",
    "            state_rewards[step] = gae\n",
    "            \n",
    "    states_reps = torch.cat(states_reps)\n",
    "    states_values = torch.cat(states_values[:-1])\n",
    "    state_rewards = torch.cat(state_rewards)\n",
    "    state_dones = torch.cat(state_dones)\n",
    "    log_probabilities = torch.cat(log_probabilities)\n",
    "    actions_used = torch.cat(actions_used)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        advantage = state_rewards - states_values\n",
    "    # we compute the clipped loss function\n",
    "    kl_divergeance = []\n",
    "    for epoch in range(ppo_epoch):\n",
    "        \n",
    "        permutation = torch.randperm(STEP_BATCH)\n",
    "        \n",
    "        # we normalize the advantage\n",
    "        with torch.no_grad():\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        for i in range(0, STEP_BATCH, minibatch_size):\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            indices = permutation[i: i+minibatch_size]\n",
    "            \n",
    "            advantage_indicies = advantage[indices]\n",
    "            \n",
    "            new_probabilities, new_states_values = model(states_reps[indices])\n",
    "            categorical  = Categorical(new_probabilities)\n",
    "            new_probabilities = categorical.log_prob(actions_used[indices])\n",
    "\n",
    "            # minus because we get log probabilities\n",
    "            ratio = (new_probabilities - log_probabilities[indices]).exp()\n",
    "            entropy = categorical.entropy().mean()\n",
    "\n",
    "            surrogate_non_clipp  = (ratio * advantage_indicies).mean()\n",
    "            surrogate_clipp = (torch.clamp(ratio, 1 - clipping_param, 1 + clipping_param) * advantage_indicies).mean()\n",
    "            actor_loss = -torch.min(surrogate_non_clipp, surrogate_clipp)\n",
    "            \n",
    "            # non clipped advantage\n",
    "            new_advantage = state_rewards[indices] - new_states_values\n",
    "            advantage_loss = new_advantage.pow(2).mean()\n",
    "            if clipping_param_vf is not None:\n",
    "                # clipped advantage\n",
    "                clipped_new_states_values = torch.max(torch.min(new_states_values, states_values[indices] + clipping_param_vf), states_values[indices] - clipping_param_vf)\n",
    "                clipped_new_advantage = state_rewards[indices] - clipped_new_states_values\n",
    "                clipped_advantage_loss = clipped_new_advantage.pow(2).mean()\n",
    "                critic_loss = torch.min(advantage_loss, clipped_advantage_loss)\n",
    "            else:\n",
    "                critic_loss = advantage_loss\n",
    "            \n",
    "            advantage[indices] = new_advantage.detach()\n",
    "            \n",
    "            loss = actor_loss + value_coefficient * critic_loss - entropy_regularization * entropy\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_norm_clipping)\n",
    "            optimizer.step()\n",
    "            kl_divergeance.append(torch.mean(ratio))\n",
    "    \n",
    "    if max_kl_div is not None and sum(kl_divergeance) / len(kl_divergeance) > max_kl_div:\n",
    "        print(\"We have exceed the maximum KL divergeance alowed, we risk to have a policy crash\")\n",
    "        episode_nb += number_episodes\n",
    "        break\n",
    "    \n",
    "    episode_nb += torch.sum(state_dones).item()\n",
    "    print('\\rEpisode {}'.format(episode_nb), end=\"\")\n",
    "    \n",
    "# to check that it's a new episode\n",
    "total_reward = 0\n",
    "for i in range(100):\n",
    "    done = False\n",
    "    state = env_infos.reset()\n",
    "    ep = 0\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state)\n",
    "            distribution, _ = model(state_tensor)\n",
    "            categorical  = Categorical(distribution)\n",
    "            action = categorical.sample().numpy()\n",
    "        if i % 33 == 0:\n",
    "            env_infos.render()\n",
    "        state, reward, done, _ = env_infos.step(action)\n",
    "        total_reward += reward\n",
    "        ep += 1\n",
    "        if done:\n",
    "            env_infos.close()\n",
    "mean_last_eval_scores = total_reward / 100.0\n",
    "torch.save(model.state_dict(), 'model.pt')\n",
    "print('\\rEpisode {}\\tMean current return: {:.2f}'.format(episode_nb, mean_last_eval_scores), end=\"\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
