2020-10-11 19:47:20,236	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_93d7d_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=21052)[0m 2020-10-11 19:47:22,964	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=20932)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20932)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20951)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20951)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21040)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21040)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21006)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21006)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20963)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20963)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21030)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21030)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20996)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20996)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21012)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21012)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20947)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20947)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20927)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20927)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20943)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20943)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21007)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21007)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21019)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21019)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21043)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21043)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20995)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20995)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20931)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20931)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20930)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20930)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20926)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20926)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20991)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20991)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21048)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21048)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20988)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20988)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21013)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21013)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21004)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21004)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21021)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21021)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20928)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20928)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21010)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21010)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21029)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21029)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21034)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21034)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20945)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20945)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20939)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20939)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21025)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21025)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20949)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20949)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21002)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21002)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20933)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20933)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21016)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21016)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20929)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20929)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21015)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21015)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20940)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20940)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21041)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21041)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20999)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20999)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21039)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21039)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21047)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21047)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20986)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20986)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20957)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20957)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20998)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20998)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20936)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20936)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20962)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20962)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20997)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20997)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20942)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20942)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21005)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21005)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20938)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20938)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20956)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20956)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20941)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20941)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21032)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21032)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21008)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21008)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21038)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21038)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20935)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20935)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21037)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21037)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21055)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21055)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21003)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21003)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20965)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20965)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20955)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20955)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21020)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21020)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20946)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20946)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21000)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21000)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21023)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21023)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21009)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21009)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21054)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21054)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21036)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21036)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21046)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21046)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21027)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21027)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20994)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20994)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20992)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20992)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21022)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21022)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20944)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20944)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20964)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20964)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20959)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20959)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21001)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21001)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=21058)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=21058)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_19-48-00
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1838011741638184
        entropy_coeff: 0.0001
        kl: 0.005163811866871335
        model: {}
        policy_loss: -0.010684763929351571
        total_loss: 503.6856966885653
        vf_explained_var: 0.5566456317901611
        vf_loss: 503.6954706365412
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.762162162162163
    gpu_util_percent0: 0.2897297297297297
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5783783783783787
    vram_util_percent0: 0.08969489331903238
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17145763101604258
    mean_env_wait_ms: 1.1768857296987536
    mean_inference_ms: 6.051324150671964
    mean_raw_obs_processing_ms: 0.46350531829805713
  time_since_restore: 32.02044677734375
  time_this_iter_s: 32.02044677734375
  time_total_s: 32.02044677734375
  timers:
    learn_throughput: 7231.527
    learn_time_ms: 22373.146
    sample_throughput: 16894.382
    sample_time_ms: 9576.675
    update_time_ms: 24.383
  timestamp: 1602445680
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 27.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      1 |          32.0204 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3621.5138888888887
    time_step_min: 3338
  date: 2020-10-11_19-48-30
  done: false
  episode_len_mean: 887.4715189873418
  episode_reward_max: 260.26262626262593
  episode_reward_mean: 218.09922004858691
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1555498188192195
        entropy_coeff: 0.0001
        kl: 0.00642134134911678
        model: {}
        policy_loss: -0.01293836475815624
        total_loss: 129.75687408447266
        vf_explained_var: 0.8092939257621765
        vf_loss: 129.7686441594904
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.511428571428574
    gpu_util_percent0: 0.29257142857142865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.757142857142857
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1668623629559187
    mean_env_wait_ms: 1.1719730397701653
    mean_inference_ms: 5.760926288807452
    mean_raw_obs_processing_ms: 0.4508868226722193
  time_since_restore: 61.82877731323242
  time_this_iter_s: 29.808330535888672
  time_total_s: 61.82877731323242
  timers:
    learn_throughput: 7288.335
    learn_time_ms: 22198.762
    sample_throughput: 18713.227
    sample_time_ms: 8645.863
    update_time_ms: 22.963
  timestamp: 1602445710
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      2 |          61.8288 | 323584 |  218.099 |              260.263 |              126.172 |            887.472 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3621.42600896861
    time_step_min: 3338
  date: 2020-10-11_19-48-59
  done: false
  episode_len_mean: 884.3080168776371
  episode_reward_max: 260.26262626262593
  episode_reward_mean: 218.4326173123639
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1476071856238625
        entropy_coeff: 0.0001
        kl: 0.006541634486480193
        model: {}
        policy_loss: -0.014222289340316573
        total_loss: 59.208427082408555
        vf_explained_var: 0.8983936905860901
        vf_loss: 59.22145600752397
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.874285714285715
    gpu_util_percent0: 0.3214285714285714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774285714285715
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16374678843410115
    mean_env_wait_ms: 1.1699348523606121
    mean_inference_ms: 5.54923734027948
    mean_raw_obs_processing_ms: 0.4417913157436294
  time_since_restore: 91.54978656768799
  time_this_iter_s: 29.721009254455566
  time_total_s: 91.54978656768799
  timers:
    learn_throughput: 7269.702
    learn_time_ms: 22255.657
    sample_throughput: 19759.687
    sample_time_ms: 8187.984
    update_time_ms: 24.488
  timestamp: 1602445739
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      3 |          91.5498 | 485376 |  218.433 |              260.263 |              126.172 |            884.308 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3609.5579470198677
    time_step_min: 3338
  date: 2020-10-11_19-49-29
  done: false
  episode_len_mean: 881.742088607595
  episode_reward_max: 260.26262626262593
  episode_reward_mean: 219.5961513872904
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1362413167953491
        entropy_coeff: 0.0001
        kl: 0.007407618728889661
        model: {}
        policy_loss: -0.015796244144439697
        total_loss: 39.005800073797054
        vf_explained_var: 0.9301210045814514
        vf_loss: 39.020228992808946
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.608823529411765
    gpu_util_percent0: 0.2679411764705883
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7823529411764705
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16151517787401787
    mean_env_wait_ms: 1.1691481666261911
    mean_inference_ms: 5.39388562055279
    mean_raw_obs_processing_ms: 0.4345691740205826
  time_since_restore: 120.86913776397705
  time_this_iter_s: 29.319351196289062
  time_total_s: 120.86913776397705
  timers:
    learn_throughput: 7275.919
    learn_time_ms: 22236.64
    sample_throughput: 20470.008
    sample_time_ms: 7903.856
    update_time_ms: 26.635
  timestamp: 1602445769
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      4 |          120.869 | 647168 |  219.596 |              260.263 |              126.172 |            881.742 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3603.740157480315
    time_step_min: 3287
  date: 2020-10-11_19-49-58
  done: false
  episode_len_mean: 879.0
  episode_reward_max: 275.4141414141414
  episode_reward_mean: 220.5833013681113
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1094145666469226
        entropy_coeff: 0.0001
        kl: 0.007309528448703614
        model: {}
        policy_loss: -0.015877019169486382
        total_loss: 35.69229992953214
        vf_explained_var: 0.9402286410331726
        vf_loss: 35.70682560313832
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.47647058823529
    gpu_util_percent0: 0.3508823529411765
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7735294117647062
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15984507025338307
    mean_env_wait_ms: 1.1690575283875422
    mean_inference_ms: 5.274903356561905
    mean_raw_obs_processing_ms: 0.4286882983051666
  time_since_restore: 150.14062452316284
  time_this_iter_s: 29.27148675918579
  time_total_s: 150.14062452316284
  timers:
    learn_throughput: 7279.301
    learn_time_ms: 22226.31
    sample_throughput: 20954.287
    sample_time_ms: 7721.188
    update_time_ms: 30.631
  timestamp: 1602445798
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      5 |          150.141 | 808960 |  220.583 |              275.414 |              126.172 |                879 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3585.7966101694915
    time_step_min: 3222
  date: 2020-10-11_19-50-28
  done: false
  episode_len_mean: 872.0082568807339
  episode_reward_max: 277.83838383838327
  episode_reward_mean: 223.1953479751643
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 300
  episodes_total: 1090
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0947759043086658
        entropy_coeff: 0.0001
        kl: 0.007307220843027939
        model: {}
        policy_loss: -0.014191401233388619
        total_loss: 33.74857538396662
        vf_explained_var: 0.960421621799469
        vf_loss: 33.761413921009414
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.594285714285718
    gpu_util_percent0: 0.29200000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7714285714285714
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1577235009177227
    mean_env_wait_ms: 1.1713421610826151
    mean_inference_ms: 5.119295875197107
    mean_raw_obs_processing_ms: 0.42121204669263024
  time_since_restore: 179.7077920436859
  time_this_iter_s: 29.56716752052307
  time_total_s: 179.7077920436859
  timers:
    learn_throughput: 7261.182
    learn_time_ms: 22281.773
    sample_throughput: 21317.733
    sample_time_ms: 7589.55
    update_time_ms: 30.199
  timestamp: 1602445828
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      6 |          179.708 | 970752 |  223.195 |              277.838 |              126.172 |            872.008 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3572.276699029126
    time_step_min: 3222
  date: 2020-10-11_19-50-57
  done: false
  episode_len_mean: 867.3837025316456
  episode_reward_max: 281.17171717171686
  episode_reward_mean: 225.27329465541473
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 174
  episodes_total: 1264
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0974578640677712
        entropy_coeff: 0.0001
        kl: 0.006937279746952382
        model: {}
        policy_loss: -0.01453719080679796
        total_loss: 17.95092079856179
        vf_explained_var: 0.9665346145629883
        vf_loss: 17.964180166071113
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.2235294117647
    gpu_util_percent0: 0.4167647058823529
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7882352941176474
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15678539156024118
    mean_env_wait_ms: 1.1723800963638422
    mean_inference_ms: 5.053303894007508
    mean_raw_obs_processing_ms: 0.4179966752572687
  time_since_restore: 208.92204093933105
  time_this_iter_s: 29.21424889564514
  time_total_s: 208.92204093933105
  timers:
    learn_throughput: 7258.195
    learn_time_ms: 22290.941
    sample_throughput: 21641.33
    sample_time_ms: 7476.065
    update_time_ms: 29.074
  timestamp: 1602445857
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      7 |          208.922 | 1132544 |  225.273 |              281.172 |              126.172 |            867.384 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3560.970588235294
    time_step_min: 3222
  date: 2020-10-11_19-51-26
  done: false
  episode_len_mean: 863.6005625879044
  episode_reward_max: 281.17171717171686
  episode_reward_mean: 226.79386693943638
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0780203125693582
        entropy_coeff: 0.0001
        kl: 0.0068403754637322645
        model: {}
        policy_loss: -0.015216219895095988
        total_loss: 18.283373746004973
        vf_explained_var: 0.9662302136421204
        vf_loss: 18.297328775579278
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.391176470588235
    gpu_util_percent0: 0.34911764705882353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7823529411764705
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1560862181459908
    mean_env_wait_ms: 1.1734060247433131
    mean_inference_ms: 5.00244358343099
    mean_raw_obs_processing_ms: 0.41550090300287146
  time_since_restore: 238.11030435562134
  time_this_iter_s: 29.188263416290283
  time_total_s: 238.11030435562134
  timers:
    learn_throughput: 7263.94
    learn_time_ms: 22273.313
    sample_throughput: 21828.787
    sample_time_ms: 7411.864
    update_time_ms: 28.797
  timestamp: 1602445886
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      8 |           238.11 | 1294336 |  226.794 |              281.172 |              126.172 |            863.601 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3550.722293814433
    time_step_min: 3222
  date: 2020-10-11_19-51-55
  done: false
  episode_len_mean: 860.3550632911392
  episode_reward_max: 281.17171717171686
  episode_reward_mean: 228.48548778928506
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0541057153181597
        entropy_coeff: 0.0001
        kl: 0.006940083197233352
        model: {}
        policy_loss: -0.015763346613808113
        total_loss: 14.273174112493342
        vf_explained_var: 0.9712476134300232
        vf_loss: 14.28765513680198
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.60588235294118
    gpu_util_percent0: 0.29823529411764704
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779411764705883
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15547445563220483
    mean_env_wait_ms: 1.1744002854051698
    mean_inference_ms: 4.9580061919190435
    mean_raw_obs_processing_ms: 0.4132552856665159
  time_since_restore: 267.0965311527252
  time_this_iter_s: 28.986226797103882
  time_total_s: 267.0965311527252
  timers:
    learn_throughput: 7273.321
    learn_time_ms: 22244.585
    sample_throughput: 21998.577
    sample_time_ms: 7354.658
    update_time_ms: 27.922
  timestamp: 1602445915
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |      9 |          267.097 | 1456128 |  228.485 |              281.172 |              126.172 |            860.355 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3537.3643454038997
    time_step_min: 3204
  date: 2020-10-11_19-52-25
  done: false
  episode_len_mean: 854.5792649478881
  episode_reward_max: 288.1414141414138
  episode_reward_mean: 230.81890213157337
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 243
  episodes_total: 1823
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0152668194337324
        entropy_coeff: 0.0001
        kl: 0.006765558596023105
        model: {}
        policy_loss: -0.015756549144333058
        total_loss: 21.14260378750888
        vf_explained_var: 0.9712116122245789
        vf_loss: 21.157107960094105
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.058823529411764
    gpu_util_percent0: 0.27588235294117647
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.764705882352941
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15469502576748936
    mean_env_wait_ms: 1.1764270620340807
    mean_inference_ms: 4.900755216933601
    mean_raw_obs_processing_ms: 0.4103799585567721
  time_since_restore: 296.24453616142273
  time_this_iter_s: 29.14800500869751
  time_total_s: 296.24453616142273
  timers:
    learn_throughput: 7278.986
    learn_time_ms: 22227.272
    sample_throughput: 22104.411
    sample_time_ms: 7319.444
    update_time_ms: 27.172
  timestamp: 1602445945
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     10 |          296.245 | 1617920 |  230.819 |              288.141 |              126.172 |            854.579 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3523.2418558736426
    time_step_min: 3204
  date: 2020-10-11_19-52-54
  done: false
  episode_len_mean: 849.5876338851023
  episode_reward_max: 288.1414141414138
  episode_reward_mean: 232.61155370649024
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 231
  episodes_total: 2054
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0204124233939431
        entropy_coeff: 0.0001
        kl: 0.0060271186838773165
        model: {}
        policy_loss: -0.01423721737228334
        total_loss: 14.337976975874467
        vf_explained_var: 0.9752159714698792
        vf_loss: 14.351110805164684
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54705882352941
    gpu_util_percent0: 0.2852941176470588
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7735294117647062
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15407627920319428
    mean_env_wait_ms: 1.178085138431977
    mean_inference_ms: 4.855676039472889
    mean_raw_obs_processing_ms: 0.40809294355719006
  time_since_restore: 325.26196360588074
  time_this_iter_s: 29.017427444458008
  time_total_s: 325.26196360588074
  timers:
    learn_throughput: 7289.706
    learn_time_ms: 22194.586
    sample_throughput: 22944.392
    sample_time_ms: 7051.483
    update_time_ms: 26.688
  timestamp: 1602445974
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     11 |          325.262 | 1779712 |  232.612 |              288.141 |              126.172 |            849.588 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3515.2266483516482
    time_step_min: 3204
  date: 2020-10-11_19-53-23
  done: false
  episode_len_mean: 846.2825497287523
  episode_reward_max: 288.1414141414138
  episode_reward_mean: 233.78329862823517
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.001401890407909
        entropy_coeff: 0.0001
        kl: 0.006522362840107896
        model: {}
        policy_loss: -0.015361127231947401
        total_loss: 11.37528844313188
        vf_explained_var: 0.9776011109352112
        vf_loss: 11.389445304870605
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.923529411764708
    gpu_util_percent0: 0.27941176470588236
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7735294117647062
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15371473008756528
    mean_env_wait_ms: 1.1792171224829138
    mean_inference_ms: 4.8289680939454405
    mean_raw_obs_processing_ms: 0.4067531825821237
  time_since_restore: 354.5268726348877
  time_this_iter_s: 29.264909029006958
  time_total_s: 354.5268726348877
  timers:
    learn_throughput: 7287.125
    learn_time_ms: 22202.447
    sample_throughput: 23156.823
    sample_time_ms: 6986.796
    update_time_ms: 28.304
  timestamp: 1602446003
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     12 |          354.527 | 1941504 |  233.783 |              288.141 |              126.172 |            846.283 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3507.3495311167944
    time_step_min: 3162
  date: 2020-10-11_19-53-52
  done: false
  episode_len_mean: 843.3917438921651
  episode_reward_max: 288.1414141414138
  episode_reward_mean: 234.99980427697346
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 162
  episodes_total: 2374
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9799625711007551
        entropy_coeff: 0.0001
        kl: 0.006172410932115533
        model: {}
        policy_loss: -0.013553760607134212
        total_loss: 13.612575617703525
        vf_explained_var: 0.973476231098175
        vf_loss: 13.624992977489125
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.661764705882355
    gpu_util_percent0: 0.41117647058823537
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7794117647058822
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1533806835222506
    mean_env_wait_ms: 1.18038700173139
    mean_inference_ms: 4.804262799464633
    mean_raw_obs_processing_ms: 0.40550374626940133
  time_since_restore: 383.6045708656311
  time_this_iter_s: 29.077698230743408
  time_total_s: 383.6045708656311
  timers:
    learn_throughput: 7300.448
    learn_time_ms: 22161.926
    sample_throughput: 23233.472
    sample_time_ms: 6963.746
    update_time_ms: 27.607
  timestamp: 1602446032
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     13 |          383.605 | 2103296 |      235 |              288.141 |              126.172 |            843.392 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3495.5613171839514
    time_step_min: 3153
  date: 2020-10-11_19-54-22
  done: false
  episode_len_mean: 839.0250936329588
  episode_reward_max: 288.2929292929292
  episode_reward_mean: 236.83299284984668
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 296
  episodes_total: 2670
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9559003439816561
        entropy_coeff: 0.0001
        kl: 0.00579497158866037
        model: {}
        policy_loss: -0.012460902671922337
        total_loss: 17.59640953757546
        vf_explained_var: 0.9766353964805603
        vf_loss: 17.607806465842508
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.51764705882353
    gpu_util_percent0: 0.3014705882352941
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.764705882352941
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15284820561901935
    mean_env_wait_ms: 1.1824203463670637
    mean_inference_ms: 4.764617966437111
    mean_raw_obs_processing_ms: 0.40350836945527685
  time_since_restore: 412.864652633667
  time_this_iter_s: 29.26008176803589
  time_total_s: 412.864652633667
  timers:
    learn_throughput: 7300.366
    learn_time_ms: 22162.177
    sample_throughput: 23256.601
    sample_time_ms: 6956.821
    update_time_ms: 28.409
  timestamp: 1602446062
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     14 |          412.865 | 2265088 |  236.833 |              288.293 |              126.172 |            839.025 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3490.4609375
    time_step_min: 3153
  date: 2020-10-11_19-54-51
  done: false
  episode_len_mean: 836.7978199718706
  episode_reward_max: 288.2929292929292
  episode_reward_mean: 237.71052295102913
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 174
  episodes_total: 2844
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9504945874214172
        entropy_coeff: 0.0001
        kl: 0.0057300583205439825
        model: {}
        policy_loss: -0.014623514926907692
        total_loss: 11.53172423622825
        vf_explained_var: 0.9788700342178345
        vf_loss: 11.545296842401678
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.48235294117647
    gpu_util_percent0: 0.4488235294117647
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7823529411764705
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15257846908474287
    mean_env_wait_ms: 1.183539241739263
    mean_inference_ms: 4.744569104868732
    mean_raw_obs_processing_ms: 0.4025072581227037
  time_since_restore: 441.9203221797943
  time_this_iter_s: 29.05566954612732
  time_total_s: 441.9203221797943
  timers:
    learn_throughput: 7305.183
    learn_time_ms: 22147.562
    sample_throughput: 23279.622
    sample_time_ms: 6949.941
    update_time_ms: 27.515
  timestamp: 1602446091
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     15 |           441.92 | 2426880 |  237.711 |              288.293 |              126.172 |            836.798 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3484.600537995965
    time_step_min: 3153
  date: 2020-10-11_19-55-20
  done: false
  episode_len_mean: 835.0739506995336
  episode_reward_max: 288.2929292929292
  episode_reward_mean: 238.54089529539218
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 3002
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9346523339098151
        entropy_coeff: 0.0001
        kl: 0.006312972315671769
        model: {}
        policy_loss: -0.014420091174542904
        total_loss: 10.676065444946289
        vf_explained_var: 0.9785726070404053
        vf_loss: 10.689316489479758
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.194117647058825
    gpu_util_percent0: 0.28764705882352937
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7911764705882356
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15235313442028847
    mean_env_wait_ms: 1.1844775160719572
    mean_inference_ms: 4.727744210418092
    mean_raw_obs_processing_ms: 0.40166329056769867
  time_since_restore: 471.06262254714966
  time_this_iter_s: 29.142300367355347
  time_total_s: 471.06262254714966
  timers:
    learn_throughput: 7317.806
    learn_time_ms: 22109.358
    sample_throughput: 23295.543
    sample_time_ms: 6945.191
    update_time_ms: 27.135
  timestamp: 1602446120
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     16 |          471.063 | 2588672 |  238.541 |              288.293 |              126.172 |            835.074 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3477.843720341664
    time_step_min: 3153
  date: 2020-10-11_19-55-49
  done: false
  episode_len_mean: 833.0661649419881
  episode_reward_max: 288.2929292929292
  episode_reward_mean: 239.4796316884745
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 187
  episodes_total: 3189
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9027818983251398
        entropy_coeff: 0.0001
        kl: 0.006948164875873111
        model: {}
        policy_loss: -0.014334690587764437
        total_loss: 11.005608992143111
        vf_explained_var: 0.9815151691436768
        vf_loss: 11.018644766374068
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.691176470588236
    gpu_util_percent0: 0.2961764705882353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7735294117647062
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15211080301899177
    mean_env_wait_ms: 1.185559777020482
    mean_inference_ms: 4.70954039141936
    mean_raw_obs_processing_ms: 0.4007398063882595
  time_since_restore: 500.0516245365143
  time_this_iter_s: 28.989001989364624
  time_total_s: 500.0516245365143
  timers:
    learn_throughput: 7330.161
    learn_time_ms: 22072.093
    sample_throughput: 23252.248
    sample_time_ms: 6958.123
    update_time_ms: 28.122
  timestamp: 1602446149
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     17 |          500.052 | 2750464 |   239.48 |              288.293 |              126.172 |            833.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3468.6519721577724
    time_step_min: 3147
  date: 2020-10-11_19-56-18
  done: false
  episode_len_mean: 830.3449367088608
  episode_reward_max: 289.2020202020204
  episode_reward_mean: 240.82527228557137
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 287
  episodes_total: 3476
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8946625102650035
        entropy_coeff: 0.0001
        kl: 0.006113460499115966
        model: {}
        policy_loss: -0.013128449791111052
        total_loss: 11.671645337885076
        vf_explained_var: 0.9829363226890564
        vf_loss: 11.68364056673917
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.435294117647057
    gpu_util_percent0: 0.30323529411764705
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7764705882352945
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15177334203273393
    mean_env_wait_ms: 1.1870932723531622
    mean_inference_ms: 4.684368630329786
    mean_raw_obs_processing_ms: 0.39946377994218946
  time_since_restore: 528.939297914505
  time_this_iter_s: 28.887673377990723
  time_total_s: 528.939297914505
  timers:
    learn_throughput: 7337.907
    learn_time_ms: 22048.795
    sample_throughput: 23275.258
    sample_time_ms: 6951.244
    update_time_ms: 27.338
  timestamp: 1602446178
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     18 |          528.939 | 2912256 |  240.825 |              289.202 |              126.172 |            830.345 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3463.885191347754
    time_step_min: 3147
  date: 2020-10-11_19-56-48
  done: false
  episode_len_mean: 828.9433131535498
  episode_reward_max: 289.2020202020204
  episode_reward_mean: 241.57627179889138
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 158
  episodes_total: 3634
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8847840103236112
        entropy_coeff: 0.0001
        kl: 0.006220871497961608
        model: {}
        policy_loss: -0.01448176123350012
        total_loss: 8.768974564292215
        vf_explained_var: 0.9826990365982056
        vf_loss: 8.782300689003684
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.211764705882356
    gpu_util_percent0: 0.4497058823529412
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.785294117647059
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15160739675042462
    mean_env_wait_ms: 1.1878580619601686
    mean_inference_ms: 4.671953257423221
    mean_raw_obs_processing_ms: 0.3988336707530648
  time_since_restore: 558.1717095375061
  time_this_iter_s: 29.2324116230011
  time_total_s: 558.1717095375061
  timers:
    learn_throughput: 7332.607
    learn_time_ms: 22064.731
    sample_throughput: 23256.181
    sample_time_ms: 6956.946
    update_time_ms: 28.922
  timestamp: 1602446208
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     19 |          558.172 | 3074048 |  241.576 |              289.202 |              126.172 |            828.943 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3458.203290870488
    time_step_min: 3147
  date: 2020-10-11_19-57-17
  done: false
  episode_len_mean: 827.4971022128557
  episode_reward_max: 289.2020202020204
  episode_reward_mean: 242.36248416727858
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 162
  episodes_total: 3796
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8720677386630665
        entropy_coeff: 0.0001
        kl: 0.006227376405149698
        model: {}
        policy_loss: -0.015219807836481115
        total_loss: 9.142887809059836
        vf_explained_var: 0.981269121170044
        vf_loss: 9.156949303366922
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.05714285714286
    gpu_util_percent0: 0.3985714285714286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.788571428571429
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15144783037380902
    mean_env_wait_ms: 1.1886242813536705
    mean_inference_ms: 4.660114049962952
    mean_raw_obs_processing_ms: 0.3982334541812821
  time_since_restore: 587.6119403839111
  time_this_iter_s: 29.44023084640503
  time_total_s: 587.6119403839111
  timers:
    learn_throughput: 7321.281
    learn_time_ms: 22098.865
    sample_throughput: 23280.753
    sample_time_ms: 6949.603
    update_time_ms: 30.738
  timestamp: 1602446237
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | RUNNING  | 172.17.0.4:21052 |     20 |          587.612 | 3235840 |  242.362 |              289.202 |              126.172 |            827.497 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_93d7d_00000:
  custom_metrics:
    time_step_max: 4223
    time_step_mean: 3451.7338949454906
    time_step_min: 3147
  date: 2020-10-11_19-57-47
  done: true
  episode_len_mean: 825.5504429133858
  episode_reward_max: 296.62626262626287
  episode_reward_mean: 243.31969796389077
  episode_reward_min: 126.17171717171705
  episodes_this_iter: 268
  episodes_total: 4064
  experiment_id: a055faac0a434f109aca788a5575c1ff
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8471026095477018
        entropy_coeff: 0.0001
        kl: 0.006117314438928257
        model: {}
        policy_loss: -0.014143136329948902
        total_loss: 11.25829809362238
        vf_explained_var: 0.9842967391014099
        vf_loss: 11.271302743391557
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.35294117647059
    gpu_util_percent0: 0.31205882352941183
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7735294117647062
    vram_util_percent0: 0.1070022958346999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 21052
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15120480364192343
    mean_env_wait_ms: 1.189768610077951
    mean_inference_ms: 4.641742185739638
    mean_raw_obs_processing_ms: 0.39728531199334505
  time_since_restore: 616.8343117237091
  time_this_iter_s: 29.222371339797974
  time_total_s: 616.8343117237091
  timers:
    learn_throughput: 7319.973
    learn_time_ms: 22102.814
    sample_throughput: 23230.689
    sample_time_ms: 6964.58
    update_time_ms: 30.8
  timestamp: 1602446267
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 93d7d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | TERMINATED |       |     21 |          616.834 | 3397632 |   243.32 |              296.626 |              126.172 |             825.55 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_93d7d_00000 | TERMINATED |       |     21 |          616.834 | 3397632 |   243.32 |              296.626 |              126.172 |             825.55 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


2020-10-11 19:57:47,308	WARNING worker.py:1072 -- A worker died or was killed while executing task ffffffffffffffffd8f83c3801000000.
[2m[36m(pid=20959)[0m 2020-10-11 19:57:47,275	ERROR worker.py:372 -- SystemExit was raised from the worker
[2m[36m(pid=20959)[0m Traceback (most recent call last):
[2m[36m(pid=20959)[0m   File "python/ray/_raylet.pyx", line 553, in ray._raylet.task_execution_handler
[2m[36m(pid=20959)[0m   File "python/ray/_raylet.pyx", line 440, in ray._raylet.execute_task
[2m[36m(pid=20959)[0m   File "python/ray/_raylet.pyx", line 479, in ray._raylet.execute_task
[2m[36m(pid=20959)[0m   File "python/ray/_raylet.pyx", line 483, in ray._raylet.execute_task
[2m[36m(pid=20959)[0m   File "python/ray/_raylet.pyx", line 484, in ray._raylet.execute_task
[2m[36m(pid=20959)[0m   File "python/ray/_raylet.pyx", line 438, in ray._raylet.execute_task.function_executor
[2m[36m(pid=20959)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/function_manager.py", line 553, in actor_method_executor
[2m[36m(pid=20959)[0m     return method(actor, *args, **kwargs)
[2m[36m(pid=20959)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 929, in __ray_terminate__
[2m[36m(pid=20959)[0m     ray.actor.exit_actor()
[2m[36m(pid=20959)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/actor.py", line 991, in exit_actor
[2m[36m(pid=20959)[0m     ray.state.state.disconnect()
[2m[36m(pid=20959)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/state.py", line 60, in disconnect
[2m[36m(pid=20959)[0m     self.global_state_accessor.disconnect()
[2m[36m(pid=20959)[0m   File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 369, in sigterm_handler
[2m[36m(pid=20959)[0m     sys.exit(1)
[2m[36m(pid=20959)[0m SystemExit: 1
