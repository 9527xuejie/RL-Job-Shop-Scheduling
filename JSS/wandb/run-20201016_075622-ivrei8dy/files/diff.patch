diff --git a/JSS/.ipynb_checkpoints/PPO-checkpoint.ipynb b/JSS/.ipynb_checkpoints/PPO-checkpoint.ipynb
index 4ec3a0c..ee79e38 100644
--- a/JSS/.ipynb_checkpoints/PPO-checkpoint.ipynb
+++ b/JSS/.ipynb_checkpoints/PPO-checkpoint.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [
     {
@@ -56,7 +56,7 @@
     "    print(\"I have detected {} CPUs here, so I'm going to create {} actors\".format(mp.cpu_count(), mp.cpu_count() - 1))\n",
     "    os.environ[\"WANDB_API_KEY\"] = '3487a01956bf67cc7882bca2a38f70c8c95f8463'\n",
     "    sweep_config = {\n",
-    "        'program': 'train.py',\n",
+    "        'program': 'MTWR.py',\n",
     "        'method': 'grid',\n",
     "        'metric': {\n",
     "            'name': 'time_step_min',\n",
@@ -74,15 +74,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Create sweep with ID: 1x8v92mc\n",
-      "Sweep URL: https://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/1x8v92mc\n"
+      "Create sweep with ID: 2ex0mmbf\n",
+      "Sweep URL: https://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/2ex0mmbf\n"
      ]
     }
    ],
@@ -92,7 +92,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -100,67 +100,2987 @@
      "output_type": "stream",
      "text": [
       "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
-      "2020-10-13 11:45:51,946 - wandb.wandb_agent - INFO - Running runs: []\n",
-      "2020-10-13 11:45:52,259 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-13 11:45:52,260 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
+      "2020-10-14 21:02:46,189 - wandb.wandb_agent - INFO - Running runs: []\n",
+      "2020-10-14 21:02:46,519 - wandb.wandb_agent - INFO - Agent received command: run\n",
+      "2020-10-14 21:02:46,519 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
       "\tinstance_path: /JSS/JSS/env/instances/ta51\n",
-      "2020-10-13 11:45:52,261 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python RandomGreedy.py --instance_path=/JSS/JSS/env/instances/ta51\n",
+      "2020-10-14 21:02:46,521 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta51\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
+      "2020-10-14 21:02:51,538 - wandb.wandb_agent - INFO - Running runs: ['z2o08s2g']\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrandom\u001b[0m\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mupbeat-sweep-1\u001b[0m\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/1x8v92mc\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/3qwfavbb\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201013_114553-3qwfavbb\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/2ex0mmbf\u001b[0m\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/z2o08s2g\u001b[0m\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_210248-z2o08s2g\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
-      "2020-10-13 11:45:57,278 - wandb.wandb_agent - INFO - Running runs: ['3qwfavbb']\n",
-      "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 39394\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201013_114553-3qwfavbb/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201013_114553-3qwfavbb/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 4473\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 261.82891\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 280.71717\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     time_step_min 3203\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 607\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602590160\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     time_step_min ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mrandom\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/3qwfavbb\u001b[0m\n",
-      "2020-10-13 11:56:07,517 - wandb.wandb_agent - INFO - Cleaning up finished run: 3qwfavbb\n",
-      "2020-10-13 11:56:07,847 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-13 11:56:07,847 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta52\n",
-      "2020-10-13 11:56:07,849 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python RandomGreedy.py --instance_path=/JSS/JSS/env/instances/ta52\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrandom\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/1x8v92mc\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/av30c7rd\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201013_115608-av30c7rd\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
+      "2020-10-14 21:02:52,176\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
+      "== Status ==\n",
+      "Memory usage on this node: 11.5/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+-------+\n",
+      "| Trial name              | status   | loc   |\n",
+      "|-------------------------+----------+-------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  |       |\n",
+      "+-------------------------+----------+-------+\n",
+      "\n",
+      "\n",
+      "\u001b[2m\u001b[36m(pid=422)\u001b[0m 2020-10-14 21:02:54,925\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
+      "\u001b[2m\u001b[36m(pid=336)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=336)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=301)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=321)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=321)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=367)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=404)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=404)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=412)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=412)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=326)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=326)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=407)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=407)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=308)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=308)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=384)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=384)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=388)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=388)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=333)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=333)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=414)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=414)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=383)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=383)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=364)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=364)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=373)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=373)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=379)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=379)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=411)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=411)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=417)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=417)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=392)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=392)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=390)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=390)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=419)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=419)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=374)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=374)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=362)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=362)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=315)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=315)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=305)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=305)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=408)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=408)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=413)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=413)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=423)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=423)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=316)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=316)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=429)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=429)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=313)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=313)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=376)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=376)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=389)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=389)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=370)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=370)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=409)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=409)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=300)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=300)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=369)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=369)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=297)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=297)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=381)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=381)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=378)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=378)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=401)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=401)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=426)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=426)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=399)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=399)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=433)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=433)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=380)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=380)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=396)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=396)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=400)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=400)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=318)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=318)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=309)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=309)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=314)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=314)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=395)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=395)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=319)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=319)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=307)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=307)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=361)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=361)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=328)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=328)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=317)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=317)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=375)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=375)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=330)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=330)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=386)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=386)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=356)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=356)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=310)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=310)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=320)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=320)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=359)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=359)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=377)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=377)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=357)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=357)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=299)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=299)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=304)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=298)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=298)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=302)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=302)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=368)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=368)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=329)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=329)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=312)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=312)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=397)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=397)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=365)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=365)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=303)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=303)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=394)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=394)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=363)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=363)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=324)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=324)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3615.0923076923077\n",
+      "    time_step_min: 3379\n",
+      "  date: 2020-10-14_21-03-28\n",
+      "  done: false\n",
+      "  episode_len_mean: 891.1139240506329\n",
+      "  episode_reward_max: 258.59595959595964\n",
+      "  episode_reward_mean: 216.07678046285614\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 158\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.20000000000000004\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1851047078768413\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.004071502441850801\n",
+      "        model: {}\n",
+      "        policy_loss: -0.00785889983914482\n",
+      "        total_loss: 507.07567087809247\n",
+      "        vf_explained_var: 0.540532648563385\n",
+      "        vf_loss: 507.0832926432292\n",
+      "    num_steps_sampled: 161792\n",
+      "    num_steps_trained: 161792\n",
+      "  iterations_since_restore: 1\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 28.05757575757576\n",
+      "    gpu_util_percent0: 0.36818181818181817\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.5575757575757576\n",
+      "    vram_util_percent0: 0.08736346740610434\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.16797264772787918\n",
+      "    mean_env_wait_ms: 1.1681326777525027\n",
+      "    mean_inference_ms: 5.3567412135850585\n",
+      "    mean_raw_obs_processing_ms: 0.43960491202796964\n",
+      "  time_since_restore: 28.13359045982361\n",
+      "  time_this_iter_s: 28.13359045982361\n",
+      "  time_total_s: 28.13359045982361\n",
+      "  timers:\n",
+      "    learn_throughput: 8285.934\n",
+      "    learn_time_ms: 19526.102\n",
+      "    sample_throughput: 18950.001\n",
+      "    sample_time_ms: 8537.836\n",
+      "    update_time_ms: 44.19\n",
+      "  timestamp: 1602709408\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 161792\n",
+      "  training_iteration: 1\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 27.6/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      1 |          28.1336 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3614.4305555555557\n",
+      "    time_step_min: 3250\n",
+      "  date: 2020-10-14_21-03-56\n",
+      "  done: false\n",
+      "  episode_len_mean: 890.8607594936709\n",
+      "  episode_reward_max: 273.5959595959592\n",
+      "  episode_reward_mean: 217.6365234624726\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 316\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1561074058214824\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007923512797181806\n",
+      "        model: {}\n",
+      "        policy_loss: -0.010965243893830726\n",
+      "        total_loss: 127.46906661987305\n",
+      "        vf_explained_var: 0.8076093792915344\n",
+      "        vf_loss: 127.47981770833333\n",
+      "    num_steps_sampled: 323584\n",
+      "    num_steps_trained: 323584\n",
+      "  iterations_since_restore: 2\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 24.90625\n",
+      "    gpu_util_percent0: 0.28125\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7593750000000004\n",
+      "    vram_util_percent0: 0.10437848474909807\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1644697490894654\n",
+      "    mean_env_wait_ms: 1.1643447442552444\n",
+      "    mean_inference_ms: 5.274918357821516\n",
+      "    mean_raw_obs_processing_ms: 0.43312069147187726\n",
+      "  time_since_restore: 55.80924940109253\n",
+      "  time_this_iter_s: 27.67565894126892\n",
+      "  time_total_s: 55.80924940109253\n",
+      "  timers:\n",
+      "    learn_throughput: 8278.375\n",
+      "    learn_time_ms: 19543.932\n",
+      "    sample_throughput: 19526.756\n",
+      "    sample_time_ms: 8285.657\n",
+      "    update_time_ms: 39.034\n",
+      "  timestamp: 1602709436\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 323584\n",
+      "  training_iteration: 2\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      2 |          55.8092 | 323584 |  217.637 |              273.596 |              145.717 |            890.861 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3601.8677130044844\n",
+      "    time_step_min: 3250\n",
+      "  date: 2020-10-14_21-04-23\n",
+      "  done: false\n",
+      "  episode_len_mean: 885.132911392405\n",
+      "  episode_reward_max: 273.5959595959592\n",
+      "  episode_reward_mean: 219.87009333844756\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 474\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1456398169199626\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.008224547879459957\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013529085864623388\n",
+      "        total_loss: 61.275455474853516\n",
+      "        vf_explained_var: 0.8916645646095276\n",
+      "        vf_loss: 61.28873507181803\n",
+      "    num_steps_sampled: 485376\n",
+      "    num_steps_trained: 485376\n",
+      "  iterations_since_restore: 3\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.758064516129032\n",
+      "    gpu_util_percent0: 0.2945161290322581\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7677419354838704\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.16202246416545443\n",
+      "    mean_env_wait_ms: 1.1639664955356654\n",
+      "    mean_inference_ms: 5.163724414780632\n",
+      "    mean_raw_obs_processing_ms: 0.4268547752872556\n",
+      "  time_since_restore: 82.59170770645142\n",
+      "  time_this_iter_s: 26.782458305358887\n",
+      "  time_total_s: 82.59170770645142\n",
+      "  timers:\n",
+      "    learn_throughput: 8286.405\n",
+      "    learn_time_ms: 19524.994\n",
+      "    sample_throughput: 20463.34\n",
+      "    sample_time_ms: 7906.432\n",
+      "    update_time_ms: 40.76\n",
+      "  timestamp: 1602709463\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 485376\n",
+      "  training_iteration: 3\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      3 |          82.5917 | 485376 |   219.87 |              273.596 |              145.717 |            885.133 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3596.0099337748343\n",
+      "    time_step_min: 3231\n",
+      "  date: 2020-10-14_21-04-49\n",
+      "  done: false\n",
+      "  episode_len_mean: 878.7689873417721\n",
+      "  episode_reward_max: 276.47474747474763\n",
+      "  episode_reward_mean: 220.6047340493541\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 632\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1263898611068726\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.008100568510902425\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013406771836647144\n",
+      "        total_loss: 47.16934140523275\n",
+      "        vf_explained_var: 0.9198758602142334\n",
+      "        vf_loss: 47.18250052134196\n",
+      "    num_steps_sampled: 647168\n",
+      "    num_steps_trained: 647168\n",
+      "  iterations_since_restore: 4\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.72666666666667\n",
+      "    gpu_util_percent0: 0.3456666666666667\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.16012064049546954\n",
+      "    mean_env_wait_ms: 1.164612439662183\n",
+      "    mean_inference_ms: 5.066839245558316\n",
+      "    mean_raw_obs_processing_ms: 0.4213159869942824\n",
+      "  time_since_restore: 109.37071919441223\n",
+      "  time_this_iter_s: 26.779011487960815\n",
+      "  time_total_s: 109.37071919441223\n",
+      "  timers:\n",
+      "    learn_throughput: 8258.447\n",
+      "    learn_time_ms: 19591.094\n",
+      "    sample_throughput: 21127.332\n",
+      "    sample_time_ms: 7657.947\n",
+      "    update_time_ms: 36.581\n",
+      "  timestamp: 1602709489\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 647168\n",
+      "  training_iteration: 4\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      4 |          109.371 | 647168 |  220.605 |              276.475 |              145.717 |            878.769 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3581.6985583224114\n",
+      "    time_step_min: 3204\n",
+      "  date: 2020-10-14_21-05-16\n",
+      "  done: false\n",
+      "  episode_len_mean: 872.4867256637168\n",
+      "  episode_reward_max: 280.5656565656565\n",
+      "  episode_reward_mean: 222.48133675567283\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 159\n",
+      "  episodes_total: 791\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0873714486757915\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006956188706681132\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011262792395427823\n",
+      "        total_loss: 34.19948164621989\n",
+      "        vf_explained_var: 0.9459590911865234\n",
+      "        vf_loss: 34.2105925877889\n",
+      "    num_steps_sampled: 808960\n",
+      "    num_steps_trained: 808960\n",
+      "  iterations_since_restore: 5\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.203225806451613\n",
+      "    gpu_util_percent0: 0.2932258064516129\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7645161290322577\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15862506856835393\n",
+      "    mean_env_wait_ms: 1.1666529609326048\n",
+      "    mean_inference_ms: 4.987482402804091\n",
+      "    mean_raw_obs_processing_ms: 0.4166214436489198\n",
+      "  time_since_restore: 135.77992701530457\n",
+      "  time_this_iter_s: 26.409207820892334\n",
+      "  time_total_s: 135.77992701530457\n",
+      "  timers:\n",
+      "    learn_throughput: 8267.911\n",
+      "    learn_time_ms: 19568.667\n",
+      "    sample_throughput: 21578.792\n",
+      "    sample_time_ms: 7497.732\n",
+      "    update_time_ms: 35.062\n",
+      "  timestamp: 1602709516\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 808960\n",
+      "  training_iteration: 5\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      5 |           135.78 | 808960 |  222.481 |              280.566 |              145.717 |            872.487 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3564.887755102041\n",
+      "    time_step_min: 3204\n",
+      "  date: 2020-10-14_21-05-43\n",
+      "  done: false\n",
+      "  episode_len_mean: 860.6943942133815\n",
+      "  episode_reward_max: 280.5656565656565\n",
+      "  episode_reward_mean: 225.7112809834327\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 315\n",
+      "  episodes_total: 1106\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0886386533578236\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007588425030310948\n",
+      "        model: {}\n",
+      "        policy_loss: -0.01092883839737624\n",
+      "        total_loss: 30.730765342712402\n",
+      "        vf_explained_var: 0.9611188769340515\n",
+      "        vf_loss: 30.741480032602947\n",
+      "    num_steps_sampled: 970752\n",
+      "    num_steps_trained: 970752\n",
+      "  iterations_since_restore: 6\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.506666666666664\n",
+      "    gpu_util_percent0: 0.31133333333333335\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.766666666666666\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15665935939519685\n",
+      "    mean_env_wait_ms: 1.1717385763775916\n",
+      "    mean_inference_ms: 4.878261275270092\n",
+      "    mean_raw_obs_processing_ms: 0.41058148949868417\n",
+      "  time_since_restore: 162.52765536308289\n",
+      "  time_this_iter_s: 26.74772834777832\n",
+      "  time_total_s: 162.52765536308289\n",
+      "  timers:\n",
+      "    learn_throughput: 8262.231\n",
+      "    learn_time_ms: 19582.119\n",
+      "    sample_throughput: 21815.562\n",
+      "    sample_time_ms: 7416.357\n",
+      "    update_time_ms: 35.692\n",
+      "  timestamp: 1602709543\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 970752\n",
+      "  training_iteration: 6\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      6 |          162.528 | 970752 |  225.711 |              280.566 |              145.717 |            860.694 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3552.704692556634\n",
+      "    time_step_min: 3179\n",
+      "  date: 2020-10-14_21-06-09\n",
+      "  done: false\n",
+      "  episode_len_mean: 855.0387658227849\n",
+      "  episode_reward_max: 284.35353535353545\n",
+      "  episode_reward_mean: 227.46978487405684\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 1264\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0706470410029094\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007387861027382314\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013462736414415607\n",
+      "        total_loss: 19.742233912150066\n",
+      "        vf_explained_var: 0.9632093906402588\n",
+      "        vf_loss: 19.75549300511678\n",
+      "    num_steps_sampled: 1132544\n",
+      "    num_steps_trained: 1132544\n",
+      "  iterations_since_restore: 7\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.490322580645167\n",
+      "    gpu_util_percent0: 0.2838709677419355\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.770967741935483\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1559362762181637\n",
+      "    mean_env_wait_ms: 1.1739296740367058\n",
+      "    mean_inference_ms: 4.838086770027385\n",
+      "    mean_raw_obs_processing_ms: 0.4083166221477109\n",
+      "  time_since_restore: 189.12698912620544\n",
+      "  time_this_iter_s: 26.59933376312256\n",
+      "  time_total_s: 189.12698912620544\n",
+      "  timers:\n",
+      "    learn_throughput: 8266.464\n",
+      "    learn_time_ms: 19572.093\n",
+      "    sample_throughput: 21985.221\n",
+      "    sample_time_ms: 7359.125\n",
+      "    update_time_ms: 33.432\n",
+      "  timestamp: 1602709569\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1132544\n",
+      "  training_iteration: 7\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      7 |          189.127 | 1132544 |   227.47 |              284.354 |              145.717 |            855.039 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3540.6398852223815\n",
+      "    time_step_min: 3179\n",
+      "  date: 2020-10-14_21-06-36\n",
+      "  done: false\n",
+      "  episode_len_mean: 850.7552742616034\n",
+      "  episode_reward_max: 284.35353535353545\n",
+      "  episode_reward_mean: 229.23441162681647\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 1422\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0475670397281647\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007399068369219701\n",
+      "        model: {}\n",
+      "        policy_loss: -0.009183195322596779\n",
+      "        total_loss: 16.652963479359943\n",
+      "        vf_explained_var: 0.9667003154754639\n",
+      "        vf_loss: 16.661930561065674\n",
+      "    num_steps_sampled: 1294336\n",
+      "    num_steps_trained: 1294336\n",
+      "  iterations_since_restore: 8\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.71666666666666\n",
+      "    gpu_util_percent0: 0.318\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.773333333333333\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15529763017321097\n",
+      "    mean_env_wait_ms: 1.1758429577289389\n",
+      "    mean_inference_ms: 4.802703090940908\n",
+      "    mean_raw_obs_processing_ms: 0.40625843821594676\n",
+      "  time_since_restore: 215.48828315734863\n",
+      "  time_this_iter_s: 26.36129403114319\n",
+      "  time_total_s: 215.48828315734863\n",
+      "  timers:\n",
+      "    learn_throughput: 8278.422\n",
+      "    learn_time_ms: 19543.82\n",
+      "    sample_throughput: 22141.326\n",
+      "    sample_time_ms: 7307.241\n",
+      "    update_time_ms: 31.549\n",
+      "  timestamp: 1602709596\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1294336\n",
+      "  training_iteration: 8\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      8 |          215.488 | 1294336 |  229.234 |              284.354 |              145.717 |            850.755 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3526.8721374045804\n",
+      "    time_step_min: 3179\n",
+      "  date: 2020-10-14_21-07-03\n",
+      "  done: false\n",
+      "  episode_len_mean: 845.9875\n",
+      "  episode_reward_max: 285.7171717171716\n",
+      "  episode_reward_mean: 231.28724747474732\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 178\n",
+      "  episodes_total: 1600\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9999773452679316\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007928823702968657\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011458211791856835\n",
+      "        total_loss: 16.58501172065735\n",
+      "        vf_explained_var: 0.9729644656181335\n",
+      "        vf_loss: 16.596176783243816\n",
+      "    num_steps_sampled: 1456128\n",
+      "    num_steps_trained: 1456128\n",
+      "  iterations_since_restore: 9\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 22.993548387096773\n",
+      "    gpu_util_percent0: 0.29387096774193544\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7741935483870965\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15464999594543705\n",
+      "    mean_env_wait_ms: 1.1780557616549023\n",
+      "    mean_inference_ms: 4.76792634372993\n",
+      "    mean_raw_obs_processing_ms: 0.40415644333563394\n",
+      "  time_since_restore: 242.3112816810608\n",
+      "  time_this_iter_s: 26.822998523712158\n",
+      "  time_total_s: 242.3112816810608\n",
+      "  timers:\n",
+      "    learn_throughput: 8272.764\n",
+      "    learn_time_ms: 19557.188\n",
+      "    sample_throughput: 22220.441\n",
+      "    sample_time_ms: 7281.224\n",
+      "    update_time_ms: 31.962\n",
+      "  timestamp: 1602709623\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1456128\n",
+      "  training_iteration: 9\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      9 |          242.311 | 1456128 |  231.287 |              285.717 |              145.717 |            845.987 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3508.0337259100643\n",
+      "    time_step_min: 3173\n",
+      "  date: 2020-10-14_21-07-29\n",
+      "  done: false\n",
+      "  episode_len_mean: 839.0052742616034\n",
+      "  episode_reward_max: 285.7171717171716\n",
+      "  episode_reward_mean: 234.27066551591852\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 296\n",
+      "  episodes_total: 1896\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9948871235052744\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006681857941051324\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011002168253374597\n",
+      "        total_loss: 15.828110535939535\n",
+      "        vf_explained_var: 0.9757750630378723\n",
+      "        vf_loss: 15.838941733042398\n",
+      "    num_steps_sampled: 1617920\n",
+      "    num_steps_trained: 1617920\n",
+      "  iterations_since_restore: 10\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.62666666666667\n",
+      "    gpu_util_percent0: 0.243\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15383279552979012\n",
+      "    mean_env_wait_ms: 1.1816130020394002\n",
+      "    mean_inference_ms: 4.721096556773406\n",
+      "    mean_raw_obs_processing_ms: 0.401484041778798\n",
+      "  time_since_restore: 268.6884000301361\n",
+      "  time_this_iter_s: 26.377118349075317\n",
+      "  time_total_s: 268.6884000301361\n",
+      "  timers:\n",
+      "    learn_throughput: 8283.974\n",
+      "    learn_time_ms: 19530.724\n",
+      "    sample_throughput: 22303.452\n",
+      "    sample_time_ms: 7254.124\n",
+      "    update_time_ms: 30.675\n",
+      "  timestamp: 1602709649\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1617920\n",
+      "  training_iteration: 10\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     10 |          268.688 | 1617920 |  234.271 |              285.717 |              145.717 |            839.005 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3499.19842053307\n",
+      "    time_step_min: 3171\n",
+      "  date: 2020-10-14_21-07-56\n",
+      "  done: false\n",
+      "  episode_len_mean: 835.7263875365142\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 235.87525203347977\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 2054\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.975288137793541\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007294710182274382\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012727556153549813\n",
+      "        total_loss: 11.962000767389933\n",
+      "        vf_explained_var: 0.9750909805297852\n",
+      "        vf_loss: 11.974486589431763\n",
+      "    num_steps_sampled: 1779712\n",
+      "    num_steps_trained: 1779712\n",
+      "  iterations_since_restore: 11\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.49032258064517\n",
+      "    gpu_util_percent0: 0.2429032258064516\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7741935483870965\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15346055964066002\n",
+      "    mean_env_wait_ms: 1.1832618180585688\n",
+      "    mean_inference_ms: 4.700248117692935\n",
+      "    mean_raw_obs_processing_ms: 0.4002798984804907\n",
+      "  time_since_restore: 295.3983256816864\n",
+      "  time_this_iter_s: 26.709925651550293\n",
+      "  time_total_s: 295.3983256816864\n",
+      "  timers:\n",
+      "    learn_throughput: 8281.307\n",
+      "    learn_time_ms: 19537.012\n",
+      "    sample_throughput: 22772.438\n",
+      "    sample_time_ms: 7104.729\n",
+      "    update_time_ms: 28.523\n",
+      "  timestamp: 1602709676\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1779712\n",
+      "  training_iteration: 11\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     11 |          295.398 | 1779712 |  235.875 |              294.202 |              145.717 |            835.726 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3490.923534798535\n",
+      "    time_step_min: 3159\n",
+      "  date: 2020-10-14_21-08-23\n",
+      "  done: false\n",
+      "  episode_len_mean: 832.6595840867993\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 237.1319752680511\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 2212\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9521185209353765\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006661186693236232\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013089668517447231\n",
+      "        total_loss: 12.603836615880331\n",
+      "        vf_explained_var: 0.9737562537193298\n",
+      "        vf_loss: 12.61673672993978\n",
+      "    num_steps_sampled: 1941504\n",
+      "    num_steps_trained: 1941504\n",
+      "  iterations_since_restore: 12\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.477419354838712\n",
+      "    gpu_util_percent0: 0.31483870967741934\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.77741935483871\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15312252956972325\n",
+      "    mean_env_wait_ms: 1.1848090383105752\n",
+      "    mean_inference_ms: 4.681388267276605\n",
+      "    mean_raw_obs_processing_ms: 0.399154195717825\n",
+      "  time_since_restore: 322.18752455711365\n",
+      "  time_this_iter_s: 26.789198875427246\n",
+      "  time_total_s: 322.18752455711365\n",
+      "  timers:\n",
+      "    learn_throughput: 8286.795\n",
+      "    learn_time_ms: 19524.074\n",
+      "    sample_throughput: 23017.742\n",
+      "    sample_time_ms: 7029.013\n",
+      "    update_time_ms: 27.778\n",
+      "  timestamp: 1602709703\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1941504\n",
+      "  training_iteration: 12\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     12 |          322.188 | 1941504 |  237.132 |              294.202 |              145.717 |             832.66 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3477.030998389694\n",
+      "    time_step_min: 3151\n",
+      "  date: 2020-10-14_21-08-50\n",
+      "  done: false\n",
+      "  episode_len_mean: 827.7671178343949\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 239.24816235604442\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 300\n",
+      "  episodes_total: 2512\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9211943199237188\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.0069003046955913305\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011187698284629732\n",
+      "        total_loss: 15.527917702992758\n",
+      "        vf_explained_var: 0.9792836308479309\n",
+      "        vf_loss: 15.538876056671143\n",
+      "    num_steps_sampled: 2103296\n",
+      "    num_steps_trained: 2103296\n",
+      "  iterations_since_restore: 13\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.738709677419354\n",
+      "    gpu_util_percent0: 0.22483870967741937\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7677419354838704\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15257686397002923\n",
+      "    mean_env_wait_ms: 1.1876912397652413\n",
+      "    mean_inference_ms: 4.650923491552926\n",
+      "    mean_raw_obs_processing_ms: 0.39736320906724354\n",
+      "  time_since_restore: 349.16273403167725\n",
+      "  time_this_iter_s: 26.9752094745636\n",
+      "  time_total_s: 349.16273403167725\n",
+      "  timers:\n",
+      "    learn_throughput: 8272.827\n",
+      "    learn_time_ms: 19557.038\n",
+      "    sample_throughput: 23048.074\n",
+      "    sample_time_ms: 7019.762\n",
+      "    update_time_ms: 28.051\n",
+      "  timestamp: 1602709730\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2103296\n",
+      "  training_iteration: 13\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     13 |          349.163 | 2103296 |  239.248 |              294.202 |              145.717 |            827.767 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3468.7953348382243\n",
+      "    time_step_min: 3151\n",
+      "  date: 2020-10-14_21-09-17\n",
+      "  done: false\n",
+      "  episode_len_mean: 825.1623231571109\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 240.4743300465563\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 174\n",
+      "  episodes_total: 2686\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9143994450569153\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.0060155229875817895\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012139652118397256\n",
+      "        total_loss: 10.54153060913086\n",
+      "        vf_explained_var: 0.979185163974762\n",
+      "        vf_loss: 10.553526004155477\n",
+      "    num_steps_sampled: 2265088\n",
+      "    num_steps_trained: 2265088\n",
+      "  iterations_since_restore: 14\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.141935483870967\n",
+      "    gpu_util_percent0: 0.3683870967741936\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.770967741935483\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.152289456661776\n",
+      "    mean_env_wait_ms: 1.1891260627894187\n",
+      "    mean_inference_ms: 4.635358873544114\n",
+      "    mean_raw_obs_processing_ms: 0.3964728387235993\n",
+      "  time_since_restore: 375.75564908981323\n",
+      "  time_this_iter_s: 26.592915058135986\n",
+      "  time_total_s: 375.75564908981323\n",
+      "  timers:\n",
+      "    learn_throughput: 8285.665\n",
+      "    learn_time_ms: 19526.737\n",
+      "    sample_throughput: 23035.609\n",
+      "    sample_time_ms: 7023.561\n",
+      "    update_time_ms: 28.254\n",
+      "  timestamp: 1602709757\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2265088\n",
+      "  training_iteration: 14\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     14 |          375.756 | 2265088 |  240.474 |              294.202 |              145.717 |            825.162 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3462.4602272727275\n",
+      "    time_step_min: 3131\n",
+      "  date: 2020-10-14_21-09-44\n",
+      "  done: false\n",
+      "  episode_len_mean: 822.9542897327707\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 241.45540851553497\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 2844\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9019962549209595\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006133316123547654\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012229806568939239\n",
+      "        total_loss: 9.555021127065023\n",
+      "        vf_explained_var: 0.9795403480529785\n",
+      "        vf_loss: 9.567088762919107\n",
+      "    num_steps_sampled: 2426880\n",
+      "    num_steps_trained: 2426880\n",
+      "  iterations_since_restore: 15\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.84333333333333\n",
+      "    gpu_util_percent0: 0.38433333333333325\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7733333333333325\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1520571188276099\n",
+      "    mean_env_wait_ms: 1.1903605198268268\n",
+      "    mean_inference_ms: 4.622489399949343\n",
+      "    mean_raw_obs_processing_ms: 0.39571469986441193\n",
+      "  time_since_restore: 402.6329152584076\n",
+      "  time_this_iter_s: 26.87726616859436\n",
+      "  time_total_s: 402.6329152584076\n",
+      "  timers:\n",
+      "    learn_throughput: 8271.367\n",
+      "    learn_time_ms: 19560.491\n",
+      "    sample_throughput: 22997.017\n",
+      "    sample_time_ms: 7035.347\n",
+      "    update_time_ms: 27.999\n",
+      "  timestamp: 1602709784\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2426880\n",
+      "  training_iteration: 15\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     15 |          402.633 | 2426880 |  241.455 |              294.202 |              145.717 |            822.954 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3453.8821989528797\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-10-10\n",
+      "  done: false\n",
+      "  episode_len_mean: 819.9792477302204\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 242.75903981448718\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 240\n",
+      "  episodes_total: 3084\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8659086326758066\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00634678197093308\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012134946203635385\n",
+      "        total_loss: 13.195513248443604\n",
+      "        vf_explained_var: 0.980156421661377\n",
+      "        vf_loss: 13.207446098327637\n",
+      "    num_steps_sampled: 2588672\n",
+      "    num_steps_trained: 2588672\n",
+      "  iterations_since_restore: 16\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.309677419354838\n",
+      "    gpu_util_percent0: 0.36096774193548387\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7709677419354835\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1517332658779976\n",
+      "    mean_env_wait_ms: 1.192259437186598\n",
+      "    mean_inference_ms: 4.604579073734175\n",
+      "    mean_raw_obs_processing_ms: 0.3946603100703554\n",
+      "  time_since_restore: 429.12290596961975\n",
+      "  time_this_iter_s: 26.489990711212158\n",
+      "  time_total_s: 429.12290596961975\n",
+      "  timers:\n",
+      "    learn_throughput: 8279.016\n",
+      "    learn_time_ms: 19542.42\n",
+      "    sample_throughput: 23020.64\n",
+      "    sample_time_ms: 7028.128\n",
+      "    update_time_ms: 26.01\n",
+      "  timestamp: 1602709810\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2588672\n",
+      "  training_iteration: 16\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     16 |          429.123 | 2588672 |  242.759 |              298.899 |              145.717 |            819.979 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3445.377811550152\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-10-37\n",
+      "  done: false\n",
+      "  episode_len_mean: 817.4445449065702\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 243.9846110289147\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 234\n",
+      "  episodes_total: 3318\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8606445689996084\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005582816433161497\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011729711521184072\n",
+      "        total_loss: 9.780861934026083\n",
+      "        vf_explained_var: 0.9827695488929749\n",
+      "        vf_loss: 9.792463779449463\n",
+      "    num_steps_sampled: 2750464\n",
+      "    num_steps_trained: 2750464\n",
+      "  iterations_since_restore: 17\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.576666666666664\n",
+      "    gpu_util_percent0: 0.37900000000000006\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7633333333333328\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1514577798692161\n",
+      "    mean_env_wait_ms: 1.1938746610135567\n",
+      "    mean_inference_ms: 4.589150214465355\n",
+      "    mean_raw_obs_processing_ms: 0.3937904501711353\n",
+      "  time_since_restore: 455.6629276275635\n",
+      "  time_this_iter_s: 26.540021657943726\n",
+      "  time_total_s: 455.6629276275635\n",
+      "  timers:\n",
+      "    learn_throughput: 8274.015\n",
+      "    learn_time_ms: 19554.23\n",
+      "    sample_throughput: 23082.272\n",
+      "    sample_time_ms: 7009.362\n",
+      "    update_time_ms: 25.933\n",
+      "  timestamp: 1602709837\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2750464\n",
+      "  training_iteration: 17\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     17 |          455.663 | 2750464 |  243.985 |              298.899 |              145.717 |            817.445 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3440.181554524362\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-11-04\n",
+      "  done: false\n",
+      "  episode_len_mean: 815.873417721519\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 244.73267194383405\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 3476\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8570553759733835\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00693794801676025\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012595690621916825\n",
+      "        total_loss: 9.302302360534668\n",
+      "        vf_explained_var: 0.9800246357917786\n",
+      "        vf_loss: 9.314632733662924\n",
+      "    num_steps_sampled: 2912256\n",
+      "    num_steps_trained: 2912256\n",
+      "  iterations_since_restore: 18\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.28064516129032\n",
+      "    gpu_util_percent0: 0.28935483870967743\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7709677419354835\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1512869388842221\n",
+      "    mean_env_wait_ms: 1.1948860891963016\n",
+      "    mean_inference_ms: 4.579551596598977\n",
+      "    mean_raw_obs_processing_ms: 0.39323684175907914\n",
+      "  time_since_restore: 482.1069633960724\n",
+      "  time_this_iter_s: 26.44403576850891\n",
+      "  time_total_s: 482.1069633960724\n",
+      "  timers:\n",
+      "    learn_throughput: 8269.598\n",
+      "    learn_time_ms: 19564.676\n",
+      "    sample_throughput: 23100.114\n",
+      "    sample_time_ms: 7003.948\n",
+      "    update_time_ms: 28.338\n",
+      "  timestamp: 1602709864\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2912256\n",
+      "  training_iteration: 18\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     18 |          482.107 | 2912256 |  244.733 |              298.899 |              145.717 |            815.873 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3435.160891089109\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-11-30\n",
+      "  done: false\n",
+      "  episode_len_mean: 814.1853165938865\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 245.55176216311577\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 188\n",
+      "  episodes_total: 3664\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8260929683844248\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00627721450291574\n",
+      "        model: {}\n",
+      "        policy_loss: -0.010709817167177485\n",
+      "        total_loss: 11.524338483810425\n",
+      "        vf_explained_var: 0.9801642894744873\n",
+      "        vf_loss: 11.534833749135336\n",
+      "    num_steps_sampled: 3074048\n",
+      "    num_steps_trained: 3074048\n",
+      "  iterations_since_restore: 19\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.583870967741937\n",
+      "    gpu_util_percent0: 0.4041935483870968\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.770967741935483\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15108964699814104\n",
+      "    mean_env_wait_ms: 1.1960450147791832\n",
+      "    mean_inference_ms: 4.568663786078855\n",
+      "    mean_raw_obs_processing_ms: 0.39260767936611063\n",
+      "  time_since_restore: 508.7718939781189\n",
+      "  time_this_iter_s: 26.66493058204651\n",
+      "  time_total_s: 508.7718939781189\n",
+      "  timers:\n",
+      "    learn_throughput: 8268.954\n",
+      "    learn_time_ms: 19566.198\n",
+      "    sample_throughput: 23189.124\n",
+      "    sample_time_ms: 6977.064\n",
+      "    update_time_ms: 35.073\n",
+      "  timestamp: 1602709890\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3074048\n",
+      "  training_iteration: 19\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     19 |          508.772 | 3074048 |  245.552 |              298.899 |              145.717 |            814.185 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3428.4441611422744\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-11-57\n",
+      "  done: false\n",
+      "  episode_len_mean: 812.0630379746835\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 246.60976857179378\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 286\n",
+      "  episodes_total: 3950\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8063790599505106\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00581474454763035\n",
+      "        model: {}\n",
+      "        policy_loss: -0.010059737542178482\n",
+      "        total_loss: 10.378987232844034\n",
+      "        vf_explained_var: 0.9842923283576965\n",
+      "        vf_loss: 10.388868490854898\n",
+      "    num_steps_sampled: 3235840\n",
+      "    num_steps_trained: 3235840\n",
+      "  iterations_since_restore: 20\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 24.36666666666667\n",
+      "    gpu_util_percent0: 0.31966666666666665\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.763333333333333\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15083686241602978\n",
+      "    mean_env_wait_ms: 1.1977366106159508\n",
+      "    mean_inference_ms: 4.554172739039668\n",
+      "    mean_raw_obs_processing_ms: 0.3917865708011254\n",
+      "  time_since_restore: 535.195830821991\n",
+      "  time_this_iter_s: 26.42393684387207\n",
+      "  time_total_s: 535.195830821991\n",
+      "  timers:\n",
+      "    learn_throughput: 8263.14\n",
+      "    learn_time_ms: 19579.966\n",
+      "    sample_throughput: 23222.566\n",
+      "    sample_time_ms: 6967.016\n",
+      "    update_time_ms: 35.303\n",
+      "  timestamp: 1602709917\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3235840\n",
+      "  training_iteration: 20\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     20 |          535.196 | 3235840 |   246.61 |              298.899 |              145.717 |            812.063 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3424.633333333333\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-12-24\n",
+      "  done: false\n",
+      "  episode_len_mean: 811.1747809152872\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 247.171761431255\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 4108\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8050975054502487\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006411496355819206\n",
+      "        model: {}\n",
+      "        policy_loss: -0.0109325938198405\n",
+      "        total_loss: 8.397321462631226\n",
+      "        vf_explained_var: 0.9822394847869873\n",
+      "        vf_loss: 8.408015330632528\n",
+      "    num_steps_sampled: 3397632\n",
+      "    num_steps_trained: 3397632\n",
+      "  iterations_since_restore: 21\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.345161290322583\n",
+      "    gpu_util_percent0: 0.33387096774193553\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7741935483870965\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1507044803921526\n",
+      "    mean_env_wait_ms: 1.1985632518212732\n",
+      "    mean_inference_ms: 4.546739803666223\n",
+      "    mean_raw_obs_processing_ms: 0.39136140122967195\n",
+      "  time_since_restore: 561.7221903800964\n",
+      "  time_this_iter_s: 26.52635955810547\n",
+      "  time_total_s: 561.7221903800964\n",
+      "  timers:\n",
+      "    learn_throughput: 8265.424\n",
+      "    learn_time_ms: 19574.556\n",
+      "    sample_throughput: 23297.173\n",
+      "    sample_time_ms: 6944.705\n",
+      "    update_time_ms: 43.065\n",
+      "  timestamp: 1602709944\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3397632\n",
+      "  training_iteration: 21\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     21 |          561.722 | 3397632 |  247.172 |              298.899 |              145.717 |            811.175 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3421.5082547169814\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-12-50\n",
+      "  done: false\n",
+      "  episode_len_mean: 810.3659793814433\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 247.6299262541061\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 160\n",
+      "  episodes_total: 4268\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7943403224150339\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006092905105712513\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012508889408006022\n",
+      "        total_loss: 10.069480101267496\n",
+      "        vf_explained_var: 0.9799533486366272\n",
+      "        vf_loss: 10.08177661895752\n",
+      "    num_steps_sampled: 3559424\n",
+      "    num_steps_trained: 3559424\n",
+      "  iterations_since_restore: 22\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.81666666666667\n",
+      "    gpu_util_percent0: 0.253\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7766666666666664\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15057738067312423\n",
+      "    mean_env_wait_ms: 1.1993669989655864\n",
+      "    mean_inference_ms: 4.539606631295978\n",
+      "    mean_raw_obs_processing_ms: 0.390950346310237\n",
+      "  time_since_restore: 588.3143429756165\n",
+      "  time_this_iter_s: 26.59215259552002\n",
+      "  time_total_s: 588.3143429756165\n",
+      "  timers:\n",
+      "    learn_throughput: 8259.281\n",
+      "    learn_time_ms: 19589.114\n",
+      "    sample_throughput: 23421.01\n",
+      "    sample_time_ms: 6907.986\n",
+      "    update_time_ms: 44.326\n",
+      "  timestamp: 1602709970\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3559424\n",
+      "  training_iteration: 22\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     22 |          588.314 | 3559424 |   247.63 |              298.899 |              145.717 |            810.366 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3415.407570422535\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-13-17\n",
+      "  done: false\n",
+      "  episode_len_mean: 808.5879265091863\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 248.62850287653427\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 304\n",
+      "  episodes_total: 4572\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7644506990909576\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005771325357879202\n",
+      "        model: {}\n",
+      "        policy_loss: -0.009549629636846172\n",
+      "        total_loss: 10.615382512410482\n",
+      "        vf_explained_var: 0.9846494197845459\n",
+      "        vf_loss: 10.624737024307251\n",
+      "    num_steps_sampled: 3721216\n",
+      "    num_steps_trained: 3721216\n",
+      "  iterations_since_restore: 23\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.08709677419355\n",
+      "    gpu_util_percent0: 0.3470967741935484\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7645161290322577\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15036060576180685\n",
+      "    mean_env_wait_ms: 1.200837895461041\n",
+      "    mean_inference_ms: 4.527163047815564\n",
+      "    mean_raw_obs_processing_ms: 0.3902497259773404\n",
+      "  time_since_restore: 614.8627808094025\n",
+      "  time_this_iter_s: 26.54843783378601\n",
+      "  time_total_s: 614.8627808094025\n",
+      "  timers:\n",
+      "    learn_throughput: 8272.154\n",
+      "    learn_time_ms: 19558.63\n",
+      "    sample_throughput: 23487.2\n",
+      "    sample_time_ms: 6888.518\n",
+      "    update_time_ms: 42.454\n",
+      "  timestamp: 1602709997\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3721216\n",
+      "  training_iteration: 23\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     23 |          614.863 | 3721216 |  248.629 |              298.899 |              145.717 |            808.588 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3412.3495331069607\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-13-44\n",
+      "  done: false\n",
+      "  episode_len_mean: 807.6881856540084\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 249.11699271192933\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 168\n",
+      "  episodes_total: 4740\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7512289037307104\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006618439607943098\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011942399355272451\n",
+      "        total_loss: 7.106495062510173\n",
+      "        vf_explained_var: 0.9852812886238098\n",
+      "        vf_loss: 7.118151148160298\n",
+      "    num_steps_sampled: 3883008\n",
+      "    num_steps_trained: 3883008\n",
+      "  iterations_since_restore: 24\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.763333333333332\n",
+      "    gpu_util_percent0: 0.272\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.783333333333333\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1502473661939675\n",
+      "    mean_env_wait_ms: 1.2015818529322149\n",
+      "    mean_inference_ms: 4.520789877874915\n",
+      "    mean_raw_obs_processing_ms: 0.38989796231221446\n",
+      "  time_since_restore: 641.3350374698639\n",
+      "  time_this_iter_s: 26.472256660461426\n",
+      "  time_total_s: 641.3350374698639\n",
+      "  timers:\n",
+      "    learn_throughput: 8271.35\n",
+      "    learn_time_ms: 19560.532\n",
+      "    sample_throughput: 23514.635\n",
+      "    sample_time_ms: 6880.481\n",
+      "    update_time_ms: 42.61\n",
+      "  timestamp: 1602710024\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3883008\n",
+      "  training_iteration: 24\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     24 |          641.335 | 3883008 |  249.117 |              298.899 |              145.717 |            807.688 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3409.523613963039\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-14-10\n",
+      "  done: false\n",
+      "  episode_len_mean: 806.9183340138832\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 249.55621548271597\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 4898\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7577964961528778\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005755245918408036\n",
+      "        model: {}\n",
+      "        policy_loss: -0.00993577616463881\n",
+      "        total_loss: 7.848556240399678\n",
+      "        vf_explained_var: 0.9825068116188049\n",
+      "        vf_loss: 7.858295281728108\n",
+      "    num_steps_sampled: 4044800\n",
+      "    num_steps_trained: 4044800\n",
+      "  iterations_since_restore: 25\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.06774193548387\n",
+      "    gpu_util_percent0: 0.2922580645161291\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.780645161290322\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1501490253393215\n",
+      "    mean_env_wait_ms: 1.2022478349453032\n",
+      "    mean_inference_ms: 4.515108016272416\n",
+      "    mean_raw_obs_processing_ms: 0.3895752935542475\n",
+      "  time_since_restore: 668.0025424957275\n",
+      "  time_this_iter_s: 26.667505025863647\n",
+      "  time_total_s: 668.0025424957275\n",
+      "  timers:\n",
+      "    learn_throughput: 8278.009\n",
+      "    learn_time_ms: 19544.795\n",
+      "    sample_throughput: 23536.559\n",
+      "    sample_time_ms: 6874.072\n",
+      "    update_time_ms: 43.337\n",
+      "  timestamp: 1602710050\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4044800\n",
+      "  training_iteration: 25\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     25 |          668.003 | 4044800 |  249.556 |              298.899 |              145.717 |            806.918 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3405.1021997274675\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-14-37\n",
+      "  done: false\n",
+      "  episode_len_mean: 805.7744433688287\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 250.1943931082362\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 267\n",
+      "  episodes_total: 5165\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.734145000576973\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00586831111771365\n",
+      "        model: {}\n",
+      "        policy_loss: -0.009772113577734368\n",
+      "        total_loss: 11.755430380503336\n",
+      "        vf_explained_var: 0.9823409914970398\n",
+      "        vf_loss: 11.76498262087504\n",
+      "    num_steps_sampled: 4206592\n",
+      "    num_steps_trained: 4206592\n",
+      "  iterations_since_restore: 26\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 24.22666666666667\n",
+      "    gpu_util_percent0: 0.29566666666666663\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.766666666666666\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14999721678332353\n",
+      "    mean_env_wait_ms: 1.2033614261477694\n",
+      "    mean_inference_ms: 4.506158258902504\n",
+      "    mean_raw_obs_processing_ms: 0.3890744939361603\n",
+      "  time_since_restore: 694.2319819927216\n",
+      "  time_this_iter_s: 26.22943949699402\n",
+      "  time_total_s: 694.2319819927216\n",
+      "  timers:\n",
+      "    learn_throughput: 8283.812\n",
+      "    learn_time_ms: 19531.104\n",
+      "    sample_throughput: 23581.891\n",
+      "    sample_time_ms: 6860.858\n",
+      "    update_time_ms: 43.287\n",
+      "  timestamp: 1602710077\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4206592\n",
+      "  training_iteration: 26\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     26 |          694.232 | 4206592 |  250.194 |              298.899 |              145.717 |            805.774 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3401.4015718562873\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-15-03\n",
+      "  done: false\n",
+      "  episode_len_mean: 804.8773268801191\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 250.75784276119336\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 207\n",
+      "  episodes_total: 5372\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7132567266623179\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.0055438703469311195\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011437343899160624\n",
+      "        total_loss: 6.541075627009074\n",
+      "        vf_explained_var: 0.9872210621833801\n",
+      "        vf_loss: 6.5523152351379395\n",
+      "    num_steps_sampled: 4368384\n",
+      "    num_steps_trained: 4368384\n",
+      "  iterations_since_restore: 27\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.353333333333335\n",
+      "    gpu_util_percent0: 0.35100000000000003\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.78\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14988135384306817\n",
+      "    mean_env_wait_ms: 1.2041428049451586\n",
+      "    mean_inference_ms: 4.499582938127375\n",
+      "    mean_raw_obs_processing_ms: 0.38871711747737686\n",
+      "  time_since_restore: 720.3953409194946\n",
+      "  time_this_iter_s: 26.16335892677307\n",
+      "  time_total_s: 720.3953409194946\n",
+      "  timers:\n",
+      "    learn_throughput: 8297.771\n",
+      "    learn_time_ms: 19498.248\n",
+      "    sample_throughput: 23599.897\n",
+      "    sample_time_ms: 6855.623\n",
+      "    update_time_ms: 43.091\n",
+      "  timestamp: 1602710103\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4368384\n",
+      "  training_iteration: 27\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     27 |          720.395 | 4368384 |  250.758 |              298.899 |              145.717 |            804.877 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3398.81697564522\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-15-30\n",
+      "  done: false\n",
+      "  episode_len_mean: 804.1676311030741\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 251.131851973624\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 5530\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7216055691242218\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005685570455777149\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011318465374642983\n",
+      "        total_loss: 6.673397620519002\n",
+      "        vf_explained_var: 0.9849072098731995\n",
+      "        vf_loss: 6.684508363405864\n",
+      "    num_steps_sampled: 4530176\n",
+      "    num_steps_trained: 4530176\n",
+      "  iterations_since_restore: 28\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.886666666666667\n",
+      "    gpu_util_percent0: 0.2823333333333334\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7766666666666664\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14980043216178882\n",
+      "    mean_env_wait_ms: 1.2047185869915957\n",
+      "    mean_inference_ms: 4.494854897407615\n",
+      "    mean_raw_obs_processing_ms: 0.38845720586260357\n",
+      "  time_since_restore: 746.6311824321747\n",
+      "  time_this_iter_s: 26.235841512680054\n",
+      "  time_total_s: 746.6311824321747\n",
+      "  timers:\n",
+      "    learn_throughput: 8303.173\n",
+      "    learn_time_ms: 19485.562\n",
+      "    sample_throughput: 23620.984\n",
+      "    sample_time_ms: 6849.503\n",
+      "    update_time_ms: 40.457\n",
+      "  timestamp: 1602710130\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4530176\n",
+      "  training_iteration: 28\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     28 |          746.631 | 4530176 |  251.132 |              298.899 |              145.717 |            804.168 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3395.749869178441\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-15-56\n",
+      "  done: false\n",
+      "  episode_len_mean: 803.1786148238153\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 251.58607600041373\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 231\n",
+      "  episodes_total: 5761\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6937208970387777\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005959675957759221\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011802961448362717\n",
+      "        total_loss: 10.309924920399984\n",
+      "        vf_explained_var: 0.983536958694458\n",
+      "        vf_loss: 10.321478684743246\n",
+      "    num_steps_sampled: 4691968\n",
+      "    num_steps_trained: 4691968\n",
+      "  iterations_since_restore: 29\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 22.92258064516129\n",
+      "    gpu_util_percent0: 0.417741935483871\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7677419354838704\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1496903041319913\n",
+      "    mean_env_wait_ms: 1.205558648276445\n",
+      "    mean_inference_ms: 4.488274076300299\n",
+      "    mean_raw_obs_processing_ms: 0.38809971432498575\n",
+      "  time_since_restore: 773.0392935276031\n",
+      "  time_this_iter_s: 26.408111095428467\n",
+      "  time_total_s: 773.0392935276031\n",
+      "  timers:\n",
+      "    learn_throughput: 8317.418\n",
+      "    learn_time_ms: 19452.189\n",
+      "    sample_throughput: 23590.607\n",
+      "    sample_time_ms: 6858.323\n",
+      "    update_time_ms: 32.563\n",
+      "  timestamp: 1602710156\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4691968\n",
+      "  training_iteration: 29\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     29 |          773.039 | 4691968 |  251.586 |              298.899 |              145.717 |            803.179 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3392.510374832664\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-16-23\n",
+      "  done: false\n",
+      "  episode_len_mean: 802.1863757495004\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 252.1170717837939\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 243\n",
+      "  episodes_total: 6004\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6723145395517349\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005279912458111842\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011053321950991327\n",
+      "        total_loss: 7.6361691157023115\n",
+      "        vf_explained_var: 0.9863631129264832\n",
+      "        vf_loss: 7.647030512491862\n",
+      "    num_steps_sampled: 4853760\n",
+      "    num_steps_trained: 4853760\n",
+      "  iterations_since_restore: 30\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.860000000000003\n",
+      "    gpu_util_percent0: 0.3133333333333333\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14957739547666185\n",
+      "    mean_env_wait_ms: 1.2063717178260402\n",
+      "    mean_inference_ms: 4.481825393409482\n",
+      "    mean_raw_obs_processing_ms: 0.3877499422841646\n",
+      "  time_since_restore: 799.4389925003052\n",
+      "  time_this_iter_s: 26.399698972702026\n",
+      "  time_total_s: 799.4389925003052\n",
+      "  timers:\n",
+      "    learn_throughput: 8320.988\n",
+      "    learn_time_ms: 19443.845\n",
+      "    sample_throughput: 23585.905\n",
+      "    sample_time_ms: 6859.69\n",
+      "    update_time_ms: 34.74\n",
+      "  timestamp: 1602710183\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4853760\n",
+      "  training_iteration: 30\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     30 |          799.439 | 4853760 |  252.117 |              298.899 |              145.717 |            802.186 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3390.042386697098\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-16-49\n",
+      "  done: false\n",
+      "  episode_len_mean: 801.4599156118144\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 252.45022769073393\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 6162\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6794097969929377\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005776377705236276\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011071474878311468\n",
+      "        total_loss: 6.609892050425212\n",
+      "        vf_explained_var: 0.9848344326019287\n",
+      "        vf_loss: 6.620725552241008\n",
+      "    num_steps_sampled: 5015552\n",
+      "    num_steps_trained: 5015552\n",
+      "  iterations_since_restore: 31\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.277419354838713\n",
+      "    gpu_util_percent0: 0.25258064516129036\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7806451612903222\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1495093060246751\n",
+      "    mean_env_wait_ms: 1.2068785550500694\n",
+      "    mean_inference_ms: 4.477842010054414\n",
+      "    mean_raw_obs_processing_ms: 0.3875324536178687\n",
+      "  time_since_restore: 826.0708198547363\n",
+      "  time_this_iter_s: 26.631827354431152\n",
+      "  time_total_s: 826.0708198547363\n",
+      "  timers:\n",
+      "    learn_throughput: 8316.229\n",
+      "    learn_time_ms: 19454.972\n",
+      "    sample_throughput: 23563.199\n",
+      "    sample_time_ms: 6866.3\n",
+      "    update_time_ms: 27.804\n",
+      "  timestamp: 1602710209\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5015552\n",
+      "  training_iteration: 31\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     31 |          826.071 | 5015552 |   252.45 |              298.899 |              145.717 |             801.46 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3386.696788413098\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-17-16\n",
+      "  done: false\n",
+      "  episode_len_mean: 800.385893416928\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 252.93216649251121\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 218\n",
+      "  episodes_total: 6380\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6597986618677775\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005507051052215199\n",
+      "        model: {}\n",
+      "        policy_loss: -0.01063559478886115\n",
+      "        total_loss: 8.718894402186075\n",
+      "        vf_explained_var: 0.9843838214874268\n",
+      "        vf_loss: 8.729309240976969\n",
+      "    num_steps_sampled: 5177344\n",
+      "    num_steps_trained: 5177344\n",
+      "  iterations_since_restore: 32\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.65666666666667\n",
+      "    gpu_util_percent0: 0.31466666666666665\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14942336945631843\n",
+      "    mean_env_wait_ms: 1.2075933530118947\n",
+      "    mean_inference_ms: 4.472619260274858\n",
+      "    mean_raw_obs_processing_ms: 0.3872484802290064\n",
+      "  time_since_restore: 852.6489214897156\n",
+      "  time_this_iter_s: 26.578101634979248\n",
+      "  time_total_s: 852.6489214897156\n",
+      "  timers:\n",
+      "    learn_throughput: 8318.686\n",
+      "    learn_time_ms: 19449.227\n",
+      "    sample_throughput: 23545.491\n",
+      "    sample_time_ms: 6871.464\n",
+      "    update_time_ms: 25.913\n",
+      "  timestamp: 1602710236\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5177344\n",
+      "  training_iteration: 32\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     32 |          852.649 | 5177344 |  252.932 |              298.899 |              145.717 |            800.386 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3383.374754048736\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-17-43\n",
+      "  done: false\n",
+      "  episode_len_mean: 799.2391861341372\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 253.41443827879388\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 255\n",
+      "  episodes_total: 6635\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6365775416294733\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005616956856101751\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012718923583937189\n",
+      "        total_loss: 7.9217236042022705\n",
+      "        vf_explained_var: 0.9864194393157959\n",
+      "        vf_loss: 7.934199412663777\n",
+      "    num_steps_sampled: 5339136\n",
+      "    num_steps_trained: 5339136\n",
+      "  iterations_since_restore: 33\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 22.948387096774198\n",
+      "    gpu_util_percent0: 0.27838709677419354\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.774193548387097\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14932189346086894\n",
+      "    mean_env_wait_ms: 1.2083754344986544\n",
+      "    mean_inference_ms: 4.466813879680601\n",
+      "    mean_raw_obs_processing_ms: 0.38693984548896015\n",
+      "  time_since_restore: 879.1936430931091\n",
+      "  time_this_iter_s: 26.544721603393555\n",
+      "  time_total_s: 879.1936430931091\n",
+      "  timers:\n",
+      "    learn_throughput: 8314.788\n",
+      "    learn_time_ms: 19458.343\n",
+      "    sample_throughput: 23551.139\n",
+      "    sample_time_ms: 6869.816\n",
+      "    update_time_ms: 25.193\n",
+      "  timestamp: 1602710263\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5339136\n",
+      "  training_iteration: 33\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     33 |          879.194 | 5339136 |  253.414 |              298.899 |              145.717 |            799.239 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3381.266627253917\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-18-10\n",
+      "  done: false\n",
+      "  episode_len_mean: 798.6435089785105\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 253.7262884957909\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 159\n",
+      "  episodes_total: 6794\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6548023074865341\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005482170420388381\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012428849945232892\n",
+      "        total_loss: 6.324912707010905\n",
+      "        vf_explained_var: 0.9852306842803955\n",
+      "        vf_loss: 6.337120532989502\n",
+      "    num_steps_sampled: 5500928\n",
+      "    num_steps_trained: 5500928\n",
+      "  iterations_since_restore: 34\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.380645161290325\n",
+      "    gpu_util_percent0: 0.3251612903225806\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.780645161290322\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1492620448104757\n",
+      "    mean_env_wait_ms: 1.2088387513992318\n",
+      "    mean_inference_ms: 4.463358913349285\n",
+      "    mean_raw_obs_processing_ms: 0.38675634808095327\n",
+      "  time_since_restore: 905.8512279987335\n",
+      "  time_this_iter_s: 26.65758490562439\n",
+      "  time_total_s: 905.8512279987335\n",
+      "  timers:\n",
+      "    learn_throughput: 8309.566\n",
+      "    learn_time_ms: 19470.571\n",
+      "    sample_throughput: 23555.989\n",
+      "    sample_time_ms: 6868.402\n",
+      "    update_time_ms: 31.554\n",
+      "  timestamp: 1602710290\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5500928\n",
+      "  training_iteration: 34\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     34 |          905.851 | 5500928 |  253.726 |              298.899 |              145.717 |            798.644 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "2020-10-13 11:56:12,862 - wandb.wandb_agent - INFO - Running runs: ['av30c7rd']\n"
+      "\n"
      ]
     }
    ],
    "source": [
-    "!wandb agent 1x8v92mc"
+    "!wandb agent 2ex0mmbf"
    ]
   },
   {
diff --git a/JSS/.ipynb_checkpoints/default_config-checkpoint.py b/JSS/.ipynb_checkpoints/default_config-checkpoint.py
index 46f4d10..4729210 100644
--- a/JSS/.ipynb_checkpoints/default_config-checkpoint.py
+++ b/JSS/.ipynb_checkpoints/default_config-checkpoint.py
@@ -6,7 +6,7 @@ default_config = {
     'env': 'jss_env',
     'seed': 0,
     'framework': 'torch',
-    'log_level': 'INFO',
+    'log_level': 'WARN',
     'num_gpus': 1,
     'instance_path': '/JSS/JSS/env/instances/ta51',
     'num_envs_per_worker': 2,
diff --git a/JSS/.ipynb_checkpoints/train-checkpoint.py b/JSS/.ipynb_checkpoints/train-checkpoint.py
index d4ac941..3400140 100644
--- a/JSS/.ipynb_checkpoints/train-checkpoint.py
+++ b/JSS/.ipynb_checkpoints/train-checkpoint.py
@@ -50,7 +50,7 @@ def train_func():
     ray.init()
 
     stop = {
-        "time_total_s": 60 * 60,
+        "time_total_s": 3 * 60 * 60,
     }
 
     analysis = tune.run(PPOTrainer, config=config, stop=stop, name="ppo-jss")
diff --git a/JSS/MTWR.py b/JSS/MTWR.py
index 1b7b946..7a5796b 100644
--- a/JSS/MTWR.py
+++ b/JSS/MTWR.py
@@ -21,11 +21,11 @@ def MTWR_worker(default_config):
         real_state = np.copy(state['real_obs'])
         legal_actions = state['action_mask'][:-1]
         reshaped = np.reshape(real_state, (env.jobs, 7))
-        remaining_time = reshaped[:, 3]
+        remaining_time = (reshaped[:, 3] * env.max_time_jobs) / env.jobs_length
         illegal_actions = np.invert(legal_actions)
         mask = illegal_actions * -1e8
         remaining_time += mask
-        MTWR_action = np.argmax(remaining_time)
+        MTWR_action = np.argmin(remaining_time)
         assert legal_actions[MTWR_action]
         state, reward, done, _ = env.step(MTWR_action)
     env.reset()
diff --git a/JSS/PPO.ipynb b/JSS/PPO.ipynb
index 23b572b..ee79e38 100644
--- a/JSS/PPO.ipynb
+++ b/JSS/PPO.ipynb
@@ -2,7 +2,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [
     {
@@ -74,15 +74,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Create sweep with ID: po3ygyxo\n",
-      "Sweep URL: https://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\n"
+      "Create sweep with ID: 2ex0mmbf\n",
+      "Sweep URL: https://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/2ex0mmbf\n"
      ]
     }
    ],
@@ -92,7 +92,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": null,
    "metadata": {},
    "outputs": [
     {
@@ -100,425 +100,2987 @@
      "output_type": "stream",
      "text": [
       "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent üïµÔ∏è\n",
-      "2020-10-14 20:43:27,735 - wandb.wandb_agent - INFO - Running runs: []\n",
-      "2020-10-14 20:43:31,145 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:43:31,145 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
+      "2020-10-14 21:02:46,189 - wandb.wandb_agent - INFO - Running runs: []\n",
+      "2020-10-14 21:02:46,519 - wandb.wandb_agent - INFO - Agent received command: run\n",
+      "2020-10-14 21:02:46,519 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
       "\tinstance_path: /JSS/JSS/env/instances/ta51\n",
-      "2020-10-14 20:43:31,147 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta51\n",
+      "2020-10-14 21:02:46,521 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python train.py --instance_path=/JSS/JSS/env/instances/ta51\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
+      "2020-10-14 21:02:51,538 - wandb.wandb_agent - INFO - Running runs: ['z2o08s2g']\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mupbeat-sweep-1\u001b[0m\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/9bbl2cxc\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204333-9bbl2cxc\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/2ex0mmbf\u001b[0m\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/z2o08s2g\u001b[0m\n",
+      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_210248-z2o08s2g\n",
       "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "2020-10-14 21:02:52,176\tINFO services.py:1164 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
+      "== Status ==\n",
+      "Memory usage on this node: 11.5/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+-------+\n",
+      "| Trial name              | status   | loc   |\n",
+      "|-------------------------+----------+-------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  |       |\n",
+      "+-------------------------+----------+-------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3282\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:43:36,159 - wandb.wandb_agent - INFO - Running runs: ['9bbl2cxc']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204333-9bbl2cxc/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204333-9bbl2cxc/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 197.38384\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 197.38384\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3753\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708214\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/9bbl2cxc\u001b[0m\n",
-      "2020-10-14 20:43:41,380 - wandb.wandb_agent - INFO - Cleaning up finished run: 9bbl2cxc\n",
-      "2020-10-14 20:43:41,772 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:43:41,772 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta52\n",
-      "2020-10-14 20:43:41,774 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta52\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/frw3hck3\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204343-frw3hck3\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "\u001b[2m\u001b[36m(pid=422)\u001b[0m 2020-10-14 21:02:54,925\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
+      "\u001b[2m\u001b[36m(pid=336)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=336)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=301)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=301)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=321)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=321)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=367)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=367)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=404)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=404)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=412)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=412)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=326)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=326)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=407)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=407)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=308)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=308)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=384)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=384)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=388)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=388)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=333)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=333)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=414)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=414)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=383)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=383)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=364)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=364)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=373)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=373)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=379)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=379)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=411)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=411)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=417)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=417)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=392)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=392)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=390)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=390)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=419)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=419)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=374)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=374)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=362)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=362)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=315)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=315)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=305)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=305)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=408)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=408)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=413)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=413)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=423)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=423)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=316)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=316)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=429)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=429)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=313)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=313)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=376)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=376)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=389)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=389)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=370)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=370)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=409)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=409)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=300)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=300)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=369)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=369)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=297)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=297)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=381)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=381)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=378)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=378)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=401)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=401)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=426)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=426)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=399)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=399)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=433)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=433)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=380)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=380)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=396)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=396)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=400)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=400)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=318)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=318)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=309)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=309)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=314)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=314)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=395)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=395)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=319)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=319)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=307)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=307)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=361)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=361)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=328)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=328)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=317)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=317)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=375)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=375)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=330)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=330)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=386)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=386)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=356)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=356)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=310)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=310)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=320)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=320)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=359)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=359)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=377)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=377)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=357)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=357)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=299)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=299)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=304)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=304)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=298)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=298)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=302)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=302)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=368)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=368)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=329)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=329)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=312)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=312)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=397)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=397)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=365)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=365)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=303)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=303)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=394)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=394)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=363)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=363)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "\u001b[2m\u001b[36m(pid=324)\u001b[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
+      "\u001b[2m\u001b[36m(pid=324)\u001b[0m   tensor = torch.from_numpy(np.asarray(item))\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3615.0923076923077\n",
+      "    time_step_min: 3379\n",
+      "  date: 2020-10-14_21-03-28\n",
+      "  done: false\n",
+      "  episode_len_mean: 891.1139240506329\n",
+      "  episode_reward_max: 258.59595959595964\n",
+      "  episode_reward_mean: 216.07678046285614\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 158\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.20000000000000004\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1851047078768413\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.004071502441850801\n",
+      "        model: {}\n",
+      "        policy_loss: -0.00785889983914482\n",
+      "        total_loss: 507.07567087809247\n",
+      "        vf_explained_var: 0.540532648563385\n",
+      "        vf_loss: 507.0832926432292\n",
+      "    num_steps_sampled: 161792\n",
+      "    num_steps_trained: 161792\n",
+      "  iterations_since_restore: 1\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 28.05757575757576\n",
+      "    gpu_util_percent0: 0.36818181818181817\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.5575757575757576\n",
+      "    vram_util_percent0: 0.08736346740610434\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.16797264772787918\n",
+      "    mean_env_wait_ms: 1.1681326777525027\n",
+      "    mean_inference_ms: 5.3567412135850585\n",
+      "    mean_raw_obs_processing_ms: 0.43960491202796964\n",
+      "  time_since_restore: 28.13359045982361\n",
+      "  time_this_iter_s: 28.13359045982361\n",
+      "  time_total_s: 28.13359045982361\n",
+      "  timers:\n",
+      "    learn_throughput: 8285.934\n",
+      "    learn_time_ms: 19526.102\n",
+      "    sample_throughput: 18950.001\n",
+      "    sample_time_ms: 8537.836\n",
+      "    update_time_ms: 44.19\n",
+      "  timestamp: 1602709408\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 161792\n",
+      "  training_iteration: 1\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 27.6/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      1 |          28.1336 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3372\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:43:46,790 - wandb.wandb_agent - INFO - Running runs: ['frw3hck3']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204343-frw3hck3/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204343-frw3hck3/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 163.86869\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 163.86869\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3871\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 2\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708225\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/frw3hck3\u001b[0m\n",
-      "2020-10-14 20:43:52,006 - wandb.wandb_agent - INFO - Cleaning up finished run: frw3hck3\n",
-      "2020-10-14 20:43:52,326 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:43:52,327 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta53\n",
-      "2020-10-14 20:43:52,329 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta53\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/o0hyb863\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204354-o0hyb863\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3614.4305555555557\n",
+      "    time_step_min: 3250\n",
+      "  date: 2020-10-14_21-03-56\n",
+      "  done: false\n",
+      "  episode_len_mean: 890.8607594936709\n",
+      "  episode_reward_max: 273.5959595959592\n",
+      "  episode_reward_mean: 217.6365234624726\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 316\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1561074058214824\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007923512797181806\n",
+      "        model: {}\n",
+      "        policy_loss: -0.010965243893830726\n",
+      "        total_loss: 127.46906661987305\n",
+      "        vf_explained_var: 0.8076093792915344\n",
+      "        vf_loss: 127.47981770833333\n",
+      "    num_steps_sampled: 323584\n",
+      "    num_steps_trained: 323584\n",
+      "  iterations_since_restore: 2\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 24.90625\n",
+      "    gpu_util_percent0: 0.28125\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7593750000000004\n",
+      "    vram_util_percent0: 0.10437848474909807\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1644697490894654\n",
+      "    mean_env_wait_ms: 1.1643447442552444\n",
+      "    mean_inference_ms: 5.274918357821516\n",
+      "    mean_raw_obs_processing_ms: 0.43312069147187726\n",
+      "  time_since_restore: 55.80924940109253\n",
+      "  time_this_iter_s: 27.67565894126892\n",
+      "  time_total_s: 55.80924940109253\n",
+      "  timers:\n",
+      "    learn_throughput: 8278.375\n",
+      "    learn_time_ms: 19543.932\n",
+      "    sample_throughput: 19526.756\n",
+      "    sample_time_ms: 8285.657\n",
+      "    update_time_ms: 39.034\n",
+      "  timestamp: 1602709436\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 323584\n",
+      "  training_iteration: 2\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      2 |          55.8092 | 323584 |  217.637 |              273.596 |              145.717 |            890.861 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3461\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:43:57,342 - wandb.wandb_agent - INFO - Running runs: ['o0hyb863']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204354-o0hyb863/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204354-o0hyb863/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 180.92929\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 180.92929\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3790\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708235\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/o0hyb863\u001b[0m\n",
-      "2020-10-14 20:44:02,563 - wandb.wandb_agent - INFO - Cleaning up finished run: o0hyb863\n",
-      "2020-10-14 20:44:02,910 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:44:02,911 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta54\n",
-      "2020-10-14 20:44:02,913 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta54\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/802owiob\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204405-802owiob\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3601.8677130044844\n",
+      "    time_step_min: 3250\n",
+      "  date: 2020-10-14_21-04-23\n",
+      "  done: false\n",
+      "  episode_len_mean: 885.132911392405\n",
+      "  episode_reward_max: 273.5959595959592\n",
+      "  episode_reward_mean: 219.87009333844756\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 474\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1456398169199626\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.008224547879459957\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013529085864623388\n",
+      "        total_loss: 61.275455474853516\n",
+      "        vf_explained_var: 0.8916645646095276\n",
+      "        vf_loss: 61.28873507181803\n",
+      "    num_steps_sampled: 485376\n",
+      "    num_steps_trained: 485376\n",
+      "  iterations_since_restore: 3\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.758064516129032\n",
+      "    gpu_util_percent0: 0.2945161290322581\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7677419354838704\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.16202246416545443\n",
+      "    mean_env_wait_ms: 1.1639664955356654\n",
+      "    mean_inference_ms: 5.163724414780632\n",
+      "    mean_raw_obs_processing_ms: 0.4268547752872556\n",
+      "  time_since_restore: 82.59170770645142\n",
+      "  time_this_iter_s: 26.782458305358887\n",
+      "  time_total_s: 82.59170770645142\n",
+      "  timers:\n",
+      "    learn_throughput: 8286.405\n",
+      "    learn_time_ms: 19524.994\n",
+      "    sample_throughput: 20463.34\n",
+      "    sample_time_ms: 7906.432\n",
+      "    update_time_ms: 40.76\n",
+      "  timestamp: 1602709463\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 485376\n",
+      "  training_iteration: 3\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      3 |          82.5917 | 485376 |   219.87 |              273.596 |              145.717 |            885.133 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3552\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:44:07,929 - wandb.wandb_agent - INFO - Running runs: ['802owiob']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204405-802owiob/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204405-802owiob/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 201.68687\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 201.68687\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3601\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708246\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/802owiob\u001b[0m\n",
-      "2020-10-14 20:44:13,147 - wandb.wandb_agent - INFO - Cleaning up finished run: 802owiob\n",
-      "2020-10-14 20:44:13,451 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:44:13,451 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta55\n",
-      "2020-10-14 20:44:13,453 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta55\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/ix8moovg\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204415-ix8moovg\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3596.0099337748343\n",
+      "    time_step_min: 3231\n",
+      "  date: 2020-10-14_21-04-49\n",
+      "  done: false\n",
+      "  episode_len_mean: 878.7689873417721\n",
+      "  episode_reward_max: 276.47474747474763\n",
+      "  episode_reward_mean: 220.6047340493541\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 632\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.1263898611068726\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.008100568510902425\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013406771836647144\n",
+      "        total_loss: 47.16934140523275\n",
+      "        vf_explained_var: 0.9198758602142334\n",
+      "        vf_loss: 47.18250052134196\n",
+      "    num_steps_sampled: 647168\n",
+      "    num_steps_trained: 647168\n",
+      "  iterations_since_restore: 4\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.72666666666667\n",
+      "    gpu_util_percent0: 0.3456666666666667\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.16012064049546954\n",
+      "    mean_env_wait_ms: 1.164612439662183\n",
+      "    mean_inference_ms: 5.066839245558316\n",
+      "    mean_raw_obs_processing_ms: 0.4213159869942824\n",
+      "  time_since_restore: 109.37071919441223\n",
+      "  time_this_iter_s: 26.779011487960815\n",
+      "  time_total_s: 109.37071919441223\n",
+      "  timers:\n",
+      "    learn_throughput: 8258.447\n",
+      "    learn_time_ms: 19591.094\n",
+      "    sample_throughput: 21127.332\n",
+      "    sample_time_ms: 7657.947\n",
+      "    update_time_ms: 36.581\n",
+      "  timestamp: 1602709489\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 647168\n",
+      "  training_iteration: 4\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      4 |          109.371 | 647168 |  220.605 |              276.475 |              145.717 |            878.769 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3642\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:44:18,469 - wandb.wandb_agent - INFO - Running runs: ['ix8moovg']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204415-ix8moovg/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204415-ix8moovg/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 224.37374\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 224.37374\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3435\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708256\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/ix8moovg\u001b[0m\n",
-      "2020-10-14 20:44:23,694 - wandb.wandb_agent - INFO - Cleaning up finished run: ix8moovg\n",
-      "2020-10-14 20:44:24,023 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:44:24,023 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta56\n",
-      "2020-10-14 20:44:24,025 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta56\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/zda8eskt\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204426-zda8eskt\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3581.6985583224114\n",
+      "    time_step_min: 3204\n",
+      "  date: 2020-10-14_21-05-16\n",
+      "  done: false\n",
+      "  episode_len_mean: 872.4867256637168\n",
+      "  episode_reward_max: 280.5656565656565\n",
+      "  episode_reward_mean: 222.48133675567283\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 159\n",
+      "  episodes_total: 791\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0873714486757915\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006956188706681132\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011262792395427823\n",
+      "        total_loss: 34.19948164621989\n",
+      "        vf_explained_var: 0.9459590911865234\n",
+      "        vf_loss: 34.2105925877889\n",
+      "    num_steps_sampled: 808960\n",
+      "    num_steps_trained: 808960\n",
+      "  iterations_since_restore: 5\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.203225806451613\n",
+      "    gpu_util_percent0: 0.2932258064516129\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7645161290322577\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15862506856835393\n",
+      "    mean_env_wait_ms: 1.1666529609326048\n",
+      "    mean_inference_ms: 4.987482402804091\n",
+      "    mean_raw_obs_processing_ms: 0.4166214436489198\n",
+      "  time_since_restore: 135.77992701530457\n",
+      "  time_this_iter_s: 26.409207820892334\n",
+      "  time_total_s: 135.77992701530457\n",
+      "  timers:\n",
+      "    learn_throughput: 8267.911\n",
+      "    learn_time_ms: 19568.667\n",
+      "    sample_throughput: 21578.792\n",
+      "    sample_time_ms: 7497.732\n",
+      "    update_time_ms: 35.062\n",
+      "  timestamp: 1602709516\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 808960\n",
+      "  training_iteration: 5\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      5 |           135.78 | 808960 |  222.481 |              280.566 |              145.717 |            872.487 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3726\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:44:29,037 - wandb.wandb_agent - INFO - Running runs: ['zda8eskt']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204426-zda8eskt/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204426-zda8eskt/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 178.59596\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 178.59596\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3881\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708267\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/zda8eskt\u001b[0m\n",
-      "2020-10-14 20:44:34,252 - wandb.wandb_agent - INFO - Cleaning up finished run: zda8eskt\n",
-      "2020-10-14 20:44:36,273 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:44:36,273 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta57\n",
-      "2020-10-14 20:44:36,275 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta57\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/uz9lk4hk\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204438-uz9lk4hk\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3564.887755102041\n",
+      "    time_step_min: 3204\n",
+      "  date: 2020-10-14_21-05-43\n",
+      "  done: false\n",
+      "  episode_len_mean: 860.6943942133815\n",
+      "  episode_reward_max: 280.5656565656565\n",
+      "  episode_reward_mean: 225.7112809834327\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 315\n",
+      "  episodes_total: 1106\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0886386533578236\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007588425030310948\n",
+      "        model: {}\n",
+      "        policy_loss: -0.01092883839737624\n",
+      "        total_loss: 30.730765342712402\n",
+      "        vf_explained_var: 0.9611188769340515\n",
+      "        vf_loss: 30.741480032602947\n",
+      "    num_steps_sampled: 970752\n",
+      "    num_steps_trained: 970752\n",
+      "  iterations_since_restore: 6\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.506666666666664\n",
+      "    gpu_util_percent0: 0.31133333333333335\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.766666666666666\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15665935939519685\n",
+      "    mean_env_wait_ms: 1.1717385763775916\n",
+      "    mean_inference_ms: 4.878261275270092\n",
+      "    mean_raw_obs_processing_ms: 0.41058148949868417\n",
+      "  time_since_restore: 162.52765536308289\n",
+      "  time_this_iter_s: 26.74772834777832\n",
+      "  time_total_s: 162.52765536308289\n",
+      "  timers:\n",
+      "    learn_throughput: 8262.231\n",
+      "    learn_time_ms: 19582.119\n",
+      "    sample_throughput: 21815.562\n",
+      "    sample_time_ms: 7416.357\n",
+      "    update_time_ms: 35.692\n",
+      "  timestamp: 1602709543\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 970752\n",
+      "  training_iteration: 6\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      6 |          162.528 | 970752 |  225.711 |              280.566 |              145.717 |            860.694 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3824\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:44:41,292 - wandb.wandb_agent - INFO - Running runs: ['uz9lk4hk']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204438-uz9lk4hk/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204438-uz9lk4hk/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 209.43434\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 209.43434\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3742\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708279\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/uz9lk4hk\u001b[0m\n",
-      "2020-10-14 20:44:46,515 - wandb.wandb_agent - INFO - Cleaning up finished run: uz9lk4hk\n",
-      "2020-10-14 20:44:46,834 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:44:46,834 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta58\n",
-      "2020-10-14 20:44:46,836 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta58\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/i1pzxngg\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204448-i1pzxngg\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3552.704692556634\n",
+      "    time_step_min: 3179\n",
+      "  date: 2020-10-14_21-06-09\n",
+      "  done: false\n",
+      "  episode_len_mean: 855.0387658227849\n",
+      "  episode_reward_max: 284.35353535353545\n",
+      "  episode_reward_mean: 227.46978487405684\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 1264\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0706470410029094\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007387861027382314\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013462736414415607\n",
+      "        total_loss: 19.742233912150066\n",
+      "        vf_explained_var: 0.9632093906402588\n",
+      "        vf_loss: 19.75549300511678\n",
+      "    num_steps_sampled: 1132544\n",
+      "    num_steps_trained: 1132544\n",
+      "  iterations_since_restore: 7\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.490322580645167\n",
+      "    gpu_util_percent0: 0.2838709677419355\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.770967741935483\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1559362762181637\n",
+      "    mean_env_wait_ms: 1.1739296740367058\n",
+      "    mean_inference_ms: 4.838086770027385\n",
+      "    mean_raw_obs_processing_ms: 0.4083166221477109\n",
+      "  time_since_restore: 189.12698912620544\n",
+      "  time_this_iter_s: 26.59933376312256\n",
+      "  time_total_s: 189.12698912620544\n",
+      "  timers:\n",
+      "    learn_throughput: 8266.464\n",
+      "    learn_time_ms: 19572.093\n",
+      "    sample_throughput: 21985.221\n",
+      "    sample_time_ms: 7359.125\n",
+      "    update_time_ms: 33.432\n",
+      "  timestamp: 1602709569\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1132544\n",
+      "  training_iteration: 7\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      7 |          189.127 | 1132544 |   227.47 |              284.354 |              145.717 |            855.039 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3912\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:44:51,849 - wandb.wandb_agent - INFO - Running runs: ['i1pzxngg']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204448-i1pzxngg/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204448-i1pzxngg/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 209.89899\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 209.89899\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3826\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708290\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/i1pzxngg\u001b[0m\n",
-      "2020-10-14 20:44:57,073 - wandb.wandb_agent - INFO - Cleaning up finished run: i1pzxngg\n",
-      "2020-10-14 20:44:57,383 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:44:57,384 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta59\n",
-      "2020-10-14 20:44:57,386 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta59\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/3kcee9dt\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204459-3kcee9dt\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3540.6398852223815\n",
+      "    time_step_min: 3179\n",
+      "  date: 2020-10-14_21-06-36\n",
+      "  done: false\n",
+      "  episode_len_mean: 850.7552742616034\n",
+      "  episode_reward_max: 284.35353535353545\n",
+      "  episode_reward_mean: 229.23441162681647\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 1422\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 1.0475670397281647\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007399068369219701\n",
+      "        model: {}\n",
+      "        policy_loss: -0.009183195322596779\n",
+      "        total_loss: 16.652963479359943\n",
+      "        vf_explained_var: 0.9667003154754639\n",
+      "        vf_loss: 16.661930561065674\n",
+      "    num_steps_sampled: 1294336\n",
+      "    num_steps_trained: 1294336\n",
+      "  iterations_since_restore: 8\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.71666666666666\n",
+      "    gpu_util_percent0: 0.318\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.773333333333333\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15529763017321097\n",
+      "    mean_env_wait_ms: 1.1758429577289389\n",
+      "    mean_inference_ms: 4.802703090940908\n",
+      "    mean_raw_obs_processing_ms: 0.40625843821594676\n",
+      "  time_since_restore: 215.48828315734863\n",
+      "  time_this_iter_s: 26.36129403114319\n",
+      "  time_total_s: 215.48828315734863\n",
+      "  timers:\n",
+      "    learn_throughput: 8278.422\n",
+      "    learn_time_ms: 19543.82\n",
+      "    sample_throughput: 22141.326\n",
+      "    sample_time_ms: 7307.241\n",
+      "    update_time_ms: 31.549\n",
+      "  timestamp: 1602709596\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1294336\n",
+      "  training_iteration: 8\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      8 |          215.488 | 1294336 |  229.234 |              284.354 |              145.717 |            850.755 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3526.8721374045804\n",
+      "    time_step_min: 3179\n",
+      "  date: 2020-10-14_21-07-03\n",
+      "  done: false\n",
+      "  episode_len_mean: 845.9875\n",
+      "  episode_reward_max: 285.7171717171716\n",
+      "  episode_reward_mean: 231.28724747474732\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 178\n",
+      "  episodes_total: 1600\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9999773452679316\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007928823702968657\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011458211791856835\n",
+      "        total_loss: 16.58501172065735\n",
+      "        vf_explained_var: 0.9729644656181335\n",
+      "        vf_loss: 16.596176783243816\n",
+      "    num_steps_sampled: 1456128\n",
+      "    num_steps_trained: 1456128\n",
+      "  iterations_since_restore: 9\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 22.993548387096773\n",
+      "    gpu_util_percent0: 0.29387096774193544\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7741935483870965\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15464999594543705\n",
+      "    mean_env_wait_ms: 1.1780557616549023\n",
+      "    mean_inference_ms: 4.76792634372993\n",
+      "    mean_raw_obs_processing_ms: 0.40415644333563394\n",
+      "  time_since_restore: 242.3112816810608\n",
+      "  time_this_iter_s: 26.822998523712158\n",
+      "  time_total_s: 242.3112816810608\n",
+      "  timers:\n",
+      "    learn_throughput: 8272.764\n",
+      "    learn_time_ms: 19557.188\n",
+      "    sample_throughput: 22220.441\n",
+      "    sample_time_ms: 7281.224\n",
+      "    update_time_ms: 31.962\n",
+      "  timestamp: 1602709623\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1456128\n",
+      "  training_iteration: 9\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |      9 |          242.311 | 1456128 |  231.287 |              285.717 |              145.717 |            845.987 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3508.0337259100643\n",
+      "    time_step_min: 3173\n",
+      "  date: 2020-10-14_21-07-29\n",
+      "  done: false\n",
+      "  episode_len_mean: 839.0052742616034\n",
+      "  episode_reward_max: 285.7171717171716\n",
+      "  episode_reward_mean: 234.27066551591852\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 296\n",
+      "  episodes_total: 1896\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9948871235052744\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006681857941051324\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011002168253374597\n",
+      "        total_loss: 15.828110535939535\n",
+      "        vf_explained_var: 0.9757750630378723\n",
+      "        vf_loss: 15.838941733042398\n",
+      "    num_steps_sampled: 1617920\n",
+      "    num_steps_trained: 1617920\n",
+      "  iterations_since_restore: 10\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.62666666666667\n",
+      "    gpu_util_percent0: 0.243\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15383279552979012\n",
+      "    mean_env_wait_ms: 1.1816130020394002\n",
+      "    mean_inference_ms: 4.721096556773406\n",
+      "    mean_raw_obs_processing_ms: 0.401484041778798\n",
+      "  time_since_restore: 268.6884000301361\n",
+      "  time_this_iter_s: 26.377118349075317\n",
+      "  time_total_s: 268.6884000301361\n",
+      "  timers:\n",
+      "    learn_throughput: 8283.974\n",
+      "    learn_time_ms: 19530.724\n",
+      "    sample_throughput: 22303.452\n",
+      "    sample_time_ms: 7254.124\n",
+      "    update_time_ms: 30.675\n",
+      "  timestamp: 1602709649\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1617920\n",
+      "  training_iteration: 10\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     10 |          268.688 | 1617920 |  234.271 |              285.717 |              145.717 |            839.005 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3499.19842053307\n",
+      "    time_step_min: 3171\n",
+      "  date: 2020-10-14_21-07-56\n",
+      "  done: false\n",
+      "  episode_len_mean: 835.7263875365142\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 235.87525203347977\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 2054\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.975288137793541\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.007294710182274382\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012727556153549813\n",
+      "        total_loss: 11.962000767389933\n",
+      "        vf_explained_var: 0.9750909805297852\n",
+      "        vf_loss: 11.974486589431763\n",
+      "    num_steps_sampled: 1779712\n",
+      "    num_steps_trained: 1779712\n",
+      "  iterations_since_restore: 11\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.49032258064517\n",
+      "    gpu_util_percent0: 0.2429032258064516\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7741935483870965\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15346055964066002\n",
+      "    mean_env_wait_ms: 1.1832618180585688\n",
+      "    mean_inference_ms: 4.700248117692935\n",
+      "    mean_raw_obs_processing_ms: 0.4002798984804907\n",
+      "  time_since_restore: 295.3983256816864\n",
+      "  time_this_iter_s: 26.709925651550293\n",
+      "  time_total_s: 295.3983256816864\n",
+      "  timers:\n",
+      "    learn_throughput: 8281.307\n",
+      "    learn_time_ms: 19537.012\n",
+      "    sample_throughput: 22772.438\n",
+      "    sample_time_ms: 7104.729\n",
+      "    update_time_ms: 28.523\n",
+      "  timestamp: 1602709676\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1779712\n",
+      "  training_iteration: 11\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     11 |          295.398 | 1779712 |  235.875 |              294.202 |              145.717 |            835.726 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3490.923534798535\n",
+      "    time_step_min: 3159\n",
+      "  date: 2020-10-14_21-08-23\n",
+      "  done: false\n",
+      "  episode_len_mean: 832.6595840867993\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 237.1319752680511\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 2212\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9521185209353765\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006661186693236232\n",
+      "        model: {}\n",
+      "        policy_loss: -0.013089668517447231\n",
+      "        total_loss: 12.603836615880331\n",
+      "        vf_explained_var: 0.9737562537193298\n",
+      "        vf_loss: 12.61673672993978\n",
+      "    num_steps_sampled: 1941504\n",
+      "    num_steps_trained: 1941504\n",
+      "  iterations_since_restore: 12\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.477419354838712\n",
+      "    gpu_util_percent0: 0.31483870967741934\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.77741935483871\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15312252956972325\n",
+      "    mean_env_wait_ms: 1.1848090383105752\n",
+      "    mean_inference_ms: 4.681388267276605\n",
+      "    mean_raw_obs_processing_ms: 0.399154195717825\n",
+      "  time_since_restore: 322.18752455711365\n",
+      "  time_this_iter_s: 26.789198875427246\n",
+      "  time_total_s: 322.18752455711365\n",
+      "  timers:\n",
+      "    learn_throughput: 8286.795\n",
+      "    learn_time_ms: 19524.074\n",
+      "    sample_throughput: 23017.742\n",
+      "    sample_time_ms: 7029.013\n",
+      "    update_time_ms: 27.778\n",
+      "  timestamp: 1602709703\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 1941504\n",
+      "  training_iteration: 12\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     12 |          322.188 | 1941504 |  237.132 |              294.202 |              145.717 |             832.66 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3477.030998389694\n",
+      "    time_step_min: 3151\n",
+      "  date: 2020-10-14_21-08-50\n",
+      "  done: false\n",
+      "  episode_len_mean: 827.7671178343949\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 239.24816235604442\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 300\n",
+      "  episodes_total: 2512\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9211943199237188\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.0069003046955913305\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011187698284629732\n",
+      "        total_loss: 15.527917702992758\n",
+      "        vf_explained_var: 0.9792836308479309\n",
+      "        vf_loss: 15.538876056671143\n",
+      "    num_steps_sampled: 2103296\n",
+      "    num_steps_trained: 2103296\n",
+      "  iterations_since_restore: 13\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.738709677419354\n",
+      "    gpu_util_percent0: 0.22483870967741937\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7677419354838704\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15257686397002923\n",
+      "    mean_env_wait_ms: 1.1876912397652413\n",
+      "    mean_inference_ms: 4.650923491552926\n",
+      "    mean_raw_obs_processing_ms: 0.39736320906724354\n",
+      "  time_since_restore: 349.16273403167725\n",
+      "  time_this_iter_s: 26.9752094745636\n",
+      "  time_total_s: 349.16273403167725\n",
+      "  timers:\n",
+      "    learn_throughput: 8272.827\n",
+      "    learn_time_ms: 19557.038\n",
+      "    sample_throughput: 23048.074\n",
+      "    sample_time_ms: 7019.762\n",
+      "    update_time_ms: 28.051\n",
+      "  timestamp: 1602709730\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2103296\n",
+      "  training_iteration: 13\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.1/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     13 |          349.163 | 2103296 |  239.248 |              294.202 |              145.717 |            827.767 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3468.7953348382243\n",
+      "    time_step_min: 3151\n",
+      "  date: 2020-10-14_21-09-17\n",
+      "  done: false\n",
+      "  episode_len_mean: 825.1623231571109\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 240.4743300465563\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 174\n",
+      "  episodes_total: 2686\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9143994450569153\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.0060155229875817895\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012139652118397256\n",
+      "        total_loss: 10.54153060913086\n",
+      "        vf_explained_var: 0.979185163974762\n",
+      "        vf_loss: 10.553526004155477\n",
+      "    num_steps_sampled: 2265088\n",
+      "    num_steps_trained: 2265088\n",
+      "  iterations_since_restore: 14\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.141935483870967\n",
+      "    gpu_util_percent0: 0.3683870967741936\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.770967741935483\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.152289456661776\n",
+      "    mean_env_wait_ms: 1.1891260627894187\n",
+      "    mean_inference_ms: 4.635358873544114\n",
+      "    mean_raw_obs_processing_ms: 0.3964728387235993\n",
+      "  time_since_restore: 375.75564908981323\n",
+      "  time_this_iter_s: 26.592915058135986\n",
+      "  time_total_s: 375.75564908981323\n",
+      "  timers:\n",
+      "    learn_throughput: 8285.665\n",
+      "    learn_time_ms: 19526.737\n",
+      "    sample_throughput: 23035.609\n",
+      "    sample_time_ms: 7023.561\n",
+      "    update_time_ms: 28.254\n",
+      "  timestamp: 1602709757\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2265088\n",
+      "  training_iteration: 14\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     14 |          375.756 | 2265088 |  240.474 |              294.202 |              145.717 |            825.162 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3462.4602272727275\n",
+      "    time_step_min: 3131\n",
+      "  date: 2020-10-14_21-09-44\n",
+      "  done: false\n",
+      "  episode_len_mean: 822.9542897327707\n",
+      "  episode_reward_max: 294.20202020201987\n",
+      "  episode_reward_mean: 241.45540851553497\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 2844\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.9019962549209595\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006133316123547654\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012229806568939239\n",
+      "        total_loss: 9.555021127065023\n",
+      "        vf_explained_var: 0.9795403480529785\n",
+      "        vf_loss: 9.567088762919107\n",
+      "    num_steps_sampled: 2426880\n",
+      "    num_steps_trained: 2426880\n",
+      "  iterations_since_restore: 15\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.84333333333333\n",
+      "    gpu_util_percent0: 0.38433333333333325\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7733333333333325\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1520571188276099\n",
+      "    mean_env_wait_ms: 1.1903605198268268\n",
+      "    mean_inference_ms: 4.622489399949343\n",
+      "    mean_raw_obs_processing_ms: 0.39571469986441193\n",
+      "  time_since_restore: 402.6329152584076\n",
+      "  time_this_iter_s: 26.87726616859436\n",
+      "  time_total_s: 402.6329152584076\n",
+      "  timers:\n",
+      "    learn_throughput: 8271.367\n",
+      "    learn_time_ms: 19560.491\n",
+      "    sample_throughput: 22997.017\n",
+      "    sample_time_ms: 7035.347\n",
+      "    update_time_ms: 27.999\n",
+      "  timestamp: 1602709784\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2426880\n",
+      "  training_iteration: 15\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     15 |          402.633 | 2426880 |  241.455 |              294.202 |              145.717 |            822.954 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3453.8821989528797\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-10-10\n",
+      "  done: false\n",
+      "  episode_len_mean: 819.9792477302204\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 242.75903981448718\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 240\n",
+      "  episodes_total: 3084\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8659086326758066\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00634678197093308\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012134946203635385\n",
+      "        total_loss: 13.195513248443604\n",
+      "        vf_explained_var: 0.980156421661377\n",
+      "        vf_loss: 13.207446098327637\n",
+      "    num_steps_sampled: 2588672\n",
+      "    num_steps_trained: 2588672\n",
+      "  iterations_since_restore: 16\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.309677419354838\n",
+      "    gpu_util_percent0: 0.36096774193548387\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7709677419354835\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1517332658779976\n",
+      "    mean_env_wait_ms: 1.192259437186598\n",
+      "    mean_inference_ms: 4.604579073734175\n",
+      "    mean_raw_obs_processing_ms: 0.3946603100703554\n",
+      "  time_since_restore: 429.12290596961975\n",
+      "  time_this_iter_s: 26.489990711212158\n",
+      "  time_total_s: 429.12290596961975\n",
+      "  timers:\n",
+      "    learn_throughput: 8279.016\n",
+      "    learn_time_ms: 19542.42\n",
+      "    sample_throughput: 23020.64\n",
+      "    sample_time_ms: 7028.128\n",
+      "    update_time_ms: 26.01\n",
+      "  timestamp: 1602709810\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2588672\n",
+      "  training_iteration: 16\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     16 |          429.123 | 2588672 |  242.759 |              298.899 |              145.717 |            819.979 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3445.377811550152\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-10-37\n",
+      "  done: false\n",
+      "  episode_len_mean: 817.4445449065702\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 243.9846110289147\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 234\n",
+      "  episodes_total: 3318\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8606445689996084\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005582816433161497\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011729711521184072\n",
+      "        total_loss: 9.780861934026083\n",
+      "        vf_explained_var: 0.9827695488929749\n",
+      "        vf_loss: 9.792463779449463\n",
+      "    num_steps_sampled: 2750464\n",
+      "    num_steps_trained: 2750464\n",
+      "  iterations_since_restore: 17\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.576666666666664\n",
+      "    gpu_util_percent0: 0.37900000000000006\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7633333333333328\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1514577798692161\n",
+      "    mean_env_wait_ms: 1.1938746610135567\n",
+      "    mean_inference_ms: 4.589150214465355\n",
+      "    mean_raw_obs_processing_ms: 0.3937904501711353\n",
+      "  time_since_restore: 455.6629276275635\n",
+      "  time_this_iter_s: 26.540021657943726\n",
+      "  time_total_s: 455.6629276275635\n",
+      "  timers:\n",
+      "    learn_throughput: 8274.015\n",
+      "    learn_time_ms: 19554.23\n",
+      "    sample_throughput: 23082.272\n",
+      "    sample_time_ms: 7009.362\n",
+      "    update_time_ms: 25.933\n",
+      "  timestamp: 1602709837\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2750464\n",
+      "  training_iteration: 17\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     17 |          455.663 | 2750464 |  243.985 |              298.899 |              145.717 |            817.445 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3440.181554524362\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-11-04\n",
+      "  done: false\n",
+      "  episode_len_mean: 815.873417721519\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 244.73267194383405\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 3476\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8570553759733835\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00693794801676025\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012595690621916825\n",
+      "        total_loss: 9.302302360534668\n",
+      "        vf_explained_var: 0.9800246357917786\n",
+      "        vf_loss: 9.314632733662924\n",
+      "    num_steps_sampled: 2912256\n",
+      "    num_steps_trained: 2912256\n",
+      "  iterations_since_restore: 18\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.28064516129032\n",
+      "    gpu_util_percent0: 0.28935483870967743\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7709677419354835\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1512869388842221\n",
+      "    mean_env_wait_ms: 1.1948860891963016\n",
+      "    mean_inference_ms: 4.579551596598977\n",
+      "    mean_raw_obs_processing_ms: 0.39323684175907914\n",
+      "  time_since_restore: 482.1069633960724\n",
+      "  time_this_iter_s: 26.44403576850891\n",
+      "  time_total_s: 482.1069633960724\n",
+      "  timers:\n",
+      "    learn_throughput: 8269.598\n",
+      "    learn_time_ms: 19564.676\n",
+      "    sample_throughput: 23100.114\n",
+      "    sample_time_ms: 7003.948\n",
+      "    update_time_ms: 28.338\n",
+      "  timestamp: 1602709864\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 2912256\n",
+      "  training_iteration: 18\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     18 |          482.107 | 2912256 |  244.733 |              298.899 |              145.717 |            815.873 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3435.160891089109\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-11-30\n",
+      "  done: false\n",
+      "  episode_len_mean: 814.1853165938865\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 245.55176216311577\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 188\n",
+      "  episodes_total: 3664\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8260929683844248\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00627721450291574\n",
+      "        model: {}\n",
+      "        policy_loss: -0.010709817167177485\n",
+      "        total_loss: 11.524338483810425\n",
+      "        vf_explained_var: 0.9801642894744873\n",
+      "        vf_loss: 11.534833749135336\n",
+      "    num_steps_sampled: 3074048\n",
+      "    num_steps_trained: 3074048\n",
+      "  iterations_since_restore: 19\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.583870967741937\n",
+      "    gpu_util_percent0: 0.4041935483870968\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.770967741935483\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15108964699814104\n",
+      "    mean_env_wait_ms: 1.1960450147791832\n",
+      "    mean_inference_ms: 4.568663786078855\n",
+      "    mean_raw_obs_processing_ms: 0.39260767936611063\n",
+      "  time_since_restore: 508.7718939781189\n",
+      "  time_this_iter_s: 26.66493058204651\n",
+      "  time_total_s: 508.7718939781189\n",
+      "  timers:\n",
+      "    learn_throughput: 8268.954\n",
+      "    learn_time_ms: 19566.198\n",
+      "    sample_throughput: 23189.124\n",
+      "    sample_time_ms: 6977.064\n",
+      "    update_time_ms: 35.073\n",
+      "  timestamp: 1602709890\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3074048\n",
+      "  training_iteration: 19\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     19 |          508.772 | 3074048 |  245.552 |              298.899 |              145.717 |            814.185 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3428.4441611422744\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-11-57\n",
+      "  done: false\n",
+      "  episode_len_mean: 812.0630379746835\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 246.60976857179378\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 286\n",
+      "  episodes_total: 3950\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8063790599505106\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00581474454763035\n",
+      "        model: {}\n",
+      "        policy_loss: -0.010059737542178482\n",
+      "        total_loss: 10.378987232844034\n",
+      "        vf_explained_var: 0.9842923283576965\n",
+      "        vf_loss: 10.388868490854898\n",
+      "    num_steps_sampled: 3235840\n",
+      "    num_steps_trained: 3235840\n",
+      "  iterations_since_restore: 20\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 24.36666666666667\n",
+      "    gpu_util_percent0: 0.31966666666666665\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.763333333333333\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15083686241602978\n",
+      "    mean_env_wait_ms: 1.1977366106159508\n",
+      "    mean_inference_ms: 4.554172739039668\n",
+      "    mean_raw_obs_processing_ms: 0.3917865708011254\n",
+      "  time_since_restore: 535.195830821991\n",
+      "  time_this_iter_s: 26.42393684387207\n",
+      "  time_total_s: 535.195830821991\n",
+      "  timers:\n",
+      "    learn_throughput: 8263.14\n",
+      "    learn_time_ms: 19579.966\n",
+      "    sample_throughput: 23222.566\n",
+      "    sample_time_ms: 6967.016\n",
+      "    update_time_ms: 35.303\n",
+      "  timestamp: 1602709917\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3235840\n",
+      "  training_iteration: 20\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     20 |          535.196 | 3235840 |   246.61 |              298.899 |              145.717 |            812.063 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3424.633333333333\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-12-24\n",
+      "  done: false\n",
+      "  episode_len_mean: 811.1747809152872\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 247.171761431255\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 4108\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.8050975054502487\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006411496355819206\n",
+      "        model: {}\n",
+      "        policy_loss: -0.0109325938198405\n",
+      "        total_loss: 8.397321462631226\n",
+      "        vf_explained_var: 0.9822394847869873\n",
+      "        vf_loss: 8.408015330632528\n",
+      "    num_steps_sampled: 3397632\n",
+      "    num_steps_trained: 3397632\n",
+      "  iterations_since_restore: 21\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.345161290322583\n",
+      "    gpu_util_percent0: 0.33387096774193553\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7741935483870965\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1507044803921526\n",
+      "    mean_env_wait_ms: 1.1985632518212732\n",
+      "    mean_inference_ms: 4.546739803666223\n",
+      "    mean_raw_obs_processing_ms: 0.39136140122967195\n",
+      "  time_since_restore: 561.7221903800964\n",
+      "  time_this_iter_s: 26.52635955810547\n",
+      "  time_total_s: 561.7221903800964\n",
+      "  timers:\n",
+      "    learn_throughput: 8265.424\n",
+      "    learn_time_ms: 19574.556\n",
+      "    sample_throughput: 23297.173\n",
+      "    sample_time_ms: 6944.705\n",
+      "    update_time_ms: 43.065\n",
+      "  timestamp: 1602709944\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3397632\n",
+      "  training_iteration: 21\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     21 |          561.722 | 3397632 |  247.172 |              298.899 |              145.717 |            811.175 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3421.5082547169814\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-12-50\n",
+      "  done: false\n",
+      "  episode_len_mean: 810.3659793814433\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 247.6299262541061\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 160\n",
+      "  episodes_total: 4268\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7943403224150339\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006092905105712513\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012508889408006022\n",
+      "        total_loss: 10.069480101267496\n",
+      "        vf_explained_var: 0.9799533486366272\n",
+      "        vf_loss: 10.08177661895752\n",
+      "    num_steps_sampled: 3559424\n",
+      "    num_steps_trained: 3559424\n",
+      "  iterations_since_restore: 22\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.81666666666667\n",
+      "    gpu_util_percent0: 0.253\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7766666666666664\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15057738067312423\n",
+      "    mean_env_wait_ms: 1.1993669989655864\n",
+      "    mean_inference_ms: 4.539606631295978\n",
+      "    mean_raw_obs_processing_ms: 0.390950346310237\n",
+      "  time_since_restore: 588.3143429756165\n",
+      "  time_this_iter_s: 26.59215259552002\n",
+      "  time_total_s: 588.3143429756165\n",
+      "  timers:\n",
+      "    learn_throughput: 8259.281\n",
+      "    learn_time_ms: 19589.114\n",
+      "    sample_throughput: 23421.01\n",
+      "    sample_time_ms: 6907.986\n",
+      "    update_time_ms: 44.326\n",
+      "  timestamp: 1602709970\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3559424\n",
+      "  training_iteration: 22\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     22 |          588.314 | 3559424 |   247.63 |              298.899 |              145.717 |            810.366 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3415.407570422535\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-13-17\n",
+      "  done: false\n",
+      "  episode_len_mean: 808.5879265091863\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 248.62850287653427\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 304\n",
+      "  episodes_total: 4572\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7644506990909576\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005771325357879202\n",
+      "        model: {}\n",
+      "        policy_loss: -0.009549629636846172\n",
+      "        total_loss: 10.615382512410482\n",
+      "        vf_explained_var: 0.9846494197845459\n",
+      "        vf_loss: 10.624737024307251\n",
+      "    num_steps_sampled: 3721216\n",
+      "    num_steps_trained: 3721216\n",
+      "  iterations_since_restore: 23\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.08709677419355\n",
+      "    gpu_util_percent0: 0.3470967741935484\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7645161290322577\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.15036060576180685\n",
+      "    mean_env_wait_ms: 1.200837895461041\n",
+      "    mean_inference_ms: 4.527163047815564\n",
+      "    mean_raw_obs_processing_ms: 0.3902497259773404\n",
+      "  time_since_restore: 614.8627808094025\n",
+      "  time_this_iter_s: 26.54843783378601\n",
+      "  time_total_s: 614.8627808094025\n",
+      "  timers:\n",
+      "    learn_throughput: 8272.154\n",
+      "    learn_time_ms: 19558.63\n",
+      "    sample_throughput: 23487.2\n",
+      "    sample_time_ms: 6888.518\n",
+      "    update_time_ms: 42.454\n",
+      "  timestamp: 1602709997\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3721216\n",
+      "  training_iteration: 23\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     23 |          614.863 | 3721216 |  248.629 |              298.899 |              145.717 |            808.588 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3412.3495331069607\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-13-44\n",
+      "  done: false\n",
+      "  episode_len_mean: 807.6881856540084\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 249.11699271192933\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 168\n",
+      "  episodes_total: 4740\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7512289037307104\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.006618439607943098\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011942399355272451\n",
+      "        total_loss: 7.106495062510173\n",
+      "        vf_explained_var: 0.9852812886238098\n",
+      "        vf_loss: 7.118151148160298\n",
+      "    num_steps_sampled: 3883008\n",
+      "    num_steps_trained: 3883008\n",
+      "  iterations_since_restore: 24\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.763333333333332\n",
+      "    gpu_util_percent0: 0.272\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.783333333333333\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1502473661939675\n",
+      "    mean_env_wait_ms: 1.2015818529322149\n",
+      "    mean_inference_ms: 4.520789877874915\n",
+      "    mean_raw_obs_processing_ms: 0.38989796231221446\n",
+      "  time_since_restore: 641.3350374698639\n",
+      "  time_this_iter_s: 26.472256660461426\n",
+      "  time_total_s: 641.3350374698639\n",
+      "  timers:\n",
+      "    learn_throughput: 8271.35\n",
+      "    learn_time_ms: 19560.532\n",
+      "    sample_throughput: 23514.635\n",
+      "    sample_time_ms: 6880.481\n",
+      "    update_time_ms: 42.61\n",
+      "  timestamp: 1602710024\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 3883008\n",
+      "  training_iteration: 24\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     24 |          641.335 | 3883008 |  249.117 |              298.899 |              145.717 |            807.688 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3409.523613963039\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-14-10\n",
+      "  done: false\n",
+      "  episode_len_mean: 806.9183340138832\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 249.55621548271597\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 4898\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7577964961528778\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005755245918408036\n",
+      "        model: {}\n",
+      "        policy_loss: -0.00993577616463881\n",
+      "        total_loss: 7.848556240399678\n",
+      "        vf_explained_var: 0.9825068116188049\n",
+      "        vf_loss: 7.858295281728108\n",
+      "    num_steps_sampled: 4044800\n",
+      "    num_steps_trained: 4044800\n",
+      "  iterations_since_restore: 25\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.06774193548387\n",
+      "    gpu_util_percent0: 0.2922580645161291\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.780645161290322\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1501490253393215\n",
+      "    mean_env_wait_ms: 1.2022478349453032\n",
+      "    mean_inference_ms: 4.515108016272416\n",
+      "    mean_raw_obs_processing_ms: 0.3895752935542475\n",
+      "  time_since_restore: 668.0025424957275\n",
+      "  time_this_iter_s: 26.667505025863647\n",
+      "  time_total_s: 668.0025424957275\n",
+      "  timers:\n",
+      "    learn_throughput: 8278.009\n",
+      "    learn_time_ms: 19544.795\n",
+      "    sample_throughput: 23536.559\n",
+      "    sample_time_ms: 6874.072\n",
+      "    update_time_ms: 43.337\n",
+      "  timestamp: 1602710050\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4044800\n",
+      "  training_iteration: 25\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     25 |          668.003 | 4044800 |  249.556 |              298.899 |              145.717 |            806.918 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3405.1021997274675\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-14-37\n",
+      "  done: false\n",
+      "  episode_len_mean: 805.7744433688287\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 250.1943931082362\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 267\n",
+      "  episodes_total: 5165\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.734145000576973\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.00586831111771365\n",
+      "        model: {}\n",
+      "        policy_loss: -0.009772113577734368\n",
+      "        total_loss: 11.755430380503336\n",
+      "        vf_explained_var: 0.9823409914970398\n",
+      "        vf_loss: 11.76498262087504\n",
+      "    num_steps_sampled: 4206592\n",
+      "    num_steps_trained: 4206592\n",
+      "  iterations_since_restore: 26\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 24.22666666666667\n",
+      "    gpu_util_percent0: 0.29566666666666663\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.766666666666666\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14999721678332353\n",
+      "    mean_env_wait_ms: 1.2033614261477694\n",
+      "    mean_inference_ms: 4.506158258902504\n",
+      "    mean_raw_obs_processing_ms: 0.3890744939361603\n",
+      "  time_since_restore: 694.2319819927216\n",
+      "  time_this_iter_s: 26.22943949699402\n",
+      "  time_total_s: 694.2319819927216\n",
+      "  timers:\n",
+      "    learn_throughput: 8283.812\n",
+      "    learn_time_ms: 19531.104\n",
+      "    sample_throughput: 23581.891\n",
+      "    sample_time_ms: 6860.858\n",
+      "    update_time_ms: 43.287\n",
+      "  timestamp: 1602710077\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4206592\n",
+      "  training_iteration: 26\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     26 |          694.232 | 4206592 |  250.194 |              298.899 |              145.717 |            805.774 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3401.4015718562873\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-15-03\n",
+      "  done: false\n",
+      "  episode_len_mean: 804.8773268801191\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 250.75784276119336\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 207\n",
+      "  episodes_total: 5372\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7132567266623179\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.0055438703469311195\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011437343899160624\n",
+      "        total_loss: 6.541075627009074\n",
+      "        vf_explained_var: 0.9872210621833801\n",
+      "        vf_loss: 6.5523152351379395\n",
+      "    num_steps_sampled: 4368384\n",
+      "    num_steps_trained: 4368384\n",
+      "  iterations_since_restore: 27\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.353333333333335\n",
+      "    gpu_util_percent0: 0.35100000000000003\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.78\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14988135384306817\n",
+      "    mean_env_wait_ms: 1.2041428049451586\n",
+      "    mean_inference_ms: 4.499582938127375\n",
+      "    mean_raw_obs_processing_ms: 0.38871711747737686\n",
+      "  time_since_restore: 720.3953409194946\n",
+      "  time_this_iter_s: 26.16335892677307\n",
+      "  time_total_s: 720.3953409194946\n",
+      "  timers:\n",
+      "    learn_throughput: 8297.771\n",
+      "    learn_time_ms: 19498.248\n",
+      "    sample_throughput: 23599.897\n",
+      "    sample_time_ms: 6855.623\n",
+      "    update_time_ms: 43.091\n",
+      "  timestamp: 1602710103\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4368384\n",
+      "  training_iteration: 27\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     27 |          720.395 | 4368384 |  250.758 |              298.899 |              145.717 |            804.877 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3398.81697564522\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-15-30\n",
+      "  done: false\n",
+      "  episode_len_mean: 804.1676311030741\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 251.131851973624\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 5530\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.7216055691242218\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005685570455777149\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011318465374642983\n",
+      "        total_loss: 6.673397620519002\n",
+      "        vf_explained_var: 0.9849072098731995\n",
+      "        vf_loss: 6.684508363405864\n",
+      "    num_steps_sampled: 4530176\n",
+      "    num_steps_trained: 4530176\n",
+      "  iterations_since_restore: 28\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.886666666666667\n",
+      "    gpu_util_percent0: 0.2823333333333334\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7766666666666664\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14980043216178882\n",
+      "    mean_env_wait_ms: 1.2047185869915957\n",
+      "    mean_inference_ms: 4.494854897407615\n",
+      "    mean_raw_obs_processing_ms: 0.38845720586260357\n",
+      "  time_since_restore: 746.6311824321747\n",
+      "  time_this_iter_s: 26.235841512680054\n",
+      "  time_total_s: 746.6311824321747\n",
+      "  timers:\n",
+      "    learn_throughput: 8303.173\n",
+      "    learn_time_ms: 19485.562\n",
+      "    sample_throughput: 23620.984\n",
+      "    sample_time_ms: 6849.503\n",
+      "    update_time_ms: 40.457\n",
+      "  timestamp: 1602710130\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4530176\n",
+      "  training_iteration: 28\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     28 |          746.631 | 4530176 |  251.132 |              298.899 |              145.717 |            804.168 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3395.749869178441\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-15-56\n",
+      "  done: false\n",
+      "  episode_len_mean: 803.1786148238153\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 251.58607600041373\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 231\n",
+      "  episodes_total: 5761\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6937208970387777\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005959675957759221\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011802961448362717\n",
+      "        total_loss: 10.309924920399984\n",
+      "        vf_explained_var: 0.983536958694458\n",
+      "        vf_loss: 10.321478684743246\n",
+      "    num_steps_sampled: 4691968\n",
+      "    num_steps_trained: 4691968\n",
+      "  iterations_since_restore: 29\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 22.92258064516129\n",
+      "    gpu_util_percent0: 0.417741935483871\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7677419354838704\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1496903041319913\n",
+      "    mean_env_wait_ms: 1.205558648276445\n",
+      "    mean_inference_ms: 4.488274076300299\n",
+      "    mean_raw_obs_processing_ms: 0.38809971432498575\n",
+      "  time_since_restore: 773.0392935276031\n",
+      "  time_this_iter_s: 26.408111095428467\n",
+      "  time_total_s: 773.0392935276031\n",
+      "  timers:\n",
+      "    learn_throughput: 8317.418\n",
+      "    learn_time_ms: 19452.189\n",
+      "    sample_throughput: 23590.607\n",
+      "    sample_time_ms: 6858.323\n",
+      "    update_time_ms: 32.563\n",
+      "  timestamp: 1602710156\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4691968\n",
+      "  training_iteration: 29\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     29 |          773.039 | 4691968 |  251.586 |              298.899 |              145.717 |            803.179 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3392.510374832664\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-16-23\n",
+      "  done: false\n",
+      "  episode_len_mean: 802.1863757495004\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 252.1170717837939\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 243\n",
+      "  episodes_total: 6004\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6723145395517349\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005279912458111842\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011053321950991327\n",
+      "        total_loss: 7.6361691157023115\n",
+      "        vf_explained_var: 0.9863631129264832\n",
+      "        vf_loss: 7.647030512491862\n",
+      "    num_steps_sampled: 4853760\n",
+      "    num_steps_trained: 4853760\n",
+      "  iterations_since_restore: 30\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.860000000000003\n",
+      "    gpu_util_percent0: 0.3133333333333333\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14957739547666185\n",
+      "    mean_env_wait_ms: 1.2063717178260402\n",
+      "    mean_inference_ms: 4.481825393409482\n",
+      "    mean_raw_obs_processing_ms: 0.3877499422841646\n",
+      "  time_since_restore: 799.4389925003052\n",
+      "  time_this_iter_s: 26.399698972702026\n",
+      "  time_total_s: 799.4389925003052\n",
+      "  timers:\n",
+      "    learn_throughput: 8320.988\n",
+      "    learn_time_ms: 19443.845\n",
+      "    sample_throughput: 23585.905\n",
+      "    sample_time_ms: 6859.69\n",
+      "    update_time_ms: 34.74\n",
+      "  timestamp: 1602710183\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 4853760\n",
+      "  training_iteration: 30\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     30 |          799.439 | 4853760 |  252.117 |              298.899 |              145.717 |            802.186 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3390.042386697098\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-16-49\n",
+      "  done: false\n",
+      "  episode_len_mean: 801.4599156118144\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 252.45022769073393\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 158\n",
+      "  episodes_total: 6162\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6794097969929377\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005776377705236276\n",
+      "        model: {}\n",
+      "        policy_loss: -0.011071474878311468\n",
+      "        total_loss: 6.609892050425212\n",
+      "        vf_explained_var: 0.9848344326019287\n",
+      "        vf_loss: 6.620725552241008\n",
+      "    num_steps_sampled: 5015552\n",
+      "    num_steps_trained: 5015552\n",
+      "  iterations_since_restore: 31\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.277419354838713\n",
+      "    gpu_util_percent0: 0.25258064516129036\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.7806451612903222\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1495093060246751\n",
+      "    mean_env_wait_ms: 1.2068785550500694\n",
+      "    mean_inference_ms: 4.477842010054414\n",
+      "    mean_raw_obs_processing_ms: 0.3875324536178687\n",
+      "  time_since_restore: 826.0708198547363\n",
+      "  time_this_iter_s: 26.631827354431152\n",
+      "  time_total_s: 826.0708198547363\n",
+      "  timers:\n",
+      "    learn_throughput: 8316.229\n",
+      "    learn_time_ms: 19454.972\n",
+      "    sample_throughput: 23563.199\n",
+      "    sample_time_ms: 6866.3\n",
+      "    update_time_ms: 27.804\n",
+      "  timestamp: 1602710209\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5015552\n",
+      "  training_iteration: 31\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     31 |          826.071 | 5015552 |   252.45 |              298.899 |              145.717 |             801.46 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3386.696788413098\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-17-16\n",
+      "  done: false\n",
+      "  episode_len_mean: 800.385893416928\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 252.93216649251121\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 218\n",
+      "  episodes_total: 6380\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6597986618677775\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005507051052215199\n",
+      "        model: {}\n",
+      "        policy_loss: -0.01063559478886115\n",
+      "        total_loss: 8.718894402186075\n",
+      "        vf_explained_var: 0.9843838214874268\n",
+      "        vf_loss: 8.729309240976969\n",
+      "    num_steps_sampled: 5177344\n",
+      "    num_steps_trained: 5177344\n",
+      "  iterations_since_restore: 32\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.65666666666667\n",
+      "    gpu_util_percent0: 0.31466666666666665\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.769999999999999\n",
+      "    vram_util_percent0: 0.10437848474909811\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14942336945631843\n",
+      "    mean_env_wait_ms: 1.2075933530118947\n",
+      "    mean_inference_ms: 4.472619260274858\n",
+      "    mean_raw_obs_processing_ms: 0.3872484802290064\n",
+      "  time_since_restore: 852.6489214897156\n",
+      "  time_this_iter_s: 26.578101634979248\n",
+      "  time_total_s: 852.6489214897156\n",
+      "  timers:\n",
+      "    learn_throughput: 8318.686\n",
+      "    learn_time_ms: 19449.227\n",
+      "    sample_throughput: 23545.491\n",
+      "    sample_time_ms: 6871.464\n",
+      "    update_time_ms: 25.913\n",
+      "  timestamp: 1602710236\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5177344\n",
+      "  training_iteration: 32\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.3/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     32 |          852.649 | 5177344 |  252.932 |              298.899 |              145.717 |            800.386 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "\n",
+      "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3383.374754048736\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-17-43\n",
+      "  done: false\n",
+      "  episode_len_mean: 799.2391861341372\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 253.41443827879388\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 255\n",
+      "  episodes_total: 6635\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6365775416294733\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005616956856101751\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012718923583937189\n",
+      "        total_loss: 7.9217236042022705\n",
+      "        vf_explained_var: 0.9864194393157959\n",
+      "        vf_loss: 7.934199412663777\n",
+      "    num_steps_sampled: 5339136\n",
+      "    num_steps_trained: 5339136\n",
+      "  iterations_since_restore: 33\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 22.948387096774198\n",
+      "    gpu_util_percent0: 0.27838709677419354\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.774193548387097\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.14932189346086894\n",
+      "    mean_env_wait_ms: 1.2083754344986544\n",
+      "    mean_inference_ms: 4.466813879680601\n",
+      "    mean_raw_obs_processing_ms: 0.38693984548896015\n",
+      "  time_since_restore: 879.1936430931091\n",
+      "  time_this_iter_s: 26.544721603393555\n",
+      "  time_total_s: 879.1936430931091\n",
+      "  timers:\n",
+      "    learn_throughput: 8314.788\n",
+      "    learn_time_ms: 19458.343\n",
+      "    sample_throughput: 23551.139\n",
+      "    sample_time_ms: 6869.816\n",
+      "    update_time_ms: 25.193\n",
+      "  timestamp: 1602710263\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5339136\n",
+      "  training_iteration: 33\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     33 |          879.194 | 5339136 |  253.414 |              298.899 |              145.717 |            799.239 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3998\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:45:02,401 - wandb.wandb_agent - INFO - Running runs: ['3kcee9dt']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204459-3kcee9dt/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204459-3kcee9dt/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 211.84848\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 211.84848\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3517\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708300\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/3kcee9dt\u001b[0m\n",
-      "2020-10-14 20:45:07,613 - wandb.wandb_agent - INFO - Cleaning up finished run: 3kcee9dt\n",
-      "2020-10-14 20:45:10,058 - wandb.wandb_agent - INFO - Agent received command: run\n",
-      "2020-10-14 20:45:10,059 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
-      "\tinstance_path: /JSS/JSS/env/instances/ta60\n",
-      "2020-10-14 20:45:10,061 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python MTWR.py --instance_path=/JSS/JSS/env/instances/ta60\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mingambe\u001b[0m (use `wandb login --relogin` to force relogin)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.5\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMTWR\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üßπ View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/sweeps/po3ygyxo\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/kkype8ue\u001b[0m\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in wandb/run-20201014_204512-kkype8ue\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb off` to turn off syncing.\n",
       "\n",
+      "Result for PPO_jss_env_a052f_00000:\n",
+      "  custom_metrics:\n",
+      "    time_step_max: 4054\n",
+      "    time_step_mean: 3381.266627253917\n",
+      "    time_step_min: 3083\n",
+      "  date: 2020-10-14_21-18-10\n",
+      "  done: false\n",
+      "  episode_len_mean: 798.6435089785105\n",
+      "  episode_reward_max: 298.8989898989898\n",
+      "  episode_reward_mean: 253.7262884957909\n",
+      "  episode_reward_min: 145.7171717171716\n",
+      "  episodes_this_iter: 159\n",
+      "  episodes_total: 6794\n",
+      "  experiment_id: 165e19af1fec47018887d66ceb7af0b3\n",
+      "  experiment_tag: '0'\n",
+      "  hostname: f85e62b52919\n",
+      "  info:\n",
+      "    learner:\n",
+      "      default_policy:\n",
+      "        allreduce_latency: 0.0\n",
+      "        cur_kl_coeff: 0.10000000000000002\n",
+      "        cur_lr: 5.0e-05\n",
+      "        entropy: 0.6548023074865341\n",
+      "        entropy_coeff: 0.0005000000000000001\n",
+      "        kl: 0.005482170420388381\n",
+      "        model: {}\n",
+      "        policy_loss: -0.012428849945232892\n",
+      "        total_loss: 6.324912707010905\n",
+      "        vf_explained_var: 0.9852306842803955\n",
+      "        vf_loss: 6.337120532989502\n",
+      "    num_steps_sampled: 5500928\n",
+      "    num_steps_trained: 5500928\n",
+      "  iterations_since_restore: 34\n",
+      "  node_ip: 172.17.0.4\n",
+      "  num_healthy_workers: 79\n",
+      "  off_policy_estimator: {}\n",
+      "  perf:\n",
+      "    cpu_util_percent: 23.380645161290325\n",
+      "    gpu_util_percent0: 0.3251612903225806\n",
+      "    gpu_util_percent1: 0.0\n",
+      "    gpu_util_percent2: 0.0\n",
+      "    ram_util_percent: 3.780645161290322\n",
+      "    vram_util_percent0: 0.10437848474909812\n",
+      "    vram_util_percent1: 0.0\n",
+      "    vram_util_percent2: 0.0\n",
+      "  pid: 422\n",
+      "  policy_reward_max: {}\n",
+      "  policy_reward_mean: {}\n",
+      "  policy_reward_min: {}\n",
+      "  sampler_perf:\n",
+      "    mean_action_processing_ms: 0.1492620448104757\n",
+      "    mean_env_wait_ms: 1.2088387513992318\n",
+      "    mean_inference_ms: 4.463358913349285\n",
+      "    mean_raw_obs_processing_ms: 0.38675634808095327\n",
+      "  time_since_restore: 905.8512279987335\n",
+      "  time_this_iter_s: 26.65758490562439\n",
+      "  time_total_s: 905.8512279987335\n",
+      "  timers:\n",
+      "    learn_throughput: 8309.566\n",
+      "    learn_time_ms: 19470.571\n",
+      "    sample_throughput: 23555.989\n",
+      "    sample_time_ms: 6868.402\n",
+      "    update_time_ms: 31.554\n",
+      "  timestamp: 1602710290\n",
+      "  timesteps_since_restore: 0\n",
+      "  timesteps_total: 5500928\n",
+      "  training_iteration: 34\n",
+      "  trial_id: a052f_00000\n",
+      "  \n",
+      "== Status ==\n",
+      "Memory usage on this node: 28.2/754.6 GiB\n",
+      "Using FIFO scheduling algorithm.\n",
+      "Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.4 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)\n",
+      "Result logdir: /root/ray_results/ppo-jss\n",
+      "Number of trials: 1 (1 RUNNING)\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
+      "| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |\n",
+      "|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|\n",
+      "| PPO_jss_env_a052f_00000 | RUNNING  | 172.17.0.4:422 |     34 |          905.851 | 5500928 |  253.726 |              298.899 |              145.717 |            798.644 |\n",
+      "+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+\n",
       "\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4096\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
-      "2020-10-14 20:45:15,074 - wandb.wandb_agent - INFO - Running runs: ['kkype8ue']\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: wandb/run-20201014_204512-kkype8ue/logs/debug.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: wandb/run-20201014_204512-kkype8ue/logs/debug-internal.log\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result 197.64646\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode 197.64646\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep 3735\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step 0\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime 1\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp 1602708313\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:       nb_episodes ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:   avg_best_result ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:      best_episode ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:     best_timestep ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:             _step ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:          _runtime ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m:        _timestamp ‚ñÅ\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mMTWR\u001b[0m: \u001b[34mhttps://wandb.ai/ingambe/RLLIB_SWEEP_2/runs/kkype8ue\u001b[0m\n",
-      "2020-10-14 20:45:20,294 - wandb.wandb_agent - INFO - Cleaning up finished run: kkype8ue\n",
-      "2020-10-14 20:45:20,607 - wandb.wandb_agent - INFO - Agent received command: exit\n",
-      "2020-10-14 20:45:20,607 - wandb.wandb_agent - INFO - Received exit command. Killing runs and quitting.\n",
-      "\u001b[34m\u001b[1mwandb\u001b[0m: Terminating and syncing runs. Press ctrl-c to kill.\n"
+      "\n"
      ]
     }
    ],
    "source": [
-    "!wandb agent po3ygyxo"
+    "!wandb agent 2ex0mmbf"
    ]
   },
   {
diff --git a/JSS/__pycache__/default_config.cpython-38.pyc b/JSS/__pycache__/default_config.cpython-38.pyc
index 14e0018..8fbfc47 100644
Binary files a/JSS/__pycache__/default_config.cpython-38.pyc and b/JSS/__pycache__/default_config.cpython-38.pyc differ
diff --git a/JSS/default_config.py b/JSS/default_config.py
index 46f4d10..4729210 100644
--- a/JSS/default_config.py
+++ b/JSS/default_config.py
@@ -6,7 +6,7 @@ default_config = {
     'env': 'jss_env',
     'seed': 0,
     'framework': 'torch',
-    'log_level': 'INFO',
+    'log_level': 'WARN',
     'num_gpus': 1,
     'instance_path': '/JSS/JSS/env/instances/ta51',
     'num_envs_per_worker': 2,
diff --git a/JSS/env/__pycache__/JSS.cpython-38.pyc b/JSS/env/__pycache__/JSS.cpython-38.pyc
index b190093..143d813 100644
Binary files a/JSS/env/__pycache__/JSS.cpython-38.pyc and b/JSS/env/__pycache__/JSS.cpython-38.pyc differ
diff --git a/JSS/train.py b/JSS/train.py
index d4ac941..3400140 100644
--- a/JSS/train.py
+++ b/JSS/train.py
@@ -50,7 +50,7 @@ def train_func():
     ray.init()
 
     stop = {
-        "time_total_s": 60 * 60,
+        "time_total_s": 3 * 60 * 60,
     }
 
     analysis = tune.run(PPOTrainer, config=config, stop=stop, name="ppo-jss")
diff --git a/JSS/wandb/debug-internal.log b/JSS/wandb/debug-internal.log
index f73d8af..289dc04 120000
--- a/JSS/wandb/debug-internal.log
+++ b/JSS/wandb/debug-internal.log
@@ -1 +1 @@
-run-20201014_204512-kkype8ue/logs/debug-internal.log
\ No newline at end of file
+run-20201016_075622-ivrei8dy/logs/debug-internal.log
\ No newline at end of file
diff --git a/JSS/wandb/debug.log b/JSS/wandb/debug.log
index db2ed2e..826599f 120000
--- a/JSS/wandb/debug.log
+++ b/JSS/wandb/debug.log
@@ -1 +1 @@
-run-20201014_204512-kkype8ue/logs/debug.log
\ No newline at end of file
+run-20201016_075622-ivrei8dy/logs/debug.log
\ No newline at end of file
diff --git a/JSS/wandb/latest-run b/JSS/wandb/latest-run
index f19e5b0..e9060ba 120000
--- a/JSS/wandb/latest-run
+++ b/JSS/wandb/latest-run
@@ -1 +1 @@
-run-20201014_204512-kkype8ue
\ No newline at end of file
+run-20201016_075622-ivrei8dy
\ No newline at end of file
diff --git a/JSS/wandb/run-20201014_185459-4qedwvw4/logs/debug-internal.log b/JSS/wandb/run-20201014_185459-4qedwvw4/logs/debug-internal.log
index 62aefa0..a469c27 100644
--- a/JSS/wandb/run-20201014_185459-4qedwvw4/logs/debug-internal.log
+++ b/JSS/wandb/run-20201014_185459-4qedwvw4/logs/debug-internal.log
@@ -3791,3 +3791,60 @@
 2020-10-14 20:48:22,755 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
 2020-10-14 20:48:24,674 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
 2020-10-14 20:48:29,293 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:48:33,903 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:48:37,761 DEBUG   HandlerThread:30187 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:48:37,761 DEBUG   SenderThread:30187 [sender.py:send():88] send: request
+2020-10-14 20:48:37,761 DEBUG   SenderThread:30187 [sender.py:send_request():97] send_request: status
+2020-10-14 20:48:37,765 DEBUG   SenderThread:30187 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:48:37,962 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:48:38,519 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:48:43,143 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:48:47,759 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:48:52,385 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:48:52,968 DEBUG   HandlerThread:30187 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:48:52,968 DEBUG   SenderThread:30187 [sender.py:send():88] send: request
+2020-10-14 20:48:52,968 DEBUG   SenderThread:30187 [sender.py:send_request():97] send_request: status
+2020-10-14 20:48:52,975 DEBUG   SenderThread:30187 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:48:53,184 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:48:57,005 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:01,622 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:06,249 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:08,189 DEBUG   HandlerThread:30187 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:08,190 DEBUG   SenderThread:30187 [sender.py:send():88] send: request
+2020-10-14 20:49:08,190 DEBUG   SenderThread:30187 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:08,195 DEBUG   SenderThread:30187 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:08,392 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:10,874 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:15,492 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:20,107 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:23,398 DEBUG   HandlerThread:30187 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:23,398 DEBUG   SenderThread:30187 [sender.py:send():88] send: request
+2020-10-14 20:49:23,398 DEBUG   SenderThread:30187 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:23,403 DEBUG   SenderThread:30187 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:23,595 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:24,725 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:29,333 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:33,953 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:38,566 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:38,601 DEBUG   HandlerThread:30187 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:38,601 DEBUG   SenderThread:30187 [sender.py:send():88] send: request
+2020-10-14 20:49:38,601 DEBUG   SenderThread:30187 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:38,605 DEBUG   SenderThread:30187 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:38,802 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:43,182 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:47,813 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:52,436 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:49:53,808 DEBUG   HandlerThread:30187 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:53,809 DEBUG   SenderThread:30187 [sender.py:send():88] send: request
+2020-10-14 20:49:53,809 DEBUG   SenderThread:30187 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:53,813 DEBUG   SenderThread:30187 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:54,014 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:57,041 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:50:01,665 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:50:06,273 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
+2020-10-14 20:50:09,020 DEBUG   HandlerThread:30187 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:50:09,020 DEBUG   SenderThread:30187 [sender.py:send():88] send: request
+2020-10-14 20:50:09,020 DEBUG   SenderThread:30187 [sender.py:send_request():97] send_request: status
+2020-10-14 20:50:09,024 DEBUG   SenderThread:30187 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:50:09,226 DEBUG   SenderThread:30187 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:50:10,902 DEBUG   SenderThread:30187 [sender.py:send():88] send: stats
diff --git a/JSS/wandb/run-20201014_185459-4qedwvw4/run-4qedwvw4.wandb b/JSS/wandb/run-20201014_185459-4qedwvw4/run-4qedwvw4.wandb
index fe32750..a173876 100644
Binary files a/JSS/wandb/run-20201014_185459-4qedwvw4/run-4qedwvw4.wandb and b/JSS/wandb/run-20201014_185459-4qedwvw4/run-4qedwvw4.wandb differ
diff --git a/JSS/wandb/run-20201014_185623-es6i30gb/logs/debug-internal.log b/JSS/wandb/run-20201014_185623-es6i30gb/logs/debug-internal.log
index 6c28b35..39056d1 100644
--- a/JSS/wandb/run-20201014_185623-es6i30gb/logs/debug-internal.log
+++ b/JSS/wandb/run-20201014_185623-es6i30gb/logs/debug-internal.log
@@ -3766,3 +3766,55 @@
 2020-10-14 20:48:28,206 DEBUG   SenderThread:34843 [sender.py:send_request():97] send_request: status
 2020-10-14 20:48:28,210 DEBUG   SenderThread:34843 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
 2020-10-14 20:48:28,411 DEBUG   SenderThread:34843 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:48:32,759 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:48:37,379 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:48:41,998 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:48:43,417 DEBUG   HandlerThread:34843 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:48:43,417 DEBUG   SenderThread:34843 [sender.py:send():88] send: request
+2020-10-14 20:48:43,418 DEBUG   SenderThread:34843 [sender.py:send_request():97] send_request: status
+2020-10-14 20:48:43,425 DEBUG   SenderThread:34843 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:48:43,622 DEBUG   SenderThread:34843 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:48:46,618 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:48:51,237 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:48:55,878 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:48:58,628 DEBUG   HandlerThread:34843 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:48:58,629 DEBUG   SenderThread:34843 [sender.py:send():88] send: request
+2020-10-14 20:48:58,629 DEBUG   SenderThread:34843 [sender.py:send_request():97] send_request: status
+2020-10-14 20:48:58,634 DEBUG   SenderThread:34843 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:48:58,828 DEBUG   SenderThread:34843 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:00,486 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:05,100 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:09,717 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:13,834 DEBUG   HandlerThread:34843 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:13,834 DEBUG   SenderThread:34843 [sender.py:send():88] send: request
+2020-10-14 20:49:13,835 DEBUG   SenderThread:34843 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:13,839 DEBUG   SenderThread:34843 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:14,032 DEBUG   SenderThread:34843 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:14,334 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:18,948 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:23,561 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:28,192 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:29,038 DEBUG   HandlerThread:34843 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:29,038 DEBUG   SenderThread:34843 [sender.py:send():88] send: request
+2020-10-14 20:49:29,039 DEBUG   SenderThread:34843 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:29,043 DEBUG   SenderThread:34843 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:29,241 DEBUG   SenderThread:34843 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:32,806 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:37,429 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:42,040 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:44,247 DEBUG   HandlerThread:34843 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:44,247 DEBUG   SenderThread:34843 [sender.py:send():88] send: request
+2020-10-14 20:49:44,247 DEBUG   SenderThread:34843 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:44,252 DEBUG   SenderThread:34843 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:44,453 DEBUG   SenderThread:34843 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:49:46,669 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:51,288 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:55,904 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:49:59,459 DEBUG   HandlerThread:34843 [handler.py:handle_request():54] handle_request: status
+2020-10-14 20:49:59,459 DEBUG   SenderThread:34843 [sender.py:send():88] send: request
+2020-10-14 20:49:59,460 DEBUG   SenderThread:34843 [sender.py:send_request():97] send_request: status
+2020-10-14 20:49:59,464 DEBUG   SenderThread:34843 [connectionpool.py:_new_conn():955] Starting new HTTPS connection (1): api.wandb.ai:443
+2020-10-14 20:49:59,667 DEBUG   SenderThread:34843 [connectionpool.py:_make_request():428] https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
+2020-10-14 20:50:00,513 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:50:05,125 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
+2020-10-14 20:50:09,744 DEBUG   SenderThread:34843 [sender.py:send():88] send: stats
diff --git a/JSS/wandb/run-20201014_185623-es6i30gb/run-es6i30gb.wandb b/JSS/wandb/run-20201014_185623-es6i30gb/run-es6i30gb.wandb
index 6b25ef1..0114685 100644
Binary files a/JSS/wandb/run-20201014_185623-es6i30gb/run-es6i30gb.wandb and b/JSS/wandb/run-20201014_185623-es6i30gb/run-es6i30gb.wandb differ
