2020-10-11 16:04:43,236	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_7a7ff_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=58108)[0m F1011 16:04:45.503923 58108 58108 service_based_gcs_client.cc:207] Couldn't reconnect to GCS server. The last attempted GCS server address was 172.17.0.4:42211
[2m[36m(pid=58108)[0m *** Check failure stack trace: ***
[2m[36m(pid=58108)[0m     @     0x7ff16efa56ed  google::LogMessage::Fail()
[2m[36m(pid=58108)[0m     @     0x7ff16efa684c  google::LogMessage::SendToLog()
[2m[36m(pid=58108)[0m     @     0x7ff16efa53c9  google::LogMessage::Flush()
[2m[36m(pid=58108)[0m     @     0x7ff16efa55e1  google::LogMessage::~LogMessage()
[2m[36m(pid=58108)[0m     @     0x7ff16ef5c789  ray::RayLog::~RayLog()
[2m[36m(pid=58108)[0m     @     0x7ff16eca01ea  ray::gcs::ServiceBasedGcsClient::ReconnectGcsServer()
[2m[36m(pid=58108)[0m     @     0x7ff16eca02ef  ray::gcs::ServiceBasedGcsClient::GcsServiceFailureDetected()
[2m[36m(pid=58108)[0m     @     0x7ff16eca0491  ray::gcs::ServiceBasedGcsClient::PeriodicallyCheckGcsServerAddress()
[2m[36m(pid=58108)[0m     @     0x7ff16eca2801  ray::gcs::ServiceBasedGcsClient::Connect()
[2m[36m(pid=58108)[0m     @     0x7ff16ebb17a8  ray::gcs::GlobalStateAccessor::Connect()
[2m[36m(pid=58108)[0m     @     0x7ff16eb22a2c  __pyx_pw_3ray_7_raylet_19GlobalStateAccessor_3connect()
[2m[36m(pid=58108)[0m     @     0x5595b1c6198a  method_vectorcall_NOARGS
[2m[36m(pid=58108)[0m     @     0x5595b1bf1b08  _PyEval_EvalFrameDefault.cold.2792
[2m[36m(pid=58108)[0m     @     0x5595b1c7c6a2  _PyEval_EvalCodeWithName
[2m[36m(pid=58108)[0m     @     0x5595b1c7da20  method_vectorcall
[2m[36m(pid=58108)[0m     @     0x5595b1bf2de6  _PyEval_EvalFrameDefault.cold.2792
[2m[36m(pid=58108)[0m     @     0x5595b1c7cbaf  _PyEval_EvalCodeWithName
[2m[36m(pid=58108)[0m     @     0x5595b1c7d643  _PyFunction_Vectorcall.localalias.353
[2m[36m(pid=58108)[0m     @     0x5595b1bf2de6  _PyEval_EvalFrameDefault.cold.2792
[2m[36m(pid=58108)[0m     @     0x5595b1c7c6a2  _PyEval_EvalCodeWithName
[2m[36m(pid=58108)[0m     @     0x5595b1c7d454  PyEval_EvalCodeEx
[2m[36m(pid=58108)[0m     @     0x5595b1d0bbbc  PyEval_EvalCode
[2m[36m(pid=58108)[0m     @     0x5595b1d0bc64  run_eval_code_obj
[2m[36m(pid=58108)[0m     @     0x5595b1d3dd14  run_mod
[2m[36m(pid=58108)[0m     @     0x5595b1c06625  PyRun_FileExFlags
[2m[36m(pid=58108)[0m     @     0x5595b1c06a0a  PyRun_SimpleFileExFlags
[2m[36m(pid=58108)[0m     @     0x5595b1c078cf  Py_RunMain.cold.2911
[2m[36m(pid=58108)[0m     @     0x5595b1d40829  Py_BytesMain
[2m[36m(pid=58108)[0m     @     0x7ff1702aa840  __libc_start_main
[2m[36m(pid=58108)[0m     @     0x5595b1cd0b33  (unknown)
[2m[36m(pid=58131)[0m 2020-10-11 16:04:46,089	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=58209)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58209)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58232)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58232)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58157)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58157)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58207)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58207)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58200)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58200)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58176)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58176)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58187)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58187)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58163)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58163)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58206)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58206)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58214)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58214)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58166)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58166)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58210)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58210)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58160)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58160)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58224)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58224)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58217)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58217)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58212)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58212)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58195)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58195)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58168)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58168)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58167)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58167)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58233)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58233)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58117)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58117)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58172)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58172)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58102)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58102)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58107)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58107)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58171)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58171)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58222)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58222)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58228)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58228)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58097)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58097)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58103)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58103)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58175)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58175)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58095)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58095)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58125)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58125)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58112)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58112)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58126)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58126)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58116)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58116)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58096)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58096)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58169)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58169)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58198)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58198)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58094)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58094)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58120)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58120)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58105)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58105)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58182)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58182)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58110)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58110)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58135)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58135)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58114)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58114)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58234)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58234)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58132)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58132)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58113)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58113)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58115)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58115)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58123)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58123)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58098)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58098)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58186)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58186)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58225)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58225)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58211)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58211)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58174)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58174)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58159)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58159)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58165)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58165)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58109)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58109)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58161)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58161)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58208)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58208)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58202)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58202)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58181)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58181)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58100)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58100)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58101)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58101)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58230)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58230)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58216)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58216)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=59506)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=59506)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58191)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58191)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58183)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58183)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58185)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58185)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58162)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58162)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58164)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58164)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58170)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58170)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58219)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58219)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58106)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58106)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58220)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58220)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58104)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58104)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58134)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58134)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58184)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58184)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_7a7ff_00000:
  custom_metrics: {}
  date: 2020-10-11_16-05-06
  done: false
  episode_len_mean: .nan
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1914540827274323
        entropy_coeff: 0.0001
        kl: 0.007823126390576363
        model: {}
        policy_loss: -0.015033794334158301
        total_loss: 562.9246368408203
        vf_explained_var: -0.7487409114837646
        vf_loss: 562.938232421875
    num_steps_sampled: 60672
    num_steps_trained: 60672
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 38.30555555555556
    gpu_util_percent0: 0.28833333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3166666666666664
    vram_util_percent0: 0.08632156262526876
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  time_since_restore: 14.397291421890259
  time_this_iter_s: 14.397291421890259
  time_total_s: 14.397291421890259
  timers:
    learn_throughput: 7415.073
    learn_time_ms: 8182.253
    sample_throughput: 9844.83
    sample_time_ms: 6162.829
    update_time_ms: 25.725
  timestamp: 1602432306
  timesteps_since_restore: 0
  timesteps_total: 60672
  training_iteration: 1
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      1 |          14.3973 | 60672 |      nan |                  nan |                  nan |                nan |
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4058
    time_step_mean: 3611.3670886075947
    time_step_min: 3306
  date: 2020-10-11_16-05-19
  done: false
  episode_len_mean: 886.0379746835443
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 218.84337041299042
  episode_reward_min: 151.1717171717169
  episodes_this_iter: 79
  episodes_total: 79
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1658275425434113
        entropy_coeff: 0.0001
        kl: 0.005513080162927508
        model: {}
        policy_loss: -0.010292008926626295
        total_loss: 595.6605682373047
        vf_explained_var: 0.18877363204956055
        vf_loss: 595.6699066162109
    num_steps_sampled: 121344
    num_steps_trained: 121344
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 39.51875
    gpu_util_percent0: 0.37
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1392562642493361
    mean_env_wait_ms: 0.6516056658962992
    mean_inference_ms: 5.6192679575765245
    mean_raw_obs_processing_ms: 0.3050151187445618
  time_since_restore: 28.144527196884155
  time_this_iter_s: 13.747235774993896
  time_total_s: 28.144527196884155
  timers:
    learn_throughput: 7525.277
    learn_time_ms: 8062.427
    sample_throughput: 10187.451
    sample_time_ms: 5955.562
    update_time_ms: 21.166
  timestamp: 1602432319
  timesteps_since_restore: 0
  timesteps_total: 121344
  training_iteration: 2
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      2 |          28.1445 | 121344 |  218.843 |              265.111 |              151.172 |            886.038 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4159
    time_step_mean: 3614.0443037974683
    time_step_min: 3306
  date: 2020-10-11_16-05-33
  done: false
  episode_len_mean: 880.4050632911392
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 218.43773174785815
  episode_reward_min: 135.8686868686869
  episodes_this_iter: 79
  episodes_total: 158
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1451095044612885
        entropy_coeff: 0.0001
        kl: 0.006658109603449702
        model: {}
        policy_loss: -0.009628374013118446
        total_loss: 343.8333053588867
        vf_explained_var: 0.582950234413147
        vf_loss: 343.8417205810547
    num_steps_sampled: 182016
    num_steps_trained: 182016
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 36.25
    gpu_util_percent0: 0.37749999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1365851144959723
    mean_env_wait_ms: 0.6516483429947283
    mean_inference_ms: 5.518104287901824
    mean_raw_obs_processing_ms: 0.2998649657433892
  time_since_restore: 41.348203897476196
  time_this_iter_s: 13.203676700592041
  time_total_s: 41.348203897476196
  timers:
    learn_throughput: 7564.887
    learn_time_ms: 8020.212
    sample_throughput: 10632.146
    sample_time_ms: 5706.468
    update_time_ms: 19.192
  timestamp: 1602432333
  timesteps_since_restore: 0
  timesteps_total: 182016
  training_iteration: 3
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      3 |          41.3482 | 182016 |  218.438 |              265.111 |              135.869 |            880.405 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4213
    time_step_mean: 3624.0
    time_step_min: 3306
  date: 2020-10-11_16-05-45
  done: false
  episode_len_mean: 877.9535864978903
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 216.92929292929279
  episode_reward_min: 127.68686868686834
  episodes_this_iter: 79
  episodes_total: 237
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1370092332363129
        entropy_coeff: 0.0001
        kl: 0.0060931689804419875
        model: {}
        policy_loss: -0.015206624171696603
        total_loss: 237.94325637817383
        vf_explained_var: 0.7285435795783997
        vf_loss: 237.95736694335938
    num_steps_sampled: 242688
    num_steps_trained: 242688
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.99375
    gpu_util_percent0: 0.425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13454040622665203
    mean_env_wait_ms: 0.6515288631657379
    mean_inference_ms: 5.404449168345442
    mean_raw_obs_processing_ms: 0.29550310639496924
  time_since_restore: 54.15542149543762
  time_this_iter_s: 12.807217597961426
  time_total_s: 54.15542149543762
  timers:
    learn_throughput: 7552.661
    learn_time_ms: 8033.195
    sample_throughput: 11133.455
    sample_time_ms: 5449.521
    update_time_ms: 19.784
  timestamp: 1602432345
  timesteps_since_restore: 0
  timesteps_total: 242688
  training_iteration: 4
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      4 |          54.1554 | 242688 |  216.929 |              265.111 |              127.687 |            877.954 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4220
    time_step_mean: 3626.240506329114
    time_step_min: 3306
  date: 2020-10-11_16-05-58
  done: false
  episode_len_mean: 876.5949367088608
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 216.58982227336645
  episode_reward_min: 126.62626262626257
  episodes_this_iter: 79
  episodes_total: 316
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.12206369638443
        entropy_coeff: 0.0001
        kl: 0.006890057120472193
        model: {}
        policy_loss: -0.011395521811209619
        total_loss: 132.92739486694336
        vf_explained_var: 0.8328981995582581
        vf_loss: 132.9375228881836
    num_steps_sampled: 303360
    num_steps_trained: 303360
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.593333333333334
    gpu_util_percent0: 0.3026666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1329078870095902
    mean_env_wait_ms: 0.6514872340321675
    mean_inference_ms: 5.307067246094525
    mean_raw_obs_processing_ms: 0.2919517870113773
  time_since_restore: 66.84345602989197
  time_this_iter_s: 12.688034534454346
  time_total_s: 66.84345602989197
  timers:
    learn_throughput: 7550.811
    learn_time_ms: 8035.164
    sample_throughput: 11496.568
    sample_time_ms: 5277.401
    update_time_ms: 19.59
  timestamp: 1602432358
  timesteps_since_restore: 0
  timesteps_total: 303360
  training_iteration: 5
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      5 |          66.8435 | 303360 |   216.59 |              265.111 |              126.626 |            876.595 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3624.9088607594936
    time_step_min: 3306
  date: 2020-10-11_16-06-11
  done: false
  episode_len_mean: 873.2886075949367
  episode_reward_max: 265.11111111111086
  episode_reward_mean: 216.7915867536119
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 395
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1141673624515533
        entropy_coeff: 0.0001
        kl: 0.006491948966868222
        model: {}
        policy_loss: -0.012061259825713933
        total_loss: 95.29883575439453
        vf_explained_var: 0.8731156587600708
        vf_loss: 95.30970764160156
    num_steps_sampled: 364032
    num_steps_trained: 364032
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.575
    gpu_util_percent0: 0.270625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13158014825376585
    mean_env_wait_ms: 0.6515391207968362
    mean_inference_ms: 5.225820762190736
    mean_raw_obs_processing_ms: 0.2889980741150863
  time_since_restore: 79.62222123146057
  time_this_iter_s: 12.778765201568604
  time_total_s: 79.62222123146057
  timers:
    learn_throughput: 7542.412
    learn_time_ms: 8044.111
    sample_throughput: 11734.364
    sample_time_ms: 5170.455
    update_time_ms: 19.635
  timestamp: 1602432371
  timesteps_since_restore: 0
  timesteps_total: 364032
  training_iteration: 6
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      6 |          79.6222 | 364032 |  216.792 |              265.111 |              118.293 |            873.289 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3619.0021097046415
    time_step_min: 3289
  date: 2020-10-11_16-06-24
  done: false
  episode_len_mean: 868.9451476793249
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 217.68654903465014
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 474
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0996873378753662
        entropy_coeff: 0.0001
        kl: 0.007531901705078781
        model: {}
        policy_loss: -0.010992132098181173
        total_loss: 71.36981773376465
        vf_explained_var: 0.8989866375923157
        vf_loss: 71.3794174194336
    num_steps_sampled: 424704
    num_steps_trained: 424704
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.71333333333333
    gpu_util_percent0: 0.29133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13050844491749886
    mean_env_wait_ms: 0.6516941788425266
    mean_inference_ms: 5.15856307166687
    mean_raw_obs_processing_ms: 0.2865702558167328
  time_since_restore: 92.38265776634216
  time_this_iter_s: 12.760436534881592
  time_total_s: 92.38265776634216
  timers:
    learn_throughput: 7546.712
    learn_time_ms: 8039.528
    sample_throughput: 11896.524
    sample_time_ms: 5099.977
    update_time_ms: 21.179
  timestamp: 1602432384
  timesteps_since_restore: 0
  timesteps_total: 424704
  training_iteration: 7
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      7 |          92.3827 | 424704 |  217.687 |              267.687 |              118.293 |            868.945 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3617.5623869801084
    time_step_min: 3289
  date: 2020-10-11_16-06-37
  done: false
  episode_len_mean: 863.7233273056058
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 217.9046888413976
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 553
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.080336093902588
        entropy_coeff: 0.0001
        kl: 0.00649541465099901
        model: {}
        policy_loss: -0.014029796118848026
        total_loss: 65.09321212768555
        vf_explained_var: 0.9129441976547241
        vf_loss: 65.10604953765869
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.29375
    gpu_util_percent0: 0.275
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4375
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12960126640808023
    mean_env_wait_ms: 0.6519909904724125
    mean_inference_ms: 5.101620631871957
    mean_raw_obs_processing_ms: 0.28448811359847365
  time_since_restore: 105.08664917945862
  time_this_iter_s: 12.703991413116455
  time_total_s: 105.08664917945862
  timers:
    learn_throughput: 7550.184
    learn_time_ms: 8035.831
    sample_throughput: 12037.574
    sample_time_ms: 5040.218
    update_time_ms: 23.219
  timestamp: 1602432397
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 8
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      8 |          105.087 | 485376 |  217.905 |              267.687 |              118.293 |            863.723 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3613.845276872964
    time_step_min: 3289
  date: 2020-10-11_16-06-49
  done: false
  episode_len_mean: 860.5602605863193
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 218.46788734248003
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 61
  episodes_total: 614
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0620779395103455
        entropy_coeff: 0.0001
        kl: 0.0070233645383268595
        model: {}
        policy_loss: -0.011862239392939955
        total_loss: 53.99057388305664
        vf_explained_var: 0.9240583181381226
        vf_loss: 54.00113868713379
    num_steps_sampled: 546048
    num_steps_trained: 546048
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.13333333333333
    gpu_util_percent0: 0.4153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12895817569501378
    mean_env_wait_ms: 0.6522547132371642
    mean_inference_ms: 5.062253685921348
    mean_raw_obs_processing_ms: 0.28298767768141286
  time_since_restore: 117.85798716545105
  time_this_iter_s: 12.771337985992432
  time_total_s: 117.85798716545105
  timers:
    learn_throughput: 7549.945
    learn_time_ms: 8036.085
    sample_throughput: 12134.416
    sample_time_ms: 4999.993
    update_time_ms: 23.094
  timestamp: 1602432409
  timesteps_since_restore: 0
  timesteps_total: 546048
  training_iteration: 9
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |      9 |          117.858 | 546048 |  218.468 |              267.687 |              118.293 |             860.56 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3611.2753846153846
    time_step_min: 3289
  date: 2020-10-11_16-07-02
  done: false
  episode_len_mean: 859.0492307692308
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 218.85726495726487
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 36
  episodes_total: 650
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0485284924507141
        entropy_coeff: 0.0001
        kl: 0.006600828724913299
        model: {}
        policy_loss: -0.017158268485218287
        total_loss: 33.94167709350586
        vf_explained_var: 0.9404016137123108
        vf_loss: 33.95761775970459
    num_steps_sampled: 606720
    num_steps_trained: 606720
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.25625
    gpu_util_percent0: 0.399375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12864684415887948
    mean_env_wait_ms: 0.6524636378572293
    mean_inference_ms: 5.041163744895523
    mean_raw_obs_processing_ms: 0.2822528246771891
  time_since_restore: 130.60816597938538
  time_this_iter_s: 12.750178813934326
  time_total_s: 130.60816597938538
  timers:
    learn_throughput: 7547.873
    learn_time_ms: 8038.291
    sample_throughput: 12229.181
    sample_time_ms: 4961.248
    update_time_ms: 25.366
  timestamp: 1602432422
  timesteps_since_restore: 0
  timesteps_total: 606720
  training_iteration: 10
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     10 |          130.608 | 606720 |  218.857 |              267.687 |              118.293 |            859.049 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3606.5876577840113
    time_step_min: 3289
  date: 2020-10-11_16-07-15
  done: false
  episode_len_mean: 856.6143057503506
  episode_reward_max: 267.6868686868684
  episode_reward_mean: 219.56752659838202
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 63
  episodes_total: 713
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0584369599819183
        entropy_coeff: 0.0001
        kl: 0.005829800385981798
        model: {}
        policy_loss: -0.01342546020168811
        total_loss: 33.587894439697266
        vf_explained_var: 0.931602954864502
        vf_loss: 33.60025978088379
    num_steps_sampled: 667392
    num_steps_trained: 667392
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.186666666666675
    gpu_util_percent0: 0.3693333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4199999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.128078485783574
    mean_env_wait_ms: 0.6526832097580031
    mean_inference_ms: 5.006329609058325
    mean_raw_obs_processing_ms: 0.2808894089431526
  time_since_restore: 143.25520038604736
  time_this_iter_s: 12.647034406661987
  time_total_s: 143.25520038604736
  timers:
    learn_throughput: 7569.718
    learn_time_ms: 8015.094
    sample_throughput: 12618.947
    sample_time_ms: 4808.008
    update_time_ms: 26.108
  timestamp: 1602432435
  timesteps_since_restore: 0
  timesteps_total: 667392
  training_iteration: 11
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     11 |          143.255 | 667392 |  219.568 |              267.687 |              118.293 |            856.614 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3600.558227848101
    time_step_min: 3235
  date: 2020-10-11_16-07-28
  done: false
  episode_len_mean: 853.659493670886
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 220.48107658867144
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 77
  episodes_total: 790
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.044329285621643
        entropy_coeff: 0.0001
        kl: 0.0065484626684337854
        model: {}
        policy_loss: -0.014785136096179485
        total_loss: 35.80673599243164
        vf_explained_var: 0.9390555024147034
        vf_loss: 35.820316314697266
    num_steps_sampled: 728064
    num_steps_trained: 728064
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.58
    gpu_util_percent0: 0.32933333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12748298109172757
    mean_env_wait_ms: 0.6530935558457849
    mean_inference_ms: 4.968588687024664
    mean_raw_obs_processing_ms: 0.2794120972899121
  time_since_restore: 155.83940172195435
  time_this_iter_s: 12.584201335906982
  time_total_s: 155.83940172195435
  timers:
    learn_throughput: 7569.422
    learn_time_ms: 8015.407
    sample_throughput: 12932.198
    sample_time_ms: 4691.546
    update_time_ms: 26.218
  timestamp: 1602432448
  timesteps_since_restore: 0
  timesteps_total: 728064
  training_iteration: 12
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     12 |          155.839 | 728064 |  220.481 |              275.869 |              118.293 |            853.659 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3593.910241657077
    time_step_min: 3235
  date: 2020-10-11_16-07-40
  done: false
  episode_len_mean: 850.2543153049482
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 221.48834722367513
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 869
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0323504209518433
        entropy_coeff: 0.0001
        kl: 0.005593539914116263
        model: {}
        policy_loss: -0.007711198413744569
        total_loss: 33.82645511627197
        vf_explained_var: 0.9453220367431641
        vf_loss: 33.83315181732178
    num_steps_sampled: 788736
    num_steps_trained: 788736
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.75333333333334
    gpu_util_percent0: 0.35799999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1269440350269048
    mean_env_wait_ms: 0.6535218584739464
    mean_inference_ms: 4.934719120591657
    mean_raw_obs_processing_ms: 0.2780699107334339
  time_since_restore: 168.4566102027893
  time_this_iter_s: 12.617208480834961
  time_total_s: 168.4566102027893
  timers:
    learn_throughput: 7570.15
    learn_time_ms: 8014.636
    sample_throughput: 13092.615
    sample_time_ms: 4634.063
    update_time_ms: 26.364
  timestamp: 1602432460
  timesteps_since_restore: 0
  timesteps_total: 788736
  training_iteration: 13
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     13 |          168.457 | 788736 |  221.488 |              275.869 |              118.293 |            850.254 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3588.6867088607596
    time_step_min: 3235
  date: 2020-10-11_16-07-53
  done: false
  episode_len_mean: 847.3449367088608
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 222.2797915867535
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 948
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.013579249382019
        entropy_coeff: 0.0001
        kl: 0.0068066451931372285
        model: {}
        policy_loss: -0.014376593055203557
        total_loss: 28.295745849609375
        vf_explained_var: 0.9575772881507874
        vf_loss: 28.308862686157227
    num_steps_sampled: 849408
    num_steps_trained: 849408
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.5625
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12647271860099454
    mean_env_wait_ms: 0.6539710230127329
    mean_inference_ms: 4.904906013887914
    mean_raw_obs_processing_ms: 0.2768762836434561
  time_since_restore: 181.10891199111938
  time_this_iter_s: 12.652301788330078
  time_total_s: 181.10891199111938
  timers:
    learn_throughput: 7582.305
    learn_time_ms: 8001.788
    sample_throughput: 13099.679
    sample_time_ms: 4631.564
    update_time_ms: 25.995
  timestamp: 1602432473
  timesteps_since_restore: 0
  timesteps_total: 849408
  training_iteration: 14
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     14 |          181.109 | 849408 |   222.28 |              275.869 |              118.293 |            847.345 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3583.4829600778967
    time_step_min: 3235
  date: 2020-10-11_16-08-06
  done: false
  episode_len_mean: 844.6426484907497
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 223.06823837203578
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1027
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9933836460113525
        entropy_coeff: 0.0001
        kl: 0.00562152813654393
        model: {}
        policy_loss: -0.011437032371759415
        total_loss: 25.94105339050293
        vf_explained_var: 0.9608175158500671
        vf_loss: 25.951465129852295
    num_steps_sampled: 910080
    num_steps_trained: 910080
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.85333333333333
    gpu_util_percent0: 0.362
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12605533833513857
    mean_env_wait_ms: 0.6544519960553011
    mean_inference_ms: 4.878354055195134
    mean_raw_obs_processing_ms: 0.2758113414277131
  time_since_restore: 193.74963283538818
  time_this_iter_s: 12.640720844268799
  time_total_s: 193.74963283538818
  timers:
    learn_throughput: 7592.843
    learn_time_ms: 7990.682
    sample_throughput: 13080.301
    sample_time_ms: 4638.425
    update_time_ms: 25.795
  timestamp: 1602432486
  timesteps_since_restore: 0
  timesteps_total: 910080
  training_iteration: 15
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     15 |           193.75 | 910080 |  223.068 |              275.869 |              118.293 |            844.643 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3577.5723327305604
    time_step_min: 3235
  date: 2020-10-11_16-08-18
  done: false
  episode_len_mean: 841.9891500904159
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 223.963787970117
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1106
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9824153780937195
        entropy_coeff: 0.0001
        kl: 0.005990388686768711
        model: {}
        policy_loss: -0.007877310505136847
        total_loss: 21.027120113372803
        vf_explained_var: 0.9672377705574036
        vf_loss: 21.033896446228027
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.5
    gpu_util_percent0: 0.31666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1256833967188158
    mean_env_wait_ms: 0.6549438728683441
    mean_inference_ms: 4.8545523827810815
    mean_raw_obs_processing_ms: 0.27484890394986167
  time_since_restore: 206.33544921875
  time_this_iter_s: 12.585816383361816
  time_total_s: 206.33544921875
  timers:
    learn_throughput: 7610.998
    learn_time_ms: 7971.623
    sample_throughput: 13079.829
    sample_time_ms: 4638.593
    update_time_ms: 25.306
  timestamp: 1602432498
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 16
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     16 |          206.335 | 970752 |  223.964 |              275.869 |              118.293 |            841.989 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3572.2556962025315
    time_step_min: 3235
  date: 2020-10-11_16-08-31
  done: false
  episode_len_mean: 839.464135021097
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 224.76933895921235
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1185
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9703270643949509
        entropy_coeff: 0.0001
        kl: 0.005524565000087023
        model: {}
        policy_loss: -0.01052850013365969
        total_loss: 24.28107976913452
        vf_explained_var: 0.9627320766448975
        vf_loss: 24.290600299835205
    num_steps_sampled: 1031424
    num_steps_trained: 1031424
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.2
    gpu_util_percent0: 0.3493333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12534404550034728
    mean_env_wait_ms: 0.6554369049907028
    mean_inference_ms: 4.83299518786737
    mean_raw_obs_processing_ms: 0.2739734408062774
  time_since_restore: 218.80903387069702
  time_this_iter_s: 12.473584651947021
  time_total_s: 218.80903387069702
  timers:
    learn_throughput: 7623.048
    learn_time_ms: 7959.021
    sample_throughput: 13121.081
    sample_time_ms: 4624.009
    update_time_ms: 24.086
  timestamp: 1602432511
  timesteps_since_restore: 0
  timesteps_total: 1031424
  training_iteration: 17
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     17 |          218.809 | 1031424 |  224.769 |              275.869 |              118.293 |            839.464 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3566.5245253164558
    time_step_min: 3235
  date: 2020-10-11_16-08-44
  done: false
  episode_len_mean: 837.506329113924
  episode_reward_max: 275.86868686868684
  episode_reward_mean: 225.63769818437535
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1264
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9576845765113831
        entropy_coeff: 0.0001
        kl: 0.006472750450484455
        model: {}
        policy_loss: -0.01621266547590494
        total_loss: 19.243000507354736
        vf_explained_var: 0.9701140522956848
        vf_loss: 19.258015155792236
    num_steps_sampled: 1092096
    num_steps_trained: 1092096
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.6
    gpu_util_percent0: 0.43062500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12503446974780746
    mean_env_wait_ms: 0.6559195313687357
    mean_inference_ms: 4.81334827821275
    mean_raw_obs_processing_ms: 0.27317726254777247
  time_since_restore: 231.5099401473999
  time_this_iter_s: 12.70090627670288
  time_total_s: 231.5099401473999
  timers:
    learn_throughput: 7618.755
    learn_time_ms: 7963.506
    sample_throughput: 13133.663
    sample_time_ms: 4619.579
    update_time_ms: 23.877
  timestamp: 1602432524
  timesteps_since_restore: 0
  timesteps_total: 1092096
  training_iteration: 18
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     18 |           231.51 | 1092096 |  225.638 |              275.869 |              118.293 |            837.506 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3560.9985107967236
    time_step_min: 3206
  date: 2020-10-11_16-08-56
  done: false
  episode_len_mean: 835.8816083395384
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 226.47497311160745
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1343
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9319173842668533
        entropy_coeff: 0.0001
        kl: 0.006037887651473284
        model: {}
        policy_loss: -0.012360059190541506
        total_loss: 18.858981609344482
        vf_explained_var: 0.971014678478241
        vf_loss: 18.87022590637207
    num_steps_sampled: 1152768
    num_steps_trained: 1152768
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.406666666666666
    gpu_util_percent0: 0.3486666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666663
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12475188650860593
    mean_env_wait_ms: 0.6563772532072604
    mean_inference_ms: 4.795310461019384
    mean_raw_obs_processing_ms: 0.2724444120578135
  time_since_restore: 244.0025041103363
  time_this_iter_s: 12.492563962936401
  time_total_s: 244.0025041103363
  timers:
    learn_throughput: 7627.655
    learn_time_ms: 7954.214
    sample_throughput: 13187.854
    sample_time_ms: 4600.597
    update_time_ms: 23.622
  timestamp: 1602432536
  timesteps_since_restore: 0
  timesteps_total: 1152768
  training_iteration: 19
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     19 |          244.003 | 1152768 |  226.475 |              280.263 |              118.293 |            835.882 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3557.6068917018283
    time_step_min: 3206
  date: 2020-10-11_16-09-09
  done: false
  episode_len_mean: 834.2637130801688
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 226.9888547926522
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1422
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9170082658529282
        entropy_coeff: 0.0001
        kl: 0.005881667952053249
        model: {}
        policy_loss: -0.009232628857716918
        total_loss: 21.077390670776367
        vf_explained_var: 0.9689568877220154
        vf_loss: 21.0855393409729
    num_steps_sampled: 1213440
    num_steps_trained: 1213440
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.3375
    gpu_util_percent0: 0.2875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4562500000000003
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12449303757212322
    mean_env_wait_ms: 0.6568240411041691
    mean_inference_ms: 4.778750735788818
    mean_raw_obs_processing_ms: 0.27177693900219413
  time_since_restore: 256.8121967315674
  time_this_iter_s: 12.809692621231079
  time_total_s: 256.8121967315674
  timers:
    learn_throughput: 7620.269
    learn_time_ms: 7961.924
    sample_throughput: 13189.585
    sample_time_ms: 4599.993
    update_time_ms: 22.14
  timestamp: 1602432549
  timesteps_since_restore: 0
  timesteps_total: 1213440
  training_iteration: 20
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     20 |          256.812 | 1213440 |  226.989 |              280.263 |              118.293 |            834.264 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3553.070619586942
    time_step_min: 3206
  date: 2020-10-11_16-09-22
  done: false
  episode_len_mean: 832.6069287141905
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 227.67616874945318
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1501
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9074065536260605
        entropy_coeff: 0.0001
        kl: 0.005816309945657849
        model: {}
        policy_loss: -0.015069264685735106
        total_loss: 16.58755850791931
        vf_explained_var: 0.9737280011177063
        vf_loss: 16.601556301116943
    num_steps_sampled: 1274112
    num_steps_trained: 1274112
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.49333333333333
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4199999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12425433273673535
    mean_env_wait_ms: 0.6572579299079598
    mean_inference_ms: 4.763498257013923
    mean_raw_obs_processing_ms: 0.27116589181050743
  time_since_restore: 269.43881154060364
  time_this_iter_s: 12.626614809036255
  time_total_s: 269.43881154060364
  timers:
    learn_throughput: 7617.989
    learn_time_ms: 7964.306
    sample_throughput: 13199.951
    sample_time_ms: 4596.381
    update_time_ms: 20.586
  timestamp: 1602432562
  timesteps_since_restore: 0
  timesteps_total: 1274112
  training_iteration: 21
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     21 |          269.439 | 1274112 |  227.676 |              280.263 |              118.293 |            832.607 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3546.6664556962023
    time_step_min: 3206
  date: 2020-10-11_16-09-35
  done: false
  episode_len_mean: 831.3810126582279
  episode_reward_max: 280.26262626262593
  episode_reward_mean: 228.64649661168644
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1580
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8895199149847031
        entropy_coeff: 0.0001
        kl: 0.006235960638150573
        model: {}
        policy_loss: -0.016274704947136343
        total_loss: 14.437464952468872
        vf_explained_var: 0.9757154583930969
        vf_loss: 14.45258092880249
    num_steps_sampled: 1334784
    num_steps_trained: 1334784
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.239999999999995
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666667
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12403295090588674
    mean_env_wait_ms: 0.657665481140537
    mean_inference_ms: 4.749378803791054
    mean_raw_obs_processing_ms: 0.27060251674617225
  time_since_restore: 282.1660873889923
  time_this_iter_s: 12.727275848388672
  time_total_s: 282.1660873889923
  timers:
    learn_throughput: 7613.066
    learn_time_ms: 7969.456
    sample_throughput: 13177.208
    sample_time_ms: 4604.314
    update_time_ms: 21.077
  timestamp: 1602432575
  timesteps_since_restore: 0
  timesteps_total: 1334784
  training_iteration: 22
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     22 |          282.166 | 1334784 |  228.646 |              280.263 |              118.293 |            831.381 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3541.2525617842075
    time_step_min: 3199
  date: 2020-10-11_16-09-47
  done: false
  episode_len_mean: 829.9795057263411
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 229.46678356804938
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1659
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8658313006162643
        entropy_coeff: 0.0001
        kl: 0.005380009184591472
        model: {}
        policy_loss: -0.008646945061627775
        total_loss: 20.08211898803711
        vf_explained_var: 0.9681438207626343
        vf_loss: 20.08977699279785
    num_steps_sampled: 1395456
    num_steps_trained: 1395456
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.59375
    gpu_util_percent0: 0.391875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.425
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12382703417818555
    mean_env_wait_ms: 0.658056314534902
    mean_inference_ms: 4.736261256209528
    mean_raw_obs_processing_ms: 0.2700851065081186
  time_since_restore: 294.70430302619934
  time_this_iter_s: 12.538215637207031
  time_total_s: 294.70430302619934
  timers:
    learn_throughput: 7614.448
    learn_time_ms: 7968.01
    sample_throughput: 13196.964
    sample_time_ms: 4597.421
    update_time_ms: 21.077
  timestamp: 1602432587
  timesteps_since_restore: 0
  timesteps_total: 1395456
  training_iteration: 23
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     23 |          294.704 | 1395456 |  229.467 |              281.323 |              118.293 |             829.98 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3536.58918296893
    time_step_min: 3199
  date: 2020-10-11_16-10-00
  done: false
  episode_len_mean: 829.3107019562716
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 230.1733561158187
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1738
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8665132224559784
        entropy_coeff: 0.0001
        kl: 0.005198416416533291
        model: {}
        policy_loss: -0.011439272901043296
        total_loss: 17.11276149749756
        vf_explained_var: 0.9721271991729736
        vf_loss: 17.12324619293213
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.3
    gpu_util_percent0: 0.34199999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4399999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12363491086642305
    mean_env_wait_ms: 0.6584116401779622
    mean_inference_ms: 4.724030151418645
    mean_raw_obs_processing_ms: 0.26960390783354077
  time_since_restore: 307.4050681591034
  time_this_iter_s: 12.700765132904053
  time_total_s: 307.4050681591034
  timers:
    learn_throughput: 7603.751
    learn_time_ms: 7979.22
    sample_throughput: 13221.046
    sample_time_ms: 4589.047
    update_time_ms: 22.999
  timestamp: 1602432600
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 24
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     24 |          307.405 | 1456128 |  230.173 |              281.323 |              118.293 |            829.311 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3531.2894881673087
    time_step_min: 3199
  date: 2020-10-11_16-10-13
  done: false
  episode_len_mean: 828.4171711612548
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 230.9763401766704
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 1817
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8617505133152008
        entropy_coeff: 0.0001
        kl: 0.0055166283855214715
        model: {}
        policy_loss: -0.012109191156923771
        total_loss: 12.609570026397705
        vf_explained_var: 0.9781627655029297
        vf_loss: 12.620662212371826
    num_steps_sampled: 1516800
    num_steps_trained: 1516800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.186666666666675
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666667
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12345748985022473
    mean_env_wait_ms: 0.6587396184782712
    mean_inference_ms: 4.712634606635987
    mean_raw_obs_processing_ms: 0.2691615888388835
  time_since_restore: 319.9479868412018
  time_this_iter_s: 12.542918682098389
  time_total_s: 319.9479868412018
  timers:
    learn_throughput: 7608.199
    learn_time_ms: 7974.555
    sample_throughput: 13240.633
    sample_time_ms: 4582.258
    update_time_ms: 24.345
  timestamp: 1602432613
  timesteps_since_restore: 0
  timesteps_total: 1516800
  training_iteration: 25
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     25 |          319.948 | 1516800 |  230.976 |              281.323 |              118.293 |            828.417 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3525.5443505807816
    time_step_min: 3199
  date: 2020-10-11_16-10-26
  done: false
  episode_len_mean: 827.2967265047519
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 231.84681556856845
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 77
  episodes_total: 1894
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8505872040987015
        entropy_coeff: 0.0001
        kl: 0.005504266591742635
        model: {}
        policy_loss: -0.013713978929445148
        total_loss: 14.596449613571167
        vf_explained_var: 0.9737269878387451
        vf_loss: 14.609147310256958
    num_steps_sampled: 1577472
    num_steps_trained: 1577472
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.56875
    gpu_util_percent0: 0.348125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12329297195619345
    mean_env_wait_ms: 0.6590516458679436
    mean_inference_ms: 4.702228905867046
    mean_raw_obs_processing_ms: 0.26875659849681693
  time_since_restore: 332.69026041030884
  time_this_iter_s: 12.742273569107056
  time_total_s: 332.69026041030884
  timers:
    learn_throughput: 7606.745
    learn_time_ms: 7976.079
    sample_throughput: 13222.83
    sample_time_ms: 4588.428
    update_time_ms: 31.364
  timestamp: 1602432626
  timesteps_since_restore: 0
  timesteps_total: 1577472
  training_iteration: 26
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     26 |           332.69 | 1577472 |  231.847 |              281.323 |              118.293 |            827.297 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3521.043279022403
    time_step_min: 3199
  date: 2020-10-11_16-10-38
  done: false
  episode_len_mean: 826.1883910386965
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 232.52879610771666
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 70
  episodes_total: 1964
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8386607617139816
        entropy_coeff: 0.0001
        kl: 0.006047990289516747
        model: {}
        policy_loss: -0.015535812475718558
        total_loss: 14.120278358459473
        vf_explained_var: 0.9748682379722595
        vf_loss: 14.134687900543213
    num_steps_sampled: 1638144
    num_steps_trained: 1638144
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.160000000000004
    gpu_util_percent0: 0.3973333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12315610587568344
    mean_env_wait_ms: 0.6593429320218321
    mean_inference_ms: 4.693429433131529
    mean_raw_obs_processing_ms: 0.2684223897882011
  time_since_restore: 345.326283454895
  time_this_iter_s: 12.636023044586182
  time_total_s: 345.326283454895
  timers:
    learn_throughput: 7589.995
    learn_time_ms: 7993.681
    sample_throughput: 13232.661
    sample_time_ms: 4585.019
    update_time_ms: 33.304
  timestamp: 1602432638
  timesteps_since_restore: 0
  timesteps_total: 1638144
  training_iteration: 27
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     27 |          345.326 | 1638144 |  232.529 |              281.323 |              118.293 |            826.188 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3516.673720472441
    time_step_min: 3199
  date: 2020-10-11_16-10-51
  done: false
  episode_len_mean: 825.3661417322835
  episode_reward_max: 281.3232323232324
  episode_reward_mean: 233.19085043346854
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 68
  episodes_total: 2032
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8244215697050095
        entropy_coeff: 0.0001
        kl: 0.005533277872018516
        model: {}
        policy_loss: -0.01628575964423362
        total_loss: 14.40134334564209
        vf_explained_var: 0.974392294883728
        vf_loss: 14.41660475730896
    num_steps_sampled: 1698816
    num_steps_trained: 1698816
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.400000000000006
    gpu_util_percent0: 0.286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12302757695675907
    mean_env_wait_ms: 0.6595993005928896
    mean_inference_ms: 4.685169265257335
    mean_raw_obs_processing_ms: 0.268104203669914
  time_since_restore: 357.9906647205353
  time_this_iter_s: 12.664381265640259
  time_total_s: 357.9906647205353
  timers:
    learn_throughput: 7589.693
    learn_time_ms: 7993.999
    sample_throughput: 13241.299
    sample_time_ms: 4582.028
    update_time_ms: 31.503
  timestamp: 1602432651
  timesteps_since_restore: 0
  timesteps_total: 1698816
  training_iteration: 28
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     28 |          357.991 | 1698816 |  233.191 |              281.323 |              118.293 |            825.366 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3512.8546927108146
    time_step_min: 3111
  date: 2020-10-11_16-11-04
  done: false
  episode_len_mean: 824.6441162458314
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 233.7694910034119
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 67
  episodes_total: 2099
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8053620457649231
        entropy_coeff: 0.0001
        kl: 0.004583484609611332
        model: {}
        policy_loss: -0.011118872789666057
        total_loss: 10.728185892105103
        vf_explained_var: 0.979694128036499
        vf_loss: 10.738468647003174
    num_steps_sampled: 1759488
    num_steps_trained: 1759488
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.413333333333334
    gpu_util_percent0: 0.374
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12289704902758945
    mean_env_wait_ms: 0.6597944890531547
    mean_inference_ms: 4.677136872350052
    mean_raw_obs_processing_ms: 0.26778808417806077
  time_since_restore: 370.6478157043457
  time_this_iter_s: 12.657150983810425
  time_total_s: 370.6478157043457
  timers:
    learn_throughput: 7589.4
    learn_time_ms: 7994.308
    sample_throughput: 13196.913
    sample_time_ms: 4597.439
    update_time_ms: 31.097
  timestamp: 1602432664
  timesteps_since_restore: 0
  timesteps_total: 1759488
  training_iteration: 29
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     29 |          370.648 | 1759488 |  233.769 |              294.657 |              118.293 |            824.644 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3509.5301982480405
    time_step_min: 3111
  date: 2020-10-11_16-11-16
  done: false
  episode_len_mean: 823.8326417704011
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 234.2732022856504
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 70
  episodes_total: 2169
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.8010383099317551
        entropy_coeff: 0.0001
        kl: 0.005840748781338334
        model: {}
        policy_loss: -0.00798800599295646
        total_loss: 11.50804591178894
        vf_explained_var: 0.9788138270378113
        vf_loss: 11.515530347824097
    num_steps_sampled: 1820160
    num_steps_trained: 1820160
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.800000000000004
    gpu_util_percent0: 0.378
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12277065660850758
    mean_env_wait_ms: 0.6600029964591032
    mean_inference_ms: 4.669321099994522
    mean_raw_obs_processing_ms: 0.2674844545880775
  time_since_restore: 383.0449287891388
  time_this_iter_s: 12.39711308479309
  time_total_s: 383.0449287891388
  timers:
    learn_throughput: 7614.353
    learn_time_ms: 7968.11
    sample_throughput: 13239.191
    sample_time_ms: 4582.757
    update_time_ms: 30.229
  timestamp: 1602432676
  timesteps_since_restore: 0
  timesteps_total: 1820160
  training_iteration: 30
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     30 |          383.045 | 1820160 |  234.273 |              294.657 |              118.293 |            823.833 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3506.5406976744184
    time_step_min: 3111
  date: 2020-10-11_16-11-29
  done: false
  episode_len_mean: 823.0178890876565
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 234.7261569180174
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 67
  episodes_total: 2236
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7931597828865051
        entropy_coeff: 0.0001
        kl: 0.00602615880779922
        model: {}
        policy_loss: -0.007323025201912969
        total_loss: 12.5483980178833
        vf_explained_var: 0.9768276810646057
        vf_loss: 12.555197715759277
    num_steps_sampled: 1880832
    num_steps_trained: 1880832
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.75625
    gpu_util_percent0: 0.323125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12265725095336075
    mean_env_wait_ms: 0.6602202213072201
    mean_inference_ms: 4.662258018301666
    mean_raw_obs_processing_ms: 0.2672126209689783
  time_since_restore: 395.6041238307953
  time_this_iter_s: 12.559195041656494
  time_total_s: 395.6041238307953
  timers:
    learn_throughput: 7614.934
    learn_time_ms: 7967.502
    sample_throughput: 13261.629
    sample_time_ms: 4575.004
    update_time_ms: 32.018
  timestamp: 1602432689
  timesteps_since_restore: 0
  timesteps_total: 1880832
  training_iteration: 31
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     31 |          395.604 | 1880832 |  234.726 |              294.657 |              118.293 |            823.018 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3503.4176062445795
    time_step_min: 3111
  date: 2020-10-11_16-11-42
  done: false
  episode_len_mean: 822.153512575889
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 235.19935258920518
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 70
  episodes_total: 2306
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7887319028377533
        entropy_coeff: 0.0001
        kl: 0.006466412800364196
        model: {}
        policy_loss: -0.011869820766150951
        total_loss: 10.237318277359009
        vf_explained_var: 0.9808382987976074
        vf_loss: 10.248620510101318
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.06666666666667
    gpu_util_percent0: 0.39866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666667
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12254758625828596
    mean_env_wait_ms: 0.6604370618429498
    mean_inference_ms: 4.655120295370935
    mean_raw_obs_processing_ms: 0.26694040957559567
  time_since_restore: 408.1169400215149
  time_this_iter_s: 12.512816190719604
  time_total_s: 408.1169400215149
  timers:
    learn_throughput: 7621.861
    learn_time_ms: 7960.261
    sample_throughput: 13301.639
    sample_time_ms: 4561.243
    update_time_ms: 31.613
  timestamp: 1602432702
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 32
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     32 |          408.117 | 1941504 |  235.199 |              294.657 |              118.293 |            822.154 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3499.610012620951
    time_step_min: 3111
  date: 2020-10-11_16-11-54
  done: false
  episode_len_mean: 821.4859066049643
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 235.77626071399737
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 71
  episodes_total: 2377
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7735395282506943
        entropy_coeff: 0.0001
        kl: 0.006109715439379215
        model: {}
        policy_loss: -0.013684868696145713
        total_loss: 9.02634072303772
        vf_explained_var: 0.9823529720306396
        vf_loss: 9.039491653442383
    num_steps_sampled: 2002176
    num_steps_trained: 2002176
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.22
    gpu_util_percent0: 0.3446666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4199999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12244578065618276
    mean_env_wait_ms: 0.6606534822204309
    mean_inference_ms: 4.648254413795414
    mean_raw_obs_processing_ms: 0.26667231021273746
  time_since_restore: 420.65903425216675
  time_this_iter_s: 12.542094230651855
  time_total_s: 420.65903425216675
  timers:
    learn_throughput: 7617.632
    learn_time_ms: 7964.679
    sample_throughput: 13314.046
    sample_time_ms: 4556.992
    update_time_ms: 31.635
  timestamp: 1602432714
  timesteps_since_restore: 0
  timesteps_total: 2002176
  training_iteration: 33
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     33 |          420.659 | 2002176 |  235.776 |              294.657 |              118.293 |            821.486 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3495.8756117455137
    time_step_min: 3111
  date: 2020-10-11_16-12-07
  done: false
  episode_len_mean: 820.7389885807504
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 236.34207902845748
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 75
  episodes_total: 2452
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7676493227481842
        entropy_coeff: 0.0001
        kl: 0.006269674748182297
        model: {}
        policy_loss: -0.01027127931592986
        total_loss: 10.94404411315918
        vf_explained_var: 0.9788558483123779
        vf_loss: 10.953765153884888
    num_steps_sampled: 2062848
    num_steps_trained: 2062848
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.666666666666664
    gpu_util_percent0: 0.42000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12233866219496542
    mean_env_wait_ms: 0.6608791809583239
    mean_inference_ms: 4.6413449326072955
    mean_raw_obs_processing_ms: 0.2663978147269404
  time_since_restore: 433.28397512435913
  time_this_iter_s: 12.624940872192383
  time_total_s: 433.28397512435913
  timers:
    learn_throughput: 7615.72
    learn_time_ms: 7966.68
    sample_throughput: 13344.134
    sample_time_ms: 4546.717
    update_time_ms: 31.655
  timestamp: 1602432727
  timesteps_since_restore: 0
  timesteps_total: 2062848
  training_iteration: 34
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     34 |          433.284 | 2062848 |  236.342 |              294.657 |              118.293 |            820.739 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3492.986956521739
    time_step_min: 3111
  date: 2020-10-11_16-12-20
  done: false
  episode_len_mean: 819.8332015810277
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 236.77975406236274
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 78
  episodes_total: 2530
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7546486258506775
        entropy_coeff: 0.0001
        kl: 0.0058147438103333116
        model: {}
        policy_loss: -0.012304193340241909
        total_loss: 10.538083553314209
        vf_explained_var: 0.980566680431366
        vf_loss: 10.549881935119629
    num_steps_sampled: 2123520
    num_steps_trained: 2123520
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.881249999999998
    gpu_util_percent0: 0.38749999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12222794658999028
    mean_env_wait_ms: 0.6610959002225931
    mean_inference_ms: 4.634419702177536
    mean_raw_obs_processing_ms: 0.26611683838637945
  time_since_restore: 445.942663192749
  time_this_iter_s: 12.658688068389893
  time_total_s: 445.942663192749
  timers:
    learn_throughput: 7606.23
    learn_time_ms: 7976.619
    sample_throughput: 13359.952
    sample_time_ms: 4541.334
    update_time_ms: 37.681
  timestamp: 1602432740
  timesteps_since_restore: 0
  timesteps_total: 2123520
  training_iteration: 35
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     35 |          445.943 | 2123520 |   236.78 |              294.657 |              118.293 |            819.833 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3488.9290916059795
    time_step_min: 3111
  date: 2020-10-11_16-12-32
  done: false
  episode_len_mean: 818.9126101954772
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 237.39458207990214
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 2609
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7400196939706802
        entropy_coeff: 0.0001
        kl: 0.0058068325743079185
        model: {}
        policy_loss: -0.013815922429785132
        total_loss: 10.05512261390686
        vf_explained_var: 0.9806376099586487
        vf_loss: 10.068431377410889
    num_steps_sampled: 2184192
    num_steps_trained: 2184192
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.053333333333335
    gpu_util_percent0: 0.3686666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666667
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12212225178823108
    mean_env_wait_ms: 0.6613143142351993
    mean_inference_ms: 4.627720603822091
    mean_raw_obs_processing_ms: 0.265844670572868
  time_since_restore: 458.47173833847046
  time_this_iter_s: 12.529075145721436
  time_total_s: 458.47173833847046
  timers:
    learn_throughput: 7600.836
    learn_time_ms: 7982.28
    sample_throughput: 13421.08
    sample_time_ms: 4520.65
    update_time_ms: 31.579
  timestamp: 1602432752
  timesteps_since_restore: 0
  timesteps_total: 2184192
  training_iteration: 36
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     36 |          458.472 | 2184192 |  237.395 |              294.657 |              118.293 |            818.913 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3485.9352678571427
    time_step_min: 3111
  date: 2020-10-11_16-12-45
  done: false
  episode_len_mean: 818.0517113095239
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 237.84819173881675
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 2688
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7301702946424484
        entropy_coeff: 0.0001
        kl: 0.005751452641561627
        model: {}
        policy_loss: -0.013575302669778466
        total_loss: 9.521034479141235
        vf_explained_var: 0.982122540473938
        vf_loss: 9.534107208251953
    num_steps_sampled: 2244864
    num_steps_trained: 2244864
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.106666666666676
    gpu_util_percent0: 0.334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4466666666666663
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.122021320091186
    mean_env_wait_ms: 0.6615287027514779
    mean_inference_ms: 4.621330885247032
    mean_raw_obs_processing_ms: 0.2655831882729574
  time_since_restore: 471.03045630455017
  time_this_iter_s: 12.558717966079712
  time_total_s: 471.03045630455017
  timers:
    learn_throughput: 7604.935
    learn_time_ms: 7977.978
    sample_throughput: 13433.247
    sample_time_ms: 4516.555
    update_time_ms: 31.29
  timestamp: 1602432765
  timesteps_since_restore: 0
  timesteps_total: 2244864
  training_iteration: 37
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     37 |           471.03 | 2244864 |  237.848 |              294.657 |              118.293 |            818.052 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3482.712942877802
    time_step_min: 3111
  date: 2020-10-11_16-12-58
  done: false
  episode_len_mean: 817.2541576283442
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 238.33642279629268
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 78
  episodes_total: 2766
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7123904228210449
        entropy_coeff: 0.0001
        kl: 0.006374098709784448
        model: {}
        policy_loss: -0.013502237037755549
        total_loss: 11.306285858154297
        vf_explained_var: 0.9792220592498779
        vf_loss: 11.319222211837769
    num_steps_sampled: 2305536
    num_steps_trained: 2305536
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.31333333333334
    gpu_util_percent0: 0.37133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.433333333333333
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12192696639683571
    mean_env_wait_ms: 0.6617340285726273
    mean_inference_ms: 4.615272777054371
    mean_raw_obs_processing_ms: 0.26533222492254516
  time_since_restore: 483.5480728149414
  time_this_iter_s: 12.517616510391235
  time_total_s: 483.5480728149414
  timers:
    learn_throughput: 7616.943
    learn_time_ms: 7965.4
    sample_throughput: 13440.282
    sample_time_ms: 4514.191
    update_time_ms: 31.372
  timestamp: 1602432778
  timesteps_since_restore: 0
  timesteps_total: 2305536
  training_iteration: 38
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     38 |          483.548 | 2305536 |  238.336 |              294.657 |              118.293 |            817.254 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3479.0738137082603
    time_step_min: 3111
  date: 2020-10-11_16-13-10
  done: false
  episode_len_mean: 816.4987697715289
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 238.887806003799
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 2845
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7063906639814377
        entropy_coeff: 0.0001
        kl: 0.006619708146899939
        model: {}
        policy_loss: -0.008540161070413888
        total_loss: 10.110114336013794
        vf_explained_var: 0.9805809855461121
        vf_loss: 10.118062973022461
    num_steps_sampled: 2366208
    num_steps_trained: 2366208
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.29375
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.41875
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12183449138565841
    mean_env_wait_ms: 0.6619379598742459
    mean_inference_ms: 4.609421720491569
    mean_raw_obs_processing_ms: 0.26508944925438577
  time_since_restore: 496.10764169692993
  time_this_iter_s: 12.559568881988525
  time_total_s: 496.10764169692993
  timers:
    learn_throughput: 7616.998
    learn_time_ms: 7965.343
    sample_throughput: 13468.468
    sample_time_ms: 4504.744
    update_time_ms: 31.81
  timestamp: 1602432790
  timesteps_since_restore: 0
  timesteps_total: 2366208
  training_iteration: 39
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     39 |          496.108 | 2366208 |  238.888 |              294.657 |              118.293 |            816.499 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3476.669859733151
    time_step_min: 3111
  date: 2020-10-11_16-13-23
  done: false
  episode_len_mean: 815.7071501881628
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 239.25204145457312
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 78
  episodes_total: 2923
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6949535459280014
        entropy_coeff: 0.0001
        kl: 0.005586441489867866
        model: {}
        policy_loss: -0.009916623908793554
        total_loss: 12.35196328163147
        vf_explained_var: 0.9776116609573364
        vf_loss: 12.361390352249146
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.82
    gpu_util_percent0: 0.35133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4466666666666668
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1217463072325626
    mean_env_wait_ms: 0.662131895610092
    mean_inference_ms: 4.6038412855908755
    mean_raw_obs_processing_ms: 0.2648563846585165
  time_since_restore: 508.76882886886597
  time_this_iter_s: 12.661187171936035
  time_total_s: 508.76882886886597
  timers:
    learn_throughput: 7603.181
    learn_time_ms: 7979.818
    sample_throughput: 13433.358
    sample_time_ms: 4516.518
    update_time_ms: 31.19
  timestamp: 1602432803
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 40
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     40 |          508.769 | 2426880 |  239.252 |              294.657 |              118.293 |            815.707 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3473.2671552298466
    time_step_min: 3111
  date: 2020-10-11_16-13-36
  done: false
  episode_len_mean: 814.9037308461026
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 239.76760274295256
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3002
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6896779984235764
        entropy_coeff: 0.0001
        kl: 0.00545170099940151
        model: {}
        policy_loss: -0.006760447518900037
        total_loss: 10.799469709396362
        vf_explained_var: 0.9795577526092529
        vf_loss: 10.805754661560059
    num_steps_sampled: 2487552
    num_steps_trained: 2487552
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.67333333333333
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12166107279338173
    mean_env_wait_ms: 0.6623265457389281
    mean_inference_ms: 4.598450888389196
    mean_raw_obs_processing_ms: 0.2646293661196949
  time_since_restore: 521.2429101467133
  time_this_iter_s: 12.47408127784729
  time_total_s: 521.2429101467133
  timers:
    learn_throughput: 7607.721
    learn_time_ms: 7975.056
    sample_throughput: 13441.993
    sample_time_ms: 4513.616
    update_time_ms: 30.063
  timestamp: 1602432816
  timesteps_since_restore: 0
  timesteps_total: 2487552
  training_iteration: 41
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     41 |          521.243 | 2487552 |  239.768 |              294.657 |              118.293 |            814.904 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3469.998052580331
    time_step_min: 3111
  date: 2020-10-11_16-13-48
  done: false
  episode_len_mean: 814.205452775073
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 240.26292132621253
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3081
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6865971684455872
        entropy_coeff: 0.0001
        kl: 0.005908183171413839
        model: {}
        policy_loss: -0.013723642565310001
        total_loss: 10.921100378036499
        vf_explained_var: 0.9789117574691772
        vf_loss: 10.934301376342773
    num_steps_sampled: 2548224
    num_steps_trained: 2548224
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.446666666666665
    gpu_util_percent0: 0.32599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12157909965570467
    mean_env_wait_ms: 0.6625155357831264
    mean_inference_ms: 4.593272441734872
    mean_raw_obs_processing_ms: 0.2644102576167796
  time_since_restore: 533.6973576545715
  time_this_iter_s: 12.454447507858276
  time_total_s: 533.6973576545715
  timers:
    learn_throughput: 7607.583
    learn_time_ms: 7975.201
    sample_throughput: 13462.367
    sample_time_ms: 4506.786
    update_time_ms: 31.204
  timestamp: 1602432828
  timesteps_since_restore: 0
  timesteps_total: 2548224
  training_iteration: 42
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     42 |          533.697 | 2548224 |  240.263 |              294.657 |              118.293 |            814.205 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3466.8977848101267
    time_step_min: 3111
  date: 2020-10-11_16-14-01
  done: false
  episode_len_mean: 813.7351265822784
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 240.73265886715257
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3160
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.673320397734642
        entropy_coeff: 0.0001
        kl: 0.005441875662654638
        model: {}
        policy_loss: -0.01596468430943787
        total_loss: 9.379735231399536
        vf_explained_var: 0.9821313619613647
        vf_loss: 9.395222902297974
    num_steps_sampled: 2608896
    num_steps_trained: 2608896
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.62666666666667
    gpu_util_percent0: 0.33133333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12150030433775505
    mean_env_wait_ms: 0.6626975133129722
    mean_inference_ms: 4.588298094915638
    mean_raw_obs_processing_ms: 0.26419929842819045
  time_since_restore: 546.1801235675812
  time_this_iter_s: 12.482765913009644
  time_total_s: 546.1801235675812
  timers:
    learn_throughput: 7620.357
    learn_time_ms: 7961.832
    sample_throughput: 13440.021
    sample_time_ms: 4514.279
    update_time_ms: 31.552
  timestamp: 1602432841
  timesteps_since_restore: 0
  timesteps_total: 2608896
  training_iteration: 43
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     43 |           546.18 | 2608896 |  240.733 |              294.657 |              118.293 |            813.735 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3464.2837295461563
    time_step_min: 3111
  date: 2020-10-11_16-14-13
  done: false
  episode_len_mean: 813.19851806113
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 241.128727846542
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3239
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6631857454776764
        entropy_coeff: 0.0001
        kl: 0.005600042990408838
        model: {}
        policy_loss: -0.013800082611851394
        total_loss: 8.741819620132446
        vf_explained_var: 0.983812153339386
        vf_loss: 8.755126237869263
    num_steps_sampled: 2669568
    num_steps_trained: 2669568
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.766666666666666
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12142464407322914
    mean_env_wait_ms: 0.6628767611562613
    mean_inference_ms: 4.583519697915844
    mean_raw_obs_processing_ms: 0.2639971100990282
  time_since_restore: 558.4696769714355
  time_this_iter_s: 12.28955340385437
  time_total_s: 558.4696769714355
  timers:
    learn_throughput: 7656.433
    learn_time_ms: 7924.317
    sample_throughput: 13425.228
    sample_time_ms: 4519.253
    update_time_ms: 29.502
  timestamp: 1602432853
  timesteps_since_restore: 0
  timesteps_total: 2669568
  training_iteration: 44
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     44 |           558.47 | 2669568 |  241.129 |              294.657 |              118.293 |            813.199 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3461.2525617842075
    time_step_min: 3111
  date: 2020-10-11_16-14-26
  done: false
  episode_len_mean: 812.5304400241109
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 241.58799568926156
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3318
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.6549868285655975
        entropy_coeff: 0.0001
        kl: 0.004678940866142511
        model: {}
        policy_loss: -0.012409038899932057
        total_loss: 8.848819255828857
        vf_explained_var: 0.9826209545135498
        vf_loss: 8.860825538635254
    num_steps_sampled: 2730240
    num_steps_trained: 2730240
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.51333333333334
    gpu_util_percent0: 0.27399999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4199999999999995
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12135184796651013
    mean_env_wait_ms: 0.6630532897122141
    mean_inference_ms: 4.578917370745875
    mean_raw_obs_processing_ms: 0.26380273697363726
  time_since_restore: 570.9770197868347
  time_this_iter_s: 12.50734281539917
  time_total_s: 570.9770197868347
  timers:
    learn_throughput: 7659.718
    learn_time_ms: 7920.918
    sample_throughput: 13438.696
    sample_time_ms: 4514.724
    update_time_ms: 22.245
  timestamp: 1602432866
  timesteps_since_restore: 0
  timesteps_total: 2730240
  training_iteration: 45
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     45 |          570.977 | 2730240 |  241.588 |              294.657 |              118.293 |             812.53 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3458.4942596408596
    time_step_min: 3111
  date: 2020-10-11_16-14-39
  done: false
  episode_len_mean: 811.9428907859876
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 242.00592025643545
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3397
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05
        cur_lr: 5.0e-05
        entropy: 0.6498712301254272
        entropy_coeff: 0.0001
        kl: 0.006043577450327575
        model: {}
        policy_loss: -0.013496566331014037
        total_loss: 10.97796106338501
        vf_explained_var: 0.9792823791503906
        vf_loss: 10.991219997406006
    num_steps_sampled: 2790912
    num_steps_trained: 2790912
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.27333333333333
    gpu_util_percent0: 0.27466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.46
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12128185002227779
    mean_env_wait_ms: 0.663225592862293
    mean_inference_ms: 4.574485566206917
    mean_raw_obs_processing_ms: 0.2636157158558468
  time_since_restore: 583.6050810813904
  time_this_iter_s: 12.628061294555664
  time_total_s: 583.6050810813904
  timers:
    learn_throughput: 7649.665
    learn_time_ms: 7931.328
    sample_throughput: 13441.107
    sample_time_ms: 4513.914
    update_time_ms: 22.368
  timestamp: 1602432879
  timesteps_since_restore: 0
  timesteps_total: 2790912
  training_iteration: 46
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     46 |          583.605 | 2790912 |  242.006 |              294.657 |              118.293 |            811.943 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3455.911104718067
    time_step_min: 3111
  date: 2020-10-11_16-14-51
  done: false
  episode_len_mean: 811.5350978135788
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 242.39730736594953
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3476
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05
        cur_lr: 5.0e-05
        entropy: 0.6342495381832123
        entropy_coeff: 0.0001
        kl: 0.006048538372851908
        model: {}
        policy_loss: -0.010996688564773649
        total_loss: 8.571913242340088
        vf_explained_var: 0.9842838048934937
        vf_loss: 8.582670211791992
    num_steps_sampled: 2851584
    num_steps_trained: 2851584
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.9625
    gpu_util_percent0: 0.34750000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.11634962282715644
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12121440710340929
    mean_env_wait_ms: 0.6633916334880035
    mean_inference_ms: 4.570215206532649
    mean_raw_obs_processing_ms: 0.26343558812156864
  time_since_restore: 596.10276222229
  time_this_iter_s: 12.497681140899658
  time_total_s: 596.10276222229
  timers:
    learn_throughput: 7664.847
    learn_time_ms: 7915.618
    sample_throughput: 13433.163
    sample_time_ms: 4516.583
    update_time_ms: 22.72
  timestamp: 1602432891
  timesteps_since_restore: 0
  timesteps_total: 2851584
  training_iteration: 47
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | RUNNING  | 172.17.0.4:58131 |     47 |          596.103 | 2851584 |  242.397 |              294.657 |              118.293 |            811.535 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_7a7ff_00000:
  custom_metrics:
    time_step_max: 4275
    time_step_mean: 3453.0551336146273
    time_step_min: 3111
  date: 2020-10-11_16-15-04
  done: true
  episode_len_mean: 811.0132208157524
  episode_reward_max: 294.6565656565658
  episode_reward_mean: 242.83003026041007
  episode_reward_min: 118.29292929292929
  episodes_this_iter: 79
  episodes_total: 3555
  experiment_id: 1d171a18a5d54a9daf4bf72942cfbe74
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05
        cur_lr: 5.0e-05
        entropy: 0.6302320063114166
        entropy_coeff: 0.0001
        kl: 0.005789852002635598
        model: {}
        policy_loss: -0.011886644060723484
        total_loss: 9.188382387161255
        vf_explained_var: 0.9817778468132019
        vf_loss: 9.200042247772217
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.833333333333336
    gpu_util_percent0: 0.38733333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666667
    vram_util_percent0: 0.1163496228271565
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58131
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12114967799853825
    mean_env_wait_ms: 0.6635533104163663
    mean_inference_ms: 4.566091176760414
    mean_raw_obs_processing_ms: 0.2632612772401684
  time_since_restore: 608.6148672103882
  time_this_iter_s: 12.512104988098145
  time_total_s: 608.6148672103882
  timers:
    learn_throughput: 7660.877
    learn_time_ms: 7919.719
    sample_throughput: 13450.41
    sample_time_ms: 4510.792
    update_time_ms: 23.015
  timestamp: 1602432904
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 48
  trial_id: 7a7ff_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | TERMINATED |       |     48 |          608.615 | 2912256 |   242.83 |              294.657 |              118.293 |            811.013 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_7a7ff_00000 | TERMINATED |       |     48 |          608.615 | 2912256 |   242.83 |              294.657 |              118.293 |            811.013 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Traceback (most recent call last):
  File "train.py", line 72, in <module>
    train_func()
  File "train.py", line 57, in train_func
    result = analysis.dataframe().to_dict('index')[0]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 89, in dataframe
    metric = self._validate_metric(metric)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 64, in _validate_metric
    raise ValueError(
ValueError: No `metric` has been passed and  `default_metric` has not been set. Please specify the `metric` parameter.
