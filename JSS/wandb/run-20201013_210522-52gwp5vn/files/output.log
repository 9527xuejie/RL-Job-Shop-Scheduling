2020-10-13 21:05:26,281	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_d1c11_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=58269)[0m 2020-10-13 21:05:28,989	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=58258)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58258)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58274)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58274)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58158)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58158)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58216)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58216)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58226)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58226)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58265)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58265)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58257)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58257)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58270)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58270)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58221)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58221)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58241)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58241)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58267)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58267)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58209)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58209)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58239)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58239)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58214)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58214)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58283)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58283)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58236)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58236)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58174)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58174)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58227)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58227)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58217)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58217)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58251)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58251)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58254)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58254)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58150)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58150)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58167)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58167)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58271)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58271)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58151)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58151)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58222)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58222)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58169)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58169)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58160)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58160)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58235)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58235)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58229)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58229)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58171)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58171)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58170)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58170)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58234)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58234)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58281)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58281)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58153)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58153)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58173)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58173)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58149)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58149)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58177)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58177)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58154)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58154)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58181)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58181)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58184)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58184)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58148)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58148)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58228)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58228)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58250)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58250)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58252)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58252)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58168)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58168)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58159)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58159)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58260)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58260)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58256)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58256)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58237)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58237)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58255)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58255)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58223)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58223)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58238)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58238)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58164)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58164)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58172)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58172)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58165)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58165)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58240)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58240)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58243)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58243)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58210)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58210)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58275)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58275)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58224)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58224)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58179)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58179)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58156)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58156)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58218)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58218)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58155)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58155)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58166)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58166)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58263)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58263)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58152)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58152)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58249)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58249)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58161)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58161)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58162)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58162)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58246)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58246)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58176)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58176)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58220)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58220)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58278)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58278)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58248)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58248)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58219)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58219)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58262)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58262)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=58215)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=58215)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4248
    time_step_mean: 3616.3166666666666
    time_step_min: 3355
  date: 2020-10-13_21-06-04
  done: false
  episode_len_mean: 904.8481012658228
  episode_reward_max: 246.595959595959
  episode_reward_mean: 201.8721391126452
  episode_reward_min: 106.74747474747424
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.169209361076355
        entropy_coeff: 0.0005000000000000001
        kl: 0.004283593191454808
        model: {}
        policy_loss: -0.00855300908733625
        total_loss: 369.24242401123047
        vf_explained_var: 0.588043749332428
        vf_loss: 369.2507044474284
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.22941176470588
    gpu_util_percent0: 0.3011764705882353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5058823529411764
    vram_util_percent0: 0.08473366388208285
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1739851535562749
    mean_env_wait_ms: 1.1973487539547538
    mean_inference_ms: 6.453237368685847
    mean_raw_obs_processing_ms: 0.4745959823386984
  time_since_restore: 29.549667596817017
  time_this_iter_s: 29.549667596817017
  time_total_s: 29.549667596817017
  timers:
    learn_throughput: 8372.124
    learn_time_ms: 19325.083
    sample_throughput: 15936.238
    sample_time_ms: 10152.459
    update_time_ms: 38.552
  timestamp: 1602623164
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 27.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      1 |          29.5497 | 161792 |  201.872 |              246.596 |              106.747 |            904.848 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4331
    time_step_mean: 3626.0323741007196
    time_step_min: 3254
  date: 2020-10-13_21-06-31
  done: false
  episode_len_mean: 904.6550632911392
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 199.26425648893976
  episode_reward_min: 94.17171717171675
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1384726762771606
        entropy_coeff: 0.0005000000000000001
        kl: 0.007555847250235577
        model: {}
        policy_loss: -0.009366699125772962
        total_loss: 89.9572073618571
        vf_explained_var: 0.8351356983184814
        vf_loss: 89.96638933817546
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.345161290322586
    gpu_util_percent0: 0.3332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7516129032258054
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16795592045294067
    mean_env_wait_ms: 1.1913259857781597
    mean_inference_ms: 6.005597552818194
    mean_raw_obs_processing_ms: 0.45478255856200595
  time_since_restore: 56.53344535827637
  time_this_iter_s: 26.98377776145935
  time_total_s: 56.53344535827637
  timers:
    learn_throughput: 8342.004
    learn_time_ms: 19394.861
    sample_throughput: 18396.351
    sample_time_ms: 8794.788
    update_time_ms: 37.211
  timestamp: 1602623191
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      2 |          56.5334 | 323584 |  199.264 |              257.354 |              94.1717 |            904.655 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4331
    time_step_mean: 3633.0733944954127
    time_step_min: 3254
  date: 2020-10-13_21-06-57
  done: false
  episode_len_mean: 899.1413502109705
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 199.64480245492877
  episode_reward_min: 94.17171717171675
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.123505175113678
        entropy_coeff: 0.0005000000000000001
        kl: 0.00928467131840686
        model: {}
        policy_loss: -0.012755329177404443
        total_loss: 44.79078483581543
        vf_explained_var: 0.9124049544334412
        vf_loss: 44.803174336751304
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.299999999999997
    gpu_util_percent0: 0.34935483870967743
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16430360223299678
    mean_env_wait_ms: 1.1892663568868795
    mean_inference_ms: 5.717225271485789
    mean_raw_obs_processing_ms: 0.44230548282639787
  time_since_restore: 83.2349157333374
  time_this_iter_s: 26.701470375061035
  time_total_s: 83.2349157333374
  timers:
    learn_throughput: 8320.46
    learn_time_ms: 19445.078
    sample_throughput: 19687.013
    sample_time_ms: 8218.21
    update_time_ms: 38.757
  timestamp: 1602623217
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      3 |          83.2349 | 485376 |  199.645 |              257.354 |              94.1717 |            899.141 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3626.841750841751
    time_step_min: 3254
  date: 2020-10-13_21-07-24
  done: false
  episode_len_mean: 894.5094936708861
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 200.8474939266076
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1135420103867848
        entropy_coeff: 0.0005000000000000001
        kl: 0.008282240596599877
        model: {}
        policy_loss: -0.011898660280470116
        total_loss: 39.95386600494385
        vf_explained_var: 0.9208271503448486
        vf_loss: 39.96549320220947
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.016666666666666
    gpu_util_percent0: 0.385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16179689232915279
    mean_env_wait_ms: 1.1887268789159595
    mean_inference_ms: 5.518716450786536
    mean_raw_obs_processing_ms: 0.4334874849184497
  time_since_restore: 109.47253799438477
  time_this_iter_s: 26.237622261047363
  time_total_s: 109.47253799438477
  timers:
    learn_throughput: 8342.512
    learn_time_ms: 19393.679
    sample_throughput: 20490.6
    sample_time_ms: 7895.913
    update_time_ms: 34.758
  timestamp: 1602623244
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      4 |          109.473 | 647168 |  200.847 |              257.354 |              74.7778 |            894.509 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3616.928191489362
    time_step_min: 3254
  date: 2020-10-13_21-07-50
  done: false
  episode_len_mean: 890.3
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 202.15541490857922
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0863073368867238
        entropy_coeff: 0.0005000000000000001
        kl: 0.00802083076753964
        model: {}
        policy_loss: -0.010518901399336755
        total_loss: 31.891902287801106
        vf_explained_var: 0.937171459197998
        vf_loss: 31.902161121368408
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000002
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1599941552270115
    mean_env_wait_ms: 1.1889703974524124
    mean_inference_ms: 5.373477869542964
    mean_raw_obs_processing_ms: 0.4268127874720605
  time_since_restore: 135.75310564041138
  time_this_iter_s: 26.28056764602661
  time_total_s: 135.75310564041138
  timers:
    learn_throughput: 8346.554
    learn_time_ms: 19384.287
    sample_throughput: 21043.437
    sample_time_ms: 7688.478
    update_time_ms: 34.191
  timestamp: 1602623270
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      5 |          135.753 | 808960 |  202.155 |              257.354 |              74.7778 |              890.3 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3608.4322033898306
    time_step_min: 3244
  date: 2020-10-13_21-08-16
  done: false
  episode_len_mean: 885.3126272912424
  episode_reward_max: 258.8686868686869
  episode_reward_mean: 203.83381678290004
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 982
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0417972803115845
        entropy_coeff: 0.0005000000000000001
        kl: 0.00803802243899554
        model: {}
        policy_loss: -0.011555042661105594
        total_loss: 29.145253658294678
        vf_explained_var: 0.9571675658226013
        vf_loss: 29.156526406606037
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.893333333333334
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1583639072718057
    mean_env_wait_ms: 1.1900051717428988
    mean_inference_ms: 5.242548373422128
    mean_raw_obs_processing_ms: 0.4208657364226883
  time_since_restore: 162.04360556602478
  time_this_iter_s: 26.290499925613403
  time_total_s: 162.04360556602478
  timers:
    learn_throughput: 8349.46
    learn_time_ms: 19377.541
    sample_throughput: 21421.592
    sample_time_ms: 7552.753
    update_time_ms: 31.913
  timestamp: 1602623296
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 27.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      6 |          162.044 | 970752 |  203.834 |              258.869 |              74.7778 |            885.313 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3593.012234910277
    time_step_min: 3212
  date: 2020-10-13_21-08-42
  done: false
  episode_len_mean: 879.5348101265823
  episode_reward_max: 263.7171717171712
  episode_reward_mean: 206.26859576780438
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 282
  episodes_total: 1264
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.052981048822403
        entropy_coeff: 0.0005000000000000001
        kl: 0.00756885576993227
        model: {}
        policy_loss: -0.013547717448091134
        total_loss: 24.316144307454426
        vf_explained_var: 0.9633116722106934
        vf_loss: 24.329460938771565
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90333333333333
    gpu_util_percent0: 0.38866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15670378302534818
    mean_env_wait_ms: 1.1911620488271384
    mean_inference_ms: 5.107395460926295
    mean_raw_obs_processing_ms: 0.4148297081200403
  time_since_restore: 188.13804817199707
  time_this_iter_s: 26.09444260597229
  time_total_s: 188.13804817199707
  timers:
    learn_throughput: 8361.351
    learn_time_ms: 19349.983
    sample_throughput: 21714.17
    sample_time_ms: 7450.987
    update_time_ms: 30.093
  timestamp: 1602623322
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      7 |          188.138 | 1132544 |  206.269 |              263.717 |              74.7778 |            879.535 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3584.440751445087
    time_step_min: 3212
  date: 2020-10-13_21-09-09
  done: false
  episode_len_mean: 875.0407876230661
  episode_reward_max: 263.7171717171712
  episode_reward_mean: 207.88383838383817
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.025655210018158
        entropy_coeff: 0.0005000000000000001
        kl: 0.007761195845281084
        model: {}
        policy_loss: -0.012660732085350901
        total_loss: 18.453331629435223
        vf_explained_var: 0.9655329585075378
        vf_loss: 18.465729077657063
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.910000000000004
    gpu_util_percent0: 0.31566666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.156002840708151
    mean_env_wait_ms: 1.191851013943105
    mean_inference_ms: 5.049841367861012
    mean_raw_obs_processing_ms: 0.412250488513392
  time_since_restore: 214.4148781299591
  time_this_iter_s: 26.276829957962036
  time_total_s: 214.4148781299591
  timers:
    learn_throughput: 8364.091
    learn_time_ms: 19343.644
    sample_throughput: 21914.408
    sample_time_ms: 7382.905
    update_time_ms: 28.775
  timestamp: 1602623349
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      8 |          214.415 | 1294336 |  207.884 |              263.717 |              74.7778 |            875.041 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3574.6420233463036
    time_step_min: 3212
  date: 2020-10-13_21-09-35
  done: false
  episode_len_mean: 870.4873417721519
  episode_reward_max: 267.20202020201987
  episode_reward_mean: 209.55290244214274
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0090048958857853
        entropy_coeff: 0.0005000000000000001
        kl: 0.007344354099283616
        model: {}
        policy_loss: -0.011860693940737596
        total_loss: 18.446773687998455
        vf_explained_var: 0.963460385799408
        vf_loss: 18.458404541015625
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73666666666667
    gpu_util_percent0: 0.30566666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1553958895423542
    mean_env_wait_ms: 1.1926734383849953
    mean_inference_ms: 5.000183032565089
    mean_raw_obs_processing_ms: 0.4099845718718001
  time_since_restore: 240.72117471694946
  time_this_iter_s: 26.306296586990356
  time_total_s: 240.72117471694946
  timers:
    learn_throughput: 8364.546
    learn_time_ms: 19342.593
    sample_throughput: 22074.758
    sample_time_ms: 7329.276
    update_time_ms: 28.113
  timestamp: 1602623375
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |      9 |          240.721 | 1456128 |  209.553 |              267.202 |              74.7778 |            870.487 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3564.078730904818
    time_step_min: 3179
  date: 2020-10-13_21-10-02
  done: false
  episode_len_mean: 866.4586206896552
  episode_reward_max: 268.71717171717165
  episode_reward_mean: 211.2001044932078
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 160
  episodes_total: 1740
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9668994247913361
        entropy_coeff: 0.0005000000000000001
        kl: 0.007379486147935192
        model: {}
        policy_loss: -0.011729328679696968
        total_loss: 17.816092491149902
        vf_explained_var: 0.9671626687049866
        vf_loss: 17.82756741841634
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.720000000000002
    gpu_util_percent0: 0.3823333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15486409556136682
    mean_env_wait_ms: 1.1935912864159117
    mean_inference_ms: 4.956397083789204
    mean_raw_obs_processing_ms: 0.40795607492882585
  time_since_restore: 267.22323393821716
  time_this_iter_s: 26.5020592212677
  time_total_s: 267.22323393821716
  timers:
    learn_throughput: 8366.089
    learn_time_ms: 19339.023
    sample_throughput: 22138.241
    sample_time_ms: 7308.259
    update_time_ms: 27.461
  timestamp: 1602623402
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     10 |          267.223 | 1617920 |    211.2 |              268.717 |              74.7778 |            866.459 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3544.0879284649777
    time_step_min: 3179
  date: 2020-10-13_21-10-28
  done: false
  episode_len_mean: 859.2252559726962
  episode_reward_max: 274.02020202020196
  episode_reward_mean: 214.2406414215286
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 311
  episodes_total: 2051
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9493600130081177
        entropy_coeff: 0.0005000000000000001
        kl: 0.006886738818138838
        model: {}
        policy_loss: -0.010961758438497782
        total_loss: 22.52125628789266
        vf_explained_var: 0.9688637852668762
        vf_loss: 22.53200387954712
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.016129032258064
    gpu_util_percent0: 0.3096774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15401108585452117
    mean_env_wait_ms: 1.1955164686508715
    mean_inference_ms: 4.887068789903413
    mean_raw_obs_processing_ms: 0.40480854390920545
  time_since_restore: 293.6946520805359
  time_this_iter_s: 26.471418142318726
  time_total_s: 293.6946520805359
  timers:
    learn_throughput: 8365.729
    learn_time_ms: 19339.857
    sample_throughput: 23114.362
    sample_time_ms: 6999.631
    update_time_ms: 25.34
  timestamp: 1602623428
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     11 |          293.695 | 1779712 |  214.241 |               274.02 |              74.7778 |            859.225 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3534.241030358786
    time_step_min: 3150
  date: 2020-10-13_21-10-55
  done: false
  episode_len_mean: 856.0664556962025
  episode_reward_max: 274.02020202020196
  episode_reward_mean: 215.7192677224321
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 161
  episodes_total: 2212
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9360186457633972
        entropy_coeff: 0.0005000000000000001
        kl: 0.006805013477181395
        model: {}
        policy_loss: -0.011606539716012776
        total_loss: 14.837892452875773
        vf_explained_var: 0.9714512228965759
        vf_loss: 14.849286794662476
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79333333333333
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15364505117372834
    mean_env_wait_ms: 1.1963717357143238
    mean_inference_ms: 4.857365963077378
    mean_raw_obs_processing_ms: 0.40344452418267557
  time_since_restore: 320.38268542289734
  time_this_iter_s: 26.68803334236145
  time_total_s: 320.38268542289734
  timers:
    learn_throughput: 8362.953
    learn_time_ms: 19346.276
    sample_throughput: 23235.25
    sample_time_ms: 6963.213
    update_time_ms: 25.15
  timestamp: 1602623455
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     12 |          320.383 | 1941504 |  215.719 |               274.02 |              74.7778 |            856.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3524.1796740994855
    time_step_min: 3150
  date: 2020-10-13_21-11-22
  done: false
  episode_len_mean: 854.0987341772152
  episode_reward_max: 274.02020202020196
  episode_reward_mean: 217.11411584196375
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 2370
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.917464887102445
        entropy_coeff: 0.0005000000000000001
        kl: 0.007187117318001886
        model: {}
        policy_loss: -0.01065789075801149
        total_loss: 11.536466916402182
        vf_explained_var: 0.9758577346801758
        vf_loss: 11.5468643506368
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64516129032258
    gpu_util_percent0: 0.36
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1533201204359296
    mean_env_wait_ms: 1.1971442836864046
    mean_inference_ms: 4.831019365018656
    mean_raw_obs_processing_ms: 0.4022222015989964
  time_since_restore: 346.9001064300537
  time_this_iter_s: 26.517421007156372
  time_total_s: 346.9001064300537
  timers:
    learn_throughput: 8362.686
    learn_time_ms: 19346.894
    sample_throughput: 23298.841
    sample_time_ms: 6944.208
    update_time_ms: 24.052
  timestamp: 1602623482
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     13 |            346.9 | 2103296 |  217.114 |               274.02 |              74.7778 |            854.099 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3515.129614767255
    time_step_min: 3137
  date: 2020-10-13_21-11-48
  done: false
  episode_len_mean: 851.9537549407114
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 218.34401325508028
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 160
  episodes_total: 2530
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8839244097471237
        entropy_coeff: 0.0005000000000000001
        kl: 0.006774009283011158
        model: {}
        policy_loss: -0.011904717515183924
        total_loss: 13.830247561136881
        vf_explained_var: 0.9751328825950623
        vf_loss: 13.84191664059957
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.513333333333335
    gpu_util_percent0: 0.3466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1530180232898491
    mean_env_wait_ms: 1.1979129794837584
    mean_inference_ms: 4.806697208033011
    mean_raw_obs_processing_ms: 0.4010840564984299
  time_since_restore: 373.2883839607239
  time_this_iter_s: 26.388277530670166
  time_total_s: 373.2883839607239
  timers:
    learn_throughput: 8360.938
    learn_time_ms: 19350.94
    sample_throughput: 23283.072
    sample_time_ms: 6948.911
    update_time_ms: 27.122
  timestamp: 1602623508
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     14 |          373.288 | 2265088 |  218.344 |              275.081 |              74.7778 |            851.954 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3500.4163090128754
    time_step_min: 3137
  date: 2020-10-13_21-12-15
  done: false
  episode_len_mean: 847.9442484121383
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 220.51883335828273
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 304
  episodes_total: 2834
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8736105312903722
        entropy_coeff: 0.0005000000000000001
        kl: 0.0062687892544393735
        model: {}
        policy_loss: -0.010797865291048462
        total_loss: 14.933183749516806
        vf_explained_var: 0.9792088866233826
        vf_loss: 14.943791627883911
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.036666666666676
    gpu_util_percent0: 0.335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.756666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15251720128104693
    mean_env_wait_ms: 1.1993421499403971
    mean_inference_ms: 4.766534180304526
    mean_raw_obs_processing_ms: 0.39924162604501773
  time_since_restore: 399.62347054481506
  time_this_iter_s: 26.335086584091187
  time_total_s: 399.62347054481506
  timers:
    learn_throughput: 8366.202
    learn_time_ms: 19338.764
    sample_throughput: 23226.345
    sample_time_ms: 6965.883
    update_time_ms: 26.513
  timestamp: 1602623535
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     15 |          399.623 | 2426880 |  220.519 |              275.081 |              74.7778 |            847.944 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3493.861673414305
    time_step_min: 3137
  date: 2020-10-13_21-12-41
  done: false
  episode_len_mean: 846.0689540306462
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 221.5660805254408
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 168
  episodes_total: 3002
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8535224894682566
        entropy_coeff: 0.0005000000000000001
        kl: 0.006619990609275798
        model: {}
        policy_loss: -0.010143043616532546
        total_loss: 11.625003178914389
        vf_explained_var: 0.9782131314277649
        vf_loss: 11.634911298751831
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333333
    gpu_util_percent0: 0.30333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15227653636287608
    mean_env_wait_ms: 1.2000059489352244
    mean_inference_ms: 4.747257182315863
    mean_raw_obs_processing_ms: 0.3983691420528732
  time_since_restore: 426.2638533115387
  time_this_iter_s: 26.640382766723633
  time_total_s: 426.2638533115387
  timers:
    learn_throughput: 8356.697
    learn_time_ms: 19360.759
    sample_throughput: 23189.538
    sample_time_ms: 6976.939
    update_time_ms: 27.881
  timestamp: 1602623561
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     16 |          426.264 | 2588672 |  221.566 |              275.081 |              74.7778 |            846.069 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3487.5397181294043
    time_step_min: 3137
  date: 2020-10-13_21-13-08
  done: false
  episode_len_mean: 844.1161392405063
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 222.54211417977223
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8495060205459595
        entropy_coeff: 0.0005000000000000001
        kl: 0.006349431467242539
        model: {}
        policy_loss: -0.012107226405836021
        total_loss: 10.89086365699768
        vf_explained_var: 0.9773384928703308
        vf_loss: 10.902761061986288
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28333333333334
    gpu_util_percent0: 0.303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520675257814602
    mean_env_wait_ms: 1.2006399155128964
    mean_inference_ms: 4.730465709396529
    mean_raw_obs_processing_ms: 0.3975905750628478
  time_since_restore: 452.38765263557434
  time_this_iter_s: 26.123799324035645
  time_total_s: 452.38765263557434
  timers:
    learn_throughput: 8357.11
    learn_time_ms: 19359.803
    sample_throughput: 23181.657
    sample_time_ms: 6979.311
    update_time_ms: 28.719
  timestamp: 1602623588
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     17 |          452.388 | 2750464 |  222.542 |              275.081 |              74.7778 |            844.116 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3480.486388384755
    time_step_min: 3137
  date: 2020-10-13_21-13-34
  done: false
  episode_len_mean: 841.9829545454545
  episode_reward_max: 277.35353535353516
  episode_reward_mean: 223.7197543617997
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 3344
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8174087206522623
        entropy_coeff: 0.0005000000000000001
        kl: 0.007216097398971518
        model: {}
        policy_loss: -0.011498963499131301
        total_loss: 13.150360743204752
        vf_explained_var: 0.9775157570838928
        vf_loss: 13.16154670715332
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.603333333333335
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15183716655704663
    mean_env_wait_ms: 1.2013566222478647
    mean_inference_ms: 4.7123614342723705
    mean_raw_obs_processing_ms: 0.3967379692383513
  time_since_restore: 478.6800184249878
  time_this_iter_s: 26.292365789413452
  time_total_s: 478.6800184249878
  timers:
    learn_throughput: 8356.412
    learn_time_ms: 19361.421
    sample_throughput: 23186.265
    sample_time_ms: 6977.924
    update_time_ms: 29.331
  timestamp: 1602623614
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     18 |           478.68 | 2912256 |   223.72 |              277.354 |              74.7778 |            841.983 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3468.8776077885955
    time_step_min: 3121
  date: 2020-10-13_21-14-01
  done: false
  episode_len_mean: 839.2174511423067
  episode_reward_max: 277.5050505050504
  episode_reward_mean: 225.50545087539297
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 289
  episodes_total: 3633
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8106405337651571
        entropy_coeff: 0.0005000000000000001
        kl: 0.005711381828101973
        model: {}
        policy_loss: -0.009296051983255893
        total_loss: 12.59952704111735
        vf_explained_var: 0.9803258776664734
        vf_loss: 12.608657042185465
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33870967741936
    gpu_util_percent0: 0.3635483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15152830225201447
    mean_env_wait_ms: 1.2024518233065478
    mean_inference_ms: 4.687044048820842
    mean_raw_obs_processing_ms: 0.3955946037626863
  time_since_restore: 505.2062816619873
  time_this_iter_s: 26.52626323699951
  time_total_s: 505.2062816619873
  timers:
    learn_throughput: 8348.947
    learn_time_ms: 19378.731
    sample_throughput: 23175.665
    sample_time_ms: 6981.116
    update_time_ms: 29.217
  timestamp: 1602623641
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     19 |          505.206 | 3074048 |  225.505 |              277.505 |              74.7778 |            839.217 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3462.9064997336177
    time_step_min: 3121
  date: 2020-10-13_21-14-27
  done: false
  episode_len_mean: 837.8349156118144
  episode_reward_max: 278.565656565657
  episode_reward_mean: 226.38830552359022
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 3792
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8071876466274261
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058333837271978455
        model: {}
        policy_loss: -0.01144625450069725
        total_loss: 8.772553523381552
        vf_explained_var: 0.9817249774932861
        vf_loss: 8.783819993336996
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.696666666666665
    gpu_util_percent0: 0.24966666666666662
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15137239721373305
    mean_env_wait_ms: 1.202973392204184
    mean_inference_ms: 4.674513326143569
    mean_raw_obs_processing_ms: 0.395020310315726
  time_since_restore: 531.4629094600677
  time_this_iter_s: 26.256627798080444
  time_total_s: 531.4629094600677
  timers:
    learn_throughput: 8353.345
    learn_time_ms: 19368.528
    sample_throughput: 23225.913
    sample_time_ms: 6966.013
    update_time_ms: 28.975
  timestamp: 1602623667
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     20 |          531.463 | 3235840 |  226.388 |              278.566 |              74.7778 |            837.835 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3456.5080500894455
    time_step_min: 3121
  date: 2020-10-13_21-14-54
  done: false
  episode_len_mean: 836.770437863832
  episode_reward_max: 278.565656565657
  episode_reward_mean: 227.26982045205267
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 3951
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7912125339110693
        entropy_coeff: 0.0005000000000000001
        kl: 0.006185428200600048
        model: {}
        policy_loss: -0.010615241718672527
        total_loss: 9.660934766133627
        vf_explained_var: 0.9788644313812256
        vf_loss: 9.671326955159506
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0
    gpu_util_percent0: 0.2954838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1512261820192969
    mean_env_wait_ms: 1.203474161103234
    mean_inference_ms: 4.662703681616023
    mean_raw_obs_processing_ms: 0.3944782635443422
  time_since_restore: 558.2589931488037
  time_this_iter_s: 26.796083688735962
  time_total_s: 558.2589931488037
  timers:
    learn_throughput: 8339.832
    learn_time_ms: 19399.91
    sample_throughput: 23231.735
    sample_time_ms: 6964.267
    update_time_ms: 30.774
  timestamp: 1602623694
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     21 |          558.259 | 3397632 |   227.27 |              278.566 |              74.7778 |             836.77 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3447.855593056895
    time_step_min: 3114
  date: 2020-10-13_21-15-21
  done: false
  episode_len_mean: 835.048972766364
  episode_reward_max: 279.6262626262625
  episode_reward_mean: 228.53481783916556
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 4186
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.759943018356959
        entropy_coeff: 0.0005000000000000001
        kl: 0.006279778007107477
        model: {}
        policy_loss: -0.010360730404499918
        total_loss: 12.314131418863932
        vf_explained_var: 0.9803383946418762
        vf_loss: 12.324244101842245
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.889999999999997
    gpu_util_percent0: 0.3503333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15102450345876225
    mean_env_wait_ms: 1.2041701704177816
    mean_inference_ms: 4.6464440539761025
    mean_raw_obs_processing_ms: 0.39372422263274537
  time_since_restore: 584.6352486610413
  time_this_iter_s: 26.37625551223755
  time_total_s: 584.6352486610413
  timers:
    learn_throughput: 8349.574
    learn_time_ms: 19377.276
    sample_throughput: 23259.471
    sample_time_ms: 6955.962
    update_time_ms: 29.369
  timestamp: 1602623721
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     22 |          584.635 | 3559424 |  228.535 |              279.626 |              74.7778 |            835.049 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3440.1703534777653
    time_step_min: 3114
  date: 2020-10-13_21-15-47
  done: false
  episode_len_mean: 833.3927198733891
  episode_reward_max: 279.6262626262625
  episode_reward_mean: 229.59495246381965
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 237
  episodes_total: 4423
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7440361777941386
        entropy_coeff: 0.0005000000000000001
        kl: 0.005954018871610363
        model: {}
        policy_loss: -0.011595885191733638
        total_loss: 9.518235127131144
        vf_explained_var: 0.9834404587745667
        vf_loss: 9.52960737546285
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.719354838709677
    gpu_util_percent0: 0.27645161290322584
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15084099246151877
    mean_env_wait_ms: 1.2048561820697226
    mean_inference_ms: 4.631673333106504
    mean_raw_obs_processing_ms: 0.39306630103563117
  time_since_restore: 611.2781054973602
  time_this_iter_s: 26.64285683631897
  time_total_s: 611.2781054973602
  timers:
    learn_throughput: 8348.273
    learn_time_ms: 19380.296
    sample_throughput: 23227.687
    sample_time_ms: 6965.481
    update_time_ms: 28.697
  timestamp: 1602623747
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     23 |          611.278 | 3721216 |  229.595 |              279.626 |              74.7778 |            833.393 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3435.027068661972
    time_step_min: 3114
  date: 2020-10-13_21-16-14
  done: false
  episode_len_mean: 832.1865997381057
  episode_reward_max: 279.6262626262625
  episode_reward_mean: 230.303748969397
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 4582
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7401020477215449
        entropy_coeff: 0.0005000000000000001
        kl: 0.006353482875662546
        model: {}
        policy_loss: -0.009180035073465357
        total_loss: 8.316651423772177
        vf_explained_var: 0.9818807244300842
        vf_loss: 8.325566053390503
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65666666666667
    gpu_util_percent0: 0.36533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507247503223688
    mean_env_wait_ms: 1.2052855417857289
    mean_inference_ms: 4.6223872662274035
    mean_raw_obs_processing_ms: 0.3926450718719902
  time_since_restore: 637.5115740299225
  time_this_iter_s: 26.233468532562256
  time_total_s: 637.5115740299225
  timers:
    learn_throughput: 8350.675
    learn_time_ms: 19374.721
    sample_throughput: 23246.145
    sample_time_ms: 6959.95
    update_time_ms: 25.489
  timestamp: 1602623774
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     24 |          637.512 | 3883008 |  230.304 |              279.626 |              74.7778 |            832.187 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3430.264319049639
    time_step_min: 3074
  date: 2020-10-13_21-16-41
  done: false
  episode_len_mean: 830.9013047138047
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 231.0115485664727
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 170
  episodes_total: 4752
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7222304195165634
        entropy_coeff: 0.0005000000000000001
        kl: 0.005677717078166704
        model: {}
        policy_loss: -0.010766375771102807
        total_loss: 10.416849772135416
        vf_explained_var: 0.9797022938728333
        vf_loss: 10.427409569422403
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35483870967742
    gpu_util_percent0: 0.3332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1506037624933571
    mean_env_wait_ms: 1.2057260466687028
    mean_inference_ms: 4.61291560775002
    mean_raw_obs_processing_ms: 0.39220927050632814
  time_since_restore: 664.3432669639587
  time_this_iter_s: 26.831692934036255
  time_total_s: 664.3432669639587
  timers:
    learn_throughput: 8341.26
    learn_time_ms: 19396.591
    sample_throughput: 23157.711
    sample_time_ms: 6986.528
    update_time_ms: 25.478
  timestamp: 1602623801
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     25 |          664.343 | 4044800 |  231.012 |              284.626 |              74.7778 |            830.901 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3422.069
    time_step_min: 3074
  date: 2020-10-13_21-17-07
  done: false
  episode_len_mean: 829.0049622866217
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 232.20574943560246
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 286
  episodes_total: 5038
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6987026333808899
        entropy_coeff: 0.0005000000000000001
        kl: 0.005648046149872243
        model: {}
        policy_loss: -0.011446453854053592
        total_loss: 10.859757820765177
        vf_explained_var: 0.9836289882659912
        vf_loss: 10.870988845825195
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57333333333333
    gpu_util_percent0: 0.331
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15042930595411827
    mean_env_wait_ms: 1.2065022669170768
    mean_inference_ms: 4.5984065436332395
    mean_raw_obs_processing_ms: 0.3915552405801893
  time_since_restore: 690.8237862586975
  time_this_iter_s: 26.48051929473877
  time_total_s: 690.8237862586975
  timers:
    learn_throughput: 8347.753
    learn_time_ms: 19381.502
    sample_throughput: 23159.809
    sample_time_ms: 6985.895
    update_time_ms: 24.014
  timestamp: 1602623827
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     26 |          690.824 | 4206592 |  232.206 |              284.626 |              74.7778 |            829.005 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3417.3390649149924
    time_step_min: 3074
  date: 2020-10-13_21-17-34
  done: false
  episode_len_mean: 827.6762562332183
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 232.9396612848856
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 5214
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6922043909629186
        entropy_coeff: 0.0005000000000000001
        kl: 0.005940411356277764
        model: {}
        policy_loss: -0.012036716704945624
        total_loss: 8.719095865885416
        vf_explained_var: 0.9824223518371582
        vf_loss: 8.730884631474813
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.786666666666665
    gpu_util_percent0: 0.2833333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15032403921548018
    mean_env_wait_ms: 1.206932504130428
    mean_inference_ms: 4.59006803881407
    mean_raw_obs_processing_ms: 0.39118479491998986
  time_since_restore: 717.1580054759979
  time_this_iter_s: 26.334219217300415
  time_total_s: 717.1580054759979
  timers:
    learn_throughput: 8343.223
    learn_time_ms: 19392.026
    sample_throughput: 23126.999
    sample_time_ms: 6995.806
    update_time_ms: 23.574
  timestamp: 1602623854
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     27 |          717.158 | 4368384 |   232.94 |              284.626 |              74.7778 |            827.676 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3413.4507966260544
    time_step_min: 3074
  date: 2020-10-13_21-18-00
  done: false
  episode_len_mean: 826.4688256095292
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 233.53257871850826
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 5373
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6884655207395554
        entropy_coeff: 0.0005000000000000001
        kl: 0.006287429481744766
        model: {}
        policy_loss: -0.010294094875765344
        total_loss: 8.898841540018717
        vf_explained_var: 0.9810356497764587
        vf_loss: 8.908851226170858
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.893333333333334
    gpu_util_percent0: 0.3033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15023557750758104
    mean_env_wait_ms: 1.2073213274358219
    mean_inference_ms: 4.5829380838771145
    mean_raw_obs_processing_ms: 0.39086189954937905
  time_since_restore: 743.4154720306396
  time_this_iter_s: 26.257466554641724
  time_total_s: 743.4154720306396
  timers:
    learn_throughput: 8353.936
    learn_time_ms: 19367.158
    sample_throughput: 23060.87
    sample_time_ms: 7015.867
    update_time_ms: 24.16
  timestamp: 1602623880
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     28 |          743.415 | 4530176 |  233.533 |              284.626 |              74.7778 |            826.469 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3407.133273703041
    time_step_min: 3074
  date: 2020-10-13_21-18-27
  done: false
  episode_len_mean: 824.4921819474058
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 234.46217505545854
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 255
  episodes_total: 5628
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6605731795231501
        entropy_coeff: 0.0005000000000000001
        kl: 0.005549012799747288
        model: {}
        policy_loss: -0.012337252808113893
        total_loss: 12.105455001195272
        vf_explained_var: 0.9811687469482422
        vf_loss: 12.11756706237793
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.551612903225806
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15010042781147187
    mean_env_wait_ms: 1.207962160128384
    mean_inference_ms: 4.57201850366177
    mean_raw_obs_processing_ms: 0.3903627951835015
  time_since_restore: 769.9180254936218
  time_this_iter_s: 26.502553462982178
  time_total_s: 769.9180254936218
  timers:
    learn_throughput: 8359.706
    learn_time_ms: 19353.79
    sample_throughput: 23030.449
    sample_time_ms: 7025.134
    update_time_ms: 24.754
  timestamp: 1602623907
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     29 |          769.918 | 4691968 |  234.462 |              284.626 |              74.7778 |            824.492 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3401.373450413223
    time_step_min: 3074
  date: 2020-10-13_21-18-53
  done: false
  episode_len_mean: 822.993670886076
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 235.34133327804207
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 5846
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6386907746394476
        entropy_coeff: 0.0005000000000000001
        kl: 0.005903509367878239
        model: {}
        policy_loss: -0.00886095035281187
        total_loss: 8.310452580451965
        vf_explained_var: 0.9841588139533997
        vf_loss: 8.319042523701986
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.46
    gpu_util_percent0: 0.30600000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14999194280605124
    mean_env_wait_ms: 1.208477174989982
    mean_inference_ms: 4.563509798242787
    mean_raw_obs_processing_ms: 0.3899832531634408
  time_since_restore: 796.2149534225464
  time_this_iter_s: 26.29692792892456
  time_total_s: 796.2149534225464
  timers:
    learn_throughput: 8356.933
    learn_time_ms: 19360.212
    sample_throughput: 23040.871
    sample_time_ms: 7021.957
    update_time_ms: 24.881
  timestamp: 1602623933
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     30 |          796.215 | 4853760 |  235.341 |              284.626 |              74.7778 |            822.994 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3397.6528662420383
    time_step_min: 3074
  date: 2020-10-13_21-19-20
  done: false
  episode_len_mean: 821.9566955363091
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 235.89486133823235
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 6004
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6477035929759344
        entropy_coeff: 0.0005000000000000001
        kl: 0.005589286796748638
        model: {}
        policy_loss: -0.010180077918145495
        total_loss: 9.052189270655314
        vf_explained_var: 0.9800416827201843
        vf_loss: 9.062134265899658
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.746666666666666
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14991735064689032
    mean_env_wait_ms: 1.208840836582645
    mean_inference_ms: 4.557596556613265
    mean_raw_obs_processing_ms: 0.389716640238656
  time_since_restore: 822.5506749153137
  time_this_iter_s: 26.335721492767334
  time_total_s: 822.5506749153137
  timers:
    learn_throughput: 8377.225
    learn_time_ms: 19313.316
    sample_throughput: 23039.404
    sample_time_ms: 7022.404
    update_time_ms: 24.782
  timestamp: 1602623960
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     31 |          822.551 | 5015552 |  235.895 |              284.626 |              74.7778 |            821.957 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3392.510992563854
    time_step_min: 3074
  date: 2020-10-13_21-19-46
  done: false
  episode_len_mean: 820.6823586118252
  episode_reward_max: 287.5050505050501
  episode_reward_mean: 236.66034866661465
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 220
  episodes_total: 6224
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6274365981419882
        entropy_coeff: 0.0005000000000000001
        kl: 0.005409304557057719
        model: {}
        policy_loss: -0.01206011434745354
        total_loss: 10.890360116958618
        vf_explained_var: 0.9811581969261169
        vf_loss: 10.90219267209371
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.31935483870968
    gpu_util_percent0: 0.33419354838709675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14981895256743274
    mean_env_wait_ms: 1.2093605986275922
    mean_inference_ms: 4.5498492385981475
    mean_raw_obs_processing_ms: 0.3893684947944251
  time_since_restore: 848.9541938304901
  time_this_iter_s: 26.40351891517639
  time_total_s: 848.9541938304901
  timers:
    learn_throughput: 8383.991
    learn_time_ms: 19297.73
    sample_throughput: 22987.445
    sample_time_ms: 7038.277
    update_time_ms: 26.437
  timestamp: 1602623986
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     32 |          848.954 | 5177344 |   236.66 |              287.505 |              74.7778 |            820.682 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3387.1559390547263
    time_step_min: 3043
  date: 2020-10-13_21-20-13
  done: false
  episode_len_mean: 819.4449768160742
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 237.48224126894905
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 246
  episodes_total: 6470
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6052823215723038
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051997468108311296
        model: {}
        policy_loss: -0.011560283159875931
        total_loss: 9.271020571390787
        vf_explained_var: 0.9844850897789001
        vf_loss: 9.282363414764404
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.616666666666667
    gpu_util_percent0: 0.31500000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1497133099935963
    mean_env_wait_ms: 1.2098672394462433
    mean_inference_ms: 4.541543758035458
    mean_raw_obs_processing_ms: 0.38899638174532136
  time_since_restore: 875.476434469223
  time_this_iter_s: 26.52224063873291
  time_total_s: 875.476434469223
  timers:
    learn_throughput: 8391.835
    learn_time_ms: 19279.692
    sample_throughput: 22975.667
    sample_time_ms: 7041.885
    update_time_ms: 28.05
  timestamp: 1602624013
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     33 |          875.476 | 5339136 |  237.482 |              289.323 |              74.7778 |            819.445 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3383.511821764171
    time_step_min: 3043
  date: 2020-10-13_21-20-40
  done: false
  episode_len_mean: 818.700120554551
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 238.00715564323153
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 166
  episodes_total: 6636
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6068141659100851
        entropy_coeff: 0.0005000000000000001
        kl: 0.005375104529472689
        model: {}
        policy_loss: -0.012939858686877415
        total_loss: 6.613195101420085
        vf_explained_var: 0.9857916831970215
        vf_loss: 6.625900705655416
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.183870967741935
    gpu_util_percent0: 0.307741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496457132220117
    mean_env_wait_ms: 1.2102103426766444
    mean_inference_ms: 4.536294767908478
    mean_raw_obs_processing_ms: 0.38876489510167755
  time_since_restore: 901.9606955051422
  time_this_iter_s: 26.48426103591919
  time_total_s: 901.9606955051422
  timers:
    learn_throughput: 8395.376
    learn_time_ms: 19271.561
    sample_throughput: 22898.529
    sample_time_ms: 7065.607
    update_time_ms: 36.351
  timestamp: 1602624040
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     34 |          901.961 | 5500928 |  238.007 |              289.323 |              74.7778 |              818.7 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3379.7174265899366
    time_step_min: 3043
  date: 2020-10-13_21-21-07
  done: false
  episode_len_mean: 817.8542920029347
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 238.5968711324543
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 6815
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.596410483121872
        entropy_coeff: 0.0005000000000000001
        kl: 0.005001630789289872
        model: {}
        policy_loss: -0.012441018969790699
        total_loss: 8.19734021027883
        vf_explained_var: 0.9839007258415222
        vf_loss: 8.209579269091288
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.445161290322584
    gpu_util_percent0: 0.387741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14957865588852223
    mean_env_wait_ms: 1.210573325534263
    mean_inference_ms: 4.5308955446445065
    mean_raw_obs_processing_ms: 0.3885237460958081
  time_since_restore: 928.7998685836792
  time_this_iter_s: 26.839173078536987
  time_total_s: 928.7998685836792
  timers:
    learn_throughput: 8388.662
    learn_time_ms: 19286.984
    sample_throughput: 22951.157
    sample_time_ms: 7049.405
    update_time_ms: 37.119
  timestamp: 1602624067
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     35 |            928.8 | 5662720 |  238.597 |              289.323 |              74.7778 |            817.854 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3374.4842642472354
    time_step_min: 3043
  date: 2020-10-13_21-21-33
  done: false
  episode_len_mean: 816.5851663846588
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 239.42137961681104
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 277
  episodes_total: 7092
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5682249665260315
        entropy_coeff: 0.0005000000000000001
        kl: 0.004860180236088733
        model: {}
        policy_loss: -0.010135394695680588
        total_loss: 9.11709451675415
        vf_explained_var: 0.9856011867523193
        vf_loss: 9.127027670542398
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.479999999999997
    gpu_util_percent0: 0.24533333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14947634097916856
    mean_env_wait_ms: 1.2110988225546133
    mean_inference_ms: 4.522930566582319
    mean_raw_obs_processing_ms: 0.38816696086880575
  time_since_restore: 955.2834129333496
  time_this_iter_s: 26.48354434967041
  time_total_s: 955.2834129333496
  timers:
    learn_throughput: 8396.558
    learn_time_ms: 19268.849
    sample_throughput: 22893.811
    sample_time_ms: 7067.063
    update_time_ms: 37.241
  timestamp: 1602624093
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     36 |          955.283 | 5824512 |  239.421 |              289.323 |              74.7778 |            816.585 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3371.3118948824344
    time_step_min: 3043
  date: 2020-10-13_21-22-00
  done: false
  episode_len_mean: 815.8380572372042
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 239.95424664921083
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 7268
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5694400072097778
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050308507634326816
        model: {}
        policy_loss: -0.010465515117781857
        total_loss: 6.280025005340576
        vf_explained_var: 0.986748456954956
        vf_loss: 6.290523727734883
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05483870967742
    gpu_util_percent0: 0.29129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14941363643404737
    mean_env_wait_ms: 1.2114241302214703
    mean_inference_ms: 4.51806736723159
    mean_raw_obs_processing_ms: 0.38795823155885734
  time_since_restore: 981.9770526885986
  time_this_iter_s: 26.693639755249023
  time_total_s: 981.9770526885986
  timers:
    learn_throughput: 8384.705
    learn_time_ms: 19296.087
    sample_throughput: 22873.63
    sample_time_ms: 7073.298
    update_time_ms: 38.748
  timestamp: 1602624120
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     37 |          981.977 | 5986304 |  239.954 |              289.323 |              74.7778 |            815.838 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3368.2841493102515
    time_step_min: 3043
  date: 2020-10-13_21-22-27
  done: false
  episode_len_mean: 815.0859795479009
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 240.43111551467302
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 164
  episodes_total: 7432
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5638417750597
        entropy_coeff: 0.0005000000000000001
        kl: 0.006015786590675513
        model: {}
        policy_loss: -0.010645508669161549
        total_loss: 7.669543464978536
        vf_explained_var: 0.9834616184234619
        vf_loss: 7.680170098940532
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.30333333333333
    gpu_util_percent0: 0.3996666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14936088677696577
    mean_env_wait_ms: 1.2117218014041815
    mean_inference_ms: 4.513779983414233
    mean_raw_obs_processing_ms: 0.3877697663183177
  time_since_restore: 1008.1908390522003
  time_this_iter_s: 26.213786363601685
  time_total_s: 1008.1908390522003
  timers:
    learn_throughput: 8378.509
    learn_time_ms: 19310.356
    sample_throughput: 22941.752
    sample_time_ms: 7052.295
    update_time_ms: 40.115
  timestamp: 1602624147
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     38 |          1008.19 | 6148096 |  240.431 |              289.323 |              74.7778 |            815.086 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3363.321894572025
    time_step_min: 3034
  date: 2020-10-13_21-22-54
  done: false
  episode_len_mean: 813.8701635938717
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 241.1807585069075
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 270
  episodes_total: 7702
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5422514081001282
        entropy_coeff: 0.0005000000000000001
        kl: 0.005400637979619205
        model: {}
        policy_loss: -0.009160482943116222
        total_loss: 9.511011521021524
        vf_explained_var: 0.9845170378684998
        vf_loss: 9.520172437032064
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.790322580645164
    gpu_util_percent0: 0.2854838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14927475104028992
    mean_env_wait_ms: 1.212201997839955
    mean_inference_ms: 4.50705488937929
    mean_raw_obs_processing_ms: 0.38746653417579946
  time_since_restore: 1034.7987351417542
  time_this_iter_s: 26.607896089553833
  time_total_s: 1034.7987351417542
  timers:
    learn_throughput: 8377.624
    learn_time_ms: 19312.398
    sample_throughput: 22944.187
    sample_time_ms: 7051.546
    update_time_ms: 47.824
  timestamp: 1602624174
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     39 |           1034.8 | 6309888 |  241.181 |              291.141 |              74.7778 |             813.87 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3359.8478371501274
    time_step_min: 3034
  date: 2020-10-13_21-23-20
  done: false
  episode_len_mean: 813.0749556849836
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 241.69890088527714
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 196
  episodes_total: 7898
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5340648392836252
        entropy_coeff: 0.0005000000000000001
        kl: 0.005210299471703668
        model: {}
        policy_loss: -0.0101865164275902
        total_loss: 6.542128682136536
        vf_explained_var: 0.9868605136871338
        vf_loss: 6.552321950594584
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.783333333333335
    gpu_util_percent0: 0.29266666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492147644226287
    mean_env_wait_ms: 1.2125282833903455
    mean_inference_ms: 4.50233313460513
    mean_raw_obs_processing_ms: 0.3872610007133459
  time_since_restore: 1061.3662693500519
  time_this_iter_s: 26.56753420829773
  time_total_s: 1061.3662693500519
  timers:
    learn_throughput: 8367.033
    learn_time_ms: 19336.844
    sample_throughput: 22944.47
    sample_time_ms: 7051.459
    update_time_ms: 49.812
  timestamp: 1602624200
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     40 |          1061.37 | 6471680 |  241.699 |              291.141 |              74.7778 |            813.075 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3356.760064813661
    time_step_min: 3034
  date: 2020-10-13_21-23-47
  done: false
  episode_len_mean: 812.4011909192408
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 242.15439095081814
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 163
  episodes_total: 8061
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5370505700508753
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642464306826393
        model: {}
        policy_loss: -0.011000998706246415
        total_loss: 6.366736014684041
        vf_explained_var: 0.985722005367279
        vf_loss: 6.377723256746928
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.929032258064517
    gpu_util_percent0: 0.317741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1491680443118355
    mean_env_wait_ms: 1.2127965161050145
    mean_inference_ms: 4.498607551382991
    mean_raw_obs_processing_ms: 0.38709757753760266
  time_since_restore: 1087.8879582881927
  time_this_iter_s: 26.52168893814087
  time_total_s: 1087.8879582881927
  timers:
    learn_throughput: 8358.555
    learn_time_ms: 19356.456
    sample_throughput: 22950.774
    sample_time_ms: 7049.523
    update_time_ms: 49.849
  timestamp: 1602624227
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     41 |          1087.89 | 6633472 |  242.154 |              291.141 |              74.7778 |            812.401 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3352.4010158423025
    time_step_min: 3034
  date: 2020-10-13_21-24-14
  done: false
  episode_len_mean: 811.5095702419646
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 242.76242745256823
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 246
  episodes_total: 8307
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.517037163178126
        entropy_coeff: 0.0005000000000000001
        kl: 0.005597573200551172
        model: {}
        policy_loss: -0.010149982525035739
        total_loss: 8.83277416229248
        vf_explained_var: 0.9849687218666077
        vf_loss: 8.842902421951294
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.68666666666667
    gpu_util_percent0: 0.31133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14910099146559283
    mean_env_wait_ms: 1.2132009035871418
    mean_inference_ms: 4.493170721475624
    mean_raw_obs_processing_ms: 0.3868560925625396
  time_since_restore: 1114.3232762813568
  time_this_iter_s: 26.435317993164062
  time_total_s: 1114.3232762813568
  timers:
    learn_throughput: 8349.81
    learn_time_ms: 19376.729
    sample_throughput: 23013.116
    sample_time_ms: 7030.426
    update_time_ms: 49.783
  timestamp: 1602624254
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     42 |          1114.32 | 6795264 |  242.762 |              291.141 |              74.7778 |             811.51 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3348.921554770318
    time_step_min: 3034
  date: 2020-10-13_21-24-40
  done: false
  episode_len_mean: 810.6337945590994
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 243.2807306176208
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 221
  episodes_total: 8528
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4924461245536804
        entropy_coeff: 0.0005000000000000001
        kl: 0.005016234703361988
        model: {}
        policy_loss: -0.009520547231659293
        total_loss: 6.755296031634013
        vf_explained_var: 0.9874246120452881
        vf_loss: 6.764811992645264
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.267741935483876
    gpu_util_percent0: 0.2732258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14904034309531752
    mean_env_wait_ms: 1.2135391277786955
    mean_inference_ms: 4.4885184485416305
    mean_raw_obs_processing_ms: 0.38665234967093653
  time_since_restore: 1140.9515879154205
  time_this_iter_s: 26.62831163406372
  time_total_s: 1140.9515879154205
  timers:
    learn_throughput: 8347.098
    learn_time_ms: 19383.024
    sample_throughput: 23003.388
    sample_time_ms: 7033.399
    update_time_ms: 49.84
  timestamp: 1602624280
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     43 |          1140.95 | 6957056 |  243.281 |              291.141 |              74.7778 |            810.634 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3346.3664624985554
    time_step_min: 3034
  date: 2020-10-13_21-25-07
  done: false
  episode_len_mean: 810.0192152801749
  episode_reward_max: 291.5959595959604
  episode_reward_mean: 243.67902706735978
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 163
  episodes_total: 8691
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5038283864657084
        entropy_coeff: 0.0005000000000000001
        kl: 0.00519260298460722
        model: {}
        policy_loss: -0.012088534994594132
        total_loss: 6.511295437812805
        vf_explained_var: 0.9850991368293762
        vf_loss: 6.52337630589803
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303333333333335
    gpu_util_percent0: 0.37366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14899929017446978
    mean_env_wait_ms: 1.2137858865797424
    mean_inference_ms: 4.485261946761385
    mean_raw_obs_processing_ms: 0.38651013938838624
  time_since_restore: 1167.41117811203
  time_this_iter_s: 26.459590196609497
  time_total_s: 1167.41117811203
  timers:
    learn_throughput: 8333.592
    learn_time_ms: 19414.437
    sample_throughput: 23093.623
    sample_time_ms: 7005.917
    update_time_ms: 42.85
  timestamp: 1602624307
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     44 |          1167.41 | 7118848 |  243.679 |              291.596 |              74.7778 |            810.019 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3342.93476056338
    time_step_min: 3034
  date: 2020-10-13_21-25-34
  done: false
  episode_len_mean: 809.2556939302143
  episode_reward_max: 291.5959595959604
  episode_reward_mean: 244.2004970608134
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 8913
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4844573761026065
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053589244683583575
        model: {}
        policy_loss: -0.011921500651321063
        total_loss: 7.8355962832768755
        vf_explained_var: 0.9858234524726868
        vf_loss: 7.847492138544719
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.635483870967743
    gpu_util_percent0: 0.36
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14894774999507854
    mean_env_wait_ms: 1.2141301615315265
    mean_inference_ms: 4.480957728264438
    mean_raw_obs_processing_ms: 0.38632085080905826
  time_since_restore: 1193.648298740387
  time_this_iter_s: 26.237120628356934
  time_total_s: 1193.648298740387
  timers:
    learn_throughput: 8350.447
    learn_time_ms: 19375.249
    sample_throughput: 23164.985
    sample_time_ms: 6984.334
    update_time_ms: 42.38
  timestamp: 1602624334
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     45 |          1193.65 | 7280640 |    244.2 |              291.596 |              74.7778 |            809.256 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3339.027192982456
    time_step_min: 3034
  date: 2020-10-13_21-26-00
  done: false
  episode_len_mean: 808.4010701026425
  episode_reward_max: 292.95959595959573
  episode_reward_mean: 244.7556411461194
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 245
  episodes_total: 9158
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4631484200557073
        entropy_coeff: 0.0005000000000000001
        kl: 0.005080961001416047
        model: {}
        policy_loss: -0.008889351251127664
        total_loss: 7.061241348584493
        vf_explained_var: 0.9872291088104248
        vf_loss: 7.07010833422343
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.036666666666672
    gpu_util_percent0: 0.31666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488871568594427
    mean_env_wait_ms: 1.2144803508376414
    mean_inference_ms: 4.476344962162306
    mean_raw_obs_processing_ms: 0.3861172719938036
  time_since_restore: 1220.1241834163666
  time_this_iter_s: 26.475884675979614
  time_total_s: 1220.1241834163666
  timers:
    learn_throughput: 8340.678
    learn_time_ms: 19397.944
    sample_throughput: 23247.083
    sample_time_ms: 6959.669
    update_time_ms: 42.588
  timestamp: 1602624360
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     46 |          1220.12 | 7442432 |  244.756 |               292.96 |              74.7778 |            808.401 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3336.404631125471
    time_step_min: 3034
  date: 2020-10-13_21-26-27
  done: false
  episode_len_mean: 807.995387750724
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 245.14376739615392
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 165
  episodes_total: 9323
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47594211747248966
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052220659563317895
        model: {}
        policy_loss: -0.010468926494165013
        total_loss: 5.399368405342102
        vf_explained_var: 0.9877247214317322
        vf_loss: 5.409814119338989
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17741935483871
    gpu_util_percent0: 0.3496774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14885022266366005
    mean_env_wait_ms: 1.2147066142827156
    mean_inference_ms: 4.473413216595223
    mean_raw_obs_processing_ms: 0.385991264672051
  time_since_restore: 1246.7197043895721
  time_this_iter_s: 26.595520973205566
  time_total_s: 1246.7197043895721
  timers:
    learn_throughput: 8343.129
    learn_time_ms: 19392.244
    sample_throughput: 23288.958
    sample_time_ms: 6947.155
    update_time_ms: 49.795
  timestamp: 1602624387
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     47 |          1246.72 | 7604224 |  245.144 |              296.141 |              74.7778 |            807.995 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3333.4631856540086
    time_step_min: 3023
  date: 2020-10-13_21-26-54
  done: false
  episode_len_mean: 807.5162849338096
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 245.56862383023335
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 9518
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.46213724960883457
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057576741091907024
        model: {}
        policy_loss: -0.00961345701944083
        total_loss: 6.898455818494161
        vf_explained_var: 0.9866754412651062
        vf_loss: 6.9080125490824384
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51
    gpu_util_percent0: 0.3303333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14880701505061975
    mean_env_wait_ms: 1.2149699560461662
    mean_inference_ms: 4.469968520848805
    mean_raw_obs_processing_ms: 0.38584092489787614
  time_since_restore: 1273.253693819046
  time_this_iter_s: 26.533989429473877
  time_total_s: 1273.253693819046
  timers:
    learn_throughput: 8333.157
    learn_time_ms: 19415.452
    sample_throughput: 23261.476
    sample_time_ms: 6955.363
    update_time_ms: 49.215
  timestamp: 1602624414
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     48 |          1273.25 | 7766016 |  245.569 |              296.141 |              74.7778 |            807.516 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3329.772895277207
    time_step_min: 3023
  date: 2020-10-13_21-27-20
  done: false
  episode_len_mean: 806.8021067703007
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 246.11465338597677
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 260
  episodes_total: 9778
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.435211144387722
        entropy_coeff: 0.0005000000000000001
        kl: 0.004585651952462892
        model: {}
        policy_loss: -0.010591738411070159
        total_loss: 6.905542532602946
        vf_explained_var: 0.9884118437767029
        vf_loss: 6.916122396787007
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03548387096774
    gpu_util_percent0: 0.3367741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14875257746460627
    mean_env_wait_ms: 1.2153052811832337
    mean_inference_ms: 4.465676397185308
    mean_raw_obs_processing_ms: 0.3856524829025742
  time_since_restore: 1299.674120426178
  time_this_iter_s: 26.420426607131958
  time_total_s: 1299.674120426178
  timers:
    learn_throughput: 8341.694
    learn_time_ms: 19395.581
    sample_throughput: 23258.852
    sample_time_ms: 6956.147
    update_time_ms: 43.002
  timestamp: 1602624440
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     49 |          1299.67 | 7927808 |  246.115 |              296.141 |              74.7778 |            806.802 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3327.5307583703107
    time_step_min: 3003
  date: 2020-10-13_21-27-47
  done: false
  episode_len_mean: 806.3794454490657
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 246.46908810832858
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 9954
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4466582238674164
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051932833933581906
        model: {}
        policy_loss: -0.011864040590201816
        total_loss: 5.898092150688171
        vf_explained_var: 0.9870522022247314
        vf_loss: 5.910049915313721
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.610000000000003
    gpu_util_percent0: 0.3016666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14871745562723418
    mean_env_wait_ms: 1.2155367147123777
    mean_inference_ms: 4.462896603348854
    mean_raw_obs_processing_ms: 0.3855336161590362
  time_since_restore: 1326.042935371399
  time_this_iter_s: 26.368814945220947
  time_total_s: 1326.042935371399
  timers:
    learn_throughput: 8353.52
    learn_time_ms: 19368.123
    sample_throughput: 23237.469
    sample_time_ms: 6962.548
    update_time_ms: 43.06
  timestamp: 1602624467
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     50 |          1326.04 | 8089600 |  246.469 |              296.141 |              74.7778 |            806.379 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3325.1214193676283
    time_step_min: 3003
  date: 2020-10-13_21-28-14
  done: false
  episode_len_mean: 805.9673150982522
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 246.82856609942615
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 173
  episodes_total: 10127
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4431249772508939
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053331688201675815
        model: {}
        policy_loss: -0.008290527951127539
        total_loss: 6.406747579574585
        vf_explained_var: 0.9863852858543396
        vf_loss: 6.415126125017802
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.067741935483873
    gpu_util_percent0: 0.28838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14868407395408656
    mean_env_wait_ms: 1.2157476585376272
    mean_inference_ms: 4.460158042116802
    mean_raw_obs_processing_ms: 0.385416862670676
  time_since_restore: 1352.5222675800323
  time_this_iter_s: 26.479332208633423
  time_total_s: 1352.5222675800323
  timers:
    learn_throughput: 8352.881
    learn_time_ms: 19369.604
    sample_throughput: 23288.688
    sample_time_ms: 6947.236
    update_time_ms: 43.363
  timestamp: 1602624494
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     51 |          1352.52 | 8251392 |  246.829 |              296.141 |              74.7778 |            805.967 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3321.5150608225526
    time_step_min: 3003
  date: 2020-10-13_21-28-40
  done: false
  episode_len_mean: 805.3527318199308
  episode_reward_max: 297.0505050505049
  episode_reward_mean: 247.3874499127481
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 269
  episodes_total: 10396
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.415935255587101
        entropy_coeff: 0.0005000000000000001
        kl: 0.00476513112274309
        model: {}
        policy_loss: -0.010463526453046748
        total_loss: 7.400005221366882
        vf_explained_var: 0.987629234790802
        vf_loss: 7.4105576276779175
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.850000000000005
    gpu_util_percent0: 0.31466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14863484247675748
    mean_env_wait_ms: 1.2160686366740279
    mean_inference_ms: 4.4562173104759655
    mean_raw_obs_processing_ms: 0.3852429305305342
  time_since_restore: 1378.9321999549866
  time_this_iter_s: 26.409932374954224
  time_total_s: 1378.9321999549866
  timers:
    learn_throughput: 8354.119
    learn_time_ms: 19366.733
    sample_throughput: 23285.271
    sample_time_ms: 6948.255
    update_time_ms: 42.438
  timestamp: 1602624520
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     52 |          1378.93 | 8413184 |  247.387 |              297.051 |              74.7778 |            805.353 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3319.2956290888405
    time_step_min: 3003
  date: 2020-10-13_21-29-07
  done: false
  episode_len_mean: 804.9416154936231
  episode_reward_max: 297.0505050505049
  episode_reward_mean: 247.74413955330346
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 10585
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4188198422392209
        entropy_coeff: 0.0005000000000000001
        kl: 0.005146876714813213
        model: {}
        policy_loss: -0.011319775764908021
        total_loss: 5.563521305720012
        vf_explained_var: 0.9883091449737549
        vf_loss: 5.574986298878987
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.029032258064518
    gpu_util_percent0: 0.25967741935483873
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485992240152965
    mean_env_wait_ms: 1.2162848303913312
    mean_inference_ms: 4.453488973752329
    mean_raw_obs_processing_ms: 0.3851296402237026
  time_since_restore: 1405.7191088199615
  time_this_iter_s: 26.786908864974976
  time_total_s: 1405.7191088199615
  timers:
    learn_throughput: 8349.797
    learn_time_ms: 19376.757
    sample_throughput: 23268.05
    sample_time_ms: 6953.398
    update_time_ms: 42.599
  timestamp: 1602624547
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     53 |          1405.72 | 8574976 |  247.744 |              297.051 |              74.7778 |            804.942 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3317.2911534154537
    time_step_min: 3003
  date: 2020-10-13_21-29-34
  done: false
  episode_len_mean: 804.5798772549749
  episode_reward_max: 297.0505050505049
  episode_reward_mean: 248.03962255998707
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 169
  episodes_total: 10754
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4299461667736371
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056602720481654005
        model: {}
        policy_loss: -0.009629145381040871
        total_loss: 6.223726034164429
        vf_explained_var: 0.986443817615509
        vf_loss: 6.23349932829539
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.396666666666665
    gpu_util_percent0: 0.3496666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14856925379171917
    mean_env_wait_ms: 1.2164692261397978
    mean_inference_ms: 4.451098812095244
    mean_raw_obs_processing_ms: 0.385028696601552
  time_since_restore: 1432.227737903595
  time_this_iter_s: 26.508629083633423
  time_total_s: 1432.227737903595
  timers:
    learn_throughput: 8351.519
    learn_time_ms: 19372.762
    sample_throughput: 23238.151
    sample_time_ms: 6962.344
    update_time_ms: 41.509
  timestamp: 1602624574
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     54 |          1432.23 | 8736768 |   248.04 |              297.051 |              74.7778 |             804.58 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3314.206145149526
    time_step_min: 2974
  date: 2020-10-13_21-30-01
  done: false
  episode_len_mean: 804.1219334908232
  episode_reward_max: 299.7777777777774
  episode_reward_mean: 248.53024888169352
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 252
  episodes_total: 11006
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4075028871496518
        entropy_coeff: 0.0005000000000000001
        kl: 0.005991729791276157
        model: {}
        policy_loss: -0.007751054183851617
        total_loss: 7.360698779424031
        vf_explained_var: 0.9872749447822571
        vf_loss: 7.368578831354777
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49032258064516
    gpu_util_percent0: 0.4090322580645162
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852834559336017
    mean_env_wait_ms: 1.2167388065277425
    mean_inference_ms: 4.447766099988453
    mean_raw_obs_processing_ms: 0.3848805065439447
  time_since_restore: 1458.5880453586578
  time_this_iter_s: 26.360307455062866
  time_total_s: 1458.5880453586578
  timers:
    learn_throughput: 8351.04
    learn_time_ms: 19373.875
    sample_throughput: 23205.57
    sample_time_ms: 6972.119
    update_time_ms: 42.436
  timestamp: 1602624601
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     55 |          1458.59 | 8898560 |   248.53 |              299.778 |              74.7778 |            804.122 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3311.512974230494
    time_step_min: 2974
  date: 2020-10-13_21-30-27
  done: false
  episode_len_mean: 803.8080078473337
  episode_reward_max: 299.7777777777774
  episode_reward_mean: 248.91747599050964
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 11214
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.39224373549222946
        entropy_coeff: 0.0005000000000000001
        kl: 0.005173111334443092
        model: {}
        policy_loss: -0.009124329410648594
        total_loss: 6.454073071479797
        vf_explained_var: 0.9874857068061829
        vf_loss: 6.4633287986119585
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.67
    gpu_util_percent0: 0.2896666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14849265969834
    mean_env_wait_ms: 1.2169468888101285
    mean_inference_ms: 4.445054773376617
    mean_raw_obs_processing_ms: 0.38476782481704924
  time_since_restore: 1485.0084948539734
  time_this_iter_s: 26.42044949531555
  time_total_s: 1485.0084948539734
  timers:
    learn_throughput: 8355.06
    learn_time_ms: 19364.553
    sample_throughput: 23195.322
    sample_time_ms: 6975.2
    update_time_ms: 42.139
  timestamp: 1602624627
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     56 |          1485.01 | 9060352 |  248.917 |              299.778 |              74.7778 |            803.808 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3309.548443974257
    time_step_min: 2974
  date: 2020-10-13_21-30-54
  done: false
  episode_len_mean: 803.582461998067
  episode_reward_max: 299.7777777777774
  episode_reward_mean: 249.22837726176624
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 11381
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4059066101908684
        entropy_coeff: 0.0005000000000000001
        kl: 0.005832684226334095
        model: {}
        policy_loss: -0.007933409598384364
        total_loss: 5.164769212404887
        vf_explained_var: 0.9883384704589844
        vf_loss: 5.172832767168681
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.158064516129034
    gpu_util_percent0: 0.38354838709677425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14846610776278993
    mean_env_wait_ms: 1.217103394919041
    mean_inference_ms: 4.442950328575833
    mean_raw_obs_processing_ms: 0.3846792196251251
  time_since_restore: 1511.629446029663
  time_this_iter_s: 26.620951175689697
  time_total_s: 1511.629446029663
  timers:
    learn_throughput: 8363.926
    learn_time_ms: 19344.027
    sample_throughput: 23101.448
    sample_time_ms: 7003.544
    update_time_ms: 35.73
  timestamp: 1602624654
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     57 |          1511.63 | 9222144 |  249.228 |              299.778 |              74.7778 |            803.582 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3306.7504758608757
    time_step_min: 2949
  date: 2020-10-13_21-31-21
  done: false
  episode_len_mean: 803.2393928941015
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 249.65313971031458
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 215
  episodes_total: 11596
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3887382075190544
        entropy_coeff: 0.0005000000000000001
        kl: 0.004961162572726607
        model: {}
        policy_loss: -0.00886147262644954
        total_loss: 6.426156918207805
        vf_explained_var: 0.9878730773925781
        vf_loss: 6.435150623321533
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.969999999999995
    gpu_util_percent0: 0.30366666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14843383039367652
    mean_env_wait_ms: 1.2173046172570248
    mean_inference_ms: 4.440322698640058
    mean_raw_obs_processing_ms: 0.384566261116641
  time_since_restore: 1537.8289685249329
  time_this_iter_s: 26.199522495269775
  time_total_s: 1537.8289685249329
  timers:
    learn_throughput: 8376.584
    learn_time_ms: 19314.795
    sample_throughput: 23125.29
    sample_time_ms: 6996.323
    update_time_ms: 35.34
  timestamp: 1602624681
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     58 |          1537.83 | 9383936 |  249.653 |              303.566 |              74.7778 |            803.239 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3303.5622299415404
    time_step_min: 2949
  date: 2020-10-13_21-31-48
  done: false
  episode_len_mean: 802.8824423612871
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 250.1446847497012
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 245
  episodes_total: 11841
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3674825330575307
        entropy_coeff: 0.0005000000000000001
        kl: 0.004915991177161534
        model: {}
        policy_loss: -0.009662117537421485
        total_loss: 5.518146475156148
        vf_explained_var: 0.9897975921630859
        vf_loss: 5.5279615720113116
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.677419354838708
    gpu_util_percent0: 0.34225806451612895
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14839721209179907
    mean_env_wait_ms: 1.2175249462451412
    mean_inference_ms: 4.437487744258351
    mean_raw_obs_processing_ms: 0.38444477480213796
  time_since_restore: 1564.2984373569489
  time_this_iter_s: 26.46946883201599
  time_total_s: 1564.2984373569489
  timers:
    learn_throughput: 8369.944
    learn_time_ms: 19330.117
    sample_throughput: 23133.349
    sample_time_ms: 6993.886
    update_time_ms: 33.329
  timestamp: 1602624708
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     59 |           1564.3 | 9545728 |  250.145 |              303.566 |              74.7778 |            802.882 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3301.4739348370927
    time_step_min: 2949
  date: 2020-10-13_21-32-14
  done: false
  episode_len_mean: 802.6105929380413
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 250.44571632379754
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 12008
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.37949976076682407
        entropy_coeff: 0.0005000000000000001
        kl: 0.005423328140750527
        model: {}
        policy_loss: -0.011651926053067049
        total_loss: 5.225569645563762
        vf_explained_var: 0.9881870150566101
        vf_loss: 5.237394571304321
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.923333333333332
    gpu_util_percent0: 0.322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14837310747484375
    mean_env_wait_ms: 1.2176658236823898
    mean_inference_ms: 4.4356006059480135
    mean_raw_obs_processing_ms: 0.384366625055222
  time_since_restore: 1590.729211807251
  time_this_iter_s: 26.430774450302124
  time_total_s: 1590.729211807251
  timers:
    learn_throughput: 8369.091
    learn_time_ms: 19332.087
    sample_throughput: 23118.924
    sample_time_ms: 6998.25
    update_time_ms: 31.869
  timestamp: 1602624734
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     60 |          1590.73 | 9707520 |  250.446 |              303.566 |              74.7778 |            802.611 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3299.2453745580133
    time_step_min: 2949
  date: 2020-10-13_21-32-41
  done: false
  episode_len_mean: 802.3424051151734
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 250.7686563147666
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 12199
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3746684566140175
        entropy_coeff: 0.0005000000000000001
        kl: 0.005291714294192691
        model: {}
        policy_loss: -0.009012324328068644
        total_loss: 6.054218252499898
        vf_explained_var: 0.9877989888191223
        vf_loss: 6.0634013414382935
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612903
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14834672009116187
    mean_env_wait_ms: 1.2178210066674937
    mean_inference_ms: 4.433472710243256
    mean_raw_obs_processing_ms: 0.3842768183331704
  time_since_restore: 1617.044329404831
  time_this_iter_s: 26.315117597579956
  time_total_s: 1617.044329404831
  timers:
    learn_throughput: 8376.426
    learn_time_ms: 19315.159
    sample_throughput: 23113.49
    sample_time_ms: 6999.895
    update_time_ms: 37.65
  timestamp: 1602624761
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     61 |          1617.04 | 9869312 |  250.769 |              303.566 |              74.7778 |            802.342 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3296.2546903937514
    time_step_min: 2949
  date: 2020-10-13_21-33-07
  done: false
  episode_len_mean: 802.021835112788
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 251.2066081056207
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 258
  episodes_total: 12457
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.35425874094168347
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057503182130555315
        model: {}
        policy_loss: -0.009201771514199208
        total_loss: 5.672591805458069
        vf_explained_var: 0.990231990814209
        vf_loss: 5.681952754656474
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.656666666666663
    gpu_util_percent0: 0.3333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14831197592390952
    mean_env_wait_ms: 1.2180273212299282
    mean_inference_ms: 4.430666063793962
    mean_raw_obs_processing_ms: 0.38415756466184187
  time_since_restore: 1643.3527998924255
  time_this_iter_s: 26.308470487594604
  time_total_s: 1643.3527998924255
  timers:
    learn_throughput: 8378.105
    learn_time_ms: 19311.288
    sample_throughput: 23134.09
    sample_time_ms: 6993.662
    update_time_ms: 37.226
  timestamp: 1602624787
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     62 |          1643.35 | 10031104 |  251.207 |              303.566 |              74.7778 |            802.022 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3294.1152289500833
    time_step_min: 2949
  date: 2020-10-13_21-33-34
  done: false
  episode_len_mean: 801.7741909961231
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 251.53180031983732
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 12639
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3602635934948921
        entropy_coeff: 0.0005000000000000001
        kl: 0.005350144618811707
        model: {}
        policy_loss: -0.009856152654416897
        total_loss: 4.978982488314311
        vf_explained_var: 0.989046573638916
        vf_loss: 4.989002068837483
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.273333333333337
    gpu_util_percent0: 0.39066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14828799480797977
    mean_env_wait_ms: 1.2181665245667896
    mean_inference_ms: 4.428805331459892
    mean_raw_obs_processing_ms: 0.38408189287168554
  time_since_restore: 1669.5846409797668
  time_this_iter_s: 26.23184108734131
  time_total_s: 1669.5846409797668
  timers:
    learn_throughput: 8388.994
    learn_time_ms: 19286.222
    sample_throughput: 23232.557
    sample_time_ms: 6964.02
    update_time_ms: 35.186
  timestamp: 1602624814
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     63 |          1669.58 | 10192896 |  251.532 |              303.566 |              74.7778 |            801.774 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3292.2328584846587
    time_step_min: 2949
  date: 2020-10-13_21-34-00
  done: false
  episode_len_mean: 801.5559544248479
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 251.81233042142983
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 175
  episodes_total: 12814
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3632343610127767
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050664207665249705
        model: {}
        policy_loss: -0.00965268024689673
        total_loss: 4.904102603594462
        vf_explained_var: 0.9893550872802734
        vf_loss: 4.913921038309733
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.026666666666667
    gpu_util_percent0: 0.3646666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826530799393375
    mean_env_wait_ms: 1.2182940341380444
    mean_inference_ms: 4.42697736842079
    mean_raw_obs_processing_ms: 0.38400390378303145
  time_since_restore: 1695.849430322647
  time_this_iter_s: 26.26478934288025
  time_total_s: 1695.849430322647
  timers:
    learn_throughput: 8393.003
    learn_time_ms: 19277.01
    sample_throughput: 23283.757
    sample_time_ms: 6948.707
    update_time_ms: 34.948
  timestamp: 1602624840
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     64 |          1695.85 | 10354688 |  251.812 |              303.566 |              74.7778 |            801.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3289.4561430435115
    time_step_min: 2949
  date: 2020-10-13_21-34-27
  done: false
  episode_len_mean: 801.235442650547
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 252.2313053250386
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 255
  episodes_total: 13069
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.34738215804100037
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055323335497329635
        model: {}
        policy_loss: -0.008751247672383519
        total_loss: 7.331027666727702
        vf_explained_var: 0.9872035384178162
        vf_loss: 7.339935262997945
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.84666666666667
    gpu_util_percent0: 0.35333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14823409484054428
    mean_env_wait_ms: 1.2184721085652297
    mean_inference_ms: 4.424477206178457
    mean_raw_obs_processing_ms: 0.3838947998298698
  time_since_restore: 1722.009444475174
  time_this_iter_s: 26.160014152526855
  time_total_s: 1722.009444475174
  timers:
    learn_throughput: 8392.199
    learn_time_ms: 19278.857
    sample_throughput: 23356.831
    sample_time_ms: 6926.967
    update_time_ms: 33.399
  timestamp: 1602624867
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     65 |          1722.01 | 10516480 |  252.231 |              303.566 |              74.7778 |            801.235 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3287.3278663744236
    time_step_min: 2949
  date: 2020-10-13_21-34-53
  done: false
  episode_len_mean: 801.0252468158867
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 252.56578673919842
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 200
  episodes_total: 13269
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3430374910434087
        entropy_coeff: 0.0005000000000000001
        kl: 0.004587248161745568
        model: {}
        policy_loss: -0.010506312828511
        total_loss: 5.565648555755615
        vf_explained_var: 0.9885144233703613
        vf_loss: 5.576312144597371
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.163333333333338
    gpu_util_percent0: 0.3003333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14820904063931634
    mean_env_wait_ms: 1.2186101220328398
    mean_inference_ms: 4.422530753022776
    mean_raw_obs_processing_ms: 0.3838154412882051
  time_since_restore: 1748.2945024967194
  time_this_iter_s: 26.28505802154541
  time_total_s: 1748.2945024967194
  timers:
    learn_throughput: 8392.591
    learn_time_ms: 19277.955
    sample_throughput: 23408.973
    sample_time_ms: 6911.538
    update_time_ms: 35.994
  timestamp: 1602624893
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     66 |          1748.29 | 10678272 |  252.566 |              303.566 |              74.7778 |            801.025 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3285.5690079868627
    time_step_min: 2949
  date: 2020-10-13_21-35-20
  done: false
  episode_len_mean: 800.8935615928544
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 252.8280084056042
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 166
  episodes_total: 13435
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.35400168349345523
        entropy_coeff: 0.0005000000000000001
        kl: 0.005854726885445416
        model: {}
        policy_loss: -0.01025951478125838
        total_loss: 5.445642948150635
        vf_explained_var: 0.9879546165466309
        vf_loss: 5.456070303916931
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.85161290322581
    gpu_util_percent0: 0.3045161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14818864637366563
    mean_env_wait_ms: 1.2187156055636847
    mean_inference_ms: 4.420938437655163
    mean_raw_obs_processing_ms: 0.3837482214371118
  time_since_restore: 1774.742306470871
  time_this_iter_s: 26.44780397415161
  time_total_s: 1774.742306470871
  timers:
    learn_throughput: 8385.726
    learn_time_ms: 19293.738
    sample_throughput: 23550.434
    sample_time_ms: 6870.022
    update_time_ms: 33.614
  timestamp: 1602624920
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     67 |          1774.74 | 10840064 |  252.828 |              303.566 |              74.7778 |            800.894 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3283.3482634554666
    time_step_min: 2949
  date: 2020-10-13_21-35-47
  done: false
  episode_len_mean: 800.7516292011422
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 253.14689029860736
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 13657
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3397717500726382
        entropy_coeff: 0.0005000000000000001
        kl: 0.004544714892593523
        model: {}
        policy_loss: -0.009270166881227246
        total_loss: 6.19477625687917
        vf_explained_var: 0.9888274669647217
        vf_loss: 6.204209327697754
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583333333333332
    gpu_util_percent0: 0.3316666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14816270661731318
    mean_env_wait_ms: 1.2188457254755223
    mean_inference_ms: 4.4188527134990965
    mean_raw_obs_processing_ms: 0.3836543672980312
  time_since_restore: 1801.220811843872
  time_this_iter_s: 26.4785053730011
  time_total_s: 1801.220811843872
  timers:
    learn_throughput: 8368.157
    learn_time_ms: 19334.246
    sample_throughput: 23589.371
    sample_time_ms: 6858.682
    update_time_ms: 34.073
  timestamp: 1602624947
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     68 |          1801.22 | 11001856 |  253.147 |              303.566 |              74.7778 |            800.752 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3281.107333622059
    time_step_min: 2949
  date: 2020-10-13_21-36-14
  done: false
  episode_len_mean: 800.5938669737978
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 253.4927019983887
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 13892
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.32775454968214035
        entropy_coeff: 0.0005000000000000001
        kl: 0.004657920799218118
        model: {}
        policy_loss: -0.01063937455182895
        total_loss: 7.272084633509318
        vf_explained_var: 0.9869036674499512
        vf_loss: 7.282884279886882
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.561290322580643
    gpu_util_percent0: 0.3383870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813775881128785
    mean_env_wait_ms: 1.2189933498063592
    mean_inference_ms: 4.416799011342827
    mean_raw_obs_processing_ms: 0.38357074914845585
  time_since_restore: 1827.7364165782928
  time_this_iter_s: 26.515604734420776
  time_total_s: 1827.7364165782928
  timers:
    learn_throughput: 8366.063
    learn_time_ms: 19339.084
    sample_throughput: 23602.983
    sample_time_ms: 6854.727
    update_time_ms: 36.145
  timestamp: 1602624974
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     69 |          1827.74 | 11163648 |  253.493 |              303.566 |              74.7778 |            800.594 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3279.43696520251
    time_step_min: 2939
  date: 2020-10-13_21-36-41
  done: false
  episode_len_mean: 800.446664770303
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 253.7351225237728
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 170
  episodes_total: 14062
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3373778189222018
        entropy_coeff: 0.0005000000000000001
        kl: 0.004869511739040415
        model: {}
        policy_loss: -0.011104502812183151
        total_loss: 5.528688748677571
        vf_explained_var: 0.9876391291618347
        vf_loss: 5.539959907531738
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.78709677419355
    gpu_util_percent0: 0.35129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7967741935483863
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481177400822587
    mean_env_wait_ms: 1.219081471972259
    mean_inference_ms: 4.41529695342129
    mean_raw_obs_processing_ms: 0.3835051095487574
  time_since_restore: 1854.3367686271667
  time_this_iter_s: 26.6003520488739
  time_total_s: 1854.3367686271667
  timers:
    learn_throughput: 8357.889
    learn_time_ms: 19357.999
    sample_throughput: 23612.634
    sample_time_ms: 6851.925
    update_time_ms: 36.779
  timestamp: 1602625001
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     70 |          1854.34 | 11325440 |  253.735 |              305.081 |              74.7778 |            800.447 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3277.488110313775
    time_step_min: 2939
  date: 2020-10-13_21-37-07
  done: false
  episode_len_mean: 800.3270418186921
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.0209462007104
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 190
  episodes_total: 14252
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3390084778269132
        entropy_coeff: 0.0005000000000000001
        kl: 0.005157003877684474
        model: {}
        policy_loss: -0.010564725331884498
        total_loss: 5.454565564791362
        vf_explained_var: 0.9887983202934265
        vf_loss: 5.465298732121785
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97666666666667
    gpu_util_percent0: 0.3426666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8633333333333346
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14809648393194638
    mean_env_wait_ms: 1.219177595076902
    mean_inference_ms: 4.413675911487015
    mean_raw_obs_processing_ms: 0.38343411127263644
  time_since_restore: 1880.6533710956573
  time_this_iter_s: 26.3166024684906
  time_total_s: 1880.6533710956573
  timers:
    learn_throughput: 8358.373
    learn_time_ms: 19356.878
    sample_throughput: 23589.568
    sample_time_ms: 6858.625
    update_time_ms: 30.889
  timestamp: 1602625027
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     71 |          1880.65 | 11487232 |  254.021 |              305.081 |              74.7778 |            800.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3274.946851890248
    time_step_min: 2939
  date: 2020-10-13_21-37-34
  done: false
  episode_len_mean: 800.1033983594127
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.41545530440547
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 255
  episodes_total: 14507
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.32005507747332257
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044149528645599885
        model: {}
        policy_loss: -0.008633171014177302
        total_loss: 5.359514395395915
        vf_explained_var: 0.9904983639717102
        vf_loss: 5.368306756019592
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.823333333333327
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8566666666666682
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14807051848612066
    mean_env_wait_ms: 1.2193044933965917
    mean_inference_ms: 4.411535075545271
    mean_raw_obs_processing_ms: 0.38333629051855767
  time_since_restore: 1907.0863754749298
  time_this_iter_s: 26.43300437927246
  time_total_s: 1907.0863754749298
  timers:
    learn_throughput: 8353.325
    learn_time_ms: 19368.574
    sample_throughput: 23596.57
    sample_time_ms: 6856.59
    update_time_ms: 32.264
  timestamp: 1602625054
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     72 |          1907.09 | 11649024 |  254.415 |              305.081 |              74.7778 |            800.103 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3273.1265097236437
    time_step_min: 2939
  date: 2020-10-13_21-38-01
  done: false
  episode_len_mean: 799.99101613013
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.6782972995455
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 14693
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.32103421290715534
        entropy_coeff: 0.0005000000000000001
        kl: 0.004831296120149394
        model: {}
        policy_loss: -0.010107869062873457
        total_loss: 4.698035915692647
        vf_explained_var: 0.9900762438774109
        vf_loss: 4.708303809165955
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.86129032258065
    gpu_util_percent0: 0.3906451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14805094216606154
    mean_env_wait_ms: 1.219394034277143
    mean_inference_ms: 4.410037993081459
    mean_raw_obs_processing_ms: 0.38327220023433034
  time_since_restore: 1933.4870309829712
  time_this_iter_s: 26.400655508041382
  time_total_s: 1933.4870309829712
  timers:
    learn_throughput: 8349.189
    learn_time_ms: 19378.169
    sample_throughput: 23576.704
    sample_time_ms: 6862.367
    update_time_ms: 33.685
  timestamp: 1602625081
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     73 |          1933.49 | 11810816 |  254.678 |              305.081 |              74.7778 |            799.991 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3271.532991499123
    time_step_min: 2939
  date: 2020-10-13_21-38-27
  done: false
  episode_len_mean: 799.9195154777927
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.91845439591063
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 14860
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.3286144882440567
        entropy_coeff: 0.0005000000000000001
        kl: 0.004885840035664539
        model: {}
        policy_loss: -0.010596770531265065
        total_loss: 4.796510457992554
        vf_explained_var: 0.9895806312561035
        vf_loss: 4.807271321614583
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.006666666666668
    gpu_util_percent0: 0.3990000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480341077559924
    mean_env_wait_ms: 1.2194666488854529
    mean_inference_ms: 4.408707754369236
    mean_raw_obs_processing_ms: 0.3832122445117205
  time_since_restore: 1959.747343301773
  time_this_iter_s: 26.26031231880188
  time_total_s: 1959.747343301773
  timers:
    learn_throughput: 8350.045
    learn_time_ms: 19376.184
    sample_throughput: 23574.682
    sample_time_ms: 6862.956
    update_time_ms: 33.516
  timestamp: 1602625107
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     74 |          1959.75 | 11972608 |  254.918 |              305.081 |              74.7778 |             799.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3269.2735780181856
    time_step_min: 2939
  date: 2020-10-13_21-38-54
  done: false
  episode_len_mean: 799.7875537901357
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 255.24874364298398
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 245
  episodes_total: 15105
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.3112246294816335
        entropy_coeff: 0.0005000000000000001
        kl: 0.005289243262571593
        model: {}
        policy_loss: -0.008476760820485651
        total_loss: 5.7805165847142534
        vf_explained_var: 0.9898079037666321
        vf_loss: 5.789149085680644
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.006451612903223
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8612903225806465
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14801126953292879
    mean_env_wait_ms: 1.2195747877890937
    mean_inference_ms: 4.406812506141418
    mean_raw_obs_processing_ms: 0.38312177777888373
  time_since_restore: 1986.4440567493439
  time_this_iter_s: 26.6967134475708
  time_total_s: 1986.4440567493439
  timers:
    learn_throughput: 8331.393
    learn_time_ms: 19419.563
    sample_throughput: 23541.187
    sample_time_ms: 6872.72
    update_time_ms: 33.373
  timestamp: 1602625134
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     75 |          1986.44 | 12134400 |  255.249 |              305.081 |              74.7778 |            799.788 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3267.3525292847326
    time_step_min: 2939
  date: 2020-10-13_21-39-21
  done: false
  episode_len_mean: 799.6541549709511
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 255.5499521621331
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 214
  episodes_total: 15319
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.30035021156072617
        entropy_coeff: 0.0005000000000000001
        kl: 0.004706569171200196
        model: {}
        policy_loss: -0.01010256118994827
        total_loss: 5.084058960278829
        vf_explained_var: 0.9900557398796082
        vf_loss: 5.094311555226644
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05
    gpu_util_percent0: 0.26766666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14798887523706736
    mean_env_wait_ms: 1.2196553890557147
    mean_inference_ms: 4.405166760669258
    mean_raw_obs_processing_ms: 0.3830496370606176
  time_since_restore: 2012.9131951332092
  time_this_iter_s: 26.469138383865356
  time_total_s: 2012.9131951332092
  timers:
    learn_throughput: 8324.608
    learn_time_ms: 19435.39
    sample_throughput: 23528.992
    sample_time_ms: 6876.283
    update_time_ms: 30.971
  timestamp: 1602625161
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     76 |          2012.91 | 12296192 |   255.55 |              305.081 |              74.7778 |            799.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3265.9380462225677
    time_step_min: 2939
  date: 2020-10-13_21-39-48
  done: false
  episode_len_mean: 799.5577655795931
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 255.77696565265177
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 166
  episodes_total: 15485
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3129647398988406
        entropy_coeff: 0.0005000000000000001
        kl: 0.005494995159097016
        model: {}
        policy_loss: -0.009574915166012943
        total_loss: 4.7394436200459795
        vf_explained_var: 0.9893284440040588
        vf_loss: 4.749174912770589
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96
    gpu_util_percent0: 0.31466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14797246817584586
    mean_env_wait_ms: 1.219714886860196
    mean_inference_ms: 4.403924027717836
    mean_raw_obs_processing_ms: 0.38299206962460014
  time_since_restore: 2039.332243680954
  time_this_iter_s: 26.41904854774475
  time_total_s: 2039.332243680954
  timers:
    learn_throughput: 8326.323
    learn_time_ms: 19431.386
    sample_throughput: 23497.788
    sample_time_ms: 6885.414
    update_time_ms: 32.735
  timestamp: 1602625188
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     77 |          2039.33 | 12457984 |  255.777 |              305.081 |              74.7778 |            799.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3264.17567394915
    time_step_min: 2939
  date: 2020-10-13_21-40-14
  done: false
  episode_len_mean: 799.4864262044354
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 256.0576810676224
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 15692
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.30396267026662827
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044093645410612226
        model: {}
        policy_loss: -0.010947145744770145
        total_loss: 5.748634099960327
        vf_explained_var: 0.9888679385185242
        vf_loss: 5.7597330411275225
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216666666666672
    gpu_util_percent0: 0.3076666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14795266692995834
    mean_env_wait_ms: 1.2197854484819481
    mean_inference_ms: 4.40242301607788
    mean_raw_obs_processing_ms: 0.38292159069252846
  time_since_restore: 2065.4833657741547
  time_this_iter_s: 26.151122093200684
  time_total_s: 2065.4833657741547
  timers:
    learn_throughput: 8344.016
    learn_time_ms: 19390.184
    sample_throughput: 23471.442
    sample_time_ms: 6893.143
    update_time_ms: 32.646
  timestamp: 1602625214
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     78 |          2065.48 | 12619776 |  256.058 |              305.081 |              74.7778 |            799.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3262.04574628744
    time_step_min: 2939
  date: 2020-10-13_21-40-41
  done: false
  episode_len_mean: 799.4197740112994
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 256.3767049021286
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 15930
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.2848590662082036
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044003649381920695
        model: {}
        policy_loss: -0.008621891727671027
        total_loss: 4.8070762157440186
        vf_explained_var: 0.9914414286613464
        vf_loss: 4.815840562184651
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05483870967742
    gpu_util_percent0: 0.29935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14793248329821673
    mean_env_wait_ms: 1.2198715736068204
    mean_inference_ms: 4.400739334095046
    mean_raw_obs_processing_ms: 0.3828412420298722
  time_since_restore: 2091.9151997566223
  time_this_iter_s: 26.43183398246765
  time_total_s: 2091.9151997566223
  timers:
    learn_throughput: 8340.916
    learn_time_ms: 19397.389
    sample_throughput: 23520.0
    sample_time_ms: 6878.912
    update_time_ms: 31.272
  timestamp: 1602625241
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     79 |          2091.92 | 12781568 |  256.377 |              305.081 |              74.7778 |             799.42 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3260.5543944765814
    time_step_min: 2939
  date: 2020-10-13_21-41-07
  done: false
  episode_len_mean: 799.3571827489916
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 256.6164562158977
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 16115
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2888157243529956
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049531990274166065
        model: {}
        policy_loss: -0.010214789110856751
        total_loss: 4.23901351292928
        vf_explained_var: 0.9907038807868958
        vf_loss: 4.249372760454814
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09666666666667
    gpu_util_percent0: 0.3520000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791459675118418
    mean_env_wait_ms: 1.2199239642582245
    mean_inference_ms: 4.399436809726444
    mean_raw_obs_processing_ms: 0.3827824830746859
  time_since_restore: 2118.1333363056183
  time_this_iter_s: 26.21813654899597
  time_total_s: 2118.1333363056183
  timers:
    learn_throughput: 8347.223
    learn_time_ms: 19382.734
    sample_throughput: 23608.885
    sample_time_ms: 6853.013
    update_time_ms: 32.066
  timestamp: 1602625267
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     80 |          2118.13 | 12943360 |  256.616 |              305.081 |              74.7778 |            799.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3259.040231299213
    time_step_min: 2939
  date: 2020-10-13_21-41-34
  done: false
  episode_len_mean: 799.2862403338652
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 256.84367115366257
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 16294
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2900454079111417
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051606989388043685
        model: {}
        policy_loss: -0.008904293567563096
        total_loss: 5.0030977328618365
        vf_explained_var: 0.9894735217094421
        vf_loss: 5.012146910031636
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.683333333333334
    gpu_util_percent0: 0.381
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14789849294420504
    mean_env_wait_ms: 1.2199740984921004
    mean_inference_ms: 4.398197978046802
    mean_raw_obs_processing_ms: 0.38272393483364225
  time_since_restore: 2144.3179302215576
  time_this_iter_s: 26.18459391593933
  time_total_s: 2144.3179302215576
  timers:
    learn_throughput: 8348.169
    learn_time_ms: 19380.538
    sample_throughput: 23646.353
    sample_time_ms: 6842.155
    update_time_ms: 30.918
  timestamp: 1602625294
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     81 |          2144.32 | 13105152 |  256.844 |              305.687 |              74.7778 |            799.286 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3256.941957787482
    time_step_min: 2939
  date: 2020-10-13_21-42-01
  done: false
  episode_len_mean: 799.224313203437
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.16085458237217
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 232
  episodes_total: 16526
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.276415911813577
        entropy_coeff: 0.0005000000000000001
        kl: 0.004555096966214478
        model: {}
        policy_loss: -0.009362990567751694
        total_loss: 4.814544200897217
        vf_explained_var: 0.9913424849510193
        vf_loss: 4.824045300483704
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.467741935483875
    gpu_util_percent0: 0.37612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788013253089535
    mean_env_wait_ms: 1.220040195974116
    mean_inference_ms: 4.396653517815878
    mean_raw_obs_processing_ms: 0.382647584553975
  time_since_restore: 2170.689857006073
  time_this_iter_s: 26.37192678451538
  time_total_s: 2170.689857006073
  timers:
    learn_throughput: 8349.533
    learn_time_ms: 19377.37
    sample_throughput: 23657.018
    sample_time_ms: 6839.07
    update_time_ms: 30.013
  timestamp: 1602625321
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     82 |          2170.69 | 13266944 |  257.161 |              305.687 |              74.7778 |            799.224 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3255.099185921226
    time_step_min: 2939
  date: 2020-10-13_21-42-27
  done: false
  episode_len_mean: 799.1691949354993
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.4201444690575
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 16744
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.27031079679727554
        entropy_coeff: 0.0005000000000000001
        kl: 0.004333762840057413
        model: {}
        policy_loss: -0.008990503553529075
        total_loss: 4.899519721666972
        vf_explained_var: 0.9904859662055969
        vf_loss: 4.908645232518514
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.843333333333337
    gpu_util_percent0: 0.33199999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14785984717544798
    mean_env_wait_ms: 1.2200925666996052
    mean_inference_ms: 4.395208233349545
    mean_raw_obs_processing_ms: 0.3825802481915672
  time_since_restore: 2196.99275803566
  time_this_iter_s: 26.302901029586792
  time_total_s: 2196.99275803566
  timers:
    learn_throughput: 8353.265
    learn_time_ms: 19368.715
    sample_throughput: 23671.626
    sample_time_ms: 6834.849
    update_time_ms: 30.385
  timestamp: 1602625347
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     83 |          2196.99 | 13428736 |   257.42 |              305.687 |              74.7778 |            799.169 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3253.8381437800035
    time_step_min: 2939
  date: 2020-10-13_21-42-54
  done: false
  episode_len_mean: 799.1290284430253
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.62739332297605
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 16911
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2760617832342784
        entropy_coeff: 0.0005000000000000001
        kl: 0.004863429193695386
        model: {}
        policy_loss: -0.008182110167884579
        total_loss: 4.305712858835856
        vf_explained_var: 0.9904346466064453
        vf_loss: 4.314032951990764
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.47666666666667
    gpu_util_percent0: 0.39733333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478466039867981
    mean_env_wait_ms: 1.2201304867633453
    mean_inference_ms: 4.394140343852886
    mean_raw_obs_processing_ms: 0.3825289896442292
  time_since_restore: 2223.222185611725
  time_this_iter_s: 26.229427576065063
  time_total_s: 2223.222185611725
  timers:
    learn_throughput: 8352.557
    learn_time_ms: 19370.356
    sample_throughput: 23690.524
    sample_time_ms: 6829.397
    update_time_ms: 31.029
  timestamp: 1602625374
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     84 |          2223.22 | 13590528 |  257.627 |              305.687 |              74.7778 |            799.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3251.9842607220176
    time_step_min: 2939
  date: 2020-10-13_21-43-21
  done: false
  episode_len_mean: 799.043143207426
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.90023358106725
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 17129
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.26749257495005924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004825605894438922
        model: {}
        policy_loss: -0.00840260067464745
        total_loss: 4.5115755796432495
        vf_explained_var: 0.9911289811134338
        vf_loss: 4.520111918449402
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32903225806452
    gpu_util_percent0: 0.4045161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782928239633936
    mean_env_wait_ms: 1.2201815382914647
    mean_inference_ms: 4.392774345704514
    mean_raw_obs_processing_ms: 0.38246277186051186
  time_since_restore: 2249.7257735729218
  time_this_iter_s: 26.5035879611969
  time_total_s: 2249.7257735729218
  timers:
    learn_throughput: 8355.61
    learn_time_ms: 19363.277
    sample_throughput: 23739.394
    sample_time_ms: 6815.338
    update_time_ms: 32.268
  timestamp: 1602625401
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     85 |          2249.73 | 13752320 |    257.9 |              305.687 |              74.7778 |            799.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3249.9984991052356
    time_step_min: 2939
  date: 2020-10-13_21-43-47
  done: false
  episode_len_mean: 798.9421692298831
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.1904774372374
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 232
  episodes_total: 17361
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.25233036528031033
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048353523792078095
        model: {}
        policy_loss: -0.0055988599779084325
        total_loss: 5.573948303858439
        vf_explained_var: 0.9899200797080994
        vf_loss: 5.579673250516255
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5
    gpu_util_percent0: 0.3423333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14781068660358498
    mean_env_wait_ms: 1.2202323618698316
    mean_inference_ms: 4.391332994352615
    mean_raw_obs_processing_ms: 0.38239164652391516
  time_since_restore: 2275.881398677826
  time_this_iter_s: 26.155625104904175
  time_total_s: 2275.881398677826
  timers:
    learn_throughput: 8368.393
    learn_time_ms: 19333.7
    sample_throughput: 23749.074
    sample_time_ms: 6812.56
    update_time_ms: 32.797
  timestamp: 1602625427
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     86 |          2275.88 | 13914112 |   258.19 |              305.687 |              74.7778 |            798.942 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3248.472141265215
    time_step_min: 2939
  date: 2020-10-13_21-44-14
  done: false
  episode_len_mean: 798.8470662028853
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.4183662478695
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 17537
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2585073684652646
        entropy_coeff: 0.0005000000000000001
        kl: 0.00489986757747829
        model: {}
        policy_loss: -0.008975959771002332
        total_loss: 3.498164494832357
        vf_explained_var: 0.99190354347229
        vf_loss: 3.5072697401046753
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.032258064516128
    gpu_util_percent0: 0.32935483870967747
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477959416389357
    mean_env_wait_ms: 1.2202626562249141
    mean_inference_ms: 4.3902595850235535
    mean_raw_obs_processing_ms: 0.3823405931708238
  time_since_restore: 2302.4837684631348
  time_this_iter_s: 26.602369785308838
  time_total_s: 2302.4837684631348
  timers:
    learn_throughput: 8360.329
    learn_time_ms: 19352.349
    sample_throughput: 23778.336
    sample_time_ms: 6804.177
    update_time_ms: 31.361
  timestamp: 1602625454
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     87 |          2302.48 | 14075904 |  258.418 |              305.687 |              74.7778 |            798.847 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3246.9174674957603
    time_step_min: 2939
  date: 2020-10-13_21-44-41
  done: false
  episode_len_mean: 798.7289034296028
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.6413201281771
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 17728
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.26109440128008526
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047555947676301
        model: {}
        policy_loss: -0.008867673760202402
        total_loss: 5.586236993471782
        vf_explained_var: 0.98823481798172
        vf_loss: 5.595235109329224
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.369999999999997
    gpu_util_percent0: 0.3350000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778207264952814
    mean_env_wait_ms: 1.220296450807179
    mean_inference_ms: 4.38914255400497
    mean_raw_obs_processing_ms: 0.382285990784238
  time_since_restore: 2329.068227291107
  time_this_iter_s: 26.584458827972412
  time_total_s: 2329.068227291107
  timers:
    learn_throughput: 8346.577
    learn_time_ms: 19384.233
    sample_throughput: 23738.274
    sample_time_ms: 6815.66
    update_time_ms: 30.219
  timestamp: 1602625481
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     88 |          2329.07 | 14237696 |  258.641 |              305.687 |              74.7778 |            798.729 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3245.0426125271906
    time_step_min: 2939
  date: 2020-10-13_21-45-08
  done: false
  episode_len_mean: 798.6408415428285
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.92065082280476
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 239
  episodes_total: 17967
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.24719692642490068
        entropy_coeff: 0.0005000000000000001
        kl: 0.004926367856872578
        model: {}
        policy_loss: -0.00856569221165652
        total_loss: 5.425957957903544
        vf_explained_var: 0.990248441696167
        vf_loss: 5.434647123018901
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.338709677419356
    gpu_util_percent0: 0.314516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477640422661728
    mean_env_wait_ms: 1.2203378489418522
    mean_inference_ms: 4.387715316122769
    mean_raw_obs_processing_ms: 0.38221283607348594
  time_since_restore: 2355.5614421367645
  time_this_iter_s: 26.49321484565735
  time_total_s: 2355.5614421367645
  timers:
    learn_throughput: 8347.721
    learn_time_ms: 19381.578
    sample_throughput: 23715.552
    sample_time_ms: 6822.19
    update_time_ms: 31.072
  timestamp: 1602625508
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     89 |          2355.56 | 14399488 |  258.921 |              305.687 |              74.7778 |            798.641 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3243.501599735216
    time_step_min: 2939
  date: 2020-10-13_21-45-35
  done: false
  episode_len_mean: 798.5295056699329
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.15542800013793
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 18166
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.24320283780495325
        entropy_coeff: 0.0005000000000000001
        kl: 0.004537505408128102
        model: {}
        policy_loss: -0.009045854256934641
        total_loss: 3.8153651356697083
        vf_explained_var: 0.992069661617279
        vf_loss: 3.8245325485865274
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683870967741942
    gpu_util_percent0: 0.3480645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14774853319853887
    mean_env_wait_ms: 1.2203696133017143
    mean_inference_ms: 4.38657185096948
    mean_raw_obs_processing_ms: 0.38215911172774597
  time_since_restore: 2382.115419626236
  time_this_iter_s: 26.553977489471436
  time_total_s: 2382.115419626236
  timers:
    learn_throughput: 8336.871
    learn_time_ms: 19406.801
    sample_throughput: 23695.128
    sample_time_ms: 6828.07
    update_time_ms: 33.222
  timestamp: 1602625535
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     90 |          2382.12 | 14561280 |  259.155 |              305.687 |              74.7778 |             798.53 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3242.1898388418463
    time_step_min: 2939
  date: 2020-10-13_21-46-02
  done: false
  episode_len_mean: 798.4458921659489
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.3590977099127
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 177
  episodes_total: 18343
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.2525920110444228
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049233135068789124
        model: {}
        policy_loss: -0.011446780166200673
        total_loss: 4.483220895131429
        vf_explained_var: 0.9901518225669861
        vf_loss: 4.494793891906738
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080000000000005
    gpu_util_percent0: 0.3983333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477358872694062
    mean_env_wait_ms: 1.2203936183268653
    mean_inference_ms: 4.385576970100786
    mean_raw_obs_processing_ms: 0.38211049231809
  time_since_restore: 2408.443638086319
  time_this_iter_s: 26.328218460083008
  time_total_s: 2408.443638086319
  timers:
    learn_throughput: 8328.761
    learn_time_ms: 19425.7
    sample_throughput: 23714.824
    sample_time_ms: 6822.399
    update_time_ms: 33.252
  timestamp: 1602625562
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     91 |          2408.44 | 14723072 |  259.359 |              305.687 |              74.7778 |            798.446 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3240.496034529269
    time_step_min: 2939
  date: 2020-10-13_21-46-28
  done: false
  episode_len_mean: 798.3073816830884
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.6141237932548
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 230
  episodes_total: 18573
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.24378551791111627
        entropy_coeff: 0.0005000000000000001
        kl: 0.004815731663256884
        model: {}
        policy_loss: -0.011370247438511191
        total_loss: 5.186391592025757
        vf_explained_var: 0.990380585193634
        vf_loss: 5.19788384437561
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.276666666666664
    gpu_util_percent0: 0.285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14772052868344437
    mean_env_wait_ms: 1.2204323197605316
    mean_inference_ms: 4.384324685793424
    mean_raw_obs_processing_ms: 0.3820477176884973
  time_since_restore: 2434.5426564216614
  time_this_iter_s: 26.099018335342407
  time_total_s: 2434.5426564216614
  timers:
    learn_throughput: 8337.833
    learn_time_ms: 19404.562
    sample_throughput: 23735.285
    sample_time_ms: 6816.518
    update_time_ms: 32.663
  timestamp: 1602625588
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     92 |          2434.54 | 14884864 |  259.614 |              305.687 |              74.7778 |            798.307 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3238.8874553500027
    time_step_min: 2938
  date: 2020-10-13_21-46-55
  done: false
  episode_len_mean: 798.1329076882149
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.86059316226914
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 18795
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.2296330196162065
        entropy_coeff: 0.0005000000000000001
        kl: 0.004444085760042071
        model: {}
        policy_loss: -0.008004582765958427
        total_loss: 5.105821490287781
        vf_explained_var: 0.990095853805542
        vf_loss: 5.113940874735515
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.758064516129032
    gpu_util_percent0: 0.2935483870967741
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477041481464712
    mean_env_wait_ms: 1.2204623861789363
    mean_inference_ms: 4.383109234810904
    mean_raw_obs_processing_ms: 0.38198892065917484
  time_since_restore: 2461.143047094345
  time_this_iter_s: 26.600390672683716
  time_total_s: 2461.143047094345
  timers:
    learn_throughput: 8323.95
    learn_time_ms: 19436.926
    sample_throughput: 23752.622
    sample_time_ms: 6811.543
    update_time_ms: 32.994
  timestamp: 1602625615
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     93 |          2461.14 | 15046656 |  259.861 |              305.687 |              74.7778 |            798.133 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3237.6421325161155
    time_step_min: 2929
  date: 2020-10-13_21-47-22
  done: false
  episode_len_mean: 797.9981543978064
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.0608020726139
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 169
  episodes_total: 18964
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.2407698097328345
        entropy_coeff: 0.0005000000000000001
        kl: 0.005226831339920561
        model: {}
        policy_loss: -0.0071348769997712225
        total_loss: 3.945502817630768
        vf_explained_var: 0.9911749958992004
        vf_loss: 3.9527581135431924
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.373333333333335
    gpu_util_percent0: 0.382
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769260812911802
    mean_env_wait_ms: 1.2204825304837499
    mean_inference_ms: 4.382211448674062
    mean_raw_obs_processing_ms: 0.38194500110536267
  time_since_restore: 2487.4207055568695
  time_this_iter_s: 26.277658462524414
  time_total_s: 2487.4207055568695
  timers:
    learn_throughput: 8322.148
    learn_time_ms: 19441.135
    sample_throughput: 23761.192
    sample_time_ms: 6809.086
    update_time_ms: 34.592
  timestamp: 1602625642
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     94 |          2487.42 | 15208448 |  260.061 |              306.596 |              74.7778 |            797.998 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3235.9981716554353
    time_step_min: 2929
  date: 2020-10-13_21-47-49
  done: false
  episode_len_mean: 797.8404671289296
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.31414346794145
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 217
  episodes_total: 19181
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.240451380610466
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044286402796084685
        model: {}
        policy_loss: -0.008429673228723308
        total_loss: 4.66086208820343
        vf_explained_var: 0.9907262325286865
        vf_loss: 4.669412096341451
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.948387096774198
    gpu_util_percent0: 0.35064516129032264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476788152607698
    mean_env_wait_ms: 1.2205155344860297
    mean_inference_ms: 4.381090655101677
    mean_raw_obs_processing_ms: 0.3818884759844761
  time_since_restore: 2513.9479401111603
  time_this_iter_s: 26.52723455429077
  time_total_s: 2513.9479401111603
  timers:
    learn_throughput: 8324.545
    learn_time_ms: 19435.537
    sample_throughput: 23742.797
    sample_time_ms: 6814.361
    update_time_ms: 35.813
  timestamp: 1602625669
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     95 |          2513.95 | 15370240 |  260.314 |              306.596 |              74.7778 |             797.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3234.4301785529983
    time_step_min: 2929
  date: 2020-10-13_21-48-15
  done: false
  episode_len_mean: 797.6979295426453
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.57312307250504
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 19416
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.22621110330025354
        entropy_coeff: 0.0005000000000000001
        kl: 0.004403940790022413
        model: {}
        policy_loss: -0.01013193831507427
        total_loss: 4.560421784718831
        vf_explained_var: 0.9914420247077942
        vf_loss: 4.570666670799255
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91
    gpu_util_percent0: 0.36066666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14766292101972098
    mean_env_wait_ms: 1.2205437877525713
    mean_inference_ms: 4.37987571427023
    mean_raw_obs_processing_ms: 0.38182853755194734
  time_since_restore: 2540.185180902481
  time_this_iter_s: 26.2372407913208
  time_total_s: 2540.185180902481
  timers:
    learn_throughput: 8324.171
    learn_time_ms: 19436.411
    sample_throughput: 23722.417
    sample_time_ms: 6820.216
    update_time_ms: 35.689
  timestamp: 1602625695
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     96 |          2540.19 | 15532032 |  260.573 |              306.596 |              74.7778 |            797.698 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3233.11138386008
    time_step_min: 2929
  date: 2020-10-13_21-48-42
  done: false
  episode_len_mean: 797.6013168640261
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.7733139892184
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 19592
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.22750935579339662
        entropy_coeff: 0.0005000000000000001
        kl: 0.005514377650494377
        model: {}
        policy_loss: -0.009730751354557773
        total_loss: 3.740702509880066
        vf_explained_var: 0.9914469122886658
        vf_loss: 3.750546932220459
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.720000000000006
    gpu_util_percent0: 0.30499999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765079804868372
    mean_env_wait_ms: 1.220559751931179
    mean_inference_ms: 4.37898187619347
    mean_raw_obs_processing_ms: 0.38178590852747496
  time_since_restore: 2566.800570011139
  time_this_iter_s: 26.615389108657837
  time_total_s: 2566.800570011139
  timers:
    learn_throughput: 8328.161
    learn_time_ms: 19427.098
    sample_throughput: 23663.419
    sample_time_ms: 6837.22
    update_time_ms: 36.601
  timestamp: 1602625722
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     97 |           2566.8 | 15693824 |  260.773 |              306.596 |              74.7778 |            797.601 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3231.7170346364187
    time_step_min: 2929
  date: 2020-10-13_21-49-09
  done: false
  episode_len_mean: 797.479025573638
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.9903737669835
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 194
  episodes_total: 19786
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.2317101942996184
        entropy_coeff: 0.0005000000000000001
        kl: 0.004682655485036473
        model: {}
        policy_loss: -0.009470400982536376
        total_loss: 4.304901401201884
        vf_explained_var: 0.9910011887550354
        vf_loss: 4.31448769569397
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65483870967742
    gpu_util_percent0: 0.3493548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14763817272862884
    mean_env_wait_ms: 1.220579023132081
    mean_inference_ms: 4.378017767688997
    mean_raw_obs_processing_ms: 0.3817383589621369
  time_since_restore: 2593.240492582321
  time_this_iter_s: 26.43992257118225
  time_total_s: 2593.240492582321
  timers:
    learn_throughput: 8338.483
    learn_time_ms: 19403.05
    sample_throughput: 23639.143
    sample_time_ms: 6844.241
    update_time_ms: 38.051
  timestamp: 1602625749
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     98 |          2593.24 | 15855616 |   260.99 |              306.596 |              74.7778 |            797.479 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3230.1126401120896
    time_step_min: 2929
  date: 2020-10-13_21-49-36
  done: false
  episode_len_mean: 797.3846269103985
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.2417129036847
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 236
  episodes_total: 20022
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.2173848090072473
        entropy_coeff: 0.0005000000000000001
        kl: 0.004540561775987347
        model: {}
        policy_loss: -0.00959299021148278
        total_loss: 4.721267620722453
        vf_explained_var: 0.9914255142211914
        vf_loss: 4.730969150861104
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.983870967741936
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762357324419825
    mean_env_wait_ms: 1.2206029359650175
    mean_inference_ms: 4.37689249355956
    mean_raw_obs_processing_ms: 0.38168014987247856
  time_since_restore: 2619.9476039409637
  time_this_iter_s: 26.707111358642578
  time_total_s: 2619.9476039409637
  timers:
    learn_throughput: 8328.078
    learn_time_ms: 19427.291
    sample_throughput: 23648.179
    sample_time_ms: 6841.626
    update_time_ms: 37.136
  timestamp: 1602625776
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |     99 |          2619.95 | 16017408 |  261.242 |              306.596 |              74.7778 |            797.385 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3228.6634296189864
    time_step_min: 2929
  date: 2020-10-13_21-50-03
  done: false
  episode_len_mean: 797.3345531872806
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.458628618413
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 20221
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.20883943513035774
        entropy_coeff: 0.0005000000000000001
        kl: 0.004350139565455417
        model: {}
        policy_loss: -0.008186976653936048
        total_loss: 3.8756356040636697
        vf_explained_var: 0.9918270111083984
        vf_loss: 3.8839269479115806
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.629999999999995
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14761103661362068
    mean_env_wait_ms: 1.2206199935230826
    mean_inference_ms: 4.375924954639845
    mean_raw_obs_processing_ms: 0.3816338385262419
  time_since_restore: 2646.282563686371
  time_this_iter_s: 26.334959745407104
  time_total_s: 2646.282563686371
  timers:
    learn_throughput: 8335.656
    learn_time_ms: 19409.629
    sample_throughput: 23657.561
    sample_time_ms: 6838.913
    update_time_ms: 34.414
  timestamp: 1602625803
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    100 |          2646.28 | 16179200 |  261.459 |              306.596 |              74.7778 |            797.335 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3227.3365422396855
    time_step_min: 2929
  date: 2020-10-13_21-50-30
  done: false
  episode_len_mean: 797.2962055103442
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.6589485402114
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 177
  episodes_total: 20398
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.22018000856041908
        entropy_coeff: 0.0005000000000000001
        kl: 0.005238248811413844
        model: {}
        policy_loss: -0.011148986018573245
        total_loss: 3.3956047097841897
        vf_explained_var: 0.9924342632293701
        vf_loss: 3.4068637688954673
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49333333333334
    gpu_util_percent0: 0.271
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476002981117163
    mean_env_wait_ms: 1.220629313495421
    mean_inference_ms: 4.375100162313686
    mean_raw_obs_processing_ms: 0.38159306389729497
  time_since_restore: 2672.652087688446
  time_this_iter_s: 26.369524002075195
  time_total_s: 2672.652087688446
  timers:
    learn_throughput: 8341.147
    learn_time_ms: 19396.853
    sample_throughput: 23601.861
    sample_time_ms: 6855.053
    update_time_ms: 33.999
  timestamp: 1602625830
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    101 |          2672.65 | 16340992 |  261.659 |              306.596 |              74.7778 |            797.296 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3225.6533547101976
    time_step_min: 2929
  date: 2020-10-13_21-50-56
  done: false
  episode_len_mean: 797.2596867271228
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.91837290513394
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 20621
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21474727243185043
        entropy_coeff: 0.0005000000000000001
        kl: 0.004982666151287655
        model: {}
        policy_loss: -0.008769697775278473
        total_loss: 3.3455925782521567
        vf_explained_var: 0.9934398531913757
        vf_loss: 3.354469656944275
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.361290322580643
    gpu_util_percent0: 0.2961290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14758792260417483
    mean_env_wait_ms: 1.220649395940866
    mean_inference_ms: 4.374103960067205
    mean_raw_obs_processing_ms: 0.3815420139322737
  time_since_restore: 2698.8923695087433
  time_this_iter_s: 26.24028182029724
  time_total_s: 2698.8923695087433
  timers:
    learn_throughput: 8339.384
    learn_time_ms: 19400.953
    sample_throughput: 23570.296
    sample_time_ms: 6864.233
    update_time_ms: 34.119
  timestamp: 1602625856
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    102 |          2698.89 | 16502784 |  261.918 |              306.596 |              74.7778 |             797.26 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3223.9988947621337
    time_step_min: 2929
  date: 2020-10-13_21-51-23
  done: false
  episode_len_mean: 797.2032329240214
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 262.16717927548706
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 20848
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.20586828514933586
        entropy_coeff: 0.0005000000000000001
        kl: 0.004019270612237354
        model: {}
        policy_loss: -0.007916943733713802
        total_loss: 3.878064215183258
        vf_explained_var: 0.9924182295799255
        vf_loss: 3.8860840598742166
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.336666666666666
    gpu_util_percent0: 0.36499999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757358940498488
    mean_env_wait_ms: 1.220658206241881
    mean_inference_ms: 4.373037158504362
    mean_raw_obs_processing_ms: 0.38149007786939876
  time_since_restore: 2725.201361179352
  time_this_iter_s: 26.30899167060852
  time_total_s: 2725.201361179352
  timers:
    learn_throughput: 8352.692
    learn_time_ms: 19370.042
    sample_throughput: 23556.017
    sample_time_ms: 6868.394
    update_time_ms: 33.657
  timestamp: 1602625883
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    103 |           2725.2 | 16664576 |  262.167 |              306.596 |              74.7778 |            797.203 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3222.746496329488
    time_step_min: 2929
  date: 2020-10-13_21-51-50
  done: false
  episode_len_mean: 797.1631614008375
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 262.3537660579914
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 168
  episodes_total: 21016
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.21039297183354697
        entropy_coeff: 0.0005000000000000001
        kl: 0.004545592547704776
        model: {}
        policy_loss: -0.01044849571674907
        total_loss: 2.9206919272740683
        vf_explained_var: 0.9932246208190918
        vf_loss: 2.9312456051508584
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612907
    gpu_util_percent0: 0.29677419354838713
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756422435081748
    mean_env_wait_ms: 1.2206649799262064
    mean_inference_ms: 4.37229905576175
    mean_raw_obs_processing_ms: 0.3814532115197248
  time_since_restore: 2751.696010351181
  time_this_iter_s: 26.494649171829224
  time_total_s: 2751.696010351181
  timers:
    learn_throughput: 8346.052
    learn_time_ms: 19385.454
    sample_throughput: 23558.663
    sample_time_ms: 6867.622
    update_time_ms: 31.626
  timestamp: 1602625910
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    104 |           2751.7 | 16826368 |  262.354 |              306.596 |              74.7778 |            797.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3221.236968838527
    time_step_min: 2929
  date: 2020-10-13_21-52-17
  done: false
  episode_len_mean: 797.1100952021868
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 262.5738671472953
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 21218
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.21486473580201468
        entropy_coeff: 0.0005000000000000001
        kl: 0.00453416813009729
        model: {}
        policy_loss: -0.009656874948025992
        total_loss: 3.907158533732096
        vf_explained_var: 0.9920961260795593
        vf_loss: 3.916922847429911
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.786666666666672
    gpu_util_percent0: 0.302
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14755237783794853
    mean_env_wait_ms: 1.2206722375725156
    mean_inference_ms: 4.371399812449149
    mean_raw_obs_processing_ms: 0.38140664555514237
  time_since_restore: 2778.1842045783997
  time_this_iter_s: 26.488194227218628
  time_total_s: 2778.1842045783997
  timers:
    learn_throughput: 8352.17
    learn_time_ms: 19371.253
    sample_throughput: 23516.28
    sample_time_ms: 6880.0
    update_time_ms: 28.771
  timestamp: 1602625937
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    105 |          2778.18 | 16988160 |  262.574 |              306.596 |              74.7778 |             797.11 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3219.373937809319
    time_step_min: 2924
  date: 2020-10-13_21-52-44
  done: false
  episode_len_mean: 797.0404548844147
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 262.84718832621513
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 21456
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.19974111268917719
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049743565420309705
        model: {}
        policy_loss: -0.009263245214242488
        total_loss: 3.4402349392573037
        vf_explained_var: 0.99342280626297
        vf_loss: 3.4495980739593506
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.47741935483871
    gpu_util_percent0: 0.3309677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753908303363866
    mean_env_wait_ms: 1.2206744866299808
    mean_inference_ms: 4.370369965103189
    mean_raw_obs_processing_ms: 0.3813539399861047
  time_since_restore: 2804.8873462677
  time_this_iter_s: 26.703141689300537
  time_total_s: 2804.8873462677
  timers:
    learn_throughput: 8338.25
    learn_time_ms: 19403.591
    sample_throughput: 23475.371
    sample_time_ms: 6891.989
    update_time_ms: 30.678
  timestamp: 1602625964
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    106 |          2804.89 | 17149952 |  262.847 |              307.354 |              74.7778 |             797.04 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3217.991576413959
    time_step_min: 2924
  date: 2020-10-13_21-53-11
  done: false
  episode_len_mean: 797.0049898355203
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.0521543283509
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 21644
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.195981714874506
        entropy_coeff: 0.0005000000000000001
        kl: 0.00416200408168758
        model: {}
        policy_loss: -0.01051843585203945
        total_loss: 3.1847651402155557
        vf_explained_var: 0.9928480982780457
        vf_loss: 3.1953815619150796
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.346666666666657
    gpu_util_percent0: 0.2779999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752856855905586
    mean_env_wait_ms: 1.2206807963308057
    mean_inference_ms: 4.369575188083549
    mean_raw_obs_processing_ms: 0.38131537812218125
  time_since_restore: 2831.1374928951263
  time_this_iter_s: 26.250146627426147
  time_total_s: 2831.1374928951263
  timers:
    learn_throughput: 8345.672
    learn_time_ms: 19386.335
    sample_throughput: 23544.843
    sample_time_ms: 6871.654
    update_time_ms: 31.365
  timestamp: 1602625991
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    107 |          2831.14 | 17311744 |  263.052 |              307.354 |              74.7778 |            797.005 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3216.5273953744495
    time_step_min: 2924
  date: 2020-10-13_21-53-38
  done: false
  episode_len_mean: 796.9402198808979
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.270982847254
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 21830
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.20648310085137686
        entropy_coeff: 0.0005000000000000001
        kl: 0.004946530524951716
        model: {}
        policy_loss: -0.00904949654553396
        total_loss: 2.99210395415624
        vf_explained_var: 0.9933214783668518
        vf_loss: 3.0012566447257996
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61612903225807
    gpu_util_percent0: 0.34774193548387095
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475187731842399
    mean_env_wait_ms: 1.2206826471693337
    mean_inference_ms: 4.368803862433722
    mean_raw_obs_processing_ms: 0.3812769426559984
  time_since_restore: 2857.511229991913
  time_this_iter_s: 26.3737370967865
  time_total_s: 2857.511229991913
  timers:
    learn_throughput: 8339.231
    learn_time_ms: 19401.309
    sample_throughput: 23624.749
    sample_time_ms: 6848.411
    update_time_ms: 31.82
  timestamp: 1602626018
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    108 |          2857.51 | 17473536 |  263.271 |              307.354 |              74.7778 |             796.94 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3214.659736722651
    time_step_min: 2924
  date: 2020-10-13_21-54-04
  done: false
  episode_len_mean: 796.834556824361
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.5297620028452
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 22068
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.19852573176225027
        entropy_coeff: 0.0005000000000000001
        kl: 0.004543304055308302
        model: {}
        policy_loss: -0.009153506873796383
        total_loss: 3.8071552515029907
        vf_explained_var: 0.9927506446838379
        vf_loss: 3.816408097743988
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45
    gpu_util_percent0: 0.318
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750594103077574
    mean_env_wait_ms: 1.220683234779084
    mean_inference_ms: 4.367819114734617
    mean_raw_obs_processing_ms: 0.381226722135137
  time_since_restore: 2883.830638885498
  time_this_iter_s: 26.319408893585205
  time_total_s: 2883.830638885498
  timers:
    learn_throughput: 8352.934
    learn_time_ms: 19369.482
    sample_throughput: 23651.367
    sample_time_ms: 6840.704
    update_time_ms: 32.002
  timestamp: 1602626044
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    109 |          2883.83 | 17635328 |   263.53 |              307.354 |              74.7778 |            796.835 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3213.098979178846
    time_step_min: 2924
  date: 2020-10-13_21-54-31
  done: false
  episode_len_mean: 796.7334231200898
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.7613168724279
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 22275
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.18953226755062738
        entropy_coeff: 0.0005000000000000001
        kl: 0.00424206592530633
        model: {}
        policy_loss: -0.008345151819109256
        total_loss: 3.069048523902893
        vf_explained_var: 0.993450939655304
        vf_loss: 3.077488442262014
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.75161290322581
    gpu_util_percent0: 0.30774193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749489918179393
    mean_env_wait_ms: 1.2206843310728908
    mean_inference_ms: 4.366976340760897
    mean_raw_obs_processing_ms: 0.3811840909341199
  time_since_restore: 2910.1875450611115
  time_this_iter_s: 26.356906175613403
  time_total_s: 2910.1875450611115
  timers:
    learn_throughput: 8357.095
    learn_time_ms: 19359.837
    sample_throughput: 23636.537
    sample_time_ms: 6844.996
    update_time_ms: 38.21
  timestamp: 1602626071
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    110 |          2910.19 | 17797120 |  263.761 |              307.354 |              74.7778 |            796.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3211.845934124788
    time_step_min: 2924
  date: 2020-10-13_21-54-58
  done: false
  episode_len_mean: 796.6694885047228
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.9520728583284
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 169
  episodes_total: 22444
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.19718386108676592
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038958047516644
        model: {}
        policy_loss: -0.008478434955274375
        total_loss: 2.9254135886828103
        vf_explained_var: 0.9932602047920227
        vf_loss: 2.9339906175931296
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.476666666666667
    gpu_util_percent0: 0.3403333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748680274556314
    mean_env_wait_ms: 1.220681327027223
    mean_inference_ms: 4.366318747268299
    mean_raw_obs_processing_ms: 0.38115116321474285
  time_since_restore: 2936.6240639686584
  time_this_iter_s: 26.436518907546997
  time_total_s: 2936.6240639686584
  timers:
    learn_throughput: 8349.836
    learn_time_ms: 19376.669
    sample_throughput: 23674.048
    sample_time_ms: 6834.15
    update_time_ms: 38.244
  timestamp: 1602626098
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    111 |          2936.62 | 17958912 |  263.952 |              307.354 |              74.7778 |            796.669 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3210.136917911107
    time_step_min: 2924
  date: 2020-10-13_21-55-25
  done: false
  episode_len_mean: 796.5705275229358
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 264.21281044388843
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 228
  episodes_total: 22672
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.1978266810377439
        entropy_coeff: 0.0005000000000000001
        kl: 0.00442284275777638
        model: {}
        policy_loss: -0.01004474914225284
        total_loss: 3.4646064043045044
        vf_explained_var: 0.9931136965751648
        vf_loss: 3.474750022093455
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.712903225806453
    gpu_util_percent0: 0.27290322580645165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747499028622305
    mean_env_wait_ms: 1.220681091946533
    mean_inference_ms: 4.365443393840694
    mean_raw_obs_processing_ms: 0.3811069753244405
  time_since_restore: 2963.0388457775116
  time_this_iter_s: 26.41478180885315
  time_total_s: 2963.0388457775116
  timers:
    learn_throughput: 8345.026
    learn_time_ms: 19387.838
    sample_throughput: 23660.776
    sample_time_ms: 6837.984
    update_time_ms: 39.512
  timestamp: 1602626125
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    112 |          2963.04 | 18120704 |  264.213 |              307.354 |              74.7778 |            796.571 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3208.55666856218
    time_step_min: 2924
  date: 2020-10-13_21-55-52
  done: false
  episode_len_mean: 796.4563518057557
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 264.44784409005547
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 22899
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.18136985103289285
        entropy_coeff: 0.0005000000000000001
        kl: 0.004298216934936742
        model: {}
        policy_loss: -0.006906143156811595
        total_loss: 3.3213775555292764
        vf_explained_var: 0.9935495853424072
        vf_loss: 3.3283743262290955
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92
    gpu_util_percent0: 0.37933333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746372695762672
    mean_env_wait_ms: 1.2206761944027085
    mean_inference_ms: 4.364553754021323
    mean_raw_obs_processing_ms: 0.3810617540463669
  time_since_restore: 2989.358511209488
  time_this_iter_s: 26.31966543197632
  time_total_s: 2989.358511209488
  timers:
    learn_throughput: 8345.786
    learn_time_ms: 19386.071
    sample_throughput: 23656.472
    sample_time_ms: 6839.228
    update_time_ms: 38.185
  timestamp: 1602626152
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    113 |          2989.36 | 18282496 |  264.448 |              307.808 |              74.7778 |            796.456 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3207.35630237506
    time_step_min: 2924
  date: 2020-10-13_21-56-18
  done: false
  episode_len_mean: 796.37795309723
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 264.6342360708826
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 170
  episodes_total: 23069
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.18563113113244376
        entropy_coeff: 0.0005000000000000001
        kl: 0.004801140322039525
        model: {}
        policy_loss: -0.010135711966237674
        total_loss: 2.932145377000173
        vf_explained_var: 0.9931931495666504
        vf_loss: 2.9423738916714988
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.029999999999998
    gpu_util_percent0: 0.275
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474552105125313
    mean_env_wait_ms: 1.2206714646300643
    mean_inference_ms: 4.36389765795874
    mean_raw_obs_processing_ms: 0.38102817822290386
  time_since_restore: 3015.617859840393
  time_this_iter_s: 26.25934863090515
  time_total_s: 3015.617859840393
  timers:
    learn_throughput: 8363.794
    learn_time_ms: 19344.331
    sample_throughput: 23565.369
    sample_time_ms: 6865.668
    update_time_ms: 38.188
  timestamp: 1602626178
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    114 |          3015.62 | 18444288 |  264.634 |              307.808 |              74.7778 |            796.378 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3205.8207246751012
    time_step_min: 2924
  date: 2020-10-13_21-56-45
  done: false
  episode_len_mean: 796.2741020793951
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 264.86940551762683
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 23276
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.19106416776776314
        entropy_coeff: 0.0005000000000000001
        kl: 0.004714735162754853
        model: {}
        policy_loss: -0.008473456546198577
        total_loss: 3.303392847379049
        vf_explained_var: 0.9929433465003967
        vf_loss: 3.3119618693987527
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.412903225806453
    gpu_util_percent0: 0.37774193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744512045200164
    mean_env_wait_ms: 1.2206678235219068
    mean_inference_ms: 4.36311989858345
    mean_raw_obs_processing_ms: 0.3809885399431608
  time_since_restore: 3042.299596786499
  time_this_iter_s: 26.681736946105957
  time_total_s: 3042.299596786499
  timers:
    learn_throughput: 8354.812
    learn_time_ms: 19365.128
    sample_throughput: 23578.55
    sample_time_ms: 6861.83
    update_time_ms: 39.845
  timestamp: 1602626205
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    115 |           3042.3 | 18606080 |  264.869 |              307.808 |              74.7778 |            796.274 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3204.0219798943604
    time_step_min: 2924
  date: 2020-10-13_21-57-12
  done: false
  episode_len_mean: 796.1630092710725
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.1257200739211
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 23514
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.18019420405228934
        entropy_coeff: 0.0005000000000000001
        kl: 0.004146733049613734
        model: {}
        policy_loss: -0.008227977940502266
        total_loss: 3.778904696305593
        vf_explained_var: 0.9928925037384033
        vf_loss: 3.7872227827707925
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22903225806452
    gpu_util_percent0: 0.2983870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743396962593386
    mean_env_wait_ms: 1.2206562028960029
    mean_inference_ms: 4.362251022218853
    mean_raw_obs_processing_ms: 0.3809432698758061
  time_since_restore: 3068.563863992691
  time_this_iter_s: 26.264267206192017
  time_total_s: 3068.563863992691
  timers:
    learn_throughput: 8369.16
    learn_time_ms: 19331.927
    sample_throughput: 23612.91
    sample_time_ms: 6851.845
    update_time_ms: 38.018
  timestamp: 1602626232
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    116 |          3068.56 | 18767872 |  265.126 |              307.808 |              74.7778 |            796.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3202.6726120033813
    time_step_min: 2924
  date: 2020-10-13_21-57-39
  done: false
  episode_len_mean: 796.1047767744113
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.31463423158925
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 23698
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.1740105872352918
        entropy_coeff: 0.0005000000000000001
        kl: 0.003510866896249354
        model: {}
        policy_loss: -0.007805104488700938
        total_loss: 2.7357622186342874
        vf_explained_var: 0.9937769770622253
        vf_loss: 2.7436543107032776
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.829999999999995
    gpu_util_percent0: 0.39599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474249730951065
    mean_env_wait_ms: 1.2206507437723935
    mean_inference_ms: 4.361574595908319
    mean_raw_obs_processing_ms: 0.38090981248134875
  time_since_restore: 3094.9074652194977
  time_this_iter_s: 26.34360122680664
  time_total_s: 3094.9074652194977
  timers:
    learn_throughput: 8368.804
    learn_time_ms: 19332.75
    sample_throughput: 23592.049
    sample_time_ms: 6857.904
    update_time_ms: 38.005
  timestamp: 1602626259
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    117 |          3094.91 | 18929664 |  265.315 |              307.808 |              74.7778 |            796.105 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3201.392276081852
    time_step_min: 2924
  date: 2020-10-13_21-58-06
  done: false
  episode_len_mean: 796.054718244997
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.51212451061735
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 23886
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.18454369033376375
        entropy_coeff: 0.0005000000000000001
        kl: 0.004071201061985145
        model: {}
        policy_loss: -0.01007609095237664
        total_loss: 2.79981122414271
        vf_explained_var: 0.9939806461334229
        vf_loss: 2.809979736804962
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216129032258063
    gpu_util_percent0: 0.35129032258064513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741614079404242
    mean_env_wait_ms: 1.220640591954961
    mean_inference_ms: 4.360896254965422
    mean_raw_obs_processing_ms: 0.3808756751483167
  time_since_restore: 3121.35564160347
  time_this_iter_s: 26.448176383972168
  time_total_s: 3121.35564160347
  timers:
    learn_throughput: 8368.834
    learn_time_ms: 19332.68
    sample_throughput: 23568.221
    sample_time_ms: 6864.837
    update_time_ms: 37.534
  timestamp: 1602626286
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    118 |          3121.36 | 19091456 |  265.512 |              307.808 |              74.7778 |            796.055 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3199.693227587925
    time_step_min: 2924
  date: 2020-10-13_21-58-33
  done: false
  episode_len_mean: 795.9970150491273
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.7624786482628
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 24121
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.17652543634176254
        entropy_coeff: 0.0005000000000000001
        kl: 0.004322420184810956
        model: {}
        policy_loss: -0.008132235573915144
        total_loss: 2.433333079020182
        vf_explained_var: 0.9953238368034363
        vf_loss: 2.4415535926818848
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11666666666667
    gpu_util_percent0: 0.32333333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740616080950764
    mean_env_wait_ms: 1.22062894456466
    mean_inference_ms: 4.360079752675245
    mean_raw_obs_processing_ms: 0.3808328255649956
  time_since_restore: 3147.6747963428497
  time_this_iter_s: 26.319154739379883
  time_total_s: 3147.6747963428497
  timers:
    learn_throughput: 8367.315
    learn_time_ms: 19336.192
    sample_throughput: 23582.487
    sample_time_ms: 6860.684
    update_time_ms: 37.373
  timestamp: 1602626313
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    119 |          3147.67 | 19253248 |  265.762 |              307.808 |              74.7778 |            795.997 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3198.193832599119
    time_step_min: 2924
  date: 2020-10-13_21-58-59
  done: false
  episode_len_mean: 795.9422863485016
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.9848104923946
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 206
  episodes_total: 24327
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1611424870789051
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031780594727024436
        model: {}
        policy_loss: -0.008321759795459608
        total_loss: 2.27426278591156
        vf_explained_var: 0.9950885772705078
        vf_loss: 2.2826651334762573
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709678
    gpu_util_percent0: 0.3641935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739667861646294
    mean_env_wait_ms: 1.2206183332483933
    mean_inference_ms: 4.359362086332555
    mean_raw_obs_processing_ms: 0.38079709249791727
  time_since_restore: 3173.9439816474915
  time_this_iter_s: 26.269185304641724
  time_total_s: 3173.9439816474915
  timers:
    learn_throughput: 8368.168
    learn_time_ms: 19334.22
    sample_throughput: 23581.355
    sample_time_ms: 6861.014
    update_time_ms: 29.987
  timestamp: 1602626339
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    120 |          3173.94 | 19415040 |  265.985 |              307.808 |              74.7778 |            795.942 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3197.0685989943177
    time_step_min: 2924
  date: 2020-10-13_21-59-27
  done: false
  episode_len_mean: 795.9045267153762
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.1595719635639
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 172
  episodes_total: 24499
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.16785553097724915
        entropy_coeff: 0.0005000000000000001
        kl: 0.003992234162675838
        model: {}
        policy_loss: -0.009055366480121544
        total_loss: 2.390176753203074
        vf_explained_var: 0.9945241808891296
        vf_loss: 2.399316112200419
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.083870967741934
    gpu_util_percent0: 0.29419354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473890984717866
    mean_env_wait_ms: 1.2206059991892872
    mean_inference_ms: 4.358770187633348
    mean_raw_obs_processing_ms: 0.3807671467309108
  time_since_restore: 3200.507026910782
  time_this_iter_s: 26.563045263290405
  time_total_s: 3200.507026910782
  timers:
    learn_throughput: 8362.294
    learn_time_ms: 19347.802
    sample_throughput: 23598.333
    sample_time_ms: 6856.077
    update_time_ms: 30.319
  timestamp: 1602626367
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    121 |          3200.51 | 19576832 |   266.16 |              307.808 |              74.7778 |            795.905 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3195.478954830869
    time_step_min: 2924
  date: 2020-10-13_21-59-54
  done: false
  episode_len_mean: 795.8419690167051
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.3957534328848
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 224
  episodes_total: 24723
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.16944307213028273
        entropy_coeff: 0.0005000000000000001
        kl: 0.004014179537383218
        model: {}
        policy_loss: -0.007565232701987649
        total_loss: 2.3067109187444053
        vf_explained_var: 0.9953827261924744
        vf_loss: 2.3143608371416726
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.735483870967744
    gpu_util_percent0: 0.30612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737981720518684
    mean_env_wait_ms: 1.2205913996702336
    mean_inference_ms: 4.358036457804717
    mean_raw_obs_processing_ms: 0.3807297660178764
  time_since_restore: 3227.1290225982666
  time_this_iter_s: 26.62199568748474
  time_total_s: 3227.1290225982666
  timers:
    learn_throughput: 8350.759
    learn_time_ms: 19374.526
    sample_throughput: 23623.638
    sample_time_ms: 6848.734
    update_time_ms: 30.907
  timestamp: 1602626394
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    122 |          3227.13 | 19738624 |  266.396 |              307.808 |              74.7778 |            795.842 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3193.8962665596146
    time_step_min: 2924
  date: 2020-10-13_22-00-21
  done: false
  episode_len_mean: 795.8021083854417
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.635059104756
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 225
  episodes_total: 24948
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.15401445453365645
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046234835948174196
        model: {}
        policy_loss: -0.007929744202556321
        total_loss: 2.3736257354418435
        vf_explained_var: 0.9951786994934082
        vf_loss: 2.381632447242737
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77
    gpu_util_percent0: 0.3093333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736984993910301
    mean_env_wait_ms: 1.2205723413268927
    mean_inference_ms: 4.3572766369194404
    mean_raw_obs_processing_ms: 0.38069050570114216
  time_since_restore: 3253.619471311569
  time_this_iter_s: 26.490448713302612
  time_total_s: 3253.619471311569
  timers:
    learn_throughput: 8343.968
    learn_time_ms: 19390.294
    sample_throughput: 23620.366
    sample_time_ms: 6849.682
    update_time_ms: 32.047
  timestamp: 1602626421
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    123 |          3253.62 | 19900416 |  266.635 |              307.808 |              74.7778 |            795.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3192.713721894435
    time_step_min: 2924
  date: 2020-10-13_22-00-48
  done: false
  episode_len_mean: 795.7593742536422
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.8187813168706
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 174
  episodes_total: 25122
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.15736312294999757
        entropy_coeff: 0.0005000000000000001
        kl: 0.004359201528131962
        model: {}
        policy_loss: -0.009375940154617032
        total_loss: 1.9298242429892223
        vf_explained_var: 0.9953746795654297
        vf_loss: 1.9392788012822468
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8741935483871
    gpu_util_percent0: 0.3006451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473624163975974
    mean_env_wait_ms: 1.2205610341240072
    mean_inference_ms: 4.35670343761367
    mean_raw_obs_processing_ms: 0.38066102607943675
  time_since_restore: 3279.8391528129578
  time_this_iter_s: 26.21968150138855
  time_total_s: 3279.8391528129578
  timers:
    learn_throughput: 8339.503
    learn_time_ms: 19400.677
    sample_throughput: 23677.767
    sample_time_ms: 6833.077
    update_time_ms: 32.058
  timestamp: 1602626448
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    124 |          3279.84 | 20062208 |  266.819 |              307.808 |              74.7778 |            795.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3191.3044406659023
    time_step_min: 2924
  date: 2020-10-13_22-01-14
  done: false
  episode_len_mean: 795.6953054052987
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 267.02972832522323
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 205
  episodes_total: 25327
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.17259028926491737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520415289637943
        model: {}
        policy_loss: -0.0073667444521561265
        total_loss: 2.7579530278841653
        vf_explained_var: 0.9940579533576965
        vf_loss: 2.7654060324033103
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.47666666666667
    gpu_util_percent0: 0.3243333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473538699266201
    mean_env_wait_ms: 1.2205423235021433
    mean_inference_ms: 4.356032809426099
    mean_raw_obs_processing_ms: 0.38062534147925436
  time_since_restore: 3306.061780691147
  time_this_iter_s: 26.222627878189087
  time_total_s: 3306.061780691147
  timers:
    learn_throughput: 8354.018
    learn_time_ms: 19366.968
    sample_throughput: 23718.959
    sample_time_ms: 6821.21
    update_time_ms: 30.75
  timestamp: 1602626474
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    125 |          3306.06 | 20224000 |   267.03 |              307.808 |              74.7778 |            795.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3189.7585801598498
    time_step_min: 2924
  date: 2020-10-13_22-01-41
  done: false
  episode_len_mean: 795.6599248885063
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 267.26050545356543
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 25562
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.1551519309480985
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035465058172121644
        model: {}
        policy_loss: -0.008046312674802417
        total_loss: 2.974668542544047
        vf_explained_var: 0.9942945837974548
        vf_loss: 2.9827925165494285
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.356666666666666
    gpu_util_percent0: 0.3173333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734467752770877
    mean_env_wait_ms: 1.220520111208655
    mean_inference_ms: 4.355305429022602
    mean_raw_obs_processing_ms: 0.3805878343889927
  time_since_restore: 3332.4247834682465
  time_this_iter_s: 26.36300277709961
  time_total_s: 3332.4247834682465
  timers:
    learn_throughput: 8345.405
    learn_time_ms: 19386.957
    sample_throughput: 23761.847
    sample_time_ms: 6808.898
    update_time_ms: 32.083
  timestamp: 1602626501
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    126 |          3332.42 | 20385792 |  267.261 |              307.808 |              74.7778 |             795.66 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3188.546455100533
    time_step_min: 2924
  date: 2020-10-13_22-02-08
  done: false
  episode_len_mean: 795.6390431439555
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 267.44860668351015
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 25751
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.143906868994236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004522902697014312
        model: {}
        policy_loss: -0.00920199352549389
        total_loss: 1.7865516046682994
        vf_explained_var: 0.9959679245948792
        vf_loss: 1.795825570821762
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.770967741935486
    gpu_util_percent0: 0.39193548387096777
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733642377946168
    mean_env_wait_ms: 1.2205030621926334
    mean_inference_ms: 4.354693400236904
    mean_raw_obs_processing_ms: 0.3805574612146471
  time_since_restore: 3358.7998547554016
  time_this_iter_s: 26.37507128715515
  time_total_s: 3358.7998547554016
  timers:
    learn_throughput: 8350.92
    learn_time_ms: 19374.153
    sample_throughput: 23705.712
    sample_time_ms: 6825.022
    update_time_ms: 32.153
  timestamp: 1602626528
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    127 |           3358.8 | 20547584 |  267.449 |              307.808 |              74.7778 |            795.639 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3187.378543291882
    time_step_min: 2924
  date: 2020-10-13_22-02-35
  done: false
  episode_len_mean: 795.6139904365263
  episode_reward_max: 307.959595959596
  episode_reward_mean: 267.63031401474245
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 181
  episodes_total: 25932
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.1585683412849903
        entropy_coeff: 0.0005000000000000001
        kl: 0.004497177433222532
        model: {}
        policy_loss: -0.010205297032371163
        total_loss: 1.8996409873167674
        vf_explained_var: 0.9956421852111816
        vf_loss: 1.9099255204200745
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15666666666667
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732905810269095
    mean_env_wait_ms: 1.220480734949155
    mean_inference_ms: 4.354134199728413
    mean_raw_obs_processing_ms: 0.38052894694026956
  time_since_restore: 3385.0834550857544
  time_this_iter_s: 26.283600330352783
  time_total_s: 3385.0834550857544
  timers:
    learn_throughput: 8355.859
    learn_time_ms: 19362.702
    sample_throughput: 23725.25
    sample_time_ms: 6819.401
    update_time_ms: 30.425
  timestamp: 1602626555
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    128 |          3385.08 | 20709376 |   267.63 |               307.96 |              74.7778 |            795.614 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3185.919923444976
    time_step_min: 2924
  date: 2020-10-13_22-03-02
  done: false
  episode_len_mean: 795.5657608072469
  episode_reward_max: 307.959595959596
  episode_reward_mean: 267.8478605571829
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 231
  episodes_total: 26163
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.15509062260389328
        entropy_coeff: 0.0005000000000000001
        kl: 0.004219735545727114
        model: {}
        policy_loss: -0.009656242385972291
        total_loss: 2.255702634652456
        vf_explained_var: 0.995591938495636
        vf_loss: 2.2654364506403604
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.48709677419355
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732046920048109
    mean_env_wait_ms: 1.2204579550438137
    mean_inference_ms: 4.353440456571305
    mean_raw_obs_processing_ms: 0.38049171729305625
  time_since_restore: 3411.738035440445
  time_this_iter_s: 26.65458035469055
  time_total_s: 3411.738035440445
  timers:
    learn_throughput: 8345.459
    learn_time_ms: 19386.83
    sample_throughput: 23695.115
    sample_time_ms: 6828.074
    update_time_ms: 29.715
  timestamp: 1602626582
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    129 |          3411.74 | 20871168 |  267.848 |               307.96 |              74.7778 |            795.566 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3184.5350823904623
    time_step_min: 2924
  date: 2020-10-13_22-03-29
  done: false
  episode_len_mean: 795.5212693357598
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.05721110100086
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 213
  episodes_total: 26376
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.14217245702942213
        entropy_coeff: 0.0005000000000000001
        kl: 0.003843813940572242
        model: {}
        policy_loss: -0.00743472120181347
        total_loss: 2.0694449047247567
        vf_explained_var: 0.9955958724021912
        vf_loss: 2.0769506990909576
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.764516129032256
    gpu_util_percent0: 0.28064516129032263
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473116897784996
    mean_env_wait_ms: 1.22043445285468
    mean_inference_ms: 4.35279341883981
    mean_raw_obs_processing_ms: 0.38045851726494473
  time_since_restore: 3438.246415615082
  time_this_iter_s: 26.50838017463684
  time_total_s: 3438.246415615082
  timers:
    learn_throughput: 8336.056
    learn_time_ms: 19408.698
    sample_throughput: 23691.611
    sample_time_ms: 6829.084
    update_time_ms: 29.374
  timestamp: 1602626609
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    130 |          3438.25 | 21032960 |  268.057 |               307.96 |              74.7778 |            795.521 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3183.454449432268
    time_step_min: 2924
  date: 2020-10-13_22-03-56
  done: false
  episode_len_mean: 795.4806192790146
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.2236330989862
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 171
  episodes_total: 26547
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.1456296518445015
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036228040892941258
        model: {}
        policy_loss: -0.007293626559354986
        total_loss: 1.6113335688908894
        vf_explained_var: 0.9961920380592346
        vf_loss: 1.6187000672022502
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.945161290322584
    gpu_util_percent0: 0.3574193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730531274932557
    mean_env_wait_ms: 1.2204153185948017
    mean_inference_ms: 4.352281336143355
    mean_raw_obs_processing_ms: 0.38043219550287843
  time_since_restore: 3464.8451108932495
  time_this_iter_s: 26.598695278167725
  time_total_s: 3464.8451108932495
  timers:
    learn_throughput: 8342.67
    learn_time_ms: 19393.311
    sample_throughput: 23624.075
    sample_time_ms: 6848.607
    update_time_ms: 30.861
  timestamp: 1602626636
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    131 |          3464.85 | 21194752 |  268.224 |               307.96 |              74.7778 |            795.481 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3182.0175819242854
    time_step_min: 2924
  date: 2020-10-13_22-04-23
  done: false
  episode_len_mean: 795.4318266716474
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.4376129618938
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 26770
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.1535994050403436
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037392685771919787
        model: {}
        policy_loss: -0.008139425100428829
        total_loss: 2.259928127129873
        vf_explained_var: 0.9953439235687256
        vf_loss: 2.268144349257151
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.889999999999997
    gpu_util_percent0: 0.35633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729721039075522
    mean_env_wait_ms: 1.2203905859441526
    mean_inference_ms: 4.351660387141843
    mean_raw_obs_processing_ms: 0.38039900461452936
  time_since_restore: 3491.4315536022186
  time_this_iter_s: 26.586442708969116
  time_total_s: 3491.4315536022186
  timers:
    learn_throughput: 8343.04
    learn_time_ms: 19392.452
    sample_throughput: 23634.435
    sample_time_ms: 6845.605
    update_time_ms: 30.645
  timestamp: 1602626663
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    132 |          3491.43 | 21356544 |  268.438 |               307.96 |              74.7778 |            795.432 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3180.550037091988
    time_step_min: 2924
  date: 2020-10-13_22-04-50
  done: false
  episode_len_mean: 795.3863989925179
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.65365971740516
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 228
  episodes_total: 26998
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.13972046971321106
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035111960411692658
        model: {}
        policy_loss: -0.006567574411747046
        total_loss: 2.4955084125200906
        vf_explained_var: 0.9950327277183533
        vf_loss: 2.5021459460258484
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.174193548387095
    gpu_util_percent0: 0.2551612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728865990537637
    mean_env_wait_ms: 1.220360134677065
    mean_inference_ms: 4.350997046252725
    mean_raw_obs_processing_ms: 0.3803643694023487
  time_since_restore: 3518.0947890281677
  time_this_iter_s: 26.663235425949097
  time_total_s: 3518.0947890281677
  timers:
    learn_throughput: 8337.638
    learn_time_ms: 19405.016
    sample_throughput: 23621.358
    sample_time_ms: 6849.395
    update_time_ms: 30.467
  timestamp: 1602626690
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    133 |          3518.09 | 21518336 |  268.654 |               307.96 |              74.7778 |            795.386 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3179.419958726415
    time_step_min: 2924
  date: 2020-10-13_22-05-17
  done: false
  episode_len_mean: 795.370170015456
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.8188230282511
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 27174
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.13431883975863457
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035796826511311033
        model: {}
        policy_loss: -0.010279505474803349
        total_loss: 1.877610703309377
        vf_explained_var: 0.995654821395874
        vf_loss: 1.887957404057185
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.973333333333336
    gpu_util_percent0: 0.25466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728168845769452
    mean_env_wait_ms: 1.2203367794028053
    mean_inference_ms: 4.3504832332785695
    mean_raw_obs_processing_ms: 0.38033845861967236
  time_since_restore: 3544.234239578247
  time_this_iter_s: 26.139450550079346
  time_total_s: 3544.234239578247
  timers:
    learn_throughput: 8339.042
    learn_time_ms: 19401.749
    sample_throughput: 23640.036
    sample_time_ms: 6843.983
    update_time_ms: 30.481
  timestamp: 1602626717
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    134 |          3544.23 | 21680128 |  268.819 |               307.96 |              74.7778 |             795.37 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3178.2238363583137
    time_step_min: 2924
  date: 2020-10-13_22-05-44
  done: false
  episode_len_mean: 795.3360739603888
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.0020529788124
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 27366
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.14118772372603416
        entropy_coeff: 0.0005000000000000001
        kl: 0.003906580755331864
        model: {}
        policy_loss: -0.008181799203157425
        total_loss: 2.2890235682328544
        vf_explained_var: 0.9951767921447754
        vf_loss: 2.297275980313619
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.796774193548387
    gpu_util_percent0: 0.2945161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472745733713043
    mean_env_wait_ms: 1.2203085811794832
    mean_inference_ms: 4.349943132873935
    mean_raw_obs_processing_ms: 0.38031035699392757
  time_since_restore: 3570.5864140987396
  time_this_iter_s: 26.352174520492554
  time_total_s: 3570.5864140987396
  timers:
    learn_throughput: 8338.683
    learn_time_ms: 19402.585
    sample_throughput: 23610.364
    sample_time_ms: 6852.584
    update_time_ms: 32.686
  timestamp: 1602626744
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    135 |          3570.59 | 21841920 |  269.002 |              308.414 |              74.7778 |            795.336 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3176.7530658152527
    time_step_min: 2924
  date: 2020-10-13_22-06-11
  done: false
  episode_len_mean: 795.2995289855072
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.22945212999554
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 234
  episodes_total: 27600
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.13460138936837515
        entropy_coeff: 0.0005000000000000001
        kl: 0.003422746706443528
        model: {}
        policy_loss: -0.00847506593951645
        total_loss: 2.073725998401642
        vf_explained_var: 0.9959105849266052
        vf_loss: 2.0822683572769165
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.419999999999998
    gpu_util_percent0: 0.3633333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472670811000775
    mean_env_wait_ms: 1.220277539872865
    mean_inference_ms: 4.349319573240333
    mean_raw_obs_processing_ms: 0.38027591256074117
  time_since_restore: 3596.825201034546
  time_this_iter_s: 26.238786935806274
  time_total_s: 3596.825201034546
  timers:
    learn_throughput: 8351.242
    learn_time_ms: 19373.406
    sample_throughput: 23581.253
    sample_time_ms: 6861.043
    update_time_ms: 38.795
  timestamp: 1602626771
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | RUNNING  | 172.17.0.4:58269 |    136 |          3596.83 | 22003712 |  269.229 |              308.414 |              74.7778 |              795.3 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_d1c11_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3175.475183691111
    time_step_min: 2924
  date: 2020-10-13_22-06-38
  done: true
  episode_len_mean: 795.2709517300914
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.41611314933374
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 27802
  experiment_id: debe0f15cb1d4a3fae5c8881a4addf84
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.388131789017202e-22
        cur_lr: 5.0e-05
        entropy: 0.1251864731311798
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037129991105757654
        model: {}
        policy_loss: -0.007405182608636096
        total_loss: 2.101415902376175
        vf_explained_var: 0.9954603314399719
        vf_loss: 2.1088836193084717
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14666666666667
    gpu_util_percent0: 0.2873333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 58269
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725902661226112
    mean_env_wait_ms: 1.2202478679095279
    mean_inference_ms: 4.348753326677067
    mean_raw_obs_processing_ms: 0.380247267919873
  time_since_restore: 3623.338688850403
  time_this_iter_s: 26.513487815856934
  time_total_s: 3623.338688850403
  timers:
    learn_throughput: 8339.272
    learn_time_ms: 19401.215
    sample_throughput: 23625.963
    sample_time_ms: 6848.059
    update_time_ms: 36.683
  timestamp: 1602626798
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: d1c11_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | TERMINATED |       |    137 |          3623.34 | 22165504 |  269.416 |              308.414 |              74.7778 |            795.271 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 27.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_d1c11_00000 | TERMINATED |       |    137 |          3623.34 | 22165504 |  269.416 |              308.414 |              74.7778 |            795.271 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


