2020-10-13 23:08:18,505	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_fbf40_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=15528)[0m 2020-10-13 23:08:21,242	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=15490)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15490)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15483)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15483)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15495)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15495)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15537)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15537)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15479)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15479)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15494)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15494)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15467)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15467)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15526)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15526)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15477)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15477)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15476)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15476)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15496)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15496)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15493)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15493)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15520)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15520)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15489)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15489)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15509)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15509)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15484)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15484)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15518)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15518)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15517)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15517)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15481)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15481)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15504)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15504)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15507)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15507)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15473)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15473)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15516)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15516)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15427)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15427)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15474)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15474)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15505)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15505)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15416)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15416)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15419)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15419)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15471)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15471)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15472)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15472)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15405)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15405)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15412)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15412)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15403)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15403)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15441)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15441)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15433)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15433)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15413)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15413)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15508)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15508)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15424)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15424)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15407)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15407)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15539)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15539)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15486)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15486)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15423)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15423)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15475)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15475)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15411)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15411)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15410)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15410)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15487)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15487)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15469)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15469)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15503)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15503)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15485)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15485)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15498)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15498)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15480)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15480)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15432)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15432)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15470)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15470)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15421)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15421)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15541)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15541)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15406)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15406)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15511)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15511)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15420)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15420)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15522)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15522)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15436)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15436)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15521)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15521)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15404)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15404)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15418)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15418)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15513)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15513)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15478)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15478)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15500)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15500)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15468)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15468)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15482)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15482)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15408)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15408)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15434)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15434)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15422)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15422)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15431)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15431)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15425)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15425)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15443)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15443)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15417)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15417)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15435)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15435)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15499)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15499)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15492)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15492)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=15409)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=15409)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3561.048780487805
    time_step_min: 3288
  date: 2020-10-13_23-08-55
  done: false
  episode_len_mean: 897.367088607595
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 205.7902442142946
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1812196671962738
        entropy_coeff: 0.0005000000000000001
        kl: 0.004003119072876871
        model: {}
        policy_loss: -0.007752152906808381
        total_loss: 417.5151901245117
        vf_explained_var: 0.5535116195678711
        vf_loss: 417.5227355957031
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.926470588235293
    gpu_util_percent0: 0.24117647058823527
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.529411764705882
    vram_util_percent0: 0.08659541218914593
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17486267525607255
    mean_env_wait_ms: 1.1799933064605908
    mean_inference_ms: 6.023259167319354
    mean_raw_obs_processing_ms: 0.46472623636046106
  time_since_restore: 29.30737805366516
  time_this_iter_s: 29.30737805366516
  time_total_s: 29.30737805366516
  timers:
    learn_throughput: 8136.985
    learn_time_ms: 19883.532
    sample_throughput: 17335.5
    sample_time_ms: 9332.987
    update_time_ms: 45.565
  timestamp: 1602630535
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 27.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      1 |          29.3074 | 161792 |   205.79 |              273.505 |              128.354 |            897.367 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3569.6227758007117
    time_step_min: 3288
  date: 2020-10-13_23-09-23
  done: false
  episode_len_mean: 896.1867088607595
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 206.45959595959573
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1479040284951527
        entropy_coeff: 0.0005000000000000001
        kl: 0.008545062194267908
        model: {}
        policy_loss: -0.010045574104879051
        total_loss: 102.10054206848145
        vf_explained_var: 0.8203843235969543
        vf_loss: 102.1103064219157
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.278125
    gpu_util_percent0: 0.2703125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.75
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.169632486889447
    mean_env_wait_ms: 1.1729540394120814
    mean_inference_ms: 5.737450080383413
    mean_raw_obs_processing_ms: 0.45053597633106107
  time_since_restore: 56.78066873550415
  time_this_iter_s: 27.47329068183899
  time_total_s: 56.78066873550415
  timers:
    learn_throughput: 8179.781
    learn_time_ms: 19779.504
    sample_throughput: 18993.114
    sample_time_ms: 8518.456
    update_time_ms: 44.474
  timestamp: 1602630563
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      2 |          56.7807 | 323584 |   206.46 |              273.505 |              128.354 |            896.187 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3554.0
    time_step_min: 3195
  date: 2020-10-13_23-09-50
  done: false
  episode_len_mean: 889.2383966244726
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 208.2390998593528
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.137634406487147
        entropy_coeff: 0.0005000000000000001
        kl: 0.007693540576534967
        model: {}
        policy_loss: -0.011871370700343201
        total_loss: 50.18517557779948
        vf_explained_var: 0.8948180079460144
        vf_loss: 50.1968469619751
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.725806451612907
    gpu_util_percent0: 0.32064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1660240330906676
    mean_env_wait_ms: 1.1716044379261517
    mean_inference_ms: 5.518810937196456
    mean_raw_obs_processing_ms: 0.4401115615818572
  time_since_restore: 83.73105192184448
  time_this_iter_s: 26.950383186340332
  time_total_s: 83.73105192184448
  timers:
    learn_throughput: 8194.108
    learn_time_ms: 19744.92
    sample_throughput: 20048.912
    sample_time_ms: 8069.864
    update_time_ms: 41.983
  timestamp: 1602630590
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      3 |          83.7311 | 485376 |  208.239 |              273.505 |              128.354 |            889.238 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3551.257956448911
    time_step_min: 3195
  date: 2020-10-13_23-10-17
  done: false
  episode_len_mean: 884.371835443038
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 208.62444060861762
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1220806539058685
        entropy_coeff: 0.0005000000000000001
        kl: 0.008578164658198753
        model: {}
        policy_loss: -0.012210655714928484
        total_loss: 37.28772290547689
        vf_explained_var: 0.9263750910758972
        vf_loss: 37.29963652292887
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.670967741935485
    gpu_util_percent0: 0.2870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16349670475754108
    mean_env_wait_ms: 1.171839786158334
    mean_inference_ms: 5.360252049756311
    mean_raw_obs_processing_ms: 0.4324698185717688
  time_since_restore: 110.53070950508118
  time_this_iter_s: 26.799657583236694
  time_total_s: 110.53070950508118
  timers:
    learn_throughput: 8206.807
    learn_time_ms: 19714.366
    sample_throughput: 20679.892
    sample_time_ms: 7823.639
    update_time_ms: 40.538
  timestamp: 1602630617
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      4 |          110.531 | 647168 |  208.624 |              273.505 |              128.354 |            884.372 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3548.7311258278146
    time_step_min: 3195
  date: 2020-10-13_23-10-44
  done: false
  episode_len_mean: 878.8354430379746
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 209.16481268379982
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0830539067586262
        entropy_coeff: 0.0005000000000000001
        kl: 0.007865200944555303
        model: {}
        policy_loss: -0.012452944797890572
        total_loss: 32.10198322931925
        vf_explained_var: 0.9422528147697449
        vf_loss: 32.11419185002645
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.932258064516127
    gpu_util_percent0: 0.2887096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16164278467098753
    mean_env_wait_ms: 1.17335587837213
    mean_inference_ms: 5.2405953092411055
    mean_raw_obs_processing_ms: 0.42648356194983483
  time_since_restore: 137.3174729347229
  time_this_iter_s: 26.786763429641724
  time_total_s: 137.3174729347229
  timers:
    learn_throughput: 8202.887
    learn_time_ms: 19723.788
    sample_throughput: 21166.719
    sample_time_ms: 7643.698
    update_time_ms: 42.244
  timestamp: 1602630644
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      5 |          137.317 | 808960 |  209.165 |              273.505 |              128.354 |            878.835 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3527.9485500467727
    time_step_min: 3187
  date: 2020-10-13_23-11-10
  done: false
  episode_len_mean: 865.7038043478261
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 212.2182696530521
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 314
  episodes_total: 1104
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0722917715708415
        entropy_coeff: 0.0005000000000000001
        kl: 0.008053469937294722
        model: {}
        policy_loss: -0.011176503176102415
        total_loss: 36.43024476369222
        vf_explained_var: 0.9526867270469666
        vf_loss: 36.44115161895752
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.209677419354836
    gpu_util_percent0: 0.2696774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15918998925930677
    mean_env_wait_ms: 1.1774773698458068
    mean_inference_ms: 5.080355198035937
    mean_raw_obs_processing_ms: 0.4187785229119953
  time_since_restore: 163.8800458908081
  time_this_iter_s: 26.562572956085205
  time_total_s: 163.8800458908081
  timers:
    learn_throughput: 8214.855
    learn_time_ms: 19695.053
    sample_throughput: 21509.875
    sample_time_ms: 7521.754
    update_time_ms: 42.524
  timestamp: 1602630670
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      6 |           163.88 | 970752 |  212.218 |              273.505 |              128.354 |            865.704 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3519.5760781122863
    time_step_min: 3187
  date: 2020-10-13_23-11-37
  done: false
  episode_len_mean: 860.2832278481013
  episode_reward_max: 273.50505050505103
  episode_reward_mean: 213.76049258406843
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 160
  episodes_total: 1264
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0584441026051838
        entropy_coeff: 0.0005000000000000001
        kl: 0.007266054550806682
        model: {}
        policy_loss: -0.011817739791392038
        total_loss: 21.57445494333903
        vf_explained_var: 0.95844966173172
        vf_loss: 21.58607578277588
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39032258064516
    gpu_util_percent0: 0.35580645161290325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15829178968192933
    mean_env_wait_ms: 1.1791439688293135
    mean_inference_ms: 5.021527232151923
    mean_raw_obs_processing_ms: 0.41590737808624934
  time_since_restore: 190.5547218322754
  time_this_iter_s: 26.674675941467285
  time_total_s: 190.5547218322754
  timers:
    learn_throughput: 8219.715
    learn_time_ms: 19683.406
    sample_throughput: 21744.731
    sample_time_ms: 7440.515
    update_time_ms: 41.769
  timestamp: 1602630697
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      7 |          190.555 | 1132544 |   213.76 |              273.505 |              128.354 |            860.283 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3509.0281182408075
    time_step_min: 3120
  date: 2020-10-13_23-12-04
  done: false
  episode_len_mean: 855.4535864978903
  episode_reward_max: 274.5656565656565
  episode_reward_mean: 215.1003281762774
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0401013692220051
        entropy_coeff: 0.0005000000000000001
        kl: 0.007276753972594936
        model: {}
        policy_loss: -0.01209554991995295
        total_loss: 20.411585489908855
        vf_explained_var: 0.9587884545326233
        vf_loss: 20.423472722371418
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.070967741935487
    gpu_util_percent0: 0.3264516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15752455115386174
    mean_env_wait_ms: 1.1807516348785774
    mean_inference_ms: 4.971444350380993
    mean_raw_obs_processing_ms: 0.4134111669504617
  time_since_restore: 217.06512260437012
  time_this_iter_s: 26.510400772094727
  time_total_s: 217.06512260437012
  timers:
    learn_throughput: 8229.462
    learn_time_ms: 19660.095
    sample_throughput: 21935.246
    sample_time_ms: 7375.892
    update_time_ms: 41.442
  timestamp: 1602630724
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      8 |          217.065 | 1294336 |    215.1 |              274.566 |              128.354 |            855.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3501.862783171521
    time_step_min: 3120
  date: 2020-10-13_23-12-31
  done: false
  episode_len_mean: 851.6063291139241
  episode_reward_max: 274.5656565656565
  episode_reward_mean: 216.40263393427938
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0068644384543102
        entropy_coeff: 0.0005000000000000001
        kl: 0.007244308828376234
        model: {}
        policy_loss: -0.012283675799456736
        total_loss: 22.011494477589924
        vf_explained_var: 0.9590733051300049
        vf_loss: 22.023557662963867
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7483870967742
    gpu_util_percent0: 0.31000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870956
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1568636582769158
    mean_env_wait_ms: 1.1823548828441903
    mean_inference_ms: 4.927800520472008
    mean_raw_obs_processing_ms: 0.4111941694132998
  time_since_restore: 243.82525396347046
  time_this_iter_s: 26.760131359100342
  time_total_s: 243.82525396347046
  timers:
    learn_throughput: 8224.231
    learn_time_ms: 19672.6
    sample_throughput: 22087.381
    sample_time_ms: 7325.087
    update_time_ms: 39.149
  timestamp: 1602630751
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |      9 |          243.825 | 1456128 |  216.403 |              274.566 |              128.354 |            851.606 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3485.0683530678148
    time_step_min: 3120
  date: 2020-10-13_23-12-57
  done: false
  episode_len_mean: 845.2070787110407
  episode_reward_max: 275.6262626262626
  episode_reward_mean: 219.26062527013386
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 313
  episodes_total: 1893
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9907859414815903
        entropy_coeff: 0.0005000000000000001
        kl: 0.006813797284848988
        model: {}
        policy_loss: -0.011380279174773023
        total_loss: 25.875819365183514
        vf_explained_var: 0.9651870727539062
        vf_loss: 25.88701327641805
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803225806451607
    gpu_util_percent0: 0.27999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15581498143980813
    mean_env_wait_ms: 1.1853145086493369
    mean_inference_ms: 4.8581010577135615
    mean_raw_obs_processing_ms: 0.40773067182030787
  time_since_restore: 270.6581585407257
  time_this_iter_s: 26.83290457725525
  time_total_s: 270.6581585407257
  timers:
    learn_throughput: 8221.503
    learn_time_ms: 19679.126
    sample_throughput: 22188.901
    sample_time_ms: 7291.573
    update_time_ms: 39.67
  timestamp: 1602630777
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     10 |          270.658 | 1617920 |  219.261 |              275.626 |              128.354 |            845.207 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3476.02823179792
    time_step_min: 3099
  date: 2020-10-13_23-13-24
  done: false
  episode_len_mean: 842.4542356377799
  episode_reward_max: 277.7474747474749
  episode_reward_mean: 220.41979188181708
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 161
  episodes_total: 2054
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9728940029939016
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065002391347661614
        model: {}
        policy_loss: -0.010479286599244611
        total_loss: 17.72179587682088
        vf_explained_var: 0.9665319323539734
        vf_loss: 17.73211161295573
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97666666666667
    gpu_util_percent0: 0.28099999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1553668600835991
    mean_env_wait_ms: 1.1865387208984721
    mean_inference_ms: 4.828558728730987
    mean_raw_obs_processing_ms: 0.4062574252347746
  time_since_restore: 297.0308518409729
  time_this_iter_s: 26.372693300247192
  time_total_s: 297.0308518409729
  timers:
    learn_throughput: 8242.655
    learn_time_ms: 19628.628
    sample_throughput: 22949.705
    sample_time_ms: 7049.851
    update_time_ms: 37.573
  timestamp: 1602630804
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     11 |          297.031 | 1779712 |   220.42 |              277.747 |              128.354 |            842.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3467.6876435461645
    time_step_min: 3099
  date: 2020-10-13_23-13-51
  done: false
  episode_len_mean: 839.7314647377939
  episode_reward_max: 277.7474747474749
  episode_reward_mean: 221.8002310628892
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9636171956857046
        entropy_coeff: 0.0005000000000000001
        kl: 0.0065782947931438684
        model: {}
        policy_loss: -0.012532726783926288
        total_loss: 14.463757356007894
        vf_explained_var: 0.9673574566841125
        vf_loss: 14.47611395517985
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.941935483870974
    gpu_util_percent0: 0.3661290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15496306814906566
    mean_env_wait_ms: 1.1876583277661774
    mean_inference_ms: 4.802370571086063
    mean_raw_obs_processing_ms: 0.4049469519225511
  time_since_restore: 323.9220600128174
  time_this_iter_s: 26.891208171844482
  time_total_s: 323.9220600128174
  timers:
    learn_throughput: 8242.245
    learn_time_ms: 19629.603
    sample_throughput: 23144.306
    sample_time_ms: 6990.575
    update_time_ms: 37.115
  timestamp: 1602630831
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     12 |          323.922 | 1941504 |    221.8 |              277.747 |              128.354 |            839.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3456.561816652649
    time_step_min: 3099
  date: 2020-10-13_23-14-18
  done: false
  episode_len_mean: 836.0049730625777
  episode_reward_max: 277.7474747474749
  episode_reward_mean: 223.48370149903505
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 201
  episodes_total: 2413
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9162271420160929
        entropy_coeff: 0.0005000000000000001
        kl: 0.006539546923401455
        model: {}
        policy_loss: -0.010849950471310876
        total_loss: 16.26166319847107
        vf_explained_var: 0.9718396663665771
        vf_loss: 16.272317091623943
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.016129032258068
    gpu_util_percent0: 0.3054838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15449810210250994
    mean_env_wait_ms: 1.1892221301391652
    mean_inference_ms: 4.772598446603674
    mean_raw_obs_processing_ms: 0.4034041677471885
  time_since_restore: 350.4782407283783
  time_this_iter_s: 26.556180715560913
  time_total_s: 350.4782407283783
  timers:
    learn_throughput: 8249.283
    learn_time_ms: 19612.856
    sample_throughput: 23212.299
    sample_time_ms: 6970.098
    update_time_ms: 35.8
  timestamp: 1602630858
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     13 |          350.478 | 2103296 |  223.484 |              277.747 |              128.354 |            836.005 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3441.1576763485477
    time_step_min: 3054
  date: 2020-10-13_23-14-44
  done: false
  episode_len_mean: 831.4266567386449
  episode_reward_max: 284.56565656565675
  episode_reward_mean: 225.76486006754052
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 273
  episodes_total: 2686
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9097392161687216
        entropy_coeff: 0.0005000000000000001
        kl: 0.005902522980856399
        model: {}
        policy_loss: -0.010862464109474482
        total_loss: 15.4582626024882
        vf_explained_var: 0.9729016423225403
        vf_loss: 15.468989531199137
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.090322580645164
    gpu_util_percent0: 0.2729032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15396392028809544
    mean_env_wait_ms: 1.19105245201458
    mean_inference_ms: 4.7378403157056015
    mean_raw_obs_processing_ms: 0.4016747710357504
  time_since_restore: 377.0279350280762
  time_this_iter_s: 26.549694299697876
  time_total_s: 377.0279350280762
  timers:
    learn_throughput: 8254.239
    learn_time_ms: 19601.081
    sample_throughput: 23254.967
    sample_time_ms: 6957.309
    update_time_ms: 34.934
  timestamp: 1602630884
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     14 |          377.028 | 2265088 |  225.765 |              284.566 |              128.354 |            831.427 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3433.8907084371663
    time_step_min: 3054
  date: 2020-10-13_23-15-11
  done: false
  episode_len_mean: 829.2387482419128
  episode_reward_max: 284.56565656565675
  episode_reward_mean: 226.9806290755657
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9017603745063146
        entropy_coeff: 0.0005000000000000001
        kl: 0.005951517377980053
        model: {}
        policy_loss: -0.01264588067230458
        total_loss: 10.179479281107584
        vf_explained_var: 0.9765340685844421
        vf_loss: 10.191980838775635
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.451612903225808
    gpu_util_percent0: 0.33548387096774196
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15368958643515598
    mean_env_wait_ms: 1.1920244681923176
    mean_inference_ms: 4.720084632136882
    mean_raw_obs_processing_ms: 0.4007722060368731
  time_since_restore: 403.45982694625854
  time_this_iter_s: 26.431891918182373
  time_total_s: 403.45982694625854
  timers:
    learn_throughput: 8266.515
    learn_time_ms: 19571.973
    sample_throughput: 23270.588
    sample_time_ms: 6952.639
    update_time_ms: 32.523
  timestamp: 1602630911
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     15 |           403.46 | 2426880 |  226.981 |              284.566 |              128.354 |            829.239 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3426.399663299663
    time_step_min: 3054
  date: 2020-10-13_23-15-38
  done: false
  episode_len_mean: 826.922462562396
  episode_reward_max: 284.56565656565675
  episode_reward_mean: 228.09966554059724
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 161
  episodes_total: 3005
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8762048035860062
        entropy_coeff: 0.0005000000000000001
        kl: 0.006493303109891713
        model: {}
        policy_loss: -0.013002060974637667
        total_loss: 14.075467268625895
        vf_explained_var: 0.9701388478279114
        vf_loss: 14.088258266448975
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.148387096774197
    gpu_util_percent0: 0.2874193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15343074082664135
    mean_env_wait_ms: 1.1930204813334602
    mean_inference_ms: 4.703344376203274
    mean_raw_obs_processing_ms: 0.399911931339022
  time_since_restore: 430.344589471817
  time_this_iter_s: 26.88476252555847
  time_total_s: 430.344589471817
  timers:
    learn_throughput: 8259.179
    learn_time_ms: 19589.355
    sample_throughput: 23219.743
    sample_time_ms: 6967.864
    update_time_ms: 31.722
  timestamp: 1602630938
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     16 |          430.345 | 2588672 |    228.1 |              284.566 |              128.354 |            826.922 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3413.207800121877
    time_step_min: 3054
  date: 2020-10-13_23-16-05
  done: false
  episode_len_mean: 822.9445281881218
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 229.984344500172
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 312
  episodes_total: 3317
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.839088092247645
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055505180498585105
        model: {}
        policy_loss: -0.01150182525937756
        total_loss: 16.7764093875885
        vf_explained_var: 0.9746711850166321
        vf_loss: 16.78777551651001
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.090322580645164
    gpu_util_percent0: 0.3054838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15297916944946022
    mean_env_wait_ms: 1.1948719865097925
    mean_inference_ms: 4.674600501842904
    mean_raw_obs_processing_ms: 0.398465226504969
  time_since_restore: 457.2407314777374
  time_this_iter_s: 26.89614200592041
  time_total_s: 457.2407314777374
  timers:
    learn_throughput: 8257.148
    learn_time_ms: 19594.174
    sample_throughput: 23161.266
    sample_time_ms: 6985.456
    update_time_ms: 32.536
  timestamp: 1602630965
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     17 |          457.241 | 2750464 |  229.984 |              290.929 |              128.354 |            822.945 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3406.6451612903224
    time_step_min: 3054
  date: 2020-10-13_23-16-32
  done: false
  episode_len_mean: 821.0917721518987
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 230.9796817426276
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 3476
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8368375897407532
        entropy_coeff: 0.0005000000000000001
        kl: 0.005631982310054203
        model: {}
        policy_loss: -0.01084599293123271
        total_loss: 10.226332902908325
        vf_explained_var: 0.9766200184822083
        vf_loss: 10.23703408241272
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.671875
    gpu_util_percent0: 0.2546875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7687500000000003
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15277746578458684
    mean_env_wait_ms: 1.1957258601457175
    mean_inference_ms: 4.66158931739152
    mean_raw_obs_processing_ms: 0.3978104424763447
  time_since_restore: 484.43384861946106
  time_this_iter_s: 27.193117141723633
  time_total_s: 484.43384861946106
  timers:
    learn_throughput: 8237.572
    learn_time_ms: 19640.738
    sample_throughput: 23087.799
    sample_time_ms: 7007.684
    update_time_ms: 30.939
  timestamp: 1602630992
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     18 |          484.434 | 2912256 |   230.98 |              290.929 |              128.354 |            821.092 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3401.0494444444444
    time_step_min: 3054
  date: 2020-10-13_23-16-59
  done: false
  episode_len_mean: 819.3532324621733
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 231.91557945340608
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 3635
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8324753443400065
        entropy_coeff: 0.0005000000000000001
        kl: 0.006413979882684846
        model: {}
        policy_loss: -0.01057599096869429
        total_loss: 11.734215180079142
        vf_explained_var: 0.9724454283714294
        vf_loss: 11.744565884272257
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.890322580645165
    gpu_util_percent0: 0.3254838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.152588493144938
    mean_env_wait_ms: 1.1965715160465895
    mean_inference_ms: 4.649401512823893
    mean_raw_obs_processing_ms: 0.3971871002104243
  time_since_restore: 511.2103428840637
  time_this_iter_s: 26.77649426460266
  time_total_s: 511.2103428840637
  timers:
    learn_throughput: 8240.304
    learn_time_ms: 19634.227
    sample_throughput: 23074.593
    sample_time_ms: 7011.695
    update_time_ms: 33.498
  timestamp: 1602631019
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     19 |           511.21 | 3074048 |  231.916 |              290.929 |              128.354 |            819.353 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3390.977389516958
    time_step_min: 3054
  date: 2020-10-13_23-17-26
  done: false
  episode_len_mean: 816.5324675324675
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 233.36511023142032
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 292
  episodes_total: 3927
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7915658603111903
        entropy_coeff: 0.0005000000000000001
        kl: 0.005714234585563342
        model: {}
        policy_loss: -0.009224428193798909
        total_loss: 16.07810894648234
        vf_explained_var: 0.9758429527282715
        vf_loss: 16.08715844154358
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716129032258067
    gpu_util_percent0: 0.29903225806451617
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15227200615563005
    mean_env_wait_ms: 1.1981090603374327
    mean_inference_ms: 4.628988982557638
    mean_raw_obs_processing_ms: 0.39616724146422605
  time_since_restore: 537.6497759819031
  time_this_iter_s: 26.439433097839355
  time_total_s: 537.6497759819031
  timers:
    learn_throughput: 8250.6
    learn_time_ms: 19609.726
    sample_throughput: 23119.079
    sample_time_ms: 6998.203
    update_time_ms: 31.945
  timestamp: 1602631046
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     20 |           537.65 | 3235840 |  233.365 |              290.929 |              128.354 |            816.532 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3385.9975448072673
    time_step_min: 3019
  date: 2020-10-13_23-17-52
  done: false
  episode_len_mean: 815.0759493670886
  episode_reward_max: 290.9292929292928
  episode_reward_mean: 234.15135040767947
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 181
  episodes_total: 4108
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7875451942284902
        entropy_coeff: 0.0005000000000000001
        kl: 0.005900586722418666
        model: {}
        policy_loss: -0.011265025086080035
        total_loss: 12.707908233006796
        vf_explained_var: 0.9739854335784912
        vf_loss: 12.718976736068726
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.548387096774196
    gpu_util_percent0: 0.28193548387096773
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15209572146821768
    mean_env_wait_ms: 1.1989321977576386
    mean_inference_ms: 4.617535418557888
    mean_raw_obs_processing_ms: 0.39558110117361445
  time_since_restore: 564.290018081665
  time_this_iter_s: 26.640242099761963
  time_total_s: 564.290018081665
  timers:
    learn_throughput: 8236.161
    learn_time_ms: 19644.103
    sample_throughput: 23154.373
    sample_time_ms: 6987.535
    update_time_ms: 33.789
  timestamp: 1602631072
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     21 |           564.29 | 3397632 |  234.151 |              290.929 |              128.354 |            815.076 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3380.8260458520444
    time_step_min: 3010
  date: 2020-10-13_23-18-19
  done: false
  episode_len_mean: 813.7878574777309
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 234.91894330079978
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 4266
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.786624605456988
        entropy_coeff: 0.0005000000000000001
        kl: 0.005671430650788049
        model: {}
        policy_loss: -0.012116239978543794
        total_loss: 10.708203315734863
        vf_explained_var: 0.9747191071510315
        vf_loss: 10.72014570236206
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.990000000000002
    gpu_util_percent0: 0.30433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15194949429934146
    mean_env_wait_ms: 1.199637383352457
    mean_inference_ms: 4.608113375470312
    mean_raw_obs_processing_ms: 0.395095927862105
  time_since_restore: 590.8131859302521
  time_this_iter_s: 26.523167848587036
  time_total_s: 590.8131859302521
  timers:
    learn_throughput: 8238.573
    learn_time_ms: 19638.354
    sample_throughput: 23255.672
    sample_time_ms: 6957.099
    update_time_ms: 32.283
  timestamp: 1602631099
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     22 |          590.813 | 3559424 |  234.919 |              291.232 |              128.354 |            813.788 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3372.76435246996
    time_step_min: 3010
  date: 2020-10-13_23-18-46
  done: false
  episode_len_mean: 811.6392139545153
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 236.14765004873192
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 263
  episodes_total: 4529
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7510873725016912
        entropy_coeff: 0.0005000000000000001
        kl: 0.005892521197286745
        model: {}
        policy_loss: -0.009724960681827119
        total_loss: 13.870153268178305
        vf_explained_var: 0.9768733978271484
        vf_loss: 13.879664421081543
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.383870967741938
    gpu_util_percent0: 0.3003225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15172505219604965
    mean_env_wait_ms: 1.2008323042293298
    mean_inference_ms: 4.593658043870607
    mean_raw_obs_processing_ms: 0.39435058806704293
  time_since_restore: 617.4484400749207
  time_this_iter_s: 26.63525414466858
  time_total_s: 617.4484400749207
  timers:
    learn_throughput: 8228.085
    learn_time_ms: 19663.385
    sample_throughput: 23323.214
    sample_time_ms: 6936.951
    update_time_ms: 34.324
  timestamp: 1602631126
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     23 |          617.448 | 3721216 |  236.148 |              291.232 |              128.354 |            811.639 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3367.1287991498407
    time_step_min: 3010
  date: 2020-10-13_23-19-13
  done: false
  episode_len_mean: 810.2875527426161
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 237.05446873801301
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 4740
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7363142818212509
        entropy_coeff: 0.0005000000000000001
        kl: 0.005312932499994834
        model: {}
        policy_loss: -0.010551177416346036
        total_loss: 10.422034819920858
        vf_explained_var: 0.9792265892028809
        vf_loss: 10.432422717412313
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.629032258064516
    gpu_util_percent0: 0.36935483870967734
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1515595099996285
    mean_env_wait_ms: 1.201661556399264
    mean_inference_ms: 4.582795734018405
    mean_raw_obs_processing_ms: 0.39380861846952925
  time_since_restore: 644.003331899643
  time_this_iter_s: 26.55489182472229
  time_total_s: 644.003331899643
  timers:
    learn_throughput: 8223.459
    learn_time_ms: 19674.446
    sample_throughput: 23365.028
    sample_time_ms: 6924.537
    update_time_ms: 35.473
  timestamp: 1602631153
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     24 |          644.003 | 3883008 |  237.054 |              291.232 |              128.354 |            810.288 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3363.1451778737405
    time_step_min: 3010
  date: 2020-10-13_23-19-39
  done: false
  episode_len_mean: 809.2658227848101
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 237.67555299833774
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 4898
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.744674930969874
        entropy_coeff: 0.0005000000000000001
        kl: 0.005310616339556873
        model: {}
        policy_loss: -0.01328013397869654
        total_loss: 9.559085130691528
        vf_explained_var: 0.9773495197296143
        vf_loss: 9.572206338246664
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.664516129032258
    gpu_util_percent0: 0.36677419354838714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15144255386379712
    mean_env_wait_ms: 1.2022648637492284
    mean_inference_ms: 4.575191643494785
    mean_raw_obs_processing_ms: 0.3934214380036777
  time_since_restore: 670.763593673706
  time_this_iter_s: 26.76026177406311
  time_total_s: 670.763593673706
  timers:
    learn_throughput: 8216.424
    learn_time_ms: 19691.292
    sample_throughput: 23318.794
    sample_time_ms: 6938.266
    update_time_ms: 37.096
  timestamp: 1602631179
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     25 |          670.764 | 4044800 |  237.676 |              291.232 |              128.354 |            809.266 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3357.3968940436407
    time_step_min: 3010
  date: 2020-10-13_23-20-06
  done: false
  episode_len_mean: 807.8734869191723
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 238.5942083860865
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 224
  episodes_total: 5122
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7157353411118189
        entropy_coeff: 0.0005000000000000001
        kl: 0.006130007832931976
        model: {}
        policy_loss: -0.010868864017538726
        total_loss: 11.382584492365519
        vf_explained_var: 0.979543924331665
        vf_loss: 11.393198251724243
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.00645161290323
    gpu_util_percent0: 0.38806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1512858858945596
    mean_env_wait_ms: 1.2031396666565812
    mean_inference_ms: 4.565100285333827
    mean_raw_obs_processing_ms: 0.39290548245411433
  time_since_restore: 697.170351266861
  time_this_iter_s: 26.406757593154907
  time_total_s: 697.170351266861
  timers:
    learn_throughput: 8229.161
    learn_time_ms: 19660.813
    sample_throughput: 23380.87
    sample_time_ms: 6919.845
    update_time_ms: 37.508
  timestamp: 1602631206
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     26 |           697.17 | 4206592 |  238.594 |              291.232 |              128.354 |            807.873 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3351.399850103054
    time_step_min: 3010
  date: 2020-10-13_23-20-33
  done: false
  episode_len_mean: 806.5517498138496
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 239.4420357709635
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 250
  episodes_total: 5372
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6879154394070307
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054661074342827005
        model: {}
        policy_loss: -0.010591972202140218
        total_loss: 10.885743856430054
        vf_explained_var: 0.9807800650596619
        vf_loss: 10.896132946014404
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.419354838709676
    gpu_util_percent0: 0.41838709677419356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1511236167950856
    mean_env_wait_ms: 1.2039925050447995
    mean_inference_ms: 4.55446473983619
    mean_raw_obs_processing_ms: 0.3923629766056399
  time_since_restore: 723.7543587684631
  time_this_iter_s: 26.584007501602173
  time_total_s: 723.7543587684631
  timers:
    learn_throughput: 8230.058
    learn_time_ms: 19658.67
    sample_throughput: 23480.918
    sample_time_ms: 6890.361
    update_time_ms: 36.888
  timestamp: 1602631233
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     27 |          723.754 | 4368384 |  239.442 |              291.232 |              128.354 |            806.552 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3348.0032757051868
    time_step_min: 3010
  date: 2020-10-13_23-20-59
  done: false
  episode_len_mean: 805.7877034358047
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 239.87405702595575
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 5530
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6986237665017446
        entropy_coeff: 0.0005000000000000001
        kl: 0.005676805002925296
        model: {}
        policy_loss: -0.011504553685275217
        total_loss: 9.959116458892822
        vf_explained_var: 0.9774243235588074
        vf_loss: 9.970402399698893
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.632258064516126
    gpu_util_percent0: 0.4390322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1510266175859562
    mean_env_wait_ms: 1.2045099919675837
    mean_inference_ms: 4.548176015834361
    mean_raw_obs_processing_ms: 0.3920398757918092
  time_since_restore: 750.3654863834381
  time_this_iter_s: 26.611127614974976
  time_total_s: 750.3654863834381
  timers:
    learn_throughput: 8242.425
    learn_time_ms: 19629.175
    sample_throughput: 23615.33
    sample_time_ms: 6851.143
    update_time_ms: 38.535
  timestamp: 1602631259
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     28 |          750.365 | 4530176 |  239.874 |              291.232 |              128.354 |            805.788 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3344.2459563994375
    time_step_min: 3010
  date: 2020-10-13_23-21-26
  done: false
  episode_len_mean: 804.8137340555653
  episode_reward_max: 291.2323232323234
  episode_reward_mean: 240.4106785132471
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 193
  episodes_total: 5723
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6785935461521149
        entropy_coeff: 0.0005000000000000001
        kl: 0.006100421228135626
        model: {}
        policy_loss: -0.010813766227026159
        total_loss: 11.620836019515991
        vf_explained_var: 0.978428840637207
        vf_loss: 11.631378571192423
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.890000000000008
    gpu_util_percent0: 0.3066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1509137166538834
    mean_env_wait_ms: 1.2051395004728074
    mean_inference_ms: 4.540779175673322
    mean_raw_obs_processing_ms: 0.3916567490697188
  time_since_restore: 776.8664178848267
  time_this_iter_s: 26.50093150138855
  time_total_s: 776.8664178848267
  timers:
    learn_throughput: 8248.78
    learn_time_ms: 19614.051
    sample_throughput: 23658.82
    sample_time_ms: 6838.549
    update_time_ms: 37.979
  timestamp: 1602631286
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     29 |          776.866 | 4691968 |  240.411 |              291.232 |              128.354 |            804.814 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3339.6116602445973
    time_step_min: 3010
  date: 2020-10-13_23-21-53
  done: false
  episode_len_mean: 803.6532311792139
  episode_reward_max: 291.9898989898989
  episode_reward_mean: 241.11038432290925
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 281
  episodes_total: 6004
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6467319428920746
        entropy_coeff: 0.0005000000000000001
        kl: 0.005211092143629988
        model: {}
        policy_loss: -0.009283149343294403
        total_loss: 11.490628719329834
        vf_explained_var: 0.9812774062156677
        vf_loss: 11.499714136123657
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380645161290328
    gpu_util_percent0: 0.31354838709677424
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15076004623195124
    mean_env_wait_ms: 1.2060017101631397
    mean_inference_ms: 4.530825252566373
    mean_raw_obs_processing_ms: 0.39114739713543645
  time_since_restore: 803.3932700157166
  time_this_iter_s: 26.526852130889893
  time_total_s: 803.3932700157166
  timers:
    learn_throughput: 8248.98
    learn_time_ms: 19613.577
    sample_throughput: 23634.836
    sample_time_ms: 6845.488
    update_time_ms: 39.21
  timestamp: 1602631313
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     30 |          803.393 | 4853760 |   241.11 |               291.99 |              128.354 |            803.653 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3336.92802350253
    time_step_min: 3010
  date: 2020-10-13_23-22-19
  done: false
  episode_len_mean: 802.9754949691659
  episode_reward_max: 291.9898989898989
  episode_reward_mean: 241.49382333559547
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 6162
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.654222289721171
        entropy_coeff: 0.0005000000000000001
        kl: 0.005552779068239033
        model: {}
        policy_loss: -0.012495742392881462
        total_loss: 8.75546701749166
        vf_explained_var: 0.9802989959716797
        vf_loss: 8.767734368642172
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.543333333333337
    gpu_util_percent0: 0.29133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15067824445723677
    mean_env_wait_ms: 1.2064478459007901
    mean_inference_ms: 4.525519895043416
    mean_raw_obs_processing_ms: 0.39087357693490626
  time_since_restore: 829.8636457920074
  time_this_iter_s: 26.470375776290894
  time_total_s: 829.8636457920074
  timers:
    learn_throughput: 8256.006
    learn_time_ms: 19596.884
    sample_throughput: 23637.143
    sample_time_ms: 6844.82
    update_time_ms: 39.015
  timestamp: 1602631339
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     31 |          829.864 | 5015552 |  241.494 |               291.99 |              128.354 |            802.975 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3333.5767338517694
    time_step_min: 3005
  date: 2020-10-13_23-22-46
  done: false
  episode_len_mean: 802.2630997474747
  episode_reward_max: 291.98989898989913
  episode_reward_mean: 241.95017887205387
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 6336
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6424600233634313
        entropy_coeff: 0.0005000000000000001
        kl: 0.005638489732518792
        model: {}
        policy_loss: -0.009538379235891625
        total_loss: 9.407063643137613
        vf_explained_var: 0.9806671738624573
        vf_loss: 9.416359504063925
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.925806451612903
    gpu_util_percent0: 0.26000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15059258414686935
    mean_env_wait_ms: 1.2069452272778716
    mean_inference_ms: 4.519947853232938
    mean_raw_obs_processing_ms: 0.39058084184727404
  time_since_restore: 856.3313746452332
  time_this_iter_s: 26.467728853225708
  time_total_s: 856.3313746452332
  timers:
    learn_throughput: 8262.755
    learn_time_ms: 19580.879
    sample_throughput: 23611.573
    sample_time_ms: 6852.233
    update_time_ms: 40.618
  timestamp: 1602631366
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     32 |          856.331 | 5177344 |   241.95 |               291.99 |              128.354 |            802.263 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3328.4720242608037
    time_step_min: 2990
  date: 2020-10-13_23-23-13
  done: false
  episode_len_mean: 801.1429864253394
  episode_reward_max: 294.2626262626261
  episode_reward_mean: 242.7336258512729
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 294
  episodes_total: 6630
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6038125157356262
        entropy_coeff: 0.0005000000000000001
        kl: 0.005317067766251664
        model: {}
        policy_loss: -0.008520152943674475
        total_loss: 11.313503424326578
        vf_explained_var: 0.9816839694976807
        vf_loss: 11.321793794631958
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.041935483870972
    gpu_util_percent0: 0.3054838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1504538068895664
    mean_env_wait_ms: 1.2077142390171225
    mean_inference_ms: 4.510905106958503
    mean_raw_obs_processing_ms: 0.3901216077491919
  time_since_restore: 882.7657737731934
  time_this_iter_s: 26.434399127960205
  time_total_s: 882.7657737731934
  timers:
    learn_throughput: 8272.685
    learn_time_ms: 19557.376
    sample_throughput: 23606.74
    sample_time_ms: 6853.636
    update_time_ms: 41.755
  timestamp: 1602631393
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     33 |          882.766 | 5339136 |  242.734 |              294.263 |              128.354 |            801.143 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3325.7595798195
    time_step_min: 2990
  date: 2020-10-13_23-23-40
  done: false
  episode_len_mean: 800.6223138062996
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 243.15886715253802
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 164
  episodes_total: 6794
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6078718453645706
        entropy_coeff: 0.0005000000000000001
        kl: 0.004984718941462536
        model: {}
        policy_loss: -0.012139975135748196
        total_loss: 8.964349667231241
        vf_explained_var: 0.9798333048820496
        vf_loss: 8.976294914881388
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5
    gpu_util_percent0: 0.4083870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15038085023190517
    mean_env_wait_ms: 1.2081187559744715
    mean_inference_ms: 4.506197845308771
    mean_raw_obs_processing_ms: 0.38987923147428927
  time_since_restore: 909.4676880836487
  time_this_iter_s: 26.701914310455322
  time_total_s: 909.4676880836487
  timers:
    learn_throughput: 8272.355
    learn_time_ms: 19558.155
    sample_throughput: 23582.05
    sample_time_ms: 6860.812
    update_time_ms: 46.469
  timestamp: 1602631420
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     34 |          909.468 | 5500928 |  243.159 |              297.596 |              128.354 |            800.622 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3323.0561976307426
    time_step_min: 2990
  date: 2020-10-13_23-24-06
  done: false
  episode_len_mean: 800.114417133822
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 243.6076286800737
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 163
  episodes_total: 6957
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6069925874471664
        entropy_coeff: 0.0005000000000000001
        kl: 0.005795620886298518
        model: {}
        policy_loss: -0.010724431369453669
        total_loss: 8.029765844345093
        vf_explained_var: 0.9822105765342712
        vf_loss: 8.04050381978353
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45161290322581
    gpu_util_percent0: 0.3451612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15031099390943312
    mean_env_wait_ms: 1.2085129154293925
    mean_inference_ms: 4.501663979378934
    mean_raw_obs_processing_ms: 0.3896464258375185
  time_since_restore: 936.2186193466187
  time_this_iter_s: 26.75093126296997
  time_total_s: 936.2186193466187
  timers:
    learn_throughput: 8279.033
    learn_time_ms: 19542.38
    sample_throughput: 23560.296
    sample_time_ms: 6867.146
    update_time_ms: 52.347
  timestamp: 1602631446
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     35 |          936.219 | 5662720 |  243.608 |              297.596 |              128.354 |            800.114 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3317.868691070438
    time_step_min: 2990
  date: 2020-10-13_23-24-33
  done: false
  episode_len_mean: 799.1491651717952
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 244.38338678631214
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 290
  episodes_total: 7247
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.573741133014361
        entropy_coeff: 0.0005000000000000001
        kl: 0.005351585801690817
        model: {}
        policy_loss: -0.010270018964850655
        total_loss: 12.00859030087789
        vf_explained_var: 0.9804748892784119
        vf_loss: 12.018879493077597
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.532258064516128
    gpu_util_percent0: 0.35000000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15019323429690776
    mean_env_wait_ms: 1.2091722047793523
    mean_inference_ms: 4.493989916495516
    mean_raw_obs_processing_ms: 0.38926195839457406
  time_since_restore: 962.8942432403564
  time_this_iter_s: 26.675623893737793
  time_total_s: 962.8942432403564
  timers:
    learn_throughput: 8277.829
    learn_time_ms: 19545.221
    sample_throughput: 23508.4
    sample_time_ms: 6882.306
    update_time_ms: 58.893
  timestamp: 1602631473
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     36 |          962.894 | 5824512 |  244.383 |              297.596 |              128.354 |            799.149 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3314.9324854552833
    time_step_min: 2990
  date: 2020-10-13_23-25-01
  done: false
  episode_len_mean: 798.5587126312954
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 244.84401923898287
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 7426
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5739115178585052
        entropy_coeff: 0.0005000000000000001
        kl: 0.005629358890776833
        model: {}
        policy_loss: -0.010710357882392904
        total_loss: 6.466275771458943
        vf_explained_var: 0.9853144288063049
        vf_loss: 6.476991772651672
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44193548387097
    gpu_util_percent0: 0.24419354838709675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15012476582440737
    mean_env_wait_ms: 1.2095538056315693
    mean_inference_ms: 4.489546398299661
    mean_raw_obs_processing_ms: 0.38903245629177985
  time_since_restore: 989.9161942005157
  time_this_iter_s: 27.0219509601593
  time_total_s: 989.9161942005157
  timers:
    learn_throughput: 8270.414
    learn_time_ms: 19562.745
    sample_throughput: 23417.858
    sample_time_ms: 6908.915
    update_time_ms: 57.691
  timestamp: 1602631501
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     37 |          989.916 | 5986304 |  244.844 |              297.596 |              128.354 |            798.559 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3312.517410300543
    time_step_min: 2990
  date: 2020-10-13_23-25-27
  done: false
  episode_len_mean: 798.0349235635214
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 245.2278864022406
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 162
  episodes_total: 7588
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5793250352144241
        entropy_coeff: 0.0005000000000000001
        kl: 0.00574223801959306
        model: {}
        policy_loss: -0.0108771194354631
        total_loss: 8.031128525733948
        vf_explained_var: 0.9812270998954773
        vf_loss: 8.04200828075409
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.276666666666667
    gpu_util_percent0: 0.43133333333333346
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1500648863251376
    mean_env_wait_ms: 1.2098964172254079
    mean_inference_ms: 4.485654111401292
    mean_raw_obs_processing_ms: 0.38883318741183137
  time_since_restore: 1016.2229979038239
  time_this_iter_s: 26.306803703308105
  time_total_s: 1016.2229979038239
  timers:
    learn_throughput: 8283.188
    learn_time_ms: 19532.576
    sample_throughput: 23396.507
    sample_time_ms: 6915.22
    update_time_ms: 58.297
  timestamp: 1602631527
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     38 |          1016.22 | 6148096 |  245.228 |              297.596 |              128.354 |            798.035 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3308.2096733027056
    time_step_min: 2988
  date: 2020-10-13_23-25-54
  done: false
  episode_len_mean: 797.2153474780841
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 245.93591485943156
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 283
  episodes_total: 7871
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5480562746524811
        entropy_coeff: 0.0005000000000000001
        kl: 0.005499842731902997
        model: {}
        policy_loss: -0.008937571406325636
        total_loss: 10.389391978581747
        vf_explained_var: 0.9827632904052734
        vf_loss: 10.398328224817911
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10322580645161
    gpu_util_percent0: 0.2661290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14996300417600586
    mean_env_wait_ms: 1.2104637527380766
    mean_inference_ms: 4.479088870052137
    mean_raw_obs_processing_ms: 0.38849786379295015
  time_since_restore: 1042.863877773285
  time_this_iter_s: 26.64087986946106
  time_total_s: 1042.863877773285
  timers:
    learn_throughput: 8278.777
    learn_time_ms: 19542.983
    sample_throughput: 23384.093
    sample_time_ms: 6918.891
    update_time_ms: 57.421
  timestamp: 1602631554
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     39 |          1042.86 | 6309888 |  245.936 |              297.596 |              128.354 |            797.215 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3305.058456936308
    time_step_min: 2988
  date: 2020-10-13_23-26-20
  done: false
  episode_len_mean: 796.6839166046166
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 246.41884970328752
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 8058
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5393295685450236
        entropy_coeff: 0.0005000000000000001
        kl: 0.005108564626425505
        model: {}
        policy_loss: -0.012748330686008558
        total_loss: 7.700785279273987
        vf_explained_var: 0.9828435778617859
        vf_loss: 7.713547786076863
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.869999999999997
    gpu_util_percent0: 0.35166666666666657
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14990136175819788
    mean_env_wait_ms: 1.2108146978089116
    mean_inference_ms: 4.475061041052438
    mean_raw_obs_processing_ms: 0.388296503287186
  time_since_restore: 1069.1527481079102
  time_this_iter_s: 26.288870334625244
  time_total_s: 1069.1527481079102
  timers:
    learn_throughput: 8279.42
    learn_time_ms: 19541.466
    sample_throughput: 23462.335
    sample_time_ms: 6895.818
    update_time_ms: 57.065
  timestamp: 1602631580
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     40 |          1069.15 | 6471680 |  246.419 |              297.596 |              128.354 |            796.684 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3302.5930360415396
    time_step_min: 2988
  date: 2020-10-13_23-26-47
  done: false
  episode_len_mean: 796.2091240875912
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 246.79267123792673
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 162
  episodes_total: 8220
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5510972589254379
        entropy_coeff: 0.0005000000000000001
        kl: 0.005666492041200399
        model: {}
        policy_loss: -0.012003632000414655
        total_loss: 8.288190285364786
        vf_explained_var: 0.9800311923027039
        vf_loss: 8.300186077753702
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706451612903226
    gpu_util_percent0: 0.36806451612903224
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1498478955521746
    mean_env_wait_ms: 1.211117108723359
    mean_inference_ms: 4.471621605142911
    mean_raw_obs_processing_ms: 0.38812169545139874
  time_since_restore: 1095.4750690460205
  time_this_iter_s: 26.32232093811035
  time_total_s: 1095.4750690460205
  timers:
    learn_throughput: 8282.177
    learn_time_ms: 19534.96
    sample_throughput: 23492.432
    sample_time_ms: 6886.984
    update_time_ms: 56.688
  timestamp: 1602631607
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     41 |          1095.48 | 6633472 |  246.793 |              297.596 |              128.354 |            796.209 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3298.630501656413
    time_step_min: 2988
  date: 2020-10-13_23-27-14
  done: false
  episode_len_mean: 795.5553199010251
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 247.40916767533946
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 267
  episodes_total: 8487
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5244901676972707
        entropy_coeff: 0.0005000000000000001
        kl: 0.00515878254858156
        model: {}
        policy_loss: -0.011081617277037973
        total_loss: 10.254097620646158
        vf_explained_var: 0.9823076725006104
        vf_loss: 10.265183846155802
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02
    gpu_util_percent0: 0.27666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1497636700970083
    mean_env_wait_ms: 1.2115921161555612
    mean_inference_ms: 4.466180462102501
    mean_raw_obs_processing_ms: 0.38784818796878895
  time_since_restore: 1122.2007422447205
  time_this_iter_s: 26.72567319869995
  time_total_s: 1122.2007422447205
  timers:
    learn_throughput: 8274.483
    learn_time_ms: 19553.124
    sample_throughput: 23466.855
    sample_time_ms: 6894.49
    update_time_ms: 55.281
  timestamp: 1602631634
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     42 |           1122.2 | 6795264 |  247.409 |              297.596 |              128.354 |            795.555 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3295.431426920855
    time_step_min: 2984
  date: 2020-10-13_23-27-41
  done: false
  episode_len_mean: 795.1115074798619
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 247.86582743429693
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 203
  episodes_total: 8690
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5074903890490532
        entropy_coeff: 0.0005000000000000001
        kl: 0.005213672256407638
        model: {}
        policy_loss: -0.011148126039188355
        total_loss: 7.967456380526225
        vf_explained_var: 0.9832329750061035
        vf_loss: 7.9785975615183515
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.341935483870973
    gpu_util_percent0: 0.3503225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14970396522715262
    mean_env_wait_ms: 1.2119270742997874
    mean_inference_ms: 4.46228418801303
    mean_raw_obs_processing_ms: 0.38765430553285946
  time_since_restore: 1148.8151230812073
  time_this_iter_s: 26.614380836486816
  time_total_s: 1148.8151230812073
  timers:
    learn_throughput: 8260.879
    learn_time_ms: 19585.325
    sample_throughput: 23513.777
    sample_time_ms: 6880.732
    update_time_ms: 53.973
  timestamp: 1602631661
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     43 |          1148.82 | 6957056 |  247.866 |              297.596 |              128.354 |            795.112 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3292.996483267158
    time_step_min: 2984
  date: 2020-10-13_23-28-08
  done: false
  episode_len_mean: 794.7411299435029
  episode_reward_max: 297.5959595959596
  episode_reward_mean: 248.2568567026194
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 160
  episodes_total: 8850
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5157928119103113
        entropy_coeff: 0.0005000000000000001
        kl: 0.005249064958964785
        model: {}
        policy_loss: -0.011975913619001707
        total_loss: 6.316017548243205
        vf_explained_var: 0.9843130707740784
        vf_loss: 6.327988902727763
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.923333333333336
    gpu_util_percent0: 0.30566666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14965770314396215
    mean_env_wait_ms: 1.2121867698483373
    mean_inference_ms: 4.459291605994013
    mean_raw_obs_processing_ms: 0.38750472409064396
  time_since_restore: 1175.5915122032166
  time_this_iter_s: 26.776389122009277
  time_total_s: 1175.5915122032166
  timers:
    learn_throughput: 8251.579
    learn_time_ms: 19607.399
    sample_throughput: 23548.808
    sample_time_ms: 6870.496
    update_time_ms: 49.305
  timestamp: 1602631688
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     44 |          1175.59 | 7118848 |  248.257 |              297.596 |              128.354 |            794.741 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3288.906484340538
    time_step_min: 2984
  date: 2020-10-13_23-28-35
  done: false
  episode_len_mean: 794.1748873997583
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 248.82141307616422
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 253
  episodes_total: 9103
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4996359671155612
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045946843844528
        model: {}
        policy_loss: -0.010852797965829572
        total_loss: 11.265838940938314
        vf_explained_var: 0.9799129366874695
        vf_loss: 11.276711622873941
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.987096774193553
    gpu_util_percent0: 0.27806451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1495875861638289
    mean_env_wait_ms: 1.212578729922747
    mean_inference_ms: 4.454737330437276
    mean_raw_obs_processing_ms: 0.3872753986518466
  time_since_restore: 1202.269697189331
  time_this_iter_s: 26.678184986114502
  time_total_s: 1202.269697189331
  timers:
    learn_throughput: 8242.386
    learn_time_ms: 19629.267
    sample_throughput: 23629.582
    sample_time_ms: 6847.011
    update_time_ms: 43.583
  timestamp: 1602631715
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     45 |          1202.27 | 7280640 |  248.821 |              305.323 |              128.354 |            794.175 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3285.604070205664
    time_step_min: 2984
  date: 2020-10-13_23-29-02
  done: false
  episode_len_mean: 793.6486805406565
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 249.288631866834
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 9322
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.48180339982112247
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053388803110768395
        model: {}
        policy_loss: -0.009326927965351691
        total_loss: 7.430981198946635
        vf_explained_var: 0.9844884276390076
        vf_loss: 7.440415342648824
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.448387096774194
    gpu_util_percent0: 0.2751612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14952939318752395
    mean_env_wait_ms: 1.2129032669220847
    mean_inference_ms: 4.450945817158842
    mean_raw_obs_processing_ms: 0.3870935634430544
  time_since_restore: 1229.0805189609528
  time_this_iter_s: 26.810821771621704
  time_total_s: 1229.0805189609528
  timers:
    learn_throughput: 8219.457
    learn_time_ms: 19684.026
    sample_throughput: 23751.699
    sample_time_ms: 6811.807
    update_time_ms: 37.761
  timestamp: 1602631742
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     46 |          1229.08 | 7442432 |  249.289 |              305.323 |              128.354 |            793.649 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3283.188988883007
    time_step_min: 2984
  date: 2020-10-13_23-29-28
  done: false
  episode_len_mean: 793.2549578059072
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 249.6373385756297
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 158
  episodes_total: 9480
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4932329977552096
        entropy_coeff: 0.0005000000000000001
        kl: 0.005842235443803172
        model: {}
        policy_loss: -0.012003136109948779
        total_loss: 6.401867707570394
        vf_explained_var: 0.983512818813324
        vf_loss: 6.413971463839213
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32
    gpu_util_percent0: 0.2906666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14948882716422432
    mean_env_wait_ms: 1.2131280525909196
    mean_inference_ms: 4.4483075645536445
    mean_raw_obs_processing_ms: 0.38696387535114873
  time_since_restore: 1255.5903022289276
  time_this_iter_s: 26.509783267974854
  time_total_s: 1255.5903022289276
  timers:
    learn_throughput: 8226.708
    learn_time_ms: 19666.677
    sample_throughput: 23877.947
    sample_time_ms: 6775.792
    update_time_ms: 39.055
  timestamp: 1602631768
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     47 |          1255.59 | 7604224 |  249.637 |              305.323 |              128.354 |            793.255 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3279.770158795628
    time_step_min: 2975
  date: 2020-10-13_23-29-55
  done: false
  episode_len_mean: 792.6430699681496
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 250.1550758795185
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 253
  episodes_total: 9733
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4747646301984787
        entropy_coeff: 0.0005000000000000001
        kl: 0.00540254928637296
        model: {}
        policy_loss: -0.008263791542655477
        total_loss: 8.609153588612875
        vf_explained_var: 0.9846184253692627
        vf_loss: 8.617519617080688
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14942569634779237
    mean_env_wait_ms: 1.213485900096928
    mean_inference_ms: 4.444224666882565
    mean_raw_obs_processing_ms: 0.38676083202845585
  time_since_restore: 1281.960603237152
  time_this_iter_s: 26.370301008224487
  time_total_s: 1281.960603237152
  timers:
    learn_throughput: 8222.271
    learn_time_ms: 19677.288
    sample_throughput: 23891.277
    sample_time_ms: 6772.011
    update_time_ms: 37.349
  timestamp: 1602631795
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     48 |          1281.96 | 7766016 |  250.155 |              305.323 |              128.354 |            792.643 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3276.672245186007
    time_step_min: 2968
  date: 2020-10-13_23-30-22
  done: false
  episode_len_mean: 792.1457705445047
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 250.63314275972505
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 221
  episodes_total: 9954
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.45495350658893585
        entropy_coeff: 0.0005000000000000001
        kl: 0.005297816319701572
        model: {}
        policy_loss: -0.011545950949463682
        total_loss: 6.367646296819051
        vf_explained_var: 0.9864680767059326
        vf_loss: 6.379287123680115
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.841935483870976
    gpu_util_percent0: 0.3590322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870956
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14937327972225678
    mean_env_wait_ms: 1.213780546427839
    mean_inference_ms: 4.440804742258456
    mean_raw_obs_processing_ms: 0.38659887399680515
  time_since_restore: 1308.5479185581207
  time_this_iter_s: 26.587315320968628
  time_total_s: 1308.5479185581207
  timers:
    learn_throughput: 8215.56
    learn_time_ms: 19693.361
    sample_throughput: 23966.961
    sample_time_ms: 6750.626
    update_time_ms: 36.212
  timestamp: 1602631822
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     49 |          1308.55 | 7927808 |  250.633 |              305.323 |              128.354 |            792.146 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3274.7784282595753
    time_step_min: 2950
  date: 2020-10-13_23-30-48
  done: false
  episode_len_mean: 791.7395431622664
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 250.94052959137503
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 10113
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4651120329896609
        entropy_coeff: 0.0005000000000000001
        kl: 0.005508953550209601
        model: {}
        policy_loss: -0.011178916155283028
        total_loss: 6.434786359469096
        vf_explained_var: 0.9834200739860535
        vf_loss: 6.446059942245483
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.133333333333336
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14933621340066797
    mean_env_wait_ms: 1.2139848317314164
    mean_inference_ms: 4.438412995907615
    mean_raw_obs_processing_ms: 0.38648126134543387
  time_since_restore: 1334.934420824051
  time_this_iter_s: 26.386502265930176
  time_total_s: 1334.934420824051
  timers:
    learn_throughput: 8211.735
    learn_time_ms: 19702.535
    sample_throughput: 23960.905
    sample_time_ms: 6752.332
    update_time_ms: 34.404
  timestamp: 1602631848
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     50 |          1334.93 | 8089600 |  250.941 |              305.323 |              128.354 |             791.74 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3271.786591854503
    time_step_min: 2950
  date: 2020-10-13_23-31-16
  done: false
  episode_len_mean: 791.0770343231778
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 251.40680230768928
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 259
  episodes_total: 10372
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.44740036129951477
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051830766024068
        model: {}
        policy_loss: -0.011204511916730553
        total_loss: 10.451243956883749
        vf_explained_var: 0.9811357855796814
        vf_loss: 10.46254269282023
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.661290322580644
    gpu_util_percent0: 0.2645161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419358
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14927786810889415
    mean_env_wait_ms: 1.214326452638826
    mean_inference_ms: 4.434659301807351
    mean_raw_obs_processing_ms: 0.3862975872672191
  time_since_restore: 1361.8395717144012
  time_this_iter_s: 26.905150890350342
  time_total_s: 1361.8395717144012
  timers:
    learn_throughput: 8189.329
    learn_time_ms: 19756.441
    sample_throughput: 23950.821
    sample_time_ms: 6755.176
    update_time_ms: 34.315
  timestamp: 1602631876
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     51 |          1361.84 | 8251392 |  251.407 |              305.323 |              128.354 |            791.077 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3269.3174106719744
    time_step_min: 2950
  date: 2020-10-13_23-31-42
  done: false
  episode_len_mean: 790.5274891365955
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 251.79537391676067
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 214
  episodes_total: 10586
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42545434335867566
        entropy_coeff: 0.0005000000000000001
        kl: 0.005011683058304091
        model: {}
        policy_loss: -0.01152935406571487
        total_loss: 5.990321119626363
        vf_explained_var: 0.986952006816864
        vf_loss: 6.0019378662109375
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.27741935483871
    gpu_util_percent0: 0.37322580645161296
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870956
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14923184791965513
    mean_env_wait_ms: 1.214583634553271
    mean_inference_ms: 4.431658517152646
    mean_raw_obs_processing_ms: 0.3861538510589809
  time_since_restore: 1388.1894354820251
  time_this_iter_s: 26.3498637676239
  time_total_s: 1388.1894354820251
  timers:
    learn_throughput: 8192.793
    learn_time_ms: 19748.089
    sample_throughput: 24055.714
    sample_time_ms: 6725.72
    update_time_ms: 33.873
  timestamp: 1602631902
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     52 |          1388.19 | 8413184 |  251.795 |              305.323 |              128.354 |            790.527 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3267.739962651727
    time_step_min: 2950
  date: 2020-10-13_23-32-09
  done: false
  episode_len_mean: 790.1010702652396
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 252.04978589994877
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 159
  episodes_total: 10745
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4393023227651914
        entropy_coeff: 0.0005000000000000001
        kl: 0.005606093637955685
        model: {}
        policy_loss: -0.010608289807957286
        total_loss: 5.841294089953105
        vf_explained_var: 0.9850214123725891
        vf_loss: 5.85198179880778
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17333333333334
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14919830706288614
    mean_env_wait_ms: 1.2147752460934513
    mean_inference_ms: 4.429494502406742
    mean_raw_obs_processing_ms: 0.3860479291921575
  time_since_restore: 1414.669938325882
  time_this_iter_s: 26.48050284385681
  time_total_s: 1414.669938325882
  timers:
    learn_throughput: 8210.572
    learn_time_ms: 19705.325
    sample_throughput: 23946.174
    sample_time_ms: 6756.486
    update_time_ms: 31.603
  timestamp: 1602631929
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     53 |          1414.67 | 8574976 |   252.05 |              305.323 |              128.354 |            790.101 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3264.7645290581163
    time_step_min: 2950
  date: 2020-10-13_23-32-36
  done: false
  episode_len_mean: 789.3747389448833
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 252.48673606123896
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 268
  episodes_total: 11013
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4164704183737437
        entropy_coeff: 0.0005000000000000001
        kl: 0.004910058225505054
        model: {}
        policy_loss: -0.008640341703236723
        total_loss: 8.891689221064249
        vf_explained_var: 0.9841923713684082
        vf_loss: 8.900414943695068
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.287096774193547
    gpu_util_percent0: 0.28967741935483876
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1491437369660914
    mean_env_wait_ms: 1.2151093457091227
    mean_inference_ms: 4.4259648710668404
    mean_raw_obs_processing_ms: 0.3858770396147702
  time_since_restore: 1441.3013851642609
  time_this_iter_s: 26.631446838378906
  time_total_s: 1441.3013851642609
  timers:
    learn_throughput: 8226.075
    learn_time_ms: 19668.19
    sample_throughput: 23866.811
    sample_time_ms: 6778.953
    update_time_ms: 31.407
  timestamp: 1602631956
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     54 |           1441.3 | 8736768 |  252.487 |              305.323 |              128.354 |            789.375 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3262.4488956451755
    time_step_min: 2950
  date: 2020-10-13_23-33-03
  done: false
  episode_len_mean: 788.8435550008915
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 252.84319482937775
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 205
  episodes_total: 11218
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4035197471578916
        entropy_coeff: 0.0005000000000000001
        kl: 0.005553202470764518
        model: {}
        policy_loss: -0.009300931472656279
        total_loss: 5.939739346504211
        vf_explained_var: 0.9866606593132019
        vf_loss: 5.949172576268514
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.15483870967742
    gpu_util_percent0: 0.2929032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.790322580645161
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14910382243629883
    mean_env_wait_ms: 1.2153412228339588
    mean_inference_ms: 4.423376820675712
    mean_raw_obs_processing_ms: 0.3857527119064645
  time_since_restore: 1468.081119298935
  time_this_iter_s: 26.779734134674072
  time_total_s: 1468.081119298935
  timers:
    learn_throughput: 8222.528
    learn_time_ms: 19676.674
    sample_throughput: 23856.506
    sample_time_ms: 6781.882
    update_time_ms: 29.274
  timestamp: 1602631983
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     55 |          1468.08 | 8898560 |  252.843 |              305.323 |              128.354 |            788.844 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3260.695372410754
    time_step_min: 2950
  date: 2020-10-13_23-33-30
  done: false
  episode_len_mean: 788.4505272407733
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 253.1275274715521
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 162
  episodes_total: 11380
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4184493770202001
        entropy_coeff: 0.0005000000000000001
        kl: 0.005551290077467759
        model: {}
        policy_loss: -0.011252332527268058
        total_loss: 5.403757611910502
        vf_explained_var: 0.9860562682151794
        vf_loss: 5.415149927139282
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33666666666667
    gpu_util_percent0: 0.38833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14907261348529532
    mean_env_wait_ms: 1.2155328838212822
    mean_inference_ms: 4.421384530558689
    mean_raw_obs_processing_ms: 0.38565577673064944
  time_since_restore: 1494.4972774982452
  time_this_iter_s: 26.416158199310303
  time_total_s: 1494.4972774982452
  timers:
    learn_throughput: 8245.934
    learn_time_ms: 19620.821
    sample_throughput: 23796.946
    sample_time_ms: 6798.856
    update_time_ms: 28.064
  timestamp: 1602632010
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     56 |           1494.5 | 9060352 |  253.128 |              305.323 |              128.354 |            788.451 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3257.6070906118234
    time_step_min: 2950
  date: 2020-10-13_23-33-57
  done: false
  episode_len_mean: 787.7989018531229
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 253.62370444319654
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 276
  episodes_total: 11656
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.39494340121746063
        entropy_coeff: 0.0005000000000000001
        kl: 0.005176450125873089
        model: {}
        policy_loss: -0.010596645918364326
        total_loss: 7.448789477348328
        vf_explained_var: 0.9863360524177551
        vf_loss: 7.459518949190776
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.24193548387097
    gpu_util_percent0: 0.32322580645161286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14902135543664258
    mean_env_wait_ms: 1.2158455631672327
    mean_inference_ms: 4.418071492931422
    mean_raw_obs_processing_ms: 0.3854971801173113
  time_since_restore: 1521.3155536651611
  time_this_iter_s: 26.818276166915894
  time_total_s: 1521.3155536651611
  timers:
    learn_throughput: 8241.587
    learn_time_ms: 19631.172
    sample_throughput: 23726.767
    sample_time_ms: 6818.965
    update_time_ms: 27.698
  timestamp: 1602632037
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     57 |          1521.32 | 9222144 |  253.624 |              305.323 |              128.354 |            787.799 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3255.520694033009
    time_step_min: 2950
  date: 2020-10-13_23-34-24
  done: false
  episode_len_mean: 787.3697046413503
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 253.92976601457616
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 11850
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.38347873588403064
        entropy_coeff: 0.0005000000000000001
        kl: 0.004819899913854897
        model: {}
        policy_loss: -0.010946293051044146
        total_loss: 6.16417129834493
        vf_explained_var: 0.98579341173172
        vf_loss: 6.175249099731445
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.75483870967742
    gpu_util_percent0: 0.37612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14898768341946161
    mean_env_wait_ms: 1.216050836460579
    mean_inference_ms: 4.415861788064245
    mean_raw_obs_processing_ms: 0.38538870037805173
  time_since_restore: 1548.056583404541
  time_this_iter_s: 26.741029739379883
  time_total_s: 1548.056583404541
  timers:
    learn_throughput: 8232.974
    learn_time_ms: 19651.709
    sample_throughput: 23669.208
    sample_time_ms: 6835.548
    update_time_ms: 26.761
  timestamp: 1602632064
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     58 |          1548.06 | 9383936 |   253.93 |              305.323 |              128.354 |             787.37 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3253.8100642576983
    time_step_min: 2950
  date: 2020-10-13_23-34-50
  done: false
  episode_len_mean: 787.0014977533699
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 254.19643766673224
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 12018
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.39791788657506305
        entropy_coeff: 0.0005000000000000001
        kl: 0.005529815602737169
        model: {}
        policy_loss: -0.011331992524598414
        total_loss: 6.490147113800049
        vf_explained_var: 0.9842641353607178
        vf_loss: 6.501643260320027
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01935483870968
    gpu_util_percent0: 0.257741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489597710447148
    mean_env_wait_ms: 1.2162338416771963
    mean_inference_ms: 4.413997704116125
    mean_raw_obs_processing_ms: 0.3852977867985234
  time_since_restore: 1574.547028541565
  time_this_iter_s: 26.490445137023926
  time_total_s: 1574.547028541565
  timers:
    learn_throughput: 8240.151
    learn_time_ms: 19634.593
    sample_throughput: 23647.967
    sample_time_ms: 6841.687
    update_time_ms: 26.706
  timestamp: 1602632090
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     59 |          1574.55 | 9545728 |  254.196 |              305.323 |              128.354 |            787.001 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3250.8934920893817
    time_step_min: 2950
  date: 2020-10-13_23-35-17
  done: false
  episode_len_mean: 786.4229486866716
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 254.6451659803697
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 279
  episodes_total: 12297
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3765448456009229
        entropy_coeff: 0.0005000000000000001
        kl: 0.005654264163846771
        model: {}
        policy_loss: -0.009280701555932561
        total_loss: 7.544934113820394
        vf_explained_var: 0.9866731762886047
        vf_loss: 7.554367701212565
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5
    gpu_util_percent0: 0.4006451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1489116188047896
    mean_env_wait_ms: 1.216523628292506
    mean_inference_ms: 4.410933091067161
    mean_raw_obs_processing_ms: 0.38515004548249454
  time_since_restore: 1601.2931959629059
  time_this_iter_s: 26.746167421340942
  time_total_s: 1601.2931959629059
  timers:
    learn_throughput: 8231.806
    learn_time_ms: 19654.496
    sample_throughput: 23619.922
    sample_time_ms: 6849.811
    update_time_ms: 26.797
  timestamp: 1602632117
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     60 |          1601.29 | 9707520 |  254.645 |              305.323 |              128.354 |            786.423 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3249.0492488149753
    time_step_min: 2950
  date: 2020-10-13_23-35-45
  done: false
  episode_len_mean: 786.0809966351546
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 254.93048494883135
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 185
  episodes_total: 12482
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3720836068193118
        entropy_coeff: 0.0005000000000000001
        kl: 0.005019095997946958
        model: {}
        policy_loss: -0.011203962999085585
        total_loss: 5.8692318598429365
        vf_explained_var: 0.9862847328186035
        vf_loss: 5.880590558052063
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.674193548387095
    gpu_util_percent0: 0.302258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488814951489637
    mean_env_wait_ms: 1.216704277220547
    mean_inference_ms: 4.408993686564575
    mean_raw_obs_processing_ms: 0.38505753719725766
  time_since_restore: 1628.330150604248
  time_this_iter_s: 27.036954641342163
  time_total_s: 1628.330150604248
  timers:
    learn_throughput: 8231.045
    learn_time_ms: 19656.313
    sample_throughput: 23577.939
    sample_time_ms: 6862.008
    update_time_ms: 25.345
  timestamp: 1602632145
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     61 |          1628.33 | 9869312 |   254.93 |              305.323 |              128.354 |            786.081 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3247.1057136064665
    time_step_min: 2950
  date: 2020-10-13_23-36-12
  done: false
  episode_len_mean: 785.7563616247827
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 255.2223180117917
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 12654
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3821555897593498
        entropy_coeff: 0.0005000000000000001
        kl: 0.005336747388355434
        model: {}
        policy_loss: -0.011190150883824876
        total_loss: 5.106372197469075
        vf_explained_var: 0.9875199198722839
        vf_loss: 5.117720087369283
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.906666666666677
    gpu_util_percent0: 0.269
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14885498060198327
    mean_env_wait_ms: 1.216876141800145
    mean_inference_ms: 4.4072348962626755
    mean_raw_obs_processing_ms: 0.3849720749974584
  time_since_restore: 1655.0303375720978
  time_this_iter_s: 26.70018696784973
  time_total_s: 1655.0303375720978
  timers:
    learn_throughput: 8229.301
    learn_time_ms: 19660.478
    sample_throughput: 23483.863
    sample_time_ms: 6889.497
    update_time_ms: 27.059
  timestamp: 1602632172
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     62 |          1655.03 | 10031104 |  255.222 |              305.323 |              128.354 |            785.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3244.266020922123
    time_step_min: 2950
  date: 2020-10-13_23-36-38
  done: false
  episode_len_mean: 785.2134466769706
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 255.66669398779135
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 286
  episodes_total: 12940
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3558207228779793
        entropy_coeff: 0.0005000000000000001
        kl: 0.004898939708558221
        model: {}
        policy_loss: -0.009166850621113554
        total_loss: 7.341542323430379
        vf_explained_var: 0.9869981408119202
        vf_loss: 7.35085650285085
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.829032258064515
    gpu_util_percent0: 0.31612903225806455
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14880880717494518
    mean_env_wait_ms: 1.2171509149620707
    mean_inference_ms: 4.404372901280829
    mean_raw_obs_processing_ms: 0.3848364733610333
  time_since_restore: 1681.5901494026184
  time_this_iter_s: 26.55981183052063
  time_total_s: 1681.5901494026184
  timers:
    learn_throughput: 8223.715
    learn_time_ms: 19673.832
    sample_throughput: 23515.135
    sample_time_ms: 6880.335
    update_time_ms: 29.827
  timestamp: 1602632198
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     63 |          1681.59 | 10192896 |  255.667 |              305.323 |              128.354 |            785.213 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3242.5269516018043
    time_step_min: 2950
  date: 2020-10-13_23-37-06
  done: false
  episode_len_mean: 784.9008692999847
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 255.9368374918932
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 174
  episodes_total: 13114
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3503326053420703
        entropy_coeff: 0.0005000000000000001
        kl: 0.005159190545479457
        model: {}
        policy_loss: -0.010730534287480017
        total_loss: 4.31449298063914
        vf_explained_var: 0.9890246391296387
        vf_loss: 4.325382471084595
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.512903225806458
    gpu_util_percent0: 0.2867741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903227
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14878338922444434
    mean_env_wait_ms: 1.2173063094643024
    mean_inference_ms: 4.402682044603893
    mean_raw_obs_processing_ms: 0.3847541399670941
  time_since_restore: 1708.446962594986
  time_this_iter_s: 26.856813192367554
  time_total_s: 1708.446962594986
  timers:
    learn_throughput: 8204.55
    learn_time_ms: 19719.79
    sample_throughput: 23602.932
    sample_time_ms: 6854.742
    update_time_ms: 30.036
  timestamp: 1602632226
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     64 |          1708.45 | 10354688 |  255.937 |              305.323 |              128.354 |            784.901 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3240.8319251659627
    time_step_min: 2950
  date: 2020-10-13_23-37-32
  done: false
  episode_len_mean: 784.6386276427658
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 256.18322416095344
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 177
  episodes_total: 13291
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3594164252281189
        entropy_coeff: 0.0005000000000000001
        kl: 0.005062125584421058
        model: {}
        policy_loss: -0.011500527684499199
        total_loss: 5.815953413645427
        vf_explained_var: 0.9867269396781921
        vf_loss: 5.827617843945821
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.42258064516129
    gpu_util_percent0: 0.3903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14875822258182594
    mean_env_wait_ms: 1.2174659976311235
    mean_inference_ms: 4.4010185611045305
    mean_raw_obs_processing_ms: 0.3846719650406161
  time_since_restore: 1735.0192663669586
  time_this_iter_s: 26.572303771972656
  time_total_s: 1735.0192663669586
  timers:
    learn_throughput: 8209.579
    learn_time_ms: 19707.709
    sample_throughput: 23636.006
    sample_time_ms: 6845.15
    update_time_ms: 30.687
  timestamp: 1602632252
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     65 |          1735.02 | 10516480 |  256.183 |              305.323 |              128.354 |            784.639 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3238.2538779731126
    time_step_min: 2950
  date: 2020-10-13_23-37-59
  done: false
  episode_len_mean: 784.2552862300155
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 256.57828636322705
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 282
  episodes_total: 13573
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3380468636751175
        entropy_coeff: 0.0005000000000000001
        kl: 0.004767467073785762
        model: {}
        policy_loss: -0.011347987786090622
        total_loss: 7.793144861857097
        vf_explained_var: 0.9862980842590332
        vf_loss: 7.804647008577983
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.053333333333335
    gpu_util_percent0: 0.2963333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14871595057304487
    mean_env_wait_ms: 1.2177157745814062
    mean_inference_ms: 4.398416475177385
    mean_raw_obs_processing_ms: 0.38454915119748345
  time_since_restore: 1761.604943037033
  time_this_iter_s: 26.585676670074463
  time_total_s: 1761.604943037033
  timers:
    learn_throughput: 8201.412
    learn_time_ms: 19727.335
    sample_throughput: 23651.618
    sample_time_ms: 6840.631
    update_time_ms: 31.443
  timestamp: 1602632279
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     66 |           1761.6 | 10678272 |  256.578 |              305.323 |              128.354 |            784.255 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3236.714681642477
    time_step_min: 2950
  date: 2020-10-13_23-38-26
  done: false
  episode_len_mean: 783.9962170813327
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 256.80994581343776
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 173
  episodes_total: 13746
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3312930141886075
        entropy_coeff: 0.0005000000000000001
        kl: 0.004902967523473005
        model: {}
        policy_loss: -0.010100232805901518
        total_loss: 4.8077118794123335
        vf_explained_var: 0.9883539080619812
        vf_loss: 4.8179701169331866
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.480645161290326
    gpu_util_percent0: 0.34548387096774197
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419344
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486924364802764
    mean_env_wait_ms: 1.2178543705875113
    mean_inference_ms: 4.3968686598670175
    mean_raw_obs_processing_ms: 0.38447278853906325
  time_since_restore: 1788.0309903621674
  time_this_iter_s: 26.426047325134277
  time_total_s: 1788.0309903621674
  timers:
    learn_throughput: 8209.505
    learn_time_ms: 19707.888
    sample_throughput: 23722.24
    sample_time_ms: 6820.266
    update_time_ms: 30.713
  timestamp: 1602632306
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     67 |          1788.03 | 10840064 |   256.81 |              305.323 |              128.354 |            783.996 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3235.0881887498203
    time_step_min: 2950
  date: 2020-10-13_23-38-53
  done: false
  episode_len_mean: 783.689746717371
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.0585303418051
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 13937
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3444381132721901
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056744702548409505
        model: {}
        policy_loss: -0.012320043751969934
        total_loss: 5.890604615211487
        vf_explained_var: 0.9868749976158142
        vf_loss: 5.903092543284099
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.98
    gpu_util_percent0: 0.3423333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148665819149557
    mean_env_wait_ms: 1.2180089098940756
    mean_inference_ms: 4.39519599638494
    mean_raw_obs_processing_ms: 0.3843903276465843
  time_since_restore: 1814.6212124824524
  time_this_iter_s: 26.590222120285034
  time_total_s: 1814.6212124824524
  timers:
    learn_throughput: 8211.043
    learn_time_ms: 19704.197
    sample_throughput: 23768.88
    sample_time_ms: 6806.884
    update_time_ms: 32.242
  timestamp: 1602632333
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     68 |          1814.62 | 11001856 |  257.059 |              305.323 |              128.354 |             783.69 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3232.8164244391137
    time_step_min: 2950
  date: 2020-10-13_23-39-20
  done: false
  episode_len_mean: 783.2805264269125
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.41160141068656
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 272
  episodes_total: 14209
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3151191423336665
        entropy_coeff: 0.0005000000000000001
        kl: 0.004452489976150294
        model: {}
        policy_loss: -0.009892117969381312
        total_loss: 7.8788909912109375
        vf_explained_var: 0.9861714243888855
        vf_loss: 7.888937473297119
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.283870967741933
    gpu_util_percent0: 0.3464516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486294278741103
    mean_env_wait_ms: 1.2182301902906871
    mean_inference_ms: 4.392892473959196
    mean_raw_obs_processing_ms: 0.38428095856408356
  time_since_restore: 1840.971180677414
  time_this_iter_s: 26.349968194961548
  time_total_s: 1840.971180677414
  timers:
    learn_throughput: 8220.42
    learn_time_ms: 19681.719
    sample_throughput: 23748.169
    sample_time_ms: 6812.82
    update_time_ms: 32.489
  timestamp: 1602632360
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     69 |          1840.97 | 11163648 |  257.412 |              305.323 |              128.354 |            783.281 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3231.107648330196
    time_step_min: 2950
  date: 2020-10-13_23-39-46
  done: false
  episode_len_mean: 783.0469467241619
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.67121134842654
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 169
  episodes_total: 14378
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3170602818330129
        entropy_coeff: 0.0005000000000000001
        kl: 0.004908886000824471
        model: {}
        policy_loss: -0.010506820544833317
        total_loss: 3.9420782128969827
        vf_explained_var: 0.9898332953453064
        vf_loss: 3.952741583188375
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.029032258064515
    gpu_util_percent0: 0.3209677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14860808249371604
    mean_env_wait_ms: 1.2183548260540105
    mean_inference_ms: 4.391504191557591
    mean_raw_obs_processing_ms: 0.38421208543931834
  time_since_restore: 1867.3644211292267
  time_this_iter_s: 26.393240451812744
  time_total_s: 1867.3644211292267
  timers:
    learn_throughput: 8232.39
    learn_time_ms: 19653.101
    sample_throughput: 23753.21
    sample_time_ms: 6811.374
    update_time_ms: 33.139
  timestamp: 1602632386
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     70 |          1867.36 | 11325440 |  257.671 |              305.323 |              128.354 |            783.047 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3229.4403795901526
    time_step_min: 2950
  date: 2020-10-13_23-40-13
  done: false
  episode_len_mean: 782.7946765452425
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 257.92329136185896
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 14577
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.32189908375342685
        entropy_coeff: 0.0005000000000000001
        kl: 0.004997135916103919
        model: {}
        policy_loss: -0.010790806454527532
        total_loss: 6.395708521207173
        vf_explained_var: 0.9863850474357605
        vf_loss: 6.406659245491028
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.520000000000003
    gpu_util_percent0: 0.3576666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14858265466838397
    mean_env_wait_ms: 1.2185030755055901
    mean_inference_ms: 4.389899262183525
    mean_raw_obs_processing_ms: 0.384131765234224
  time_since_restore: 1893.775863647461
  time_this_iter_s: 26.411442518234253
  time_total_s: 1893.775863647461
  timers:
    learn_throughput: 8258.959
    learn_time_ms: 19589.878
    sample_throughput: 23752.026
    sample_time_ms: 6811.714
    update_time_ms: 32.788
  timestamp: 1602632413
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     71 |          1893.78 | 11487232 |  257.923 |              305.323 |              128.354 |            782.795 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3227.0247839005942
    time_step_min: 2950
  date: 2020-10-13_23-40-40
  done: false
  episode_len_mean: 782.4831233578118
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 258.2973105031314
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 266
  episodes_total: 14843
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.29725851863622665
        entropy_coeff: 0.0005000000000000001
        kl: 0.004627004925472041
        model: {}
        policy_loss: -0.008652853925013915
        total_loss: 5.9916768074035645
        vf_explained_var: 0.9891205430030823
        vf_loss: 6.000477910041809
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.874193548387098
    gpu_util_percent0: 0.34129032258064523
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14854904437008296
    mean_env_wait_ms: 1.218696373144666
    mean_inference_ms: 4.38781561881127
    mean_raw_obs_processing_ms: 0.3840312666655914
  time_since_restore: 1920.0774598121643
  time_this_iter_s: 26.30159616470337
  time_total_s: 1920.0774598121643
  timers:
    learn_throughput: 8270.278
    learn_time_ms: 19563.066
    sample_throughput: 23790.078
    sample_time_ms: 6800.819
    update_time_ms: 32.1
  timestamp: 1602632440
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     72 |          1920.08 | 11649024 |  258.297 |              305.323 |              128.354 |            782.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3225.64714524207
    time_step_min: 2950
  date: 2020-10-13_23-41-07
  done: false
  episode_len_mean: 782.2940706195869
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 258.5215142766775
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 15010
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.29669157912333805
        entropy_coeff: 0.0005000000000000001
        kl: 0.004874422408950825
        model: {}
        policy_loss: -0.011252956717119863
        total_loss: 4.834938367207845
        vf_explained_var: 0.988092839717865
        vf_loss: 4.8463393449783325
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.106666666666666
    gpu_util_percent0: 0.38300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852956342940607
    mean_env_wait_ms: 1.2188063517547678
    mean_inference_ms: 4.38654500487855
    mean_raw_obs_processing_ms: 0.38396672796960696
  time_since_restore: 1946.8004159927368
  time_this_iter_s: 26.72295618057251
  time_total_s: 1946.8004159927368
  timers:
    learn_throughput: 8260.012
    learn_time_ms: 19587.38
    sample_throughput: 23813.08
    sample_time_ms: 6794.249
    update_time_ms: 30.039
  timestamp: 1602632467
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     73 |           1946.8 | 11810816 |  258.522 |              305.323 |              128.354 |            782.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3223.793014827018
    time_step_min: 2950
  date: 2020-10-13_23-41-34
  done: false
  episode_len_mean: 782.0633793556871
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 258.80502261271494
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 15210
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.30901283274094266
        entropy_coeff: 0.0005000000000000001
        kl: 0.005666652228683233
        model: {}
        policy_loss: -0.010412109596624456
        total_loss: 5.1122108697891235
        vf_explained_var: 0.9888952374458313
        vf_loss: 5.122777422269185
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.703225806451616
    gpu_util_percent0: 0.2529032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14850540773989163
    mean_env_wait_ms: 1.2189364402387226
    mean_inference_ms: 4.385037557999916
    mean_raw_obs_processing_ms: 0.38388975907627465
  time_since_restore: 1973.3185350894928
  time_this_iter_s: 26.51811909675598
  time_total_s: 1973.3185350894928
  timers:
    learn_throughput: 8273.011
    learn_time_ms: 19556.605
    sample_throughput: 23818.962
    sample_time_ms: 6792.571
    update_time_ms: 28.34
  timestamp: 1602632494
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     74 |          1973.32 | 11972608 |  258.805 |              305.323 |              128.354 |            782.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3221.2739167044497
    time_step_min: 2950
  date: 2020-10-13_23-42-01
  done: false
  episode_len_mean: 781.8125242341993
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 259.1772435483176
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 264
  episodes_total: 15474
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2824097474416097
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046757976136480766
        model: {}
        policy_loss: -0.010624428505252581
        total_loss: 5.228386839230855
        vf_explained_var: 0.990327775478363
        vf_loss: 5.239152232805888
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.380645161290325
    gpu_util_percent0: 0.33838709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484745921679529
    mean_env_wait_ms: 1.2191075011523014
    mean_inference_ms: 4.383120940620776
    mean_raw_obs_processing_ms: 0.38379497223289394
  time_since_restore: 1999.9463620185852
  time_this_iter_s: 26.627826929092407
  time_total_s: 1999.9463620185852
  timers:
    learn_throughput: 8275.01
    learn_time_ms: 19551.879
    sample_throughput: 23783.704
    sample_time_ms: 6802.641
    update_time_ms: 27.319
  timestamp: 1602632521
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     75 |          1999.95 | 12134400 |  259.177 |              305.323 |              128.354 |            781.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3219.722432241943
    time_step_min: 2950
  date: 2020-10-13_23-42-28
  done: false
  episode_len_mean: 781.6298427311085
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 259.40917227511017
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 15642
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2869689092040062
        entropy_coeff: 0.0005000000000000001
        kl: 0.004641822228829066
        model: {}
        policy_loss: -0.00899923139756235
        total_loss: 3.954155921936035
        vf_explained_var: 0.9898896217346191
        vf_loss: 3.963298499584198
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12
    gpu_util_percent0: 0.31766666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14845619517371078
    mean_env_wait_ms: 1.2192053670520353
    mean_inference_ms: 4.38192784978241
    mean_raw_obs_processing_ms: 0.3837335509326087
  time_since_restore: 2026.4144051074982
  time_this_iter_s: 26.468043088912964
  time_total_s: 2026.4144051074982
  timers:
    learn_throughput: 8278.969
    learn_time_ms: 19542.531
    sample_throughput: 23789.909
    sample_time_ms: 6800.867
    update_time_ms: 25.378
  timestamp: 1602632548
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     76 |          2026.41 | 12296192 |  259.409 |              305.323 |              128.354 |             781.63 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3217.8160374375516
    time_step_min: 2950
  date: 2020-10-13_23-42-55
  done: false
  episode_len_mean: 781.4249116607774
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 259.69880595454805
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 15848
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.30046174426873523
        entropy_coeff: 0.0005000000000000001
        kl: 0.005141240156566103
        model: {}
        policy_loss: -0.010082557108641291
        total_loss: 4.380263129870097
        vf_explained_var: 0.9905273914337158
        vf_loss: 4.390495856602986
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64193548387097
    gpu_util_percent0: 0.2438709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14843326389866407
    mean_env_wait_ms: 1.2193244458342472
    mean_inference_ms: 4.380490805401675
    mean_raw_obs_processing_ms: 0.38365875806048616
  time_since_restore: 2053.128924369812
  time_this_iter_s: 26.714519262313843
  time_total_s: 2053.128924369812
  timers:
    learn_throughput: 8265.067
    learn_time_ms: 19575.401
    sample_throughput: 23812.928
    sample_time_ms: 6794.293
    update_time_ms: 26.771
  timestamp: 1602632575
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     77 |          2053.13 | 12457984 |  259.699 |              305.323 |              128.354 |            781.425 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3215.422457231726
    time_step_min: 2950
  date: 2020-10-13_23-43-22
  done: false
  episode_len_mean: 781.1581626319056
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.05554615051824
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 262
  episodes_total: 16110
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.27083553870519
        entropy_coeff: 0.0005000000000000001
        kl: 0.004197654585974912
        model: {}
        policy_loss: -0.0105372149652491
        total_loss: 4.557687163352966
        vf_explained_var: 0.9912907481193542
        vf_loss: 4.568359653155009
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.461290322580652
    gpu_util_percent0: 0.3729032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14840566670479907
    mean_env_wait_ms: 1.2194766173082665
    mean_inference_ms: 4.378705730542311
    mean_raw_obs_processing_ms: 0.38357093888139415
  time_since_restore: 2079.690046310425
  time_this_iter_s: 26.561121940612793
  time_total_s: 2079.690046310425
  timers:
    learn_throughput: 8264.153
    learn_time_ms: 19577.565
    sample_throughput: 23870.682
    sample_time_ms: 6777.854
    update_time_ms: 27.196
  timestamp: 1602632602
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     78 |          2079.69 | 12619776 |  260.056 |              305.323 |              128.354 |            781.158 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3213.8581193423242
    time_step_min: 2950
  date: 2020-10-13_23-43-49
  done: false
  episode_len_mean: 781.0087870222441
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.28060995850103
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 164
  episodes_total: 16274
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.27802975724140805
        entropy_coeff: 0.0005000000000000001
        kl: 0.004632183428232868
        model: {}
        policy_loss: -0.009663733314179504
        total_loss: 3.6808159351348877
        vf_explained_var: 0.9904757142066956
        vf_loss: 3.690618614355723
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.276666666666674
    gpu_util_percent0: 0.39966666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483878795260907
    mean_env_wait_ms: 1.2195621529948968
    mean_inference_ms: 4.377621636625613
    mean_raw_obs_processing_ms: 0.38351487845592186
  time_since_restore: 2106.3089034557343
  time_this_iter_s: 26.61885714530945
  time_total_s: 2106.3089034557343
  timers:
    learn_throughput: 8249.937
    learn_time_ms: 19611.302
    sample_throughput: 23893.923
    sample_time_ms: 6771.262
    update_time_ms: 27.928
  timestamp: 1602632629
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     79 |          2106.31 | 12781568 |  260.281 |              305.323 |              128.354 |            781.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3211.932871214885
    time_step_min: 2950
  date: 2020-10-13_23-44-15
  done: false
  episode_len_mean: 780.8264061646745
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.5720312156208
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 207
  episodes_total: 16481
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2814795871575673
        entropy_coeff: 0.0005000000000000001
        kl: 0.005099617992527783
        model: {}
        policy_loss: -0.009346065638965229
        total_loss: 4.120864848295848
        vf_explained_var: 0.9910848140716553
        vf_loss: 4.130351642767589
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32
    gpu_util_percent0: 0.2973333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14836618399454193
    mean_env_wait_ms: 1.2196687720398938
    mean_inference_ms: 4.376270250891692
    mean_raw_obs_processing_ms: 0.38344383144705413
  time_since_restore: 2132.365280866623
  time_this_iter_s: 26.056377410888672
  time_total_s: 2132.365280866623
  timers:
    learn_throughput: 8262.937
    learn_time_ms: 19580.447
    sample_throughput: 23904.332
    sample_time_ms: 6768.313
    update_time_ms: 27.188
  timestamp: 1602632655
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     80 |          2132.37 | 12943360 |  260.572 |              305.323 |              128.354 |            780.826 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3209.74371408046
    time_step_min: 2950
  date: 2020-10-13_23-44-42
  done: false
  episode_len_mean: 780.627934763128
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 260.9192456255005
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 258
  episodes_total: 16739
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.25749700516462326
        entropy_coeff: 0.0005000000000000001
        kl: 0.004436231645134588
        model: {}
        policy_loss: -0.007659326685825363
        total_loss: 3.9238884647687278
        vf_explained_var: 0.9924590587615967
        vf_loss: 3.9316764871279397
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.909677419354836
    gpu_util_percent0: 0.2645161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14834097697842055
    mean_env_wait_ms: 1.219801969053647
    mean_inference_ms: 4.374621554768669
    mean_raw_obs_processing_ms: 0.3833624277467674
  time_since_restore: 2158.876425743103
  time_this_iter_s: 26.511144876480103
  time_total_s: 2158.876425743103
  timers:
    learn_throughput: 8258.852
    learn_time_ms: 19590.132
    sample_throughput: 23917.125
    sample_time_ms: 6764.693
    update_time_ms: 29.483
  timestamp: 1602632682
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     81 |          2158.88 | 13105152 |  260.919 |              305.323 |              128.354 |            780.628 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3208.274553968348
    time_step_min: 2950
  date: 2020-10-13_23-45-08
  done: false
  episode_len_mean: 780.4979297290903
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.1398152828415
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 16906
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.26211660355329514
        entropy_coeff: 0.0005000000000000001
        kl: 0.004696245305240154
        model: {}
        policy_loss: -0.011184754732918615
        total_loss: 3.641374329725901
        vf_explained_var: 0.9907317161560059
        vf_loss: 3.65269011259079
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203333333333333
    gpu_util_percent0: 0.30033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483237751859197
    mean_env_wait_ms: 1.2198772950036456
    mean_inference_ms: 4.373585990638549
    mean_raw_obs_processing_ms: 0.38330868076299623
  time_since_restore: 2184.999586582184
  time_this_iter_s: 26.12316083908081
  time_total_s: 2184.999586582184
  timers:
    learn_throughput: 8262.166
    learn_time_ms: 19582.275
    sample_throughput: 23958.463
    sample_time_ms: 6753.021
    update_time_ms: 29.709
  timestamp: 1602632708
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     82 |             2185 | 13266944 |   261.14 |              305.323 |              128.354 |            780.498 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3206.417432535269
    time_step_min: 2950
  date: 2020-10-13_23-45-35
  done: false
  episode_len_mean: 780.3190793316976
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.42058864140887
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 17118
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2694035818179448
        entropy_coeff: 0.0005000000000000001
        kl: 0.005037655821070075
        model: {}
        policy_loss: -0.009216082204754153
        total_loss: 4.942888617515564
        vf_explained_var: 0.989602267742157
        vf_loss: 4.952239553133647
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.609677419354842
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483027217728249
    mean_env_wait_ms: 1.2199717309864786
    mean_inference_ms: 4.372290960150006
    mean_raw_obs_processing_ms: 0.3832398708743341
  time_since_restore: 2211.171103000641
  time_this_iter_s: 26.17151641845703
  time_total_s: 2211.171103000641
  timers:
    learn_throughput: 8282.301
    learn_time_ms: 19534.669
    sample_throughput: 23995.317
    sample_time_ms: 6742.649
    update_time_ms: 31.502
  timestamp: 1602632735
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     83 |          2211.17 | 13428736 |  261.421 |              305.323 |              128.354 |            780.319 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3204.318354868482
    time_step_min: 2950
  date: 2020-10-13_23-46-01
  done: false
  episode_len_mean: 780.1321167463013
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.7379203351226
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 253
  episodes_total: 17371
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2436151554187139
        entropy_coeff: 0.0005000000000000001
        kl: 0.004467451595701277
        model: {}
        policy_loss: -0.010677221541603407
        total_loss: 4.291302442550659
        vf_explained_var: 0.9917826056480408
        vf_loss: 4.302101532618205
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.500000000000004
    gpu_util_percent0: 0.3803333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14827967766624853
    mean_env_wait_ms: 1.2200857971921253
    mean_inference_ms: 4.37077043605032
    mean_raw_obs_processing_ms: 0.38316418680438313
  time_since_restore: 2237.374161720276
  time_this_iter_s: 26.20305871963501
  time_total_s: 2237.374161720276
  timers:
    learn_throughput: 8299.847
    learn_time_ms: 19493.372
    sample_throughput: 23996.613
    sample_time_ms: 6742.285
    update_time_ms: 32.705
  timestamp: 1602632761
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     84 |          2237.37 | 13590528 |  261.738 |              305.323 |              128.354 |            780.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3202.995200822716
    time_step_min: 2950
  date: 2020-10-13_23-46-28
  done: false
  episode_len_mean: 780.0091230470978
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 261.9464176489493
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 17538
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2509919007619222
        entropy_coeff: 0.0005000000000000001
        kl: 0.005146100263421734
        model: {}
        policy_loss: -0.008820091335413357
        total_loss: 3.578421711921692
        vf_explained_var: 0.9908131957054138
        vf_loss: 3.5873672564824424
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39666666666667
    gpu_util_percent0: 0.3453333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826351203505028
    mean_env_wait_ms: 1.220150037180553
    mean_inference_ms: 4.369799170259042
    mean_raw_obs_processing_ms: 0.3831135111978873
  time_since_restore: 2263.5414850711823
  time_this_iter_s: 26.167323350906372
  time_total_s: 2263.5414850711823
  timers:
    learn_throughput: 8314.226
    learn_time_ms: 19459.659
    sample_throughput: 24043.354
    sample_time_ms: 6729.178
    update_time_ms: 32.827
  timestamp: 1602632788
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     85 |          2263.54 | 13752320 |  261.946 |              305.323 |              128.354 |            780.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3201.1250070545743
    time_step_min: 2950
  date: 2020-10-13_23-46-55
  done: false
  episode_len_mean: 779.8487101498254
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 262.2342143981211
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 216
  episodes_total: 17754
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.25614098211129505
        entropy_coeff: 0.0005000000000000001
        kl: 0.004949440364725888
        model: {}
        policy_loss: -0.009649161210594078
        total_loss: 3.7816648284594216
        vf_explained_var: 0.9918193817138672
        vf_loss: 3.7914421359697976
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.130000000000006
    gpu_util_percent0: 0.32500000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482438015327037
    mean_env_wait_ms: 1.2202328323925045
    mean_inference_ms: 4.368554555313583
    mean_raw_obs_processing_ms: 0.38304751513796614
  time_since_restore: 2289.791426420212
  time_this_iter_s: 26.24994134902954
  time_total_s: 2289.791426420212
  timers:
    learn_throughput: 8324.46
    learn_time_ms: 19435.735
    sample_throughput: 24047.286
    sample_time_ms: 6728.077
    update_time_ms: 34.993
  timestamp: 1602632815
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     86 |          2289.79 | 13914112 |  262.234 |              305.323 |              128.354 |            779.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3198.94557898837
    time_step_min: 2950
  date: 2020-10-13_23-47-21
  done: false
  episode_len_mean: 779.6709430189936
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 262.55848050649786
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 252
  episodes_total: 18006
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.23144945750633875
        entropy_coeff: 0.0005000000000000001
        kl: 0.004349789000116289
        model: {}
        policy_loss: -0.010432317008962855
        total_loss: 3.5066633025805154
        vf_explained_var: 0.9931366443634033
        vf_loss: 3.517211357752482
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.703225806451613
    gpu_util_percent0: 0.33064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14822156358368502
    mean_env_wait_ms: 1.2203312766753414
    mean_inference_ms: 4.367138102469405
    mean_raw_obs_processing_ms: 0.3829748270612244
  time_since_restore: 2316.254958152771
  time_this_iter_s: 26.463531732559204
  time_total_s: 2316.254958152771
  timers:
    learn_throughput: 8349.989
    learn_time_ms: 19376.313
    sample_throughput: 23949.699
    sample_time_ms: 6755.492
    update_time_ms: 40.834
  timestamp: 1602632841
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     87 |          2316.25 | 14075904 |  262.558 |              305.323 |              128.354 |            779.671 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3197.705723423026
    time_step_min: 2950
  date: 2020-10-13_23-47-48
  done: false
  episode_len_mean: 779.5422376313907
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 262.7615664653802
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 165
  episodes_total: 18171
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.23796684419115385
        entropy_coeff: 0.0005000000000000001
        kl: 0.004746708087623119
        model: {}
        policy_loss: -0.010158667995331902
        total_loss: 3.2070122758547464
        vf_explained_var: 0.9915556907653809
        vf_loss: 3.217290004094442
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02333333333333
    gpu_util_percent0: 0.3883333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14820663329493772
    mean_env_wait_ms: 1.22038671920751
    mean_inference_ms: 4.366238905034787
    mean_raw_obs_processing_ms: 0.3829280169390219
  time_since_restore: 2342.51530957222
  time_this_iter_s: 26.260351419448853
  time_total_s: 2342.51530957222
  timers:
    learn_throughput: 8365.465
    learn_time_ms: 19340.467
    sample_throughput: 23893.331
    sample_time_ms: 6771.429
    update_time_ms: 38.986
  timestamp: 1602632868
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     88 |          2342.52 | 14237696 |  262.762 |              305.323 |              128.354 |            779.542 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3195.9664886660853
    time_step_min: 2950
  date: 2020-10-13_23-48-15
  done: false
  episode_len_mean: 779.4080600424213
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.02331192492716
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 216
  episodes_total: 18387
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.24343065917491913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554607129345338
        model: {}
        policy_loss: -0.011246437119552866
        total_loss: 3.7148165504137673
        vf_explained_var: 0.9921318888664246
        vf_loss: 3.726184686024984
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.71290322580645
    gpu_util_percent0: 0.43
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481883447854097
    mean_env_wait_ms: 1.2204589239213095
    mean_inference_ms: 4.3650651703679095
    mean_raw_obs_processing_ms: 0.38286611875485593
  time_since_restore: 2368.903007745743
  time_this_iter_s: 26.38769817352295
  time_total_s: 2368.903007745743
  timers:
    learn_throughput: 8377.167
    learn_time_ms: 19313.45
    sample_throughput: 23919.48
    sample_time_ms: 6764.027
    update_time_ms: 39.99
  timestamp: 1602632895
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     89 |           2368.9 | 14399488 |  263.023 |              305.323 |              128.354 |            779.408 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3194.079243051449
    time_step_min: 2950
  date: 2020-10-13_23-48-41
  done: false
  episode_len_mean: 779.2437754883022
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.3326487671304
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 249
  episodes_total: 18636
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.22417493412892023
        entropy_coeff: 0.0005000000000000001
        kl: 0.005401874853608509
        model: {}
        policy_loss: -0.009299968970784297
        total_loss: 3.656243145465851
        vf_explained_var: 0.992896556854248
        vf_loss: 3.665655255317688
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.963333333333335
    gpu_util_percent0: 0.38
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14816721317713646
    mean_env_wait_ms: 1.2205430090518035
    mean_inference_ms: 4.363751378421435
    mean_raw_obs_processing_ms: 0.38279795543377115
  time_since_restore: 2395.0127120018005
  time_this_iter_s: 26.10970425605774
  time_total_s: 2395.0127120018005
  timers:
    learn_throughput: 8373.285
    learn_time_ms: 19322.405
    sample_throughput: 23939.296
    sample_time_ms: 6758.428
    update_time_ms: 40.882
  timestamp: 1602632921
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     90 |          2395.01 | 14561280 |  263.333 |              305.323 |              128.354 |            779.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3192.787126338786
    time_step_min: 2950
  date: 2020-10-13_23-49-08
  done: false
  episode_len_mean: 779.1524305924902
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.5286940245987
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 166
  episodes_total: 18802
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.23339983324209848
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044905396255974965
        model: {}
        policy_loss: -0.006208823120687157
        total_loss: 2.6869796911875405
        vf_explained_var: 0.9930952191352844
        vf_loss: 2.6933053135871887
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95666666666667
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14815341827867118
    mean_env_wait_ms: 1.2205886290362047
    mean_inference_ms: 4.362893465128438
    mean_raw_obs_processing_ms: 0.3827528028156156
  time_since_restore: 2421.314378261566
  time_this_iter_s: 26.301666259765625
  time_total_s: 2421.314378261566
  timers:
    learn_throughput: 8382.36
    learn_time_ms: 19301.486
    sample_throughput: 23939.817
    sample_time_ms: 6758.281
    update_time_ms: 40.812
  timestamp: 1602632948
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     91 |          2421.31 | 14723072 |  263.529 |              305.323 |              128.354 |            779.152 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3191.1694406404717
    time_step_min: 2950
  date: 2020-10-13_23-49-34
  done: false
  episode_len_mean: 779.0157720414279
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 263.7814037541707
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 19021
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2426673285663128
        entropy_coeff: 0.0005000000000000001
        kl: 0.004914539594513674
        model: {}
        policy_loss: -0.009997313551139086
        total_loss: 3.930317997932434
        vf_explained_var: 0.9916749000549316
        vf_loss: 3.9404366612434387
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22666666666667
    gpu_util_percent0: 0.38366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14813597265243528
    mean_env_wait_ms: 1.2206507539701554
    mean_inference_ms: 4.361784335738998
    mean_raw_obs_processing_ms: 0.3826926843819505
  time_since_restore: 2447.40415930748
  time_this_iter_s: 26.089781045913696
  time_total_s: 2447.40415930748
  timers:
    learn_throughput: 8381.239
    learn_time_ms: 19304.068
    sample_throughput: 23959.568
    sample_time_ms: 6752.709
    update_time_ms: 39.79
  timestamp: 1602632974
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     92 |           2447.4 | 14884864 |  263.781 |              305.323 |              128.354 |            779.016 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3189.2560049911613
    time_step_min: 2950
  date: 2020-10-13_23-50-01
  done: false
  episode_len_mean: 778.8726970782085
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.06427029126706
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 248
  episodes_total: 19269
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.21617051089803377
        entropy_coeff: 0.0005000000000000001
        kl: 0.00392264958160619
        model: {}
        policy_loss: -0.009051351774057062
        total_loss: 3.663463215033213
        vf_explained_var: 0.9928447604179382
        vf_loss: 3.6726226607958474
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896774193548392
    gpu_util_percent0: 0.3532258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14811613931381654
    mean_env_wait_ms: 1.2207228419968839
    mean_inference_ms: 4.360544751449197
    mean_raw_obs_processing_ms: 0.3826282763796297
  time_since_restore: 2473.658504009247
  time_this_iter_s: 26.254344701766968
  time_total_s: 2473.658504009247
  timers:
    learn_throughput: 8380.35
    learn_time_ms: 19306.115
    sample_throughput: 23937.922
    sample_time_ms: 6758.816
    update_time_ms: 38.754
  timestamp: 1602633001
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     93 |          2473.66 | 15046656 |  264.064 |              305.323 |              128.354 |            778.873 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3188.015670103093
    time_step_min: 2950
  date: 2020-10-13_23-50-28
  done: false
  episode_len_mean: 778.7855930023154
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.2532087013692
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 166
  episodes_total: 19435
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.22326836735010147
        entropy_coeff: 0.0005000000000000001
        kl: 0.004347114940173924
        model: {}
        policy_loss: -0.008071325634470364
        total_loss: 3.4326427777608237
        vf_explained_var: 0.9912821650505066
        vf_loss: 3.4408257007598877
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.933333333333337
    gpu_util_percent0: 0.33199999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481028782917578
    mean_env_wait_ms: 1.2207622556074083
    mean_inference_ms: 4.359734555627009
    mean_raw_obs_processing_ms: 0.3825850496410632
  time_since_restore: 2499.887499809265
  time_this_iter_s: 26.22899580001831
  time_total_s: 2499.887499809265
  timers:
    learn_throughput: 8376.328
    learn_time_ms: 19315.386
    sample_throughput: 23936.439
    sample_time_ms: 6759.234
    update_time_ms: 39.111
  timestamp: 1602633028
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     94 |          2499.89 | 15208448 |  264.253 |              305.323 |              128.354 |            778.786 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3186.414801977675
    time_step_min: 2950
  date: 2020-10-13_23-50-54
  done: false
  episode_len_mean: 778.6659204233235
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.50424567235393
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 219
  episodes_total: 19654
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.23257563263177872
        entropy_coeff: 0.0005000000000000001
        kl: 0.00485941837541759
        model: {}
        policy_loss: -0.010026904977470016
        total_loss: 3.708245118459066
        vf_explained_var: 0.9921152591705322
        vf_loss: 3.718388338883718
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.04666666666667
    gpu_util_percent0: 0.3056666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14808595203441732
    mean_env_wait_ms: 1.22081575803136
    mean_inference_ms: 4.358677870478121
    mean_raw_obs_processing_ms: 0.3825279252605076
  time_since_restore: 2526.060644388199
  time_this_iter_s: 26.173144578933716
  time_total_s: 2526.060644388199
  timers:
    learn_throughput: 8377.848
    learn_time_ms: 19311.88
    sample_throughput: 23929.374
    sample_time_ms: 6761.23
    update_time_ms: 39.581
  timestamp: 1602633054
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     95 |          2526.06 | 15370240 |  264.504 |              305.323 |              128.354 |            778.666 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3184.50981576563
    time_step_min: 2950
  date: 2020-10-13_23-51-21
  done: false
  episode_len_mean: 778.5404753529973
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.79224788968014
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 247
  episodes_total: 19901
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.2100596303741137
        entropy_coeff: 0.0005000000000000001
        kl: 0.004346414158741633
        model: {}
        policy_loss: -0.008264342187127719
        total_loss: 3.1842907468477883
        vf_explained_var: 0.9937360286712646
        vf_loss: 3.19266011317571
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.625806451612902
    gpu_util_percent0: 0.3848387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480674901458253
    mean_env_wait_ms: 1.2208731220347473
    mean_inference_ms: 4.357523761185567
    mean_raw_obs_processing_ms: 0.3824677245699859
  time_since_restore: 2552.640457391739
  time_this_iter_s: 26.57981300354004
  time_total_s: 2552.640457391739
  timers:
    learn_throughput: 8374.066
    learn_time_ms: 19320.602
    sample_throughput: 23843.475
    sample_time_ms: 6785.588
    update_time_ms: 39.055
  timestamp: 1602633081
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     96 |          2552.64 | 15532032 |  264.792 |              305.323 |              128.354 |             778.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3183.2542931309904
    time_step_min: 2950
  date: 2020-10-13_23-51-48
  done: false
  episode_len_mean: 778.4958887726118
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 264.97838755321186
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 166
  episodes_total: 20067
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2108885571360588
        entropy_coeff: 0.0005000000000000001
        kl: 0.004517972702160478
        model: {}
        policy_loss: -0.010164422759165367
        total_loss: 2.8915368715922036
        vf_explained_var: 0.9927234053611755
        vf_loss: 2.901806732018789
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.738709677419354
    gpu_util_percent0: 0.33290322580645165
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14805547782435724
    mean_env_wait_ms: 1.2209056577730104
    mean_inference_ms: 4.356760690610891
    mean_raw_obs_processing_ms: 0.38242664793527775
  time_since_restore: 2578.9809691905975
  time_this_iter_s: 26.340511798858643
  time_total_s: 2578.9809691905975
  timers:
    learn_throughput: 8379.589
    learn_time_ms: 19307.869
    sample_throughput: 23816.499
    sample_time_ms: 6793.274
    update_time_ms: 31.137
  timestamp: 1602633108
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     97 |          2578.98 | 15693824 |  264.978 |              305.323 |              128.354 |            778.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3181.6712112230784
    time_step_min: 2950
  date: 2020-10-13_23-52-15
  done: false
  episode_len_mean: 778.4248237092559
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.21741304758217
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 212
  episodes_total: 20279
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.22524654741088548
        entropy_coeff: 0.0005000000000000001
        kl: 0.004906056371207039
        model: {}
        policy_loss: -0.009739923446128765
        total_loss: 3.3801024158795676
        vf_explained_var: 0.9928431510925293
        vf_loss: 3.3899550437927246
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.763333333333332
    gpu_util_percent0: 0.356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480403701038155
    mean_env_wait_ms: 1.2209475398295921
    mean_inference_ms: 4.355800488935337
    mean_raw_obs_processing_ms: 0.38237412927537073
  time_since_restore: 2605.233991622925
  time_this_iter_s: 26.25302243232727
  time_total_s: 2605.233991622925
  timers:
    learn_throughput: 8382.908
    learn_time_ms: 19300.223
    sample_throughput: 23803.42
    sample_time_ms: 6797.007
    update_time_ms: 33.338
  timestamp: 1602633135
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     98 |          2605.23 | 15855616 |  265.217 |              305.323 |              128.354 |            778.425 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3179.816247865333
    time_step_min: 2950
  date: 2020-10-13_23-52-41
  done: false
  episode_len_mean: 778.3734047735022
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.5078844952201
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 251
  episodes_total: 20530
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.19802668442328772
        entropy_coeff: 0.0005000000000000001
        kl: 0.004424594924785197
        model: {}
        policy_loss: -0.010245248946982125
        total_loss: 3.3691912094751992
        vf_explained_var: 0.9935426115989685
        vf_loss: 3.3795355558395386
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.779999999999998
    gpu_util_percent0: 0.30800000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14802232579605534
    mean_env_wait_ms: 1.2209929959929342
    mean_inference_ms: 4.354682271953271
    mean_raw_obs_processing_ms: 0.38231616902473126
  time_since_restore: 2631.5201318264008
  time_this_iter_s: 26.286140203475952
  time_total_s: 2631.5201318264008
  timers:
    learn_throughput: 8387.215
    learn_time_ms: 19290.313
    sample_throughput: 23770.721
    sample_time_ms: 6806.356
    update_time_ms: 32.222
  timestamp: 1602633161
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |     99 |          2631.52 | 16017408 |  265.508 |              305.323 |              128.354 |            778.373 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3178.577263708077
    time_step_min: 2950
  date: 2020-10-13_23-53-08
  done: false
  episode_len_mean: 778.3550584597546
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.69789156420717
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 20698
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.19598001117507616
        entropy_coeff: 0.0005000000000000001
        kl: 0.004606316331773996
        model: {}
        policy_loss: -0.010173791376776839
        total_loss: 2.12760066986084
        vf_explained_var: 0.994718074798584
        vf_loss: 2.1378723978996277
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.810000000000006
    gpu_util_percent0: 0.29800000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14801081222195883
    mean_env_wait_ms: 1.221014754052664
    mean_inference_ms: 4.353954633617763
    mean_raw_obs_processing_ms: 0.382277446097928
  time_since_restore: 2657.725615978241
  time_this_iter_s: 26.20548415184021
  time_total_s: 2657.725615978241
  timers:
    learn_throughput: 8384.867
    learn_time_ms: 19295.715
    sample_throughput: 23759.693
    sample_time_ms: 6809.516
    update_time_ms: 31.61
  timestamp: 1602633188
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    100 |          2657.73 | 16179200 |  265.698 |              305.323 |              128.354 |            778.355 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3177.1539383479553
    time_step_min: 2950
  date: 2020-10-13_23-53-35
  done: false
  episode_len_mean: 778.3401454963147
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 265.9129260442078
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 196
  episodes_total: 20894
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21070735653241476
        entropy_coeff: 0.0005000000000000001
        kl: 0.004244183112556736
        model: {}
        policy_loss: -0.00905375556612853
        total_loss: 2.441717485586802
        vf_explained_var: 0.9947071075439453
        vf_loss: 2.4508766531944275
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709674
    gpu_util_percent0: 0.3496774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14799695357011106
    mean_env_wait_ms: 1.221043300170711
    mean_inference_ms: 4.3531135337527305
    mean_raw_obs_processing_ms: 0.38223226524143167
  time_since_restore: 2683.97438287735
  time_this_iter_s: 26.248766899108887
  time_total_s: 2683.97438287735
  timers:
    learn_throughput: 8389.419
    learn_time_ms: 19285.244
    sample_throughput: 23739.308
    sample_time_ms: 6815.363
    update_time_ms: 29.905
  timestamp: 1602633215
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    101 |          2683.97 | 16340992 |  265.913 |              305.323 |              128.354 |             778.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3175.3479043334123
    time_step_min: 2950
  date: 2020-10-13_23-54-02
  done: false
  episode_len_mean: 778.3208037825059
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.18831578193283
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 256
  episodes_total: 21150
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19386547058820724
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038890320186813674
        model: {}
        policy_loss: -0.008659424861737838
        total_loss: 2.954138239224752
        vf_explained_var: 0.9946532845497131
        vf_loss: 2.962894638379415
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79666666666667
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14797983130852768
    mean_env_wait_ms: 1.221073442156688
    mean_inference_ms: 4.352045382616624
    mean_raw_obs_processing_ms: 0.38217564381516583
  time_since_restore: 2710.2991828918457
  time_this_iter_s: 26.32480001449585
  time_total_s: 2710.2991828918457
  timers:
    learn_throughput: 8388.466
    learn_time_ms: 19287.435
    sample_throughput: 23669.727
    sample_time_ms: 6835.398
    update_time_ms: 29.604
  timestamp: 1602633242
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    102 |           2710.3 | 16502784 |  266.188 |              305.323 |              128.354 |            778.321 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3174.0594975346326
    time_step_min: 2950
  date: 2020-10-13_23-54-28
  done: false
  episode_len_mean: 778.315611814346
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.3841509326741
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 21330
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.18441305682063103
        entropy_coeff: 0.0005000000000000001
        kl: 0.003973086772020906
        model: {}
        policy_loss: -0.008704686028067954
        total_loss: 2.045116752386093
        vf_explained_var: 0.9951183199882507
        vf_loss: 2.0539136230945587
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.441935483870967
    gpu_util_percent0: 0.35387096774193544
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14796840399868275
    mean_env_wait_ms: 1.221089087852018
    mean_inference_ms: 4.351301088477194
    mean_raw_obs_processing_ms: 0.38213515293809536
  time_since_restore: 2736.635725259781
  time_this_iter_s: 26.33654236793518
  time_total_s: 2736.635725259781
  timers:
    learn_throughput: 8393.033
    learn_time_ms: 19276.94
    sample_throughput: 23603.771
    sample_time_ms: 6854.498
    update_time_ms: 28.283
  timestamp: 1602633268
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    103 |          2736.64 | 16664576 |  266.384 |              305.323 |              128.354 |            778.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3172.7255924391266
    time_step_min: 2950
  date: 2020-10-13_23-54-55
  done: false
  episode_len_mean: 778.3073812401227
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.5851942310527
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 21514
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.19824964553117752
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040229161774429185
        model: {}
        policy_loss: -0.007387646634015255
        total_loss: 2.607377211252848
        vf_explained_var: 0.9942156672477722
        vf_loss: 2.6148639718691506
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65666666666667
    gpu_util_percent0: 0.3623333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14795615456543398
    mean_env_wait_ms: 1.2211033639975515
    mean_inference_ms: 4.350552950085631
    mean_raw_obs_processing_ms: 0.38209331831116716
  time_since_restore: 2762.8438806533813
  time_this_iter_s: 26.208155393600464
  time_total_s: 2762.8438806533813
  timers:
    learn_throughput: 8395.658
    learn_time_ms: 19270.914
    sample_throughput: 23591.122
    sample_time_ms: 6858.173
    update_time_ms: 26.72
  timestamp: 1602633295
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    104 |          2762.84 | 16826368 |  266.585 |              305.323 |              128.354 |            778.307 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3170.8945988222304
    time_step_min: 2950
  date: 2020-10-13_23-55-22
  done: false
  episode_len_mean: 778.332414680079
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 266.85784954408354
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 257
  episodes_total: 21771
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.191275334606568
        entropy_coeff: 0.0005000000000000001
        kl: 0.004122633875037233
        model: {}
        policy_loss: -0.008579501551139401
        total_loss: 3.101557950178782
        vf_explained_var: 0.994422435760498
        vf_loss: 3.110233167807261
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.848387096774196
    gpu_util_percent0: 0.3596774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479399323778289
    mean_env_wait_ms: 1.2211210589013786
    mean_inference_ms: 4.349531714297643
    mean_raw_obs_processing_ms: 0.38203997447954796
  time_since_restore: 2789.395140647888
  time_this_iter_s: 26.551259994506836
  time_total_s: 2789.395140647888
  timers:
    learn_throughput: 8391.717
    learn_time_ms: 19279.963
    sample_throughput: 23508.412
    sample_time_ms: 6882.302
    update_time_ms: 30.282
  timestamp: 1602633322
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    105 |           2789.4 | 16988160 |  266.858 |              305.323 |              128.354 |            778.332 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3169.5796962648787
    time_step_min: 2950
  date: 2020-10-13_23-55-49
  done: false
  episode_len_mean: 778.3297969219561
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.0641525904708
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 191
  episodes_total: 21962
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.17079847181836763
        entropy_coeff: 0.0005000000000000001
        kl: 0.00390712955656151
        model: {}
        policy_loss: -0.0103188930467392
        total_loss: 1.982936531305313
        vf_explained_var: 0.9954614043235779
        vf_loss: 1.9933408399422963
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.540000000000006
    gpu_util_percent0: 0.2586666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14792843276052808
    mean_env_wait_ms: 1.2211302706437297
    mean_inference_ms: 4.3488005936400125
    mean_raw_obs_processing_ms: 0.3820000005045143
  time_since_restore: 2815.7616550922394
  time_this_iter_s: 26.366514444351196
  time_total_s: 2815.7616550922394
  timers:
    learn_throughput: 8395.671
    learn_time_ms: 19270.884
    sample_throughput: 23549.01
    sample_time_ms: 6870.437
    update_time_ms: 28.826
  timestamp: 1602633349
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    106 |          2815.76 | 17149952 |  267.064 |              305.323 |              128.354 |             778.33 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3168.3918099547514
    time_step_min: 2950
  date: 2020-10-13_23-56-16
  done: false
  episode_len_mean: 778.3448836683984
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.24122864059615
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 173
  episodes_total: 22135
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.18805952370166779
        entropy_coeff: 0.0005000000000000001
        kl: 0.004009904999596377
        model: {}
        policy_loss: -0.008801199272663022
        total_loss: 2.853523035844167
        vf_explained_var: 0.9935711026191711
        vf_loss: 2.8624181946118674
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.870967741935488
    gpu_util_percent0: 0.29612903225806453
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791769731637322
    mean_env_wait_ms: 1.2211359771016599
    mean_inference_ms: 4.348143694150866
    mean_raw_obs_processing_ms: 0.3819631926686456
  time_since_restore: 2842.300229072571
  time_this_iter_s: 26.53857398033142
  time_total_s: 2842.300229072571
  timers:
    learn_throughput: 8384.574
    learn_time_ms: 19296.389
    sample_throughput: 23582.682
    sample_time_ms: 6860.628
    update_time_ms: 30.416
  timestamp: 1602633376
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    107 |           2842.3 | 17311744 |  267.241 |              305.323 |              128.354 |            778.345 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3166.675227059192
    time_step_min: 2950
  date: 2020-10-13_23-56-42
  done: false
  episode_len_mean: 778.3826051996783
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.506597738305
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 251
  episodes_total: 22386
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18865067387620607
        entropy_coeff: 0.0005000000000000001
        kl: 0.006582954782061279
        model: {}
        policy_loss: -0.007514678523875773
        total_loss: 2.3213423093159995
        vf_explained_var: 0.995720386505127
        vf_loss: 2.3289512991905212
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.687096774193552
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14790230468434715
    mean_env_wait_ms: 1.2211400023543126
    mean_inference_ms: 4.347193730585536
    mean_raw_obs_processing_ms: 0.38191275950208886
  time_since_restore: 2868.8148744106293
  time_this_iter_s: 26.51464533805847
  time_total_s: 2868.8148744106293
  timers:
    learn_throughput: 8377.635
    learn_time_ms: 19312.372
    sample_throughput: 23549.465
    sample_time_ms: 6870.305
    update_time_ms: 30.727
  timestamp: 1602633402
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    108 |          2868.81 | 17473536 |  267.507 |              305.323 |              128.354 |            778.383 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3165.2506538990115
    time_step_min: 2950
  date: 2020-10-13_23-57-09
  done: false
  episode_len_mean: 778.4289571529745
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.71874374052135
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 206
  episodes_total: 22592
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.16685236866275469
        entropy_coeff: 0.0005000000000000001
        kl: 0.003932366632701208
        model: {}
        policy_loss: -0.011335260981771475
        total_loss: 1.7410776615142822
        vf_explained_var: 0.9963397979736328
        vf_loss: 1.752496321996053
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.52903225806452
    gpu_util_percent0: 0.29064516129032264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14789082539168133
    mean_env_wait_ms: 1.2211393663526722
    mean_inference_ms: 4.346448360949933
    mean_raw_obs_processing_ms: 0.38187140168295486
  time_since_restore: 2895.3605964183807
  time_this_iter_s: 26.545722007751465
  time_total_s: 2895.3605964183807
  timers:
    learn_throughput: 8376.146
    learn_time_ms: 19315.804
    sample_throughput: 23508.456
    sample_time_ms: 6882.29
    update_time_ms: 31.695
  timestamp: 1602633429
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    109 |          2895.36 | 17635328 |  267.719 |              305.323 |              128.354 |            778.429 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3164.0901689843336
    time_step_min: 2950
  date: 2020-10-13_23-57-36
  done: false
  episode_len_mean: 778.4599499099257
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 267.89233252601593
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 167
  episodes_total: 22759
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.17553378393252692
        entropy_coeff: 0.0005000000000000001
        kl: 0.004073347450078775
        model: {}
        policy_loss: -0.0077541398131870665
        total_loss: 1.8975201845169067
        vf_explained_var: 0.9955175518989563
        vf_loss: 1.9053620994091034
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.716129032258067
    gpu_util_percent0: 0.2658064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788110679898642
    mean_env_wait_ms: 1.2211349326248409
    mean_inference_ms: 4.345847455391279
    mean_raw_obs_processing_ms: 0.3818387936815204
  time_since_restore: 2921.835550546646
  time_this_iter_s: 26.47495412826538
  time_total_s: 2921.835550546646
  timers:
    learn_throughput: 8374.499
    learn_time_ms: 19319.603
    sample_throughput: 23431.567
    sample_time_ms: 6904.873
    update_time_ms: 31.707
  timestamp: 1602633456
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    110 |          2921.84 | 17797120 |  267.892 |              305.323 |              128.354 |             778.46 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3162.607293799834
    time_step_min: 2950
  date: 2020-10-13_23-58-04
  done: false
  episode_len_mean: 778.5004350474202
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.1214507381305
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 227
  episodes_total: 22986
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1821675499280294
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037739647280735276
        model: {}
        policy_loss: -0.010040990697840849
        total_loss: 2.495137890179952
        vf_explained_var: 0.9952752590179443
        vf_loss: 2.5052700638771057
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.293333333333337
    gpu_util_percent0: 0.3436666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14786868794602806
    mean_env_wait_ms: 1.2211266637417149
    mean_inference_ms: 4.345041794779706
    mean_raw_obs_processing_ms: 0.381794224655578
  time_since_restore: 2948.41002702713
  time_this_iter_s: 26.57447648048401
  time_total_s: 2948.41002702713
  timers:
    learn_throughput: 8361.91
    learn_time_ms: 19348.69
    sample_throughput: 23423.344
    sample_time_ms: 6907.297
    update_time_ms: 31.502
  timestamp: 1602633484
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    111 |          2948.41 | 17958912 |  268.121 |              305.323 |              128.354 |              778.5 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3161.15826251995
    time_step_min: 2950
  date: 2020-10-13_23-58-30
  done: false
  episode_len_mean: 778.5304505125334
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.34338126723344
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 232
  episodes_total: 23218
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.15971247230966887
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036957418778911233
        model: {}
        policy_loss: -0.008545937540475279
        total_loss: 2.6411733428637185
        vf_explained_var: 0.9948610663414001
        vf_loss: 2.6497991482416787
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870974
    gpu_util_percent0: 0.35290322580645167
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14785513942735776
    mean_env_wait_ms: 1.221116217349734
    mean_inference_ms: 4.344241778445944
    mean_raw_obs_processing_ms: 0.3817508743297299
  time_since_restore: 2974.8033792972565
  time_this_iter_s: 26.393352270126343
  time_total_s: 2974.8033792972565
  timers:
    learn_throughput: 8364.876
    learn_time_ms: 19341.829
    sample_throughput: 23379.602
    sample_time_ms: 6920.22
    update_time_ms: 31.699
  timestamp: 1602633510
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    112 |           2974.8 | 18120704 |  268.343 |              305.323 |              128.354 |             778.53 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3160.030915474865
    time_step_min: 2950
  date: 2020-10-13_23-58-57
  done: false
  episode_len_mean: 778.5465389713113
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.51551903661874
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 171
  episodes_total: 23389
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.16117296740412712
        entropy_coeff: 0.0005000000000000001
        kl: 0.004501701216213405
        model: {}
        policy_loss: -0.007391366428540398
        total_loss: 1.9591073592503865
        vf_explained_var: 0.9953795075416565
        vf_loss: 1.9665793180465698
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14784595090891972
    mean_env_wait_ms: 1.221104732722798
    mean_inference_ms: 4.34365590669663
    mean_raw_obs_processing_ms: 0.38171919020163947
  time_since_restore: 3001.1849315166473
  time_this_iter_s: 26.38155221939087
  time_total_s: 3001.1849315166473
  timers:
    learn_throughput: 8360.413
    learn_time_ms: 19352.154
    sample_throughput: 23407.316
    sample_time_ms: 6912.027
    update_time_ms: 33.067
  timestamp: 1602633537
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    113 |          3001.18 | 18282496 |  268.516 |              305.323 |              128.354 |            778.547 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3158.648546573308
    time_step_min: 2950
  date: 2020-10-13_23-59-24
  done: false
  episode_len_mean: 778.5567372881355
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.7241568224619
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 211
  episodes_total: 23600
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.17645690714319548
        entropy_coeff: 0.0005000000000000001
        kl: 0.00455080372436593
        model: {}
        policy_loss: -0.010527246360046169
        total_loss: 2.439042647679647
        vf_explained_var: 0.995141327381134
        vf_loss: 2.449658155441284
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.583870967741937
    gpu_util_percent0: 0.29612903225806453
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14783551318580657
    mean_env_wait_ms: 1.2210885932491031
    mean_inference_ms: 4.342949190427043
    mean_raw_obs_processing_ms: 0.38167969845076055
  time_since_restore: 3027.8844056129456
  time_this_iter_s: 26.699474096298218
  time_total_s: 3027.8844056129456
  timers:
    learn_throughput: 8355.244
    learn_time_ms: 19364.127
    sample_throughput: 23291.276
    sample_time_ms: 6946.464
    update_time_ms: 33.999
  timestamp: 1602633564
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    114 |          3027.88 | 18444288 |  268.724 |              305.323 |              128.354 |            778.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3157.0572100642667
    time_step_min: 2950
  date: 2020-10-13_23-59-51
  done: false
  episode_len_mean: 778.5681989765959
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 268.9621697217117
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 242
  episodes_total: 23842
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.16001935675740242
        entropy_coeff: 0.0005000000000000001
        kl: 0.005934614804573357
        model: {}
        policy_loss: -0.010901586120477683
        total_loss: 1.85432102282842
        vf_explained_var: 0.9965662956237793
        vf_loss: 1.865302582581838
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.332258064516136
    gpu_util_percent0: 0.3277419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782140725657766
    mean_env_wait_ms: 1.2210669917991843
    mean_inference_ms: 4.342153799611642
    mean_raw_obs_processing_ms: 0.381636958777203
  time_since_restore: 3054.4788637161255
  time_this_iter_s: 26.59445810317993
  time_total_s: 3054.4788637161255
  timers:
    learn_throughput: 8352.518
    learn_time_ms: 19370.445
    sample_throughput: 23298.574
    sample_time_ms: 6944.288
    update_time_ms: 33.947
  timestamp: 1602633591
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    115 |          3054.48 | 18606080 |  268.962 |              305.323 |              128.354 |            778.568 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3155.9518825834966
    time_step_min: 2950
  date: 2020-10-14_00-00-18
  done: false
  episode_len_mean: 778.5784411691232
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.13100780475247
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 176
  episodes_total: 24018
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.15443184599280357
        entropy_coeff: 0.0005000000000000001
        kl: 0.00397810017845283
        model: {}
        policy_loss: -0.007802540688620259
        total_loss: 1.61911674340566
        vf_explained_var: 0.9962174892425537
        vf_loss: 1.6269965370496113
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01
    gpu_util_percent0: 0.36966666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14781244670624258
    mean_env_wait_ms: 1.2210467705146995
    mean_inference_ms: 4.341582056249944
    mean_raw_obs_processing_ms: 0.3816049132708672
  time_since_restore: 3080.500507593155
  time_this_iter_s: 26.02164387702942
  time_total_s: 3080.500507593155
  timers:
    learn_throughput: 8357.648
    learn_time_ms: 19358.556
    sample_throughput: 23376.642
    sample_time_ms: 6921.097
    update_time_ms: 33.489
  timestamp: 1602633618
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    116 |           3080.5 | 18767872 |  269.131 |              305.323 |              128.354 |            778.578 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3154.771683831741
    time_step_min: 2950
  date: 2020-10-14_00-00-45
  done: false
  episode_len_mean: 778.5959854617545
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.3097103531599
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 194
  episodes_total: 24212
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.1701481839021047
        entropy_coeff: 0.0005000000000000001
        kl: 0.00412475304134811
        model: {}
        policy_loss: -0.009199916142582273
        total_loss: 1.8399411340554555
        vf_explained_var: 0.9962038993835449
        vf_loss: 1.8492261171340942
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.812903225806444
    gpu_util_percent0: 0.38161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147802740425029
    mean_env_wait_ms: 1.2210210238518875
    mean_inference_ms: 4.340961994006345
    mean_raw_obs_processing_ms: 0.38156896290362274
  time_since_restore: 3106.9495844841003
  time_this_iter_s: 26.449076890945435
  time_total_s: 3106.9495844841003
  timers:
    learn_throughput: 8353.646
    learn_time_ms: 19367.832
    sample_throughput: 23436.944
    sample_time_ms: 6903.289
    update_time_ms: 34.006
  timestamp: 1602633645
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    117 |          3106.95 | 18929664 |   269.31 |              305.323 |              128.354 |            778.596 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3153.1925170903432
    time_step_min: 2950
  date: 2020-10-14_00-01-12
  done: false
  episode_len_mean: 778.6361183780249
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.5497581273824
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 252
  episodes_total: 24464
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.156033077587684
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037198655190877616
        model: {}
        policy_loss: -0.009029158842774146
        total_loss: 1.5604484577973683
        vf_explained_var: 0.9971835613250732
        vf_loss: 1.569555660088857
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94
    gpu_util_percent0: 0.319
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778937839623907
    mean_env_wait_ms: 1.220990270902806
    mean_inference_ms: 4.340167284309008
    mean_raw_obs_processing_ms: 0.3815271639372294
  time_since_restore: 3133.2842614650726
  time_this_iter_s: 26.33467698097229
  time_total_s: 3133.2842614650726
  timers:
    learn_throughput: 8355.603
    learn_time_ms: 19363.294
    sample_throughput: 23484.229
    sample_time_ms: 6889.389
    update_time_ms: 33.117
  timestamp: 1602633672
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    118 |          3133.28 | 19091456 |   269.55 |              305.323 |              128.354 |            778.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3152.048632836306
    time_step_min: 2950
  date: 2020-10-14_00-01-39
  done: false
  episode_len_mean: 778.6690603700098
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.7223783600366
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 184
  episodes_total: 24648
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.13847396398584047
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038921852440883717
        model: {}
        policy_loss: -0.006751392104585345
        total_loss: 1.3265824615955353
        vf_explained_var: 0.9970529079437256
        vf_loss: 1.3334031005700429
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.90967741935484
    gpu_util_percent0: 0.2880645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778065321117959
    mean_env_wait_ms: 1.220964017268005
    mean_inference_ms: 4.339610800089322
    mean_raw_obs_processing_ms: 0.3814950413875708
  time_since_restore: 3159.7278881073
  time_this_iter_s: 26.443626642227173
  time_total_s: 3159.7278881073
  timers:
    learn_throughput: 8351.568
    learn_time_ms: 19372.65
    sample_throughput: 23523.701
    sample_time_ms: 6877.829
    update_time_ms: 31.574
  timestamp: 1602633699
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    119 |          3159.73 | 19253248 |  269.722 |              305.323 |              128.354 |            778.669 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3150.913443310612
    time_step_min: 2950
  date: 2020-10-14_00-02-05
  done: false
  episode_len_mean: 778.7131464475592
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 269.8922123604337
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 180
  episodes_total: 24828
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.15610838681459427
        entropy_coeff: 0.0005000000000000001
        kl: 0.003907249813588957
        model: {}
        policy_loss: -0.008552311808064891
        total_loss: 1.3009407420953114
        vf_explained_var: 0.9971628785133362
        vf_loss: 1.309571127096812
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.906451612903222
    gpu_util_percent0: 0.32064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777161426235708
    mean_env_wait_ms: 1.2209351945960334
    mean_inference_ms: 4.339065995338047
    mean_raw_obs_processing_ms: 0.38146329688492553
  time_since_restore: 3186.051331281662
  time_this_iter_s: 26.323443174362183
  time_total_s: 3186.051331281662
  timers:
    learn_throughput: 8351.476
    learn_time_ms: 19372.862
    sample_throughput: 23583.358
    sample_time_ms: 6860.431
    update_time_ms: 33.385
  timestamp: 1602633725
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    120 |          3186.05 | 19415040 |  269.892 |              305.323 |              128.354 |            778.713 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3149.3702431817273
    time_step_min: 2950
  date: 2020-10-14_00-02-32
  done: false
  episode_len_mean: 778.7751814339262
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.1260294144894
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 250
  episodes_total: 25078
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.15299376845359802
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039689391463374095
        model: {}
        policy_loss: -0.008984444139059633
        total_loss: 1.3363647560278575
        vf_explained_var: 0.9976133704185486
        vf_loss: 1.3454256852467854
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57666666666667
    gpu_util_percent0: 0.31233333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477594723616974
    mean_env_wait_ms: 1.220892720166534
    mean_inference_ms: 4.3383159087445975
    mean_raw_obs_processing_ms: 0.38142343598482475
  time_since_restore: 3212.4196457862854
  time_this_iter_s: 26.368314504623413
  time_total_s: 3212.4196457862854
  timers:
    learn_throughput: 8357.87
    learn_time_ms: 19358.041
    sample_throughput: 23605.595
    sample_time_ms: 6853.968
    update_time_ms: 33.497
  timestamp: 1602633752
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    121 |          3212.42 | 19576832 |  270.126 |              305.323 |              128.354 |            778.775 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3148.1567959434296
    time_step_min: 2950
  date: 2020-10-14_00-02-59
  done: false
  episode_len_mean: 778.8494342906876
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.31145420499803
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 200
  episodes_total: 25278
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.13192211215694746
        entropy_coeff: 0.0005000000000000001
        kl: 0.003562251280527562
        model: {}
        policy_loss: -0.006472938703761126
        total_loss: 1.2065467437108357
        vf_explained_var: 0.9975103735923767
        vf_loss: 1.2130856414635975
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.31935483870968
    gpu_util_percent0: 0.4377419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477499365165822
    mean_env_wait_ms: 1.2208564804608832
    mean_inference_ms: 4.337732837077728
    mean_raw_obs_processing_ms: 0.3813895686036798
  time_since_restore: 3238.736887693405
  time_this_iter_s: 26.31724190711975
  time_total_s: 3238.736887693405
  timers:
    learn_throughput: 8356.363
    learn_time_ms: 19361.533
    sample_throughput: 23650.128
    sample_time_ms: 6841.062
    update_time_ms: 33.118
  timestamp: 1602633779
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    122 |          3238.74 | 19738624 |  270.311 |              305.323 |              128.354 |            778.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3147.1301011373025
    time_step_min: 2950
  date: 2020-10-14_00-03-26
  done: false
  episode_len_mean: 778.9041106657235
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.4646190744988
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 168
  episodes_total: 25446
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.13830991089344025
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043364263062054915
        model: {}
        policy_loss: -0.008275023118282357
        total_loss: 1.0009029010931652
        vf_explained_var: 0.9977321028709412
        vf_loss: 1.0092470596234004
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.635483870967747
    gpu_util_percent0: 0.355483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477422247409013
    mean_env_wait_ms: 1.2208235164147716
    mean_inference_ms: 4.337247145912159
    mean_raw_obs_processing_ms: 0.381361983030755
  time_since_restore: 3265.2612702846527
  time_this_iter_s: 26.52438259124756
  time_total_s: 3265.2612702846527
  timers:
    learn_throughput: 8353.987
    learn_time_ms: 19367.041
    sample_throughput: 23626.574
    sample_time_ms: 6847.882
    update_time_ms: 33.597
  timestamp: 1602633806
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    123 |          3265.26 | 19900416 |  270.465 |              305.323 |              128.354 |            778.904 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3145.8145041741436
    time_step_min: 2950
  date: 2020-10-14_00-03-53
  done: false
  episode_len_mean: 779.0039347072344
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.6624635855615
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 223
  episodes_total: 25669
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.15040000155568123
        entropy_coeff: 0.0005000000000000001
        kl: 0.004205987846944481
        model: {}
        policy_loss: -0.0084898002751288
        total_loss: 1.413439432779948
        vf_explained_var: 0.9973837733268738
        vf_loss: 1.4220044513543446
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.84193548387097
    gpu_util_percent0: 0.2793548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477322160184666
    mean_env_wait_ms: 1.220775298428853
    mean_inference_ms: 4.336612324763281
    mean_raw_obs_processing_ms: 0.3813255431928899
  time_since_restore: 3291.4791169166565
  time_this_iter_s: 26.217846632003784
  time_total_s: 3291.4791169166565
  timers:
    learn_throughput: 8351.237
    learn_time_ms: 19373.416
    sample_throughput: 23811.97
    sample_time_ms: 6794.566
    update_time_ms: 33.485
  timestamp: 1602633833
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    124 |          3291.48 | 20062208 |  270.662 |              305.323 |              128.354 |            779.004 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3144.483414520993
    time_step_min: 2950
  date: 2020-10-14_00-04-20
  done: false
  episode_len_mean: 779.0900737423266
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 270.8570481464192
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 232
  episodes_total: 25901
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.13230709110697111
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037244013510644436
        model: {}
        policy_loss: -0.007667488719259079
        total_loss: 1.4335866769154866
        vf_explained_var: 0.9973209500312805
        vf_loss: 1.4413203001022339
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.336666666666662
    gpu_util_percent0: 0.412
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14772132906253096
    mean_env_wait_ms: 1.2207267143354081
    mean_inference_ms: 4.335964931572133
    mean_raw_obs_processing_ms: 0.3812902438994605
  time_since_restore: 3317.548483610153
  time_this_iter_s: 26.069366693496704
  time_total_s: 3317.548483610153
  timers:
    learn_throughput: 8359.891
    learn_time_ms: 19353.363
    sample_throughput: 23915.972
    sample_time_ms: 6765.019
    update_time_ms: 29.682
  timestamp: 1602633860
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    125 |          3317.55 | 20224000 |  270.857 |              305.323 |              128.354 |             779.09 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3143.5189338658884
    time_step_min: 2950
  date: 2020-10-14_00-04-46
  done: false
  episode_len_mean: 779.1518045487669
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.0074185648918
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 172
  episodes_total: 26073
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.12917267034451166
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038988196562665203
        model: {}
        policy_loss: -0.006992035099149992
        total_loss: 1.0980620334545772
        vf_explained_var: 0.9975457787513733
        vf_loss: 1.1051186919212341
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10645161290323
    gpu_util_percent0: 0.2890322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477138092209224
    mean_env_wait_ms: 1.2206861972262437
    mean_inference_ms: 4.335487231422873
    mean_raw_obs_processing_ms: 0.3812627138894302
  time_since_restore: 3343.693370819092
  time_this_iter_s: 26.1448872089386
  time_total_s: 3343.693370819092
  timers:
    learn_throughput: 8354.816
    learn_time_ms: 19365.118
    sample_throughput: 23925.946
    sample_time_ms: 6762.199
    update_time_ms: 31.946
  timestamp: 1602633886
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    126 |          3343.69 | 20385792 |  271.007 |              305.323 |              128.354 |            779.152 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3142.3872393947477
    time_step_min: 2950
  date: 2020-10-14_00-05-13
  done: false
  episode_len_mean: 779.214486906212
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.17589721822367
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 199
  episodes_total: 26272
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.1453334242105484
        entropy_coeff: 0.0005000000000000001
        kl: 0.004310628632083535
        model: {}
        policy_loss: -0.009597847509818772
        total_loss: 1.1674810647964478
        vf_explained_var: 0.9976624846458435
        vf_loss: 1.1771516005198162
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.459999999999997
    gpu_util_percent0: 0.3346666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14770546698097106
    mean_env_wait_ms: 1.220636637715108
    mean_inference_ms: 4.3349553282195386
    mean_raw_obs_processing_ms: 0.38123119070475703
  time_since_restore: 3370.3396203517914
  time_this_iter_s: 26.646249532699585
  time_total_s: 3370.3396203517914
  timers:
    learn_throughput: 8356.791
    learn_time_ms: 19360.541
    sample_throughput: 23841.946
    sample_time_ms: 6786.023
    update_time_ms: 31.781
  timestamp: 1602633913
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    127 |          3370.34 | 20547584 |  271.176 |              305.323 |              128.354 |            779.214 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3141.077867150032
    time_step_min: 2950
  date: 2020-10-14_00-05-40
  done: false
  episode_len_mean: 779.2942374415447
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.373592997405
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 244
  episodes_total: 26516
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.13515925034880638
        entropy_coeff: 0.0005000000000000001
        kl: 0.003592020172315339
        model: {}
        policy_loss: -0.006902218621689826
        total_loss: 1.8067791163921356
        vf_explained_var: 0.9968041777610779
        vf_loss: 1.8137489358584087
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.012903225806454
    gpu_util_percent0: 0.37290322580645163
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769388763144947
    mean_env_wait_ms: 1.2205770148284607
    mean_inference_ms: 4.334288634489004
    mean_raw_obs_processing_ms: 0.38119547896802886
  time_since_restore: 3396.817754507065
  time_this_iter_s: 26.478134155273438
  time_total_s: 3396.817754507065
  timers:
    learn_throughput: 8355.289
    learn_time_ms: 19364.023
    sample_throughput: 23806.755
    sample_time_ms: 6796.054
    update_time_ms: 31.601
  timestamp: 1602633940
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    128 |          3396.82 | 20709376 |  271.374 |              305.323 |              128.354 |            779.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3140.0637468126592
    time_step_min: 2950
  date: 2020-10-14_00-06-07
  done: false
  episode_len_mean: 779.3489495562296
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.52590618010237
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 187
  episodes_total: 26703
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12362479977309704
        entropy_coeff: 0.0005000000000000001
        kl: 0.004788789587716262
        model: {}
        policy_loss: -0.01017092047065186
        total_loss: 1.0793224076430004
        vf_explained_var: 0.9976506233215332
        vf_loss: 1.0895551145076752
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21935483870968
    gpu_util_percent0: 0.32645161290322583
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14768634211549184
    mean_env_wait_ms: 1.2205272377846716
    mean_inference_ms: 4.333804136529902
    mean_raw_obs_processing_ms: 0.3811661769932968
  time_since_restore: 3423.0224385261536
  time_this_iter_s: 26.204684019088745
  time_total_s: 3423.0224385261536
  timers:
    learn_throughput: 8367.46
    learn_time_ms: 19335.856
    sample_throughput: 23791.287
    sample_time_ms: 6800.473
    update_time_ms: 31.485
  timestamp: 1602633967
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    129 |          3423.02 | 20871168 |  271.526 |              305.323 |              128.354 |            779.349 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3139.122322792118
    time_step_min: 2950
  date: 2020-10-14_00-06-34
  done: false
  episode_len_mean: 779.4044341938844
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.66934541456527
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 179
  episodes_total: 26882
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.13681315382321677
        entropy_coeff: 0.0005000000000000001
        kl: 0.004381068516522646
        model: {}
        policy_loss: -0.008273542117725205
        total_loss: 1.340931475162506
        vf_explained_var: 0.9971432685852051
        vf_loss: 1.3492734134197235
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.700000000000003
    gpu_util_percent0: 0.2803333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14767856271367316
    mean_env_wait_ms: 1.2204782220436612
    mean_inference_ms: 4.33334331306815
    mean_raw_obs_processing_ms: 0.3811385373016429
  time_since_restore: 3449.2346682548523
  time_this_iter_s: 26.21222972869873
  time_total_s: 3449.2346682548523
  timers:
    learn_throughput: 8367.796
    learn_time_ms: 19335.08
    sample_throughput: 23827.512
    sample_time_ms: 6790.134
    update_time_ms: 29.678
  timestamp: 1602633994
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    130 |          3449.23 | 21032960 |  271.669 |              305.323 |              128.354 |            779.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3137.8412018751615
    time_step_min: 2950
  date: 2020-10-14_00-07-01
  done: false
  episode_len_mean: 779.4864336798644
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 271.86411672576236
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 244
  episodes_total: 27126
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.13891911258300146
        entropy_coeff: 0.0005000000000000001
        kl: 0.004638209706172347
        model: {}
        policy_loss: -0.010382649403860947
        total_loss: 1.242403397957484
        vf_explained_var: 0.9978165626525879
        vf_loss: 1.2528554797172546
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55483870967742
    gpu_util_percent0: 0.3032258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14766853060243393
    mean_env_wait_ms: 1.220409656023447
    mean_inference_ms: 4.332720600147525
    mean_raw_obs_processing_ms: 0.38110375804688135
  time_since_restore: 3475.703278064728
  time_this_iter_s: 26.46860980987549
  time_total_s: 3475.703278064728
  timers:
    learn_throughput: 8362.508
    learn_time_ms: 19347.305
    sample_throughput: 23856.828
    sample_time_ms: 6781.79
    update_time_ms: 31.884
  timestamp: 1602634021
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    131 |           3475.7 | 21194752 |  271.864 |              305.323 |              128.354 |            779.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3136.780297479484
    time_step_min: 2950
  date: 2020-10-14_00-07-28
  done: false
  episode_len_mean: 779.5484248655373
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.0277514451529
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 205
  episodes_total: 27331
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.11882328987121582
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033434471115469933
        model: {}
        policy_loss: -0.007724979077465832
        total_loss: 0.9796513219674429
        vf_explained_var: 0.9980270266532898
        vf_loss: 0.9874357034762701
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.767741935483876
    gpu_util_percent0: 0.2803225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765993322830842
    mean_env_wait_ms: 1.220350523808309
    mean_inference_ms: 4.332206726516409
    mean_raw_obs_processing_ms: 0.38107347654110324
  time_since_restore: 3501.985260248184
  time_this_iter_s: 26.28198218345642
  time_total_s: 3501.985260248184
  timers:
    learn_throughput: 8360.474
    learn_time_ms: 19352.013
    sample_throughput: 23921.751
    sample_time_ms: 6763.385
    update_time_ms: 32.55
  timestamp: 1602634048
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    132 |          3501.99 | 21356544 |  272.028 |              305.323 |              128.354 |            779.548 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3135.9012925541597
    time_step_min: 2950
  date: 2020-10-14_00-07-54
  done: false
  episode_len_mean: 779.5916
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.1607199265381
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 169
  episodes_total: 27500
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.12584660512705645
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036025882000103593
        model: {}
        policy_loss: -0.0094398159320311
        total_loss: 0.8635007490714391
        vf_explained_var: 0.9980620741844177
        vf_loss: 0.8730034927527109
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.566666666666666
    gpu_util_percent0: 0.30700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765331897616019
    mean_env_wait_ms: 1.2202998258388624
    mean_inference_ms: 4.331788519634489
    mean_raw_obs_processing_ms: 0.3810486842640565
  time_since_restore: 3528.3085420131683
  time_this_iter_s: 26.32328176498413
  time_total_s: 3528.3085420131683
  timers:
    learn_throughput: 8360.629
    learn_time_ms: 19351.654
    sample_throughput: 23994.297
    sample_time_ms: 6742.936
    update_time_ms: 32.699
  timestamp: 1602634074
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    133 |          3528.31 | 21518336 |  272.161 |              305.323 |              128.354 |            779.592 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3134.736258577104
    time_step_min: 2950
  date: 2020-10-14_00-08-21
  done: false
  episode_len_mean: 779.646420198377
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.3386598172892
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 225
  episodes_total: 27725
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.13572877024610838
        entropy_coeff: 0.0005000000000000001
        kl: 0.004482988268136978
        model: {}
        policy_loss: -0.008557878199402088
        total_loss: 0.9369617253541946
        vf_explained_var: 0.9982648491859436
        vf_loss: 0.9455874810616175
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.354838709677423
    gpu_util_percent0: 0.39387096774193553
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14764457617960894
    mean_env_wait_ms: 1.2202284027786703
    mean_inference_ms: 4.331235993005479
    mean_raw_obs_processing_ms: 0.38101646670092876
  time_since_restore: 3554.3814418315887
  time_this_iter_s: 26.07289981842041
  time_total_s: 3554.3814418315887
  timers:
    learn_throughput: 8369.374
    learn_time_ms: 19331.434
    sample_throughput: 23974.733
    sample_time_ms: 6748.438
    update_time_ms: 31.456
  timestamp: 1602634101
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    134 |          3554.38 | 21680128 |  272.339 |              305.323 |              128.354 |            779.646 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3133.533039647577
    time_step_min: 2950
  date: 2020-10-14_00-08-48
  done: false
  episode_len_mean: 779.701745600229
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.51982805592047
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 231
  episodes_total: 27956
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.12009631469845772
        entropy_coeff: 0.0005000000000000001
        kl: 0.00511722903077801
        model: {}
        policy_loss: -0.008499198534991592
        total_loss: 0.8839535117149353
        vf_explained_var: 0.9983601570129395
        vf_loss: 0.8925127486387888
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.310000000000006
    gpu_util_percent0: 0.3733333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476341745705452
    mean_env_wait_ms: 1.2201585646681916
    mean_inference_ms: 4.330686230154041
    mean_raw_obs_processing_ms: 0.38098446564384053
  time_since_restore: 3580.699727535248
  time_this_iter_s: 26.318285703659058
  time_total_s: 3580.699727535248
  timers:
    learn_throughput: 8362.916
    learn_time_ms: 19346.362
    sample_throughput: 23951.754
    sample_time_ms: 6754.912
    update_time_ms: 33.432
  timestamp: 1602634128
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | RUNNING  | 172.17.0.4:15528 |    135 |           3580.7 | 21841920 |   272.52 |              305.323 |              128.354 |            779.702 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_fbf40_00000:
  custom_metrics:
    time_step_max: 4085
    time_step_mean: 3132.6958812431026
    time_step_min: 2950
  date: 2020-10-14_00-09-15
  done: true
  episode_len_mean: 779.7487022683638
  episode_reward_max: 305.3232323232323
  episode_reward_mean: 272.64552551038366
  episode_reward_min: 128.35353535353502
  episodes_this_iter: 170
  episodes_total: 28126
  experiment_id: 947be63ddfda496482668612bd4829c3
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.11492656357586384
        entropy_coeff: 0.0005000000000000001
        kl: 0.003911605939113845
        model: {}
        policy_loss: -0.007973914422715703
        total_loss: 1.179173767566681
        vf_explained_var: 0.9974175095558167
        vf_loss: 1.1872051457564037
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.593548387096774
    gpu_util_percent0: 0.2932258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 15528
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476283235118298
    mean_env_wait_ms: 1.2201021024488636
    mean_inference_ms: 4.330281174287216
    mean_raw_obs_processing_ms: 0.380960352666532
  time_since_restore: 3607.112093448639
  time_this_iter_s: 26.412365913391113
  time_total_s: 3607.112093448639
  timers:
    learn_throughput: 8357.886
    learn_time_ms: 19358.005
    sample_throughput: 23927.857
    sample_time_ms: 6761.658
    update_time_ms: 39.612
  timestamp: 1602634155
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: fbf40_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | TERMINATED |       |    136 |          3607.11 | 22003712 |  272.646 |              305.323 |              128.354 |            779.749 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.01 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_fbf40_00000 | TERMINATED |       |    136 |          3607.11 | 22003712 |  272.646 |              305.323 |              128.354 |            779.749 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


