2020-10-15 18:48:06,692	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_f76cb_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=38736)[0m 2020-10-15 18:48:09,401	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=38639)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38639)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38733)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38733)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38708)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38708)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38702)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38702)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38686)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38686)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38647)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38647)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38747)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38747)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38653)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38653)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38760)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38760)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38716)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38716)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38654)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38654)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38717)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38717)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38755)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38755)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38646)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38646)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38650)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38650)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38634)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38634)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38763)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38763)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38734)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38734)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38726)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38726)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38706)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38706)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38662)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38662)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38742)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38742)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38719)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38719)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38749)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38749)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38744)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38744)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38677)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38677)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38641)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38641)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38645)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38645)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38756)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38756)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38710)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38710)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38649)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38649)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38637)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38637)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38752)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38752)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38711)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38711)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38713)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38713)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38761)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38761)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38740)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38740)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38754)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38754)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38699)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38699)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38675)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38675)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38694)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38694)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38688)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38688)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38643)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38643)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38743)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38743)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38674)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38674)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38741)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38741)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38636)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38636)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38730)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38730)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38692)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38692)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38660)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38660)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38714)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38714)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38657)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38657)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38633)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38633)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38635)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38635)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38638)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38638)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38721)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38721)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38663)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38663)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38739)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38739)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38705)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38705)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38651)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38651)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38681)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38681)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38648)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38648)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38642)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38642)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38712)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38712)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38665)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38665)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38664)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38664)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38631)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38631)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38746)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38746)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38709)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38709)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38700)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38700)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38729)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38729)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38652)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38652)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38632)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38632)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38723)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38723)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38655)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38655)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38656)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38656)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38722)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38722)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38728)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38728)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=38750)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=38750)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3771.2241379310344
    time_step_min: 3428
  date: 2020-10-15_18-48-43
  done: false
  episode_len_mean: 902.7784810126582
  episode_reward_max: 270.95959595959573
  episode_reward_mean: 218.82112261859064
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1621165374914806
        entropy_coeff: 0.0005000000000000001
        kl: 0.004109993615808587
        model: {}
        policy_loss: -0.008680239833969003
        total_loss: 463.3200505574544
        vf_explained_var: 0.5503719449043274
        vf_loss: 463.32850392659503
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.069696969696963
    gpu_util_percent0: 0.3342424242424243
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5515151515151517
    vram_util_percent0: 0.08582297226114873
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17200570372969842
    mean_env_wait_ms: 1.1756412450749774
    mean_inference_ms: 5.990311798397204
    mean_raw_obs_processing_ms: 0.4625541761209289
  time_since_restore: 28.435828924179077
  time_this_iter_s: 28.435828924179077
  time_total_s: 28.435828924179077
  timers:
    learn_throughput: 8499.68
    learn_time_ms: 19035.07
    sample_throughput: 17331.864
    sample_time_ms: 9334.945
    update_time_ms: 31.021
  timestamp: 1602787723
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      1 |          28.4358 | 161792 |  218.821 |               270.96 |              107.323 |            902.778 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3747.29197080292
    time_step_min: 3428
  date: 2020-10-15_18-49-09
  done: false
  episode_len_mean: 900.9272151898734
  episode_reward_max: 274.14141414141386
  episode_reward_mean: 222.16164812683766
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1380364000797272
        entropy_coeff: 0.0005000000000000001
        kl: 0.00867712350251774
        model: {}
        policy_loss: -0.00887471608408911
        total_loss: 109.33460108439128
        vf_explained_var: 0.8141503930091858
        vf_loss: 109.34317715962727
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.43
    gpu_util_percent0: 0.3213333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7533333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1667755038321209
    mean_env_wait_ms: 1.1696216445131713
    mean_inference_ms: 5.684846905961418
    mean_raw_obs_processing_ms: 0.44707939940467123
  time_since_restore: 54.73858070373535
  time_this_iter_s: 26.302751779556274
  time_total_s: 54.73858070373535
  timers:
    learn_throughput: 8577.756
    learn_time_ms: 18861.809
    sample_throughput: 19191.049
    sample_time_ms: 8430.597
    update_time_ms: 37.066
  timestamp: 1602787749
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      2 |          54.7386 | 323584 |  222.162 |              274.141 |              107.323 |            900.927 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3727.0833333333335
    time_step_min: 3406
  date: 2020-10-15_18-49-35
  done: false
  episode_len_mean: 894.8776371308016
  episode_reward_max: 274.14141414141386
  episode_reward_mean: 223.82911392405032
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1231509645779927
        entropy_coeff: 0.0005000000000000001
        kl: 0.010296327294781804
        model: {}
        policy_loss: -0.01334596873978929
        total_loss: 55.0855909983317
        vf_explained_var: 0.8887900710105896
        vf_loss: 55.09846909840902
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666672
    gpu_util_percent0: 0.34833333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16332176833038398
    mean_env_wait_ms: 1.1688491566543688
    mean_inference_ms: 5.460986018290804
    mean_raw_obs_processing_ms: 0.43640516012083136
  time_since_restore: 80.33329272270203
  time_this_iter_s: 25.594712018966675
  time_total_s: 80.33329272270203
  timers:
    learn_throughput: 8591.389
    learn_time_ms: 18831.879
    sample_throughput: 20566.699
    sample_time_ms: 7866.698
    update_time_ms: 36.212
  timestamp: 1602787775
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      3 |          80.3333 | 485376 |  223.829 |              274.141 |              107.323 |            894.878 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3720.9745762711864
    time_step_min: 3388
  date: 2020-10-15_18-50-00
  done: false
  episode_len_mean: 890.2642405063291
  episode_reward_max: 276.26262626262604
  episode_reward_mean: 224.71463048203523
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1073287626107533
        entropy_coeff: 0.0005000000000000001
        kl: 0.008640646080796918
        model: {}
        policy_loss: -0.01210791828634683
        total_loss: 43.94570732116699
        vf_explained_var: 0.9174127578735352
        vf_loss: 43.95750586191813
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.806896551724137
    gpu_util_percent0: 0.38310344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16096030317130666
    mean_env_wait_ms: 1.1698257160709131
    mean_inference_ms: 5.301543224064487
    mean_raw_obs_processing_ms: 0.4286842495217658
  time_since_restore: 105.93596506118774
  time_this_iter_s: 25.602672338485718
  time_total_s: 105.93596506118774
  timers:
    learn_throughput: 8601.565
    learn_time_ms: 18809.601
    sample_throughput: 21298.109
    sample_time_ms: 7596.543
    update_time_ms: 32.833
  timestamp: 1602787800
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      4 |          105.936 | 647168 |  224.715 |              276.263 |              107.323 |            890.264 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3710.621657754011
    time_step_min: 3388
  date: 2020-10-15_18-50-26
  done: false
  episode_len_mean: 884.3227848101266
  episode_reward_max: 276.2626262626261
  episode_reward_mean: 226.2800792737499
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0708771049976349
        entropy_coeff: 0.0005000000000000001
        kl: 0.0078018741915002465
        model: {}
        policy_loss: -0.012055049980214486
        total_loss: 35.78077920277914
        vf_explained_var: 0.9343737959861755
        vf_loss: 35.792589823404946
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.693103448275863
    gpu_util_percent0: 0.29758620689655174
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15924332505010916
    mean_env_wait_ms: 1.1713015985170567
    mean_inference_ms: 5.183150439559603
    mean_raw_obs_processing_ms: 0.42281217856486203
  time_since_restore: 131.50106930732727
  time_this_iter_s: 25.565104246139526
  time_total_s: 131.50106930732727
  timers:
    learn_throughput: 8613.05
    learn_time_ms: 18784.519
    sample_throughput: 21753.124
    sample_time_ms: 7437.644
    update_time_ms: 32.67
  timestamp: 1602787826
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      5 |          131.501 | 808960 |   226.28 |              276.263 |              107.323 |            884.323 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3688.6055396370584
    time_step_min: 3373
  date: 2020-10-15_18-50-52
  done: false
  episode_len_mean: 871.2506887052342
  episode_reward_max: 278.53535353535335
  episode_reward_mean: 230.34310042574478
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 299
  episodes_total: 1089
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0434323847293854
        entropy_coeff: 0.0005000000000000001
        kl: 0.007448257490371664
        model: {}
        policy_loss: -0.012186863285023719
        total_loss: 35.14606253306071
        vf_explained_var: 0.9557506442070007
        vf_loss: 35.15802574157715
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.056666666666672
    gpu_util_percent0: 0.32266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15702038721567085
    mean_env_wait_ms: 1.1755679696177475
    mean_inference_ms: 5.03204914113174
    mean_raw_obs_processing_ms: 0.4154789341369128
  time_since_restore: 157.29221081733704
  time_this_iter_s: 25.791141510009766
  time_total_s: 157.29221081733704
  timers:
    learn_throughput: 8609.262
    learn_time_ms: 18792.785
    sample_throughput: 22031.023
    sample_time_ms: 7343.826
    update_time_ms: 32.488
  timestamp: 1602787852
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      6 |          157.292 | 970752 |  230.343 |              278.535 |              107.323 |            871.251 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3680.719312602291
    time_step_min: 3329
  date: 2020-10-15_18-51-18
  done: false
  episode_len_mean: 863.7784810126582
  episode_reward_max: 285.20202020202015
  episode_reward_mean: 231.55858426032455
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 1264
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0492589275042217
        entropy_coeff: 0.0005000000000000001
        kl: 0.006972051497238378
        model: {}
        policy_loss: -0.01170268045583119
        total_loss: 21.69623072942098
        vf_explained_var: 0.9608769416809082
        vf_loss: 21.70776096979777
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.179310344827588
    gpu_util_percent0: 0.34241379310344827
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15611134974700291
    mean_env_wait_ms: 1.1776495266403382
    mean_inference_ms: 4.968094115983268
    mean_raw_obs_processing_ms: 0.41243269320212644
  time_since_restore: 182.96785140037537
  time_this_iter_s: 25.67564058303833
  time_total_s: 182.96785140037537
  timers:
    learn_throughput: 8606.873
    learn_time_ms: 18797.999
    sample_throughput: 22281.634
    sample_time_ms: 7261.227
    update_time_ms: 31.591
  timestamp: 1602787878
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      7 |          182.968 | 1132544 |  231.559 |              285.202 |              107.323 |            863.778 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3671.960144927536
    time_step_min: 3329
  date: 2020-10-15_18-51-43
  done: false
  episode_len_mean: 858.0611814345991
  episode_reward_max: 285.20202020202015
  episode_reward_mean: 232.9800110812767
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0351685583591461
        entropy_coeff: 0.0005000000000000001
        kl: 0.006892501260153949
        model: {}
        policy_loss: -0.011367103511778017
        total_loss: 19.161619822184246
        vf_explained_var: 0.9610426425933838
        vf_loss: 19.172815481821697
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.953333333333337
    gpu_util_percent0: 0.3540000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15541108339308224
    mean_env_wait_ms: 1.179624065010627
    mean_inference_ms: 4.919620920049767
    mean_raw_obs_processing_ms: 0.41002142279691844
  time_since_restore: 208.64787435531616
  time_this_iter_s: 25.680022954940796
  time_total_s: 208.64787435531616
  timers:
    learn_throughput: 8610.315
    learn_time_ms: 18790.486
    sample_throughput: 22435.32
    sample_time_ms: 7211.486
    update_time_ms: 31.214
  timestamp: 1602787903
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      8 |          208.648 | 1294336 |   232.98 |              285.202 |              107.323 |            858.061 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3662.9545454545455
    time_step_min: 3329
  date: 2020-10-15_18-52-09
  done: false
  episode_len_mean: 853.1422250316056
  episode_reward_max: 287.0202020202019
  episode_reward_mean: 234.41216846084077
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 160
  episodes_total: 1582
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9972319106260935
        entropy_coeff: 0.0005000000000000001
        kl: 0.007517460539626579
        model: {}
        policy_loss: -0.012781010950978574
        total_loss: 18.780067920684814
        vf_explained_var: 0.9643420577049255
        vf_loss: 18.792595386505127
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.39310344827587
    gpu_util_percent0: 0.3593103448275863
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034485
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1548085973978537
    mean_env_wait_ms: 1.1816734930997788
    mean_inference_ms: 4.877014729021364
    mean_raw_obs_processing_ms: 0.4078548645832135
  time_since_restore: 234.12972354888916
  time_this_iter_s: 25.481849193572998
  time_total_s: 234.12972354888916
  timers:
    learn_throughput: 8618.223
    learn_time_ms: 18773.243
    sample_throughput: 22588.136
    sample_time_ms: 7162.698
    update_time_ms: 30.246
  timestamp: 1602787929
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |      9 |           234.13 | 1456128 |  234.412 |               287.02 |              107.323 |            853.142 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3647.884034519957
    time_step_min: 3261
  date: 2020-10-15_18-52-35
  done: false
  episode_len_mean: 843.7589662447257
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 236.817302774581
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 314
  episodes_total: 1896
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9691165437301
        entropy_coeff: 0.0005000000000000001
        kl: 0.0063012606697157025
        model: {}
        policy_loss: -0.011411164688373296
        total_loss: 24.393256028493244
        vf_explained_var: 0.9697794914245605
        vf_loss: 24.40452178319295
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.637931034482758
    gpu_util_percent0: 0.3551724137931033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15385480343093563
    mean_env_wait_ms: 1.185670877778752
    mean_inference_ms: 4.80983335845326
    mean_raw_obs_processing_ms: 0.40454517445480703
  time_since_restore: 259.6285984516144
  time_this_iter_s: 25.49887490272522
  time_total_s: 259.6285984516144
  timers:
    learn_throughput: 8624.829
    learn_time_ms: 18758.864
    sample_throughput: 22710.681
    sample_time_ms: 7124.049
    update_time_ms: 31.216
  timestamp: 1602787955
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     10 |          259.629 | 1617920 |  236.817 |              295.505 |              107.323 |            843.759 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3640.322564612326
    time_step_min: 3261
  date: 2020-10-15_18-53-00
  done: false
  episode_len_mean: 839.8476144109055
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 237.9823306089127
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9603535681962967
        entropy_coeff: 0.0005000000000000001
        kl: 0.006699105259031057
        model: {}
        policy_loss: -0.013546019990523442
        total_loss: 14.318873723347982
        vf_explained_var: 0.9735555052757263
        vf_loss: 14.332229693730673
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85
    gpu_util_percent0: 0.27666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1534623997524214
    mean_env_wait_ms: 1.1873532534463416
    mean_inference_ms: 4.782150173963429
    mean_raw_obs_processing_ms: 0.4031716870691291
  time_since_restore: 285.48706817626953
  time_this_iter_s: 25.85846972465515
  time_total_s: 285.48706817626953
  timers:
    learn_throughput: 8628.985
    learn_time_ms: 18749.83
    sample_throughput: 23541.906
    sample_time_ms: 6872.511
    update_time_ms: 32.261
  timestamp: 1602787980
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     11 |          285.487 | 1779712 |  237.982 |              295.505 |              107.323 |            839.848 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3634.2548387096776
    time_step_min: 3261
  date: 2020-10-15_18-53-27
  done: false
  episode_len_mean: 836.2640144665461
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 238.97284326081777
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.944629356265068
        entropy_coeff: 0.0005000000000000001
        kl: 0.007740743652296563
        model: {}
        policy_loss: -0.011666560147811348
        total_loss: 15.514042218526205
        vf_explained_var: 0.9695773720741272
        vf_loss: 15.525406757990519
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.970000000000002
    gpu_util_percent0: 0.3676666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15310645963719546
    mean_env_wait_ms: 1.1890168633621425
    mean_inference_ms: 4.75709527773593
    mean_raw_obs_processing_ms: 0.40189076623825115
  time_since_restore: 311.73081946372986
  time_this_iter_s: 26.243751287460327
  time_total_s: 311.73081946372986
  timers:
    learn_throughput: 8601.159
    learn_time_ms: 18810.489
    sample_throughput: 23772.876
    sample_time_ms: 6805.739
    update_time_ms: 31.761
  timestamp: 1602788007
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     12 |          311.731 | 1941504 |  238.973 |              295.505 |              107.323 |            836.264 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3624.788824979458
    time_step_min: 3261
  date: 2020-10-15_18-53-53
  done: false
  episode_len_mean: 831.7665589660743
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 240.26908829816728
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 264
  episodes_total: 2476
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8990111748377482
        entropy_coeff: 0.0005000000000000001
        kl: 0.006856335133003692
        model: {}
        policy_loss: -0.012046161340549588
        total_loss: 21.309036095937092
        vf_explained_var: 0.9721997380256653
        vf_loss: 21.32084560394287
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87
    gpu_util_percent0: 0.30199999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15258253021465273
    mean_env_wait_ms: 1.1917498414566812
    mean_inference_ms: 4.720915172115158
    mean_raw_obs_processing_ms: 0.40004681321651764
  time_since_restore: 337.81561756134033
  time_this_iter_s: 26.084798097610474
  time_total_s: 337.81561756134033
  timers:
    learn_throughput: 8587.489
    learn_time_ms: 18840.432
    sample_throughput: 23709.349
    sample_time_ms: 6823.975
    update_time_ms: 31.949
  timestamp: 1602788033
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     13 |          337.816 | 2103296 |  240.269 |              295.505 |              107.323 |            831.767 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3618.495839636914
    time_step_min: 3261
  date: 2020-10-15_18-54-19
  done: false
  episode_len_mean: 828.9441548771407
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 241.32862504418702
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 210
  episodes_total: 2686
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8919987479845682
        entropy_coeff: 0.0005000000000000001
        kl: 0.006052382290363312
        model: {}
        policy_loss: -0.010693902731873095
        total_loss: 12.854787826538086
        vf_explained_var: 0.9796460270881653
        vf_loss: 12.865322589874268
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.813333333333336
    gpu_util_percent0: 0.39099999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15224063534531168
    mean_env_wait_ms: 1.1935008815460824
    mean_inference_ms: 4.696081340822081
    mean_raw_obs_processing_ms: 0.39881015974755646
  time_since_restore: 363.6225275993347
  time_this_iter_s: 25.806910037994385
  time_total_s: 363.6225275993347
  timers:
    learn_throughput: 8589.218
    learn_time_ms: 18836.639
    sample_throughput: 23661.368
    sample_time_ms: 6837.813
    update_time_ms: 41.16
  timestamp: 1602788059
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     14 |          363.623 | 2265088 |  241.329 |              295.505 |              107.323 |            828.944 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3614.1402569593147
    time_step_min: 3261
  date: 2020-10-15_18-54-45
  done: false
  episode_len_mean: 827.4785513361463
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 242.05008950262095
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8864663541316986
        entropy_coeff: 0.0005000000000000001
        kl: 0.006266986446765562
        model: {}
        policy_loss: -0.011825939640402794
        total_loss: 11.424020608266195
        vf_explained_var: 0.9787198901176453
        vf_loss: 11.43566338221232
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.186206896551727
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1520032033056856
    mean_env_wait_ms: 1.194713158217971
    mean_inference_ms: 4.679302333304235
    mean_raw_obs_processing_ms: 0.3979645873594563
  time_since_restore: 389.3068265914917
  time_this_iter_s: 25.684298992156982
  time_total_s: 389.3068265914917
  timers:
    learn_throughput: 8589.052
    learn_time_ms: 18837.002
    sample_throughput: 23622.467
    sample_time_ms: 6849.073
    update_time_ms: 40.323
  timestamp: 1602788085
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     15 |          389.307 | 2426880 |   242.05 |              295.505 |              107.323 |            827.479 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3609.2964213369346
    time_step_min: 3261
  date: 2020-10-15_18-55-11
  done: false
  episode_len_mean: 826.3575233022636
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 242.80506462763432
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 160
  episodes_total: 3004
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8594326078891754
        entropy_coeff: 0.0005000000000000001
        kl: 0.00622925612454613
        model: {}
        policy_loss: -0.011599689605645835
        total_loss: 10.48609709739685
        vf_explained_var: 0.9815452098846436
        vf_loss: 10.497503678003946
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82666666666667
    gpu_util_percent0: 0.3236666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15178254252318074
    mean_env_wait_ms: 1.1958469653148747
    mean_inference_ms: 4.663658155052096
    mean_raw_obs_processing_ms: 0.39716285451949745
  time_since_restore: 415.1648099422455
  time_this_iter_s: 25.857983350753784
  time_total_s: 415.1648099422455
  timers:
    learn_throughput: 8589.881
    learn_time_ms: 18835.186
    sample_throughput: 23601.198
    sample_time_ms: 6855.245
    update_time_ms: 40.989
  timestamp: 1602788111
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     16 |          415.165 | 2588672 |  242.805 |              295.505 |              107.323 |            826.358 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3599.8479926448053
    time_step_min: 3261
  date: 2020-10-15_18-55-37
  done: false
  episode_len_mean: 824.7621785173978
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 244.21925762924232
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 301
  episodes_total: 3305
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8330417821804682
        entropy_coeff: 0.0005000000000000001
        kl: 0.00592450723828127
        model: {}
        policy_loss: -0.010085929917598454
        total_loss: 15.710782845815023
        vf_explained_var: 0.9800810217857361
        vf_loss: 15.720693031946817
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.489655172413794
    gpu_util_percent0: 0.39689655172413796
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15141259877410948
    mean_env_wait_ms: 1.1976960196413735
    mean_inference_ms: 4.637492248766053
    mean_raw_obs_processing_ms: 0.3958419759771981
  time_since_restore: 440.95134830474854
  time_this_iter_s: 25.78653836250305
  time_total_s: 440.95134830474854
  timers:
    learn_throughput: 8592.264
    learn_time_ms: 18829.962
    sample_throughput: 23550.844
    sample_time_ms: 6869.902
    update_time_ms: 42.173
  timestamp: 1602788137
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     17 |          440.951 | 2750464 |  244.219 |              295.505 |              107.323 |            824.762 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3595.008444962143
    time_step_min: 3261
  date: 2020-10-15_18-56-02
  done: false
  episode_len_mean: 823.8201956271577
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 244.87837233090377
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 171
  episodes_total: 3476
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.823181688785553
        entropy_coeff: 0.0005000000000000001
        kl: 0.006371172416644792
        model: {}
        policy_loss: -0.011092360441883406
        total_loss: 10.869799057642618
        vf_explained_var: 0.9816603064537048
        vf_loss: 10.880666017532349
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.466666666666665
    gpu_util_percent0: 0.33499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15122510749300375
    mean_env_wait_ms: 1.1985729176496382
    mean_inference_ms: 4.624343756587896
    mean_raw_obs_processing_ms: 0.39517869287823915
  time_since_restore: 466.6808862686157
  time_this_iter_s: 25.729537963867188
  time_total_s: 466.6808862686157
  timers:
    learn_throughput: 8583.703
    learn_time_ms: 18848.741
    sample_throughput: 23604.056
    sample_time_ms: 6854.415
    update_time_ms: 42.946
  timestamp: 1602788162
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     18 |          466.681 | 2912256 |  244.878 |              295.505 |              107.323 |             823.82 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3590.344376391982
    time_step_min: 3261
  date: 2020-10-15_18-56-28
  done: false
  episode_len_mean: 822.8646119977985
  episode_reward_max: 295.5050505050505
  episode_reward_mean: 245.54553515340507
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 3634
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.820036510626475
        entropy_coeff: 0.0005000000000000001
        kl: 0.006021461876419683
        model: {}
        policy_loss: -0.012707485565139601
        total_loss: 9.866514841715494
        vf_explained_var: 0.9809303283691406
        vf_loss: 9.879030307133993
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.120689655172416
    gpu_util_percent0: 0.3613793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15106315402583909
    mean_env_wait_ms: 1.199352431237981
    mean_inference_ms: 4.61294876823755
    mean_raw_obs_processing_ms: 0.39460185993961383
  time_since_restore: 492.4597210884094
  time_this_iter_s: 25.7788348197937
  time_total_s: 492.4597210884094
  timers:
    learn_throughput: 8573.13
    learn_time_ms: 18871.986
    sample_throughput: 23590.376
    sample_time_ms: 6858.39
    update_time_ms: 44.568
  timestamp: 1602788188
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     19 |           492.46 | 3074048 |  245.546 |              295.505 |              107.323 |            822.865 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3584.9151915455745
    time_step_min: 3249
  date: 2020-10-15_18-56-54
  done: false
  episode_len_mean: 821.9960804807944
  episode_reward_max: 297.32323232323273
  episode_reward_mean: 246.2629034003477
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 193
  episodes_total: 3827
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7892957677443823
        entropy_coeff: 0.0005000000000000001
        kl: 0.005970731726847589
        model: {}
        policy_loss: -0.010454207542352378
        total_loss: 12.134648323059082
        vf_explained_var: 0.9820330142974854
        vf_loss: 12.144899845123291
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.846666666666668
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1508829287051196
    mean_env_wait_ms: 1.2002713299190184
    mean_inference_ms: 4.600116808957753
    mean_raw_obs_processing_ms: 0.3939512622266518
  time_since_restore: 518.2627367973328
  time_this_iter_s: 25.80301570892334
  time_total_s: 518.2627367973328
  timers:
    learn_throughput: 8563.508
    learn_time_ms: 18893.191
    sample_throughput: 23561.052
    sample_time_ms: 6866.926
    update_time_ms: 44.23
  timestamp: 1602788214
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     20 |          518.263 | 3235840 |  246.263 |              297.323 |              107.323 |            821.996 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3576.8825412459987
    time_step_min: 3249
  date: 2020-10-15_18-57-20
  done: false
  episode_len_mean: 820.9390689739215
  episode_reward_max: 297.32323232323273
  episode_reward_mean: 247.46869130003404
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 276
  episodes_total: 4103
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7702900916337967
        entropy_coeff: 0.0005000000000000001
        kl: 0.005835725654227038
        model: {}
        policy_loss: -0.012004603505677855
        total_loss: 10.29564905166626
        vf_explained_var: 0.9854583144187927
        vf_loss: 10.30745498339335
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.893103448275863
    gpu_util_percent0: 0.3868965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15064596744645894
    mean_env_wait_ms: 1.2013652315398897
    mean_inference_ms: 4.583186217973878
    mean_raw_obs_processing_ms: 0.39310814238198916
  time_since_restore: 543.9974768161774
  time_this_iter_s: 25.734740018844604
  time_total_s: 543.9974768161774
  timers:
    learn_throughput: 8568.739
    learn_time_ms: 18881.659
    sample_throughput: 23565.086
    sample_time_ms: 6865.75
    update_time_ms: 42.342
  timestamp: 1602788240
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     21 |          543.997 | 3397632 |  247.469 |              297.323 |              107.323 |            820.939 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3572.4107481060605
    time_step_min: 3249
  date: 2020-10-15_18-57-46
  done: false
  episode_len_mean: 820.4036568213784
  episode_reward_max: 297.32323232323273
  episode_reward_mean: 248.11220503203612
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 163
  episodes_total: 4266
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.762136846780777
        entropy_coeff: 0.0005000000000000001
        kl: 0.00587383983656764
        model: {}
        policy_loss: -0.012054619951716935
        total_loss: 9.693988800048828
        vf_explained_var: 0.9822675585746765
        vf_loss: 9.70583693186442
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.130000000000003
    gpu_util_percent0: 0.28933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15051684167492307
    mean_env_wait_ms: 1.2019347277924746
    mean_inference_ms: 4.574089548286715
    mean_raw_obs_processing_ms: 0.39264793734532444
  time_since_restore: 569.8179416656494
  time_this_iter_s: 25.820464849472046
  time_total_s: 569.8179416656494
  timers:
    learn_throughput: 8587.511
    learn_time_ms: 18840.384
    sample_throughput: 23568.57
    sample_time_ms: 6864.736
    update_time_ms: 40.958
  timestamp: 1602788266
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     22 |          569.818 | 3559424 |  248.112 |              297.323 |              107.323 |            820.404 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3568.2400730260156
    time_step_min: 3239
  date: 2020-10-15_18-58-12
  done: false
  episode_len_mean: 819.8386075949367
  episode_reward_max: 298.838383838384
  episode_reward_mean: 248.82584890496273
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 4424
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7569443633159002
        entropy_coeff: 0.0005000000000000001
        kl: 0.005998353357426822
        model: {}
        policy_loss: -0.012208390917900639
        total_loss: 10.326914469401041
        vf_explained_var: 0.9792599081993103
        vf_loss: 10.33890144030253
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21333333333333
    gpu_util_percent0: 0.3246666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15039925535897122
    mean_env_wait_ms: 1.2024581183800387
    mean_inference_ms: 4.565738468823085
    mean_raw_obs_processing_ms: 0.3922219182037637
  time_since_restore: 595.5982401371002
  time_this_iter_s: 25.780298471450806
  time_total_s: 595.5982401371002
  timers:
    learn_throughput: 8597.81
    learn_time_ms: 18817.815
    sample_throughput: 23604.531
    sample_time_ms: 6854.277
    update_time_ms: 42.678
  timestamp: 1602788292
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     23 |          595.598 | 3721216 |  248.826 |              298.838 |              107.323 |            819.839 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3561.0428015564203
    time_step_min: 3239
  date: 2020-10-15_18-58-38
  done: false
  episode_len_mean: 819.2707797772065
  episode_reward_max: 299.2929292929294
  episode_reward_mean: 249.94563241671196
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 244
  episodes_total: 4668
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7197075833876928
        entropy_coeff: 0.0005000000000000001
        kl: 0.005537901306524873
        model: {}
        policy_loss: -0.010951873955491465
        total_loss: 11.808536291122437
        vf_explained_var: 0.9827486872673035
        vf_loss: 11.819294055302938
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.863333333333333
    gpu_util_percent0: 0.3283333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15022848847338544
    mean_env_wait_ms: 1.2032168499168727
    mean_inference_ms: 4.553740295736303
    mean_raw_obs_processing_ms: 0.3916169721530458
  time_since_restore: 621.5486600399017
  time_this_iter_s: 25.950419902801514
  time_total_s: 621.5486600399017
  timers:
    learn_throughput: 8582.816
    learn_time_ms: 18850.69
    sample_throughput: 23641.421
    sample_time_ms: 6843.582
    update_time_ms: 34.94
  timestamp: 1602788318
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     24 |          621.549 | 3883008 |  249.946 |              299.293 |              107.323 |            819.271 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3554.94686985173
    time_step_min: 3239
  date: 2020-10-15_18-59-04
  done: false
  episode_len_mean: 818.7715394038383
  episode_reward_max: 302.02020202020185
  episode_reward_mean: 250.88173486601403
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 230
  episodes_total: 4898
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.706352710723877
        entropy_coeff: 0.0005000000000000001
        kl: 0.005637997140487035
        model: {}
        policy_loss: -0.011160943327316394
        total_loss: 8.13277781009674
        vf_explained_var: 0.9867730736732483
        vf_loss: 8.143728057543436
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.14137931034483
    gpu_util_percent0: 0.3437931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034485
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1500835105476374
    mean_env_wait_ms: 1.2037690085739965
    mean_inference_ms: 4.543322346764285
    mean_raw_obs_processing_ms: 0.3910950261007484
  time_since_restore: 647.3440179824829
  time_this_iter_s: 25.795357942581177
  time_total_s: 647.3440179824829
  timers:
    learn_throughput: 8578.538
    learn_time_ms: 18860.09
    sample_throughput: 23641.37
    sample_time_ms: 6843.597
    update_time_ms: 35.116
  timestamp: 1602788344
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     25 |          647.344 | 4044800 |  250.882 |               302.02 |              107.323 |            818.772 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3551.3155165536496
    time_step_min: 3239
  date: 2020-10-15_18-59-30
  done: false
  episode_len_mean: 818.4770569620254
  episode_reward_max: 302.02020202020185
  episode_reward_mean: 251.44326972254174
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 5056
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7149504125118256
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056422999283919735
        model: {}
        policy_loss: -0.01032269986656805
        total_loss: 8.663956960042318
        vf_explained_var: 0.9832354187965393
        vf_loss: 8.674073060353598
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.065517241379315
    gpu_util_percent0: 0.33137931034482765
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.149988892211576
    mean_env_wait_ms: 1.2041304305289324
    mean_inference_ms: 4.536607608434677
    mean_raw_obs_processing_ms: 0.39075934230699055
  time_since_restore: 672.8064477443695
  time_this_iter_s: 25.462429761886597
  time_total_s: 672.8064477443695
  timers:
    learn_throughput: 8589.181
    learn_time_ms: 18836.721
    sample_throughput: 23693.706
    sample_time_ms: 6828.48
    update_time_ms: 33.727
  timestamp: 1602788370
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     26 |          672.806 | 4206592 |  251.443 |               302.02 |              107.323 |            818.477 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3547.504347826087
    time_step_min: 3239
  date: 2020-10-15_18-59-55
  done: false
  episode_len_mean: 817.9539965497412
  episode_reward_max: 302.02020202020185
  episode_reward_mean: 252.06013557077375
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 161
  episodes_total: 5217
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7004147122303644
        entropy_coeff: 0.0005000000000000001
        kl: 0.00600345351267606
        model: {}
        policy_loss: -0.010853229614440352
        total_loss: 8.496254205703735
        vf_explained_var: 0.9839918613433838
        vf_loss: 8.50685747464498
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.000000000000004
    gpu_util_percent0: 0.3086206896551725
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14989775925349103
    mean_env_wait_ms: 1.2044930038628492
    mean_inference_ms: 4.530101124527425
    mean_raw_obs_processing_ms: 0.39043418638951094
  time_since_restore: 698.2106080055237
  time_this_iter_s: 25.404160261154175
  time_total_s: 698.2106080055237
  timers:
    learn_throughput: 8601.658
    learn_time_ms: 18809.396
    sample_throughput: 23733.914
    sample_time_ms: 6816.912
    update_time_ms: 33.361
  timestamp: 1602788395
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     27 |          698.211 | 4368384 |   252.06 |               302.02 |              107.323 |            817.954 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3541.553414096916
    time_step_min: 3212
  date: 2020-10-15_19-00-21
  done: false
  episode_len_mean: 817.4360655737705
  episode_reward_max: 302.9292929292925
  episode_reward_mean: 252.9422917701605
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 273
  episodes_total: 5490
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6699864814678828
        entropy_coeff: 0.0005000000000000001
        kl: 0.005773540275792281
        model: {}
        policy_loss: -0.011313577182590961
        total_loss: 11.954463640848795
        vf_explained_var: 0.9834496974945068
        vf_loss: 11.965535004933676
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.431034482758616
    gpu_util_percent0: 0.3641379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14974932267823265
    mean_env_wait_ms: 1.205027162981708
    mean_inference_ms: 4.519705491544145
    mean_raw_obs_processing_ms: 0.38991729950888226
  time_since_restore: 723.7477731704712
  time_this_iter_s: 25.53716516494751
  time_total_s: 723.7477731704712
  timers:
    learn_throughput: 8615.713
    learn_time_ms: 18778.713
    sample_throughput: 23700.772
    sample_time_ms: 6826.444
    update_time_ms: 33.908
  timestamp: 1602788421
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     28 |          723.748 | 4530176 |  252.942 |              302.929 |              107.323 |            817.436 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3537.503365214311
    time_step_min: 3212
  date: 2020-10-15_19-00-47
  done: false
  episode_len_mean: 817.286040787623
  episode_reward_max: 302.9292929292925
  episode_reward_mean: 253.5962739206409
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 5688
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6623594363530477
        entropy_coeff: 0.0005000000000000001
        kl: 0.005243997322395444
        model: {}
        policy_loss: -0.009220112289767712
        total_loss: 7.972414493560791
        vf_explained_var: 0.9861915111541748
        vf_loss: 7.981441458066304
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63
    gpu_util_percent0: 0.32633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14965146372581895
    mean_env_wait_ms: 1.2053381334082007
    mean_inference_ms: 4.512681865965864
    mean_raw_obs_processing_ms: 0.389568119982313
  time_since_restore: 749.3378398418427
  time_this_iter_s: 25.59006667137146
  time_total_s: 749.3378398418427
  timers:
    learn_throughput: 8624.143
    learn_time_ms: 18760.356
    sample_throughput: 23700.241
    sample_time_ms: 6826.597
    update_time_ms: 32.22
  timestamp: 1602788447
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     29 |          749.338 | 4691968 |  253.596 |              302.929 |              107.323 |            817.286 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3534.2841144038593
    time_step_min: 3212
  date: 2020-10-15_19-01-13
  done: false
  episode_len_mean: 817.0797126240165
  episode_reward_max: 305.9595959595957
  episode_reward_mean: 254.0841359195788
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 5846
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6751842598120371
        entropy_coeff: 0.0005000000000000001
        kl: 0.005583611821445326
        model: {}
        policy_loss: -0.008562183628479639
        total_loss: 9.158657868703207
        vf_explained_var: 0.9818723797798157
        vf_loss: 9.166999419530233
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01379310344828
    gpu_util_percent0: 0.3544827586206896
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14957609813046485
    mean_env_wait_ms: 1.2055685027144436
    mean_inference_ms: 4.507362091384271
    mean_raw_obs_processing_ms: 0.3893049990510396
  time_since_restore: 774.9636242389679
  time_this_iter_s: 25.625784397125244
  time_total_s: 774.9636242389679
  timers:
    learn_throughput: 8636.46
    learn_time_ms: 18733.602
    sample_throughput: 23674.344
    sample_time_ms: 6834.065
    update_time_ms: 32.926
  timestamp: 1602788473
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     30 |          774.964 | 4853760 |  254.084 |               305.96 |              107.323 |             817.08 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3530.2177688710754
    time_step_min: 3169
  date: 2020-10-15_19-01-38
  done: false
  episode_len_mean: 816.7492537313433
  episode_reward_max: 309.4444444444443
  episode_reward_mean: 254.71963415246984
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 6030
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6538680344820023
        entropy_coeff: 0.0005000000000000001
        kl: 0.005568233706677954
        model: {}
        policy_loss: -0.01098362467989015
        total_loss: 10.944342613220215
        vf_explained_var: 0.9808550477027893
        vf_loss: 10.95509640375773
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703448275862073
    gpu_util_percent0: 0.40551724137931033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14949194789628645
    mean_env_wait_ms: 1.2058375344377128
    mean_inference_ms: 4.501455366546081
    mean_raw_obs_processing_ms: 0.38901004350352797
  time_since_restore: 800.4799809455872
  time_this_iter_s: 25.516356706619263
  time_total_s: 800.4799809455872
  timers:
    learn_throughput: 8646.166
    learn_time_ms: 18712.572
    sample_throughput: 23685.178
    sample_time_ms: 6830.939
    update_time_ms: 33.754
  timestamp: 1602788498
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     31 |           800.48 | 5015552 |   254.72 |              309.444 |              107.323 |            816.749 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3524.061341853035
    time_step_min: 3169
  date: 2020-10-15_19-02-04
  done: false
  episode_len_mean: 816.1202792764202
  episode_reward_max: 309.4444444444443
  episode_reward_mean: 255.6600758457311
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 272
  episodes_total: 6302
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6287688066562017
        entropy_coeff: 0.0005000000000000001
        kl: 0.005341685532281796
        model: {}
        policy_loss: -0.010679997292754706
        total_loss: 9.308965921401978
        vf_explained_var: 0.9860467910766602
        vf_loss: 9.319426457087198
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.76
    gpu_util_percent0: 0.3143333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14937605225749387
    mean_env_wait_ms: 1.2061631751103552
    mean_inference_ms: 4.493247454636889
    mean_raw_obs_processing_ms: 0.3886050322734682
  time_since_restore: 826.3830645084381
  time_this_iter_s: 25.903083562850952
  time_total_s: 826.3830645084381
  timers:
    learn_throughput: 8649.05
    learn_time_ms: 18706.333
    sample_throughput: 23642.895
    sample_time_ms: 6843.155
    update_time_ms: 35.268
  timestamp: 1602788524
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     32 |          826.383 | 5177344 |   255.66 |              309.444 |              107.323 |             816.12 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3520.1591050341826
    time_step_min: 3169
  date: 2020-10-15_19-02-30
  done: false
  episode_len_mean: 815.7110219203457
  episode_reward_max: 309.4444444444443
  episode_reward_mean: 256.26594752713913
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 6478
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6290171841780344
        entropy_coeff: 0.0005000000000000001
        kl: 0.005798434528211753
        model: {}
        policy_loss: -0.010629309714810612
        total_loss: 7.30441677570343
        vf_explained_var: 0.9854683876037598
        vf_loss: 7.314780632654826
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.630000000000003
    gpu_util_percent0: 0.3273333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14930597165767603
    mean_env_wait_ms: 1.2063483184752402
    mean_inference_ms: 4.4882297524985
    mean_raw_obs_processing_ms: 0.3883576561304741
  time_since_restore: 852.0983891487122
  time_this_iter_s: 25.715324640274048
  time_total_s: 852.0983891487122
  timers:
    learn_throughput: 8651.863
    learn_time_ms: 18700.25
    sample_throughput: 23646.186
    sample_time_ms: 6842.203
    update_time_ms: 34.867
  timestamp: 1602788550
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     33 |          852.098 | 5339136 |  256.266 |              309.444 |              107.323 |            815.711 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3516.4803639120546
    time_step_min: 3163
  date: 2020-10-15_19-02-56
  done: false
  episode_len_mean: 815.2666867560645
  episode_reward_max: 310.3535353535349
  episode_reward_mean: 256.8258599251517
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 159
  episodes_total: 6637
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6299049854278564
        entropy_coeff: 0.0005000000000000001
        kl: 0.005551038348736863
        model: {}
        policy_loss: -0.010303838042697558
        total_loss: 7.630209843317668
        vf_explained_var: 0.983752965927124
        vf_loss: 7.640273650487264
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.358620689655172
    gpu_util_percent0: 0.3737931034482757
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.789655172413793
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14924554641144214
    mean_env_wait_ms: 1.2065204492623556
    mean_inference_ms: 4.483900165963167
    mean_raw_obs_processing_ms: 0.3881444340571553
  time_since_restore: 877.5680115222931
  time_this_iter_s: 25.469622373580933
  time_total_s: 877.5680115222931
  timers:
    learn_throughput: 8675.811
    learn_time_ms: 18648.631
    sample_throughput: 23644.018
    sample_time_ms: 6842.83
    update_time_ms: 35.023
  timestamp: 1602788576
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     34 |          877.568 | 5500928 |  256.826 |              310.354 |              107.323 |            815.267 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3511.214536414156
    time_step_min: 3163
  date: 2020-10-15_19-03-22
  done: false
  episode_len_mean: 814.5463662790697
  episode_reward_max: 312.47474747474746
  episode_reward_mean: 257.59965938454303
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 6880
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6098584433396658
        entropy_coeff: 0.0005000000000000001
        kl: 0.00519350233177344
        model: {}
        policy_loss: -0.010517341147836609
        total_loss: 10.362518469492594
        vf_explained_var: 0.9834849834442139
        vf_loss: 10.37282125155131
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.99655172413793
    gpu_util_percent0: 0.4048275862068965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14915481487804427
    mean_env_wait_ms: 1.2067894808374013
    mean_inference_ms: 4.477580341203753
    mean_raw_obs_processing_ms: 0.3878442833285498
  time_since_restore: 903.1373980045319
  time_this_iter_s: 25.56938648223877
  time_total_s: 903.1373980045319
  timers:
    learn_throughput: 8680.635
    learn_time_ms: 18638.268
    sample_throughput: 23686.918
    sample_time_ms: 6830.437
    update_time_ms: 35.126
  timestamp: 1602788602
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     35 |          903.137 | 5662720 |    257.6 |              312.475 |              107.323 |            814.546 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3506.182937181664
    time_step_min: 3163
  date: 2020-10-15_19-03-48
  done: false
  episode_len_mean: 813.8714486638537
  episode_reward_max: 312.47474747474746
  episode_reward_mean: 258.40114222392697
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 230
  episodes_total: 7110
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5835195481777191
        entropy_coeff: 0.0005000000000000001
        kl: 0.005151468561962247
        model: {}
        policy_loss: -0.01129642988477523
        total_loss: 7.82729434967041
        vf_explained_var: 0.9858905673027039
        vf_loss: 7.838367303212483
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.3
    gpu_util_percent0: 0.345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14907761707366418
    mean_env_wait_ms: 1.2069716778149866
    mean_inference_ms: 4.471903943462238
    mean_raw_obs_processing_ms: 0.3875594775813732
  time_since_restore: 928.8804955482483
  time_this_iter_s: 25.74309754371643
  time_total_s: 928.8804955482483
  timers:
    learn_throughput: 8673.146
    learn_time_ms: 18654.362
    sample_throughput: 23651.133
    sample_time_ms: 6840.772
    update_time_ms: 36.336
  timestamp: 1602788628
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     36 |           928.88 | 5824512 |  258.401 |              312.475 |              107.323 |            813.871 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3502.6598394685857
    time_step_min: 3138
  date: 2020-10-15_19-04-14
  done: false
  episode_len_mean: 813.5203632361034
  episode_reward_max: 314.14141414141363
  episode_reward_mean: 258.91297398864805
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 158
  episodes_total: 7268
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.594015454252561
        entropy_coeff: 0.0005000000000000001
        kl: 0.005375155985044937
        model: {}
        policy_loss: -0.011208146383675436
        total_loss: 7.364322900772095
        vf_explained_var: 0.9836664795875549
        vf_loss: 7.375290552775065
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.256666666666664
    gpu_util_percent0: 0.4073333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14902574487243914
    mean_env_wait_ms: 1.2071039415008185
    mean_inference_ms: 4.468192701673665
    mean_raw_obs_processing_ms: 0.3873775787907227
  time_since_restore: 954.6862828731537
  time_this_iter_s: 25.805787324905396
  time_total_s: 954.6862828731537
  timers:
    learn_throughput: 8659.846
    learn_time_ms: 18683.01
    sample_throughput: 23616.546
    sample_time_ms: 6850.79
    update_time_ms: 37.445
  timestamp: 1602788654
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     37 |          954.686 | 5986304 |  258.913 |              314.141 |              107.323 |             813.52 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3498.950593952484
    time_step_min: 3131
  date: 2020-10-15_19-04-39
  done: false
  episode_len_mean: 813.0660402684564
  episode_reward_max: 315.2020202020202
  episode_reward_mean: 259.496000271168
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 7450
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.584243044257164
        entropy_coeff: 0.0005000000000000001
        kl: 0.005464488291181624
        model: {}
        policy_loss: -0.009593600155009577
        total_loss: 7.8663210074106855
        vf_explained_var: 0.9848172664642334
        vf_loss: 7.875660300254822
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.99310344827586
    gpu_util_percent0: 0.35172413793103446
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275867
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14896720669054242
    mean_env_wait_ms: 1.207268912880103
    mean_inference_ms: 4.464073379123738
    mean_raw_obs_processing_ms: 0.38717979211594766
  time_since_restore: 980.3504486083984
  time_this_iter_s: 25.66416573524475
  time_total_s: 980.3504486083984
  timers:
    learn_throughput: 8654.807
    learn_time_ms: 18693.888
    sample_throughput: 23609.885
    sample_time_ms: 6852.723
    update_time_ms: 36.955
  timestamp: 1602788679
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     38 |           980.35 | 6148096 |  259.496 |              315.202 |              107.323 |            813.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3493.6595938557666
    time_step_min: 3131
  date: 2020-10-15_19-05-05
  done: false
  episode_len_mean: 812.4883480062144
  episode_reward_max: 315.2020202020202
  episode_reward_mean: 260.30808473131094
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 274
  episodes_total: 7724
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5534585614999136
        entropy_coeff: 0.0005000000000000001
        kl: 0.005156314660174151
        model: {}
        policy_loss: -0.011462989670690149
        total_loss: 9.30127509435018
        vf_explained_var: 0.9854690432548523
        vf_loss: 9.312499205271402
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.644827586206898
    gpu_util_percent0: 0.3493103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488851042828591
    mean_env_wait_ms: 1.2074575366177938
    mean_inference_ms: 4.458174782360462
    mean_raw_obs_processing_ms: 0.38689266168574205
  time_since_restore: 1006.0789997577667
  time_this_iter_s: 25.728551149368286
  time_total_s: 1006.0789997577667
  timers:
    learn_throughput: 8646.498
    learn_time_ms: 18711.853
    sample_throughput: 23630.832
    sample_time_ms: 6846.648
    update_time_ms: 38.401
  timestamp: 1602788705
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     39 |          1006.08 | 6309888 |  260.308 |              315.202 |              107.323 |            812.488 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3490.3612878595063
    time_step_min: 3131
  date: 2020-10-15_19-05-31
  done: false
  episode_len_mean: 812.1498734177216
  episode_reward_max: 315.2020202020202
  episode_reward_mean: 260.81715253803856
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 7900
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5477771113316218
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048639319914703565
        model: {}
        policy_loss: -0.011131139976593355
        total_loss: 7.1255602439244585
        vf_explained_var: 0.9854187965393066
        vf_loss: 7.136478662490845
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.066666666666674
    gpu_util_percent0: 0.3013333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14883542987380646
    mean_env_wait_ms: 1.2075709252362348
    mean_inference_ms: 4.454577752699448
    mean_raw_obs_processing_ms: 0.38671814633352797
  time_since_restore: 1031.7767431735992
  time_this_iter_s: 25.69774341583252
  time_total_s: 1031.7767431735992
  timers:
    learn_throughput: 8635.194
    learn_time_ms: 18736.348
    sample_throughput: 23692.791
    sample_time_ms: 6828.744
    update_time_ms: 38.186
  timestamp: 1602788731
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     40 |          1031.78 | 6471680 |  260.817 |              315.202 |              107.323 |             812.15 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3487.4048628428927
    time_step_min: 3126
  date: 2020-10-15_19-05-57
  done: false
  episode_len_mean: 811.8263458198958
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 261.24589983185854
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 162
  episodes_total: 8062
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5474757154782613
        entropy_coeff: 0.0005000000000000001
        kl: 0.005657938658259809
        model: {}
        policy_loss: -0.011913959480201205
        total_loss: 7.367338418960571
        vf_explained_var: 0.9843635559082031
        vf_loss: 7.379243055979411
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37586206896552
    gpu_util_percent0: 0.31827586206896546
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14879127756796906
    mean_env_wait_ms: 1.2076772367764088
    mean_inference_ms: 4.451370499132568
    mean_raw_obs_processing_ms: 0.38656297188327865
  time_since_restore: 1057.1196875572205
  time_this_iter_s: 25.342944383621216
  time_total_s: 1057.1196875572205
  timers:
    learn_throughput: 8641.702
    learn_time_ms: 18722.238
    sample_throughput: 23699.651
    sample_time_ms: 6826.767
    update_time_ms: 37.297
  timestamp: 1602788757
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     41 |          1057.12 | 6633472 |  261.246 |               315.96 |              107.323 |            811.826 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3483.423398328691
    time_step_min: 3126
  date: 2020-10-15_19-06-22
  done: false
  episode_len_mean: 811.4016146523678
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 261.8762087680029
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 237
  episodes_total: 8299
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5258320172627767
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053638086731856065
        model: {}
        policy_loss: -0.010679875093046576
        total_loss: 8.638081908226013
        vf_explained_var: 0.9858747124671936
        vf_loss: 8.648756424585978
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09655172413794
    gpu_util_percent0: 0.38344827586206887
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7724137931034485
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14872555978823518
    mean_env_wait_ms: 1.207833563794635
    mean_inference_ms: 4.446886750586619
    mean_raw_obs_processing_ms: 0.38634784207337675
  time_since_restore: 1082.5287618637085
  time_this_iter_s: 25.409074306488037
  time_total_s: 1082.5287618637085
  timers:
    learn_throughput: 8657.552
    learn_time_ms: 18687.961
    sample_throughput: 23758.18
    sample_time_ms: 6809.949
    update_time_ms: 37.556
  timestamp: 1602788782
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     42 |          1082.53 | 6795264 |  261.876 |               315.96 |              107.323 |            811.402 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3479.495464719048
    time_step_min: 3126
  date: 2020-10-15_19-06-48
  done: false
  episode_len_mean: 811.0968233501349
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 262.46187108454126
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 8531
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5051681821544965
        entropy_coeff: 0.0005000000000000001
        kl: 0.005095119161220889
        model: {}
        policy_loss: -0.008532960782758892
        total_loss: 7.788020968437195
        vf_explained_var: 0.986561119556427
        vf_loss: 7.796551942825317
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.660000000000004
    gpu_util_percent0: 0.3586666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14866894097768793
    mean_env_wait_ms: 1.207931370485609
    mean_inference_ms: 4.442682562235831
    mean_raw_obs_processing_ms: 0.38614239337537914
  time_since_restore: 1108.3607144355774
  time_this_iter_s: 25.831952571868896
  time_total_s: 1108.3607144355774
  timers:
    learn_throughput: 8651.617
    learn_time_ms: 18700.781
    sample_throughput: 23767.628
    sample_time_ms: 6807.242
    update_time_ms: 36.352
  timestamp: 1602788808
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     43 |          1108.36 | 6957056 |  262.462 |               315.96 |              107.323 |            811.097 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3476.717622571693
    time_step_min: 3126
  date: 2020-10-15_19-07-14
  done: false
  episode_len_mean: 810.8812428078251
  episode_reward_max: 315.9595959595958
  episode_reward_mean: 262.87111041368803
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 159
  episodes_total: 8690
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5089509064952532
        entropy_coeff: 0.0005000000000000001
        kl: 0.005177010122376184
        model: {}
        policy_loss: -0.008594492411551377
        total_loss: 6.384830077489217
        vf_explained_var: 0.9861769676208496
        vf_loss: 6.393420100212097
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.42068965517241
    gpu_util_percent0: 0.2944827586206896
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14863069005455315
    mean_env_wait_ms: 1.20800209966359
    mean_inference_ms: 4.439915111833449
    mean_raw_obs_processing_ms: 0.3860085935348211
  time_since_restore: 1134.1097204685211
  time_this_iter_s: 25.749006032943726
  time_total_s: 1134.1097204685211
  timers:
    learn_throughput: 8640.253
    learn_time_ms: 18725.378
    sample_throughput: 23753.135
    sample_time_ms: 6811.396
    update_time_ms: 36.085
  timestamp: 1602788834
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     44 |          1134.11 | 7118848 |  262.871 |               315.96 |              107.323 |            810.881 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3473.827500283158
    time_step_min: 3126
  date: 2020-10-15_19-07-40
  done: false
  episode_len_mean: 810.6123323187916
  episode_reward_max: 318.838383838384
  episode_reward_mean: 263.3335382912657
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 8871
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5027876198291779
        entropy_coeff: 0.0005000000000000001
        kl: 0.00566875849229594
        model: {}
        policy_loss: -0.011540132186686
        total_loss: 6.751582582791646
        vf_explained_var: 0.9869011044502258
        vf_loss: 6.76309057076772
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.883333333333333
    gpu_util_percent0: 0.3323333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485874033502773
    mean_env_wait_ms: 1.208091041954155
    mean_inference_ms: 4.436881731895152
    mean_raw_obs_processing_ms: 0.3858631212262393
  time_since_restore: 1159.79079079628
  time_this_iter_s: 25.68107032775879
  time_total_s: 1159.79079079628
  timers:
    learn_throughput: 8638.499
    learn_time_ms: 18729.179
    sample_throughput: 23737.255
    sample_time_ms: 6815.952
    update_time_ms: 37.557
  timestamp: 1602788860
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     45 |          1159.79 | 7280640 |  263.334 |              318.838 |              107.323 |            810.612 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3469.661536770364
    time_step_min: 3126
  date: 2020-10-15_19-08-07
  done: false
  episode_len_mean: 810.1766057555532
  episode_reward_max: 318.838383838384
  episode_reward_mean: 263.9787081892344
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 268
  episodes_total: 9139
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4825672581791878
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052553107573961215
        model: {}
        policy_loss: -0.009422173636266962
        total_loss: 9.06334114074707
        vf_explained_var: 0.9857136607170105
        vf_loss: 9.072741746902466
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.323333333333338
    gpu_util_percent0: 0.3793333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852710192271665
    mean_env_wait_ms: 1.2081765678526735
    mean_inference_ms: 4.432559427022275
    mean_raw_obs_processing_ms: 0.3856565579210545
  time_since_restore: 1185.7997052669525
  time_this_iter_s: 26.008914470672607
  time_total_s: 1185.7997052669525
  timers:
    learn_throughput: 8633.571
    learn_time_ms: 18739.871
    sample_throughput: 23714.199
    sample_time_ms: 6822.579
    update_time_ms: 44.963
  timestamp: 1602788887
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     46 |           1185.8 | 7442432 |  263.979 |              318.838 |              107.323 |            810.177 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3466.5811422413794
    time_step_min: 3113
  date: 2020-10-15_19-08-32
  done: false
  episode_len_mean: 809.9514052778374
  episode_reward_max: 318.838383838384
  episode_reward_mean: 264.4207630911127
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 9322
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4710475852092107
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053789357577140135
        model: {}
        policy_loss: -0.010178057884331793
        total_loss: 5.923376639684041
        vf_explained_var: 0.9880004525184631
        vf_loss: 5.933521270751953
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.237931034482763
    gpu_util_percent0: 0.35551724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14848891351129562
    mean_env_wait_ms: 1.2082424201557307
    mean_inference_ms: 4.429754458399731
    mean_raw_obs_processing_ms: 0.38552164894208485
  time_since_restore: 1211.4596943855286
  time_this_iter_s: 25.65998911857605
  time_total_s: 1211.4596943855286
  timers:
    learn_throughput: 8644.544
    learn_time_ms: 18716.083
    sample_throughput: 23678.962
    sample_time_ms: 6832.732
    update_time_ms: 42.364
  timestamp: 1602788912
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     47 |          1211.46 | 7604224 |  264.421 |              318.838 |              107.323 |            809.951 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3464.1419040559144
    time_step_min: 3113
  date: 2020-10-15_19-08-59
  done: false
  episode_len_mean: 809.7520295202952
  episode_reward_max: 318.838383838384
  episode_reward_mean: 264.78094066654944
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 163
  episodes_total: 9485
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47280077387889224
        entropy_coeff: 0.0005000000000000001
        kl: 0.005530967842787504
        model: {}
        policy_loss: -0.010288277165576195
        total_loss: 6.600515087445577
        vf_explained_var: 0.9858220219612122
        vf_loss: 6.610763390858968
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.566666666666666
    gpu_util_percent0: 0.3493333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.79
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484556124524125
    mean_env_wait_ms: 1.2082981939321016
    mean_inference_ms: 4.427339888845137
    mean_raw_obs_processing_ms: 0.385407782889633
  time_since_restore: 1237.5944180488586
  time_this_iter_s: 26.134723663330078
  time_total_s: 1237.5944180488586
  timers:
    learn_throughput: 8642.03
    learn_time_ms: 18721.526
    sample_throughput: 23544.103
    sample_time_ms: 6871.869
    update_time_ms: 42.94
  timestamp: 1602788939
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     48 |          1237.59 | 7766016 |  264.781 |              318.838 |              107.323 |            809.752 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3460.9023582954073
    time_step_min: 3113
  date: 2020-10-15_19-09-25
  done: false
  episode_len_mean: 809.4886714727086
  episode_reward_max: 318.838383838384
  episode_reward_mean: 265.2531234070883
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 225
  episodes_total: 9710
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4582233751813571
        entropy_coeff: 0.0005000000000000001
        kl: 0.005469115450978279
        model: {}
        policy_loss: -0.011072663590312004
        total_loss: 8.390069405237833
        vf_explained_var: 0.9859001040458679
        vf_loss: 8.401097774505615
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.713333333333342
    gpu_util_percent0: 0.35633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14840646688368042
    mean_env_wait_ms: 1.208367576168628
    mean_inference_ms: 4.424078509843339
    mean_raw_obs_processing_ms: 0.3852569132476332
  time_since_restore: 1263.3259980678558
  time_this_iter_s: 25.731580018997192
  time_total_s: 1263.3259980678558
  timers:
    learn_throughput: 8642.491
    learn_time_ms: 18720.529
    sample_throughput: 23544.753
    sample_time_ms: 6871.68
    update_time_ms: 43.619
  timestamp: 1602788965
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     49 |          1263.33 | 7927808 |  265.253 |              318.838 |              107.323 |            809.489 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3457.4240589363203
    time_step_min: 3113
  date: 2020-10-15_19-09-50
  done: false
  episode_len_mean: 809.1631996784242
  episode_reward_max: 318.838383838384
  episode_reward_mean: 265.76986323896176
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 241
  episodes_total: 9951
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.44216354191303253
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046632324034969015
        model: {}
        policy_loss: -0.010077782101385916
        total_loss: 5.821465929349263
        vf_explained_var: 0.9897493720054626
        vf_loss: 5.831531683603923
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810344827586206
    gpu_util_percent0: 0.4251724137931035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14836282715076246
    mean_env_wait_ms: 1.2084076872452914
    mean_inference_ms: 4.420778170520959
    mean_raw_obs_processing_ms: 0.3850968938380583
  time_since_restore: 1288.8159625530243
  time_this_iter_s: 25.489964485168457
  time_total_s: 1288.8159625530243
  timers:
    learn_throughput: 8654.568
    learn_time_ms: 18694.404
    sample_throughput: 23522.629
    sample_time_ms: 6878.143
    update_time_ms: 41.566
  timestamp: 1602788990
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     50 |          1288.82 | 8089600 |   265.77 |              318.838 |              107.323 |            809.163 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3455.361271102284
    time_step_min: 3113
  date: 2020-10-15_19-10-16
  done: false
  episode_len_mean: 808.9449169303797
  episode_reward_max: 318.838383838384
  episode_reward_mean: 266.06398738172857
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 161
  episodes_total: 10112
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4463478699326515
        entropy_coeff: 0.0005000000000000001
        kl: 0.005972774194863935
        model: {}
        policy_loss: -0.011093871532163272
        total_loss: 6.32016642888387
        vf_explained_var: 0.9864138960838318
        vf_loss: 6.331334034601848
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.862068965517242
    gpu_util_percent0: 0.3279310344827586
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14833285001784222
    mean_env_wait_ms: 1.2084453540908762
    mean_inference_ms: 4.418630590486937
    mean_raw_obs_processing_ms: 0.3849945232014464
  time_since_restore: 1314.2188007831573
  time_this_iter_s: 25.402838230133057
  time_total_s: 1314.2188007831573
  timers:
    learn_throughput: 8654.482
    learn_time_ms: 18694.592
    sample_throughput: 23503.676
    sample_time_ms: 6883.689
    update_time_ms: 41.377
  timestamp: 1602789016
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     51 |          1314.22 | 8251392 |  266.064 |              318.838 |              107.323 |            808.945 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3453.1218655478583
    time_step_min: 3113
  date: 2020-10-15_19-10-42
  done: false
  episode_len_mean: 808.7273345641823
  episode_reward_max: 318.838383838384
  episode_reward_mean: 266.3973276639683
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 10291
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4436478490630786
        entropy_coeff: 0.0005000000000000001
        kl: 0.005474852320427696
        model: {}
        policy_loss: -0.012421490389291042
        total_loss: 7.783152262369792
        vf_explained_var: 0.9851331114768982
        vf_loss: 7.795658628145854
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.065517241379318
    gpu_util_percent0: 0.34206896551724136
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14829859228857076
    mean_env_wait_ms: 1.2084918764703234
    mean_inference_ms: 4.416298999904261
    mean_raw_obs_processing_ms: 0.3848878768937218
  time_since_restore: 1339.6466455459595
  time_this_iter_s: 25.427844762802124
  time_total_s: 1339.6466455459595
  timers:
    learn_throughput: 8651.541
    learn_time_ms: 18700.946
    sample_throughput: 23516.996
    sample_time_ms: 6879.79
    update_time_ms: 40.876
  timestamp: 1602789042
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     52 |          1339.65 | 8413184 |  266.397 |              318.838 |              107.323 |            808.727 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3449.9697632404677
    time_step_min: 3113
  date: 2020-10-15_19-11-08
  done: false
  episode_len_mean: 808.4174637749787
  episode_reward_max: 318.838383838384
  episode_reward_mean: 266.9089942899015
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 268
  episodes_total: 10559
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4222855319579442
        entropy_coeff: 0.0005000000000000001
        kl: 0.005193327243129413
        model: {}
        policy_loss: -0.01050349560197598
        total_loss: 8.29350999991099
        vf_explained_var: 0.9866161942481995
        vf_loss: 8.304094870885214
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.476666666666674
    gpu_util_percent0: 0.33866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482525202149434
    mean_env_wait_ms: 1.2085408402309294
    mean_inference_ms: 4.412956581467871
    mean_raw_obs_processing_ms: 0.38472786363553213
  time_since_restore: 1365.3486559391022
  time_this_iter_s: 25.7020103931427
  time_total_s: 1365.3486559391022
  timers:
    learn_throughput: 8660.55
    learn_time_ms: 18681.492
    sample_throughput: 23491.061
    sample_time_ms: 6887.386
    update_time_ms: 40.462
  timestamp: 1602789068
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     53 |          1365.35 | 8574976 |  266.909 |              318.838 |              107.323 |            808.417 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3447.5629788824517
    time_step_min: 3113
  date: 2020-10-15_19-11-33
  done: false
  episode_len_mean: 808.2284065524944
  episode_reward_max: 318.838383838384
  episode_reward_mean: 267.28260358612175
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 185
  episodes_total: 10744
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.41082995136578876
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055609165380398435
        model: {}
        policy_loss: -0.011005585916185131
        total_loss: 6.849740187327067
        vf_explained_var: 0.9859861731529236
        vf_loss: 6.860811988512675
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.555172413793102
    gpu_util_percent0: 0.3093103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482215931862318
    mean_env_wait_ms: 1.2085716517786311
    mean_inference_ms: 4.41072408855546
    mean_raw_obs_processing_ms: 0.38462450788595076
  time_since_restore: 1390.7619581222534
  time_this_iter_s: 25.413302183151245
  time_total_s: 1390.7619581222534
  timers:
    learn_throughput: 8671.082
    learn_time_ms: 18658.801
    sample_throughput: 23531.509
    sample_time_ms: 6875.547
    update_time_ms: 40.7
  timestamp: 1602789093
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     54 |          1390.76 | 8736768 |  267.283 |              318.838 |              107.323 |            808.228 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3445.4221036164536
    time_step_min: 3113
  date: 2020-10-15_19-11-59
  done: false
  episode_len_mean: 808.0611421761848
  episode_reward_max: 318.838383838384
  episode_reward_mean: 267.5956929270707
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 165
  episodes_total: 10909
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4211726163824399
        entropy_coeff: 0.0005000000000000001
        kl: 0.005527137080207467
        model: {}
        policy_loss: -0.009184041584376246
        total_loss: 6.2019104560216265
        vf_explained_var: 0.9864926934242249
        vf_loss: 6.211167017618815
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.862068965517246
    gpu_util_percent0: 0.35172413793103446
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481945825524551
    mean_env_wait_ms: 1.2086026622221497
    mean_inference_ms: 4.408792104385867
    mean_raw_obs_processing_ms: 0.38453642757835627
  time_since_restore: 1416.2345716953278
  time_this_iter_s: 25.47261357307434
  time_total_s: 1416.2345716953278
  timers:
    learn_throughput: 8679.365
    learn_time_ms: 18640.996
    sample_throughput: 23542.419
    sample_time_ms: 6872.361
    update_time_ms: 39.938
  timestamp: 1602789119
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     55 |          1416.23 | 8898560 |  267.596 |              318.838 |              107.323 |            808.061 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3442.654525784349
    time_step_min: 3113
  date: 2020-10-15_19-12-25
  done: false
  episode_len_mean: 807.7900125740973
  episode_reward_max: 318.838383838384
  episode_reward_mean: 267.99374198242526
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 225
  episodes_total: 11134
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.41311071813106537
        entropy_coeff: 0.0005000000000000001
        kl: 0.005275824495280783
        model: {}
        policy_loss: -0.009874430494771028
        total_loss: 9.78791888554891
        vf_explained_var: 0.9832718968391418
        vf_loss: 9.797868172327677
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333337
    gpu_util_percent0: 0.33266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481557652348994
    mean_env_wait_ms: 1.2086360850380102
    mean_inference_ms: 4.40622144856908
    mean_raw_obs_processing_ms: 0.3844213028809628
  time_since_restore: 1441.6991486549377
  time_this_iter_s: 25.464576959609985
  time_total_s: 1441.6991486549377
  timers:
    learn_throughput: 8692.732
    learn_time_ms: 18612.33
    sample_throughput: 23603.369
    sample_time_ms: 6854.615
    update_time_ms: 31.101
  timestamp: 1602789145
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     56 |           1441.7 | 9060352 |  267.994 |              318.838 |              107.323 |             807.79 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3439.825699408702
    time_step_min: 3113
  date: 2020-10-15_19-12-51
  done: false
  episode_len_mean: 807.5094522113778
  episode_reward_max: 318.838383838384
  episode_reward_mean: 268.39280432923266
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 11373
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38683905204137164
        entropy_coeff: 0.0005000000000000001
        kl: 0.005008873762562871
        model: {}
        policy_loss: -0.010195984796155244
        total_loss: 6.402861674626668
        vf_explained_var: 0.9888530373573303
        vf_loss: 6.413125991821289
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.006896551724143
    gpu_util_percent0: 0.32862068965517244
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14812121419876645
    mean_env_wait_ms: 1.2086644875446275
    mean_inference_ms: 4.403615826344997
    mean_raw_obs_processing_ms: 0.3842977736254363
  time_since_restore: 1467.3866112232208
  time_this_iter_s: 25.68746256828308
  time_total_s: 1467.3866112232208
  timers:
    learn_throughput: 8681.942
    learn_time_ms: 18635.461
    sample_throughput: 23678.688
    sample_time_ms: 6832.811
    update_time_ms: 32.225
  timestamp: 1602789171
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     57 |          1467.39 | 9222144 |  268.393 |              318.838 |              107.323 |            807.509 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3437.9520577742974
    time_step_min: 3113
  date: 2020-10-15_19-13-17
  done: false
  episode_len_mean: 807.2903337667967
  episode_reward_max: 318.838383838384
  episode_reward_mean: 268.6658260104293
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 162
  episodes_total: 11535
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.39349231868982315
        entropy_coeff: 0.0005000000000000001
        kl: 0.005527043676314254
        model: {}
        policy_loss: -0.012349717571244886
        total_loss: 5.995402574539185
        vf_explained_var: 0.986660897731781
        vf_loss: 6.007810831069946
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.503333333333334
    gpu_util_percent0: 0.32000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7966666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14809700000177986
    mean_env_wait_ms: 1.208683718125382
    mean_inference_ms: 4.401876981223866
    mean_raw_obs_processing_ms: 0.38421676395178506
  time_since_restore: 1492.9920299053192
  time_this_iter_s: 25.60541868209839
  time_total_s: 1492.9920299053192
  timers:
    learn_throughput: 8687.952
    learn_time_ms: 18622.571
    sample_throughput: 23811.196
    sample_time_ms: 6794.787
    update_time_ms: 30.105
  timestamp: 1602789197
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     58 |          1492.99 | 9383936 |  268.666 |              318.838 |              107.323 |             807.29 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3435.5406862325663
    time_step_min: 3113
  date: 2020-10-15_19-13-43
  done: false
  episode_len_mean: 806.9947992156194
  episode_reward_max: 318.838383838384
  episode_reward_mean: 269.03050024501124
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 194
  episodes_total: 11729
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3928256755073865
        entropy_coeff: 0.0005000000000000001
        kl: 0.005106201278977096
        model: {}
        policy_loss: -0.012295511667616665
        total_loss: 7.120609879493713
        vf_explained_var: 0.9857527613639832
        vf_loss: 7.132974028587341
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.353333333333335
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806678864391626
    mean_env_wait_ms: 1.208711994367719
    mean_inference_ms: 4.399859958496087
    mean_raw_obs_processing_ms: 0.38412891955005424
  time_since_restore: 1518.7605254650116
  time_this_iter_s: 25.768495559692383
  time_total_s: 1518.7605254650116
  timers:
    learn_throughput: 8686.097
    learn_time_ms: 18626.548
    sample_throughput: 23808.383
    sample_time_ms: 6795.59
    update_time_ms: 27.55
  timestamp: 1602789223
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     59 |          1518.76 | 9545728 |  269.031 |              318.838 |              107.323 |            806.995 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3432.3968852047224
    time_step_min: 3113
  date: 2020-10-15_19-14-09
  done: false
  episode_len_mean: 806.6457238214434
  episode_reward_max: 318.838383838384
  episode_reward_mean: 269.52862795666294
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 256
  episodes_total: 11985
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.37009695669015247
        entropy_coeff: 0.0005000000000000001
        kl: 0.005217065801844001
        model: {}
        policy_loss: -0.010196032354239529
        total_loss: 6.351018945376079
        vf_explained_var: 0.9890618920326233
        vf_loss: 6.361269474029541
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74137931034483
    gpu_util_percent0: 0.32827586206896553
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480318256482674
    mean_env_wait_ms: 1.2087361315238039
    mean_inference_ms: 4.397303522542773
    mean_raw_obs_processing_ms: 0.3840075036157743
  time_since_restore: 1544.5471234321594
  time_this_iter_s: 25.786597967147827
  time_total_s: 1544.5471234321594
  timers:
    learn_throughput: 8673.496
    learn_time_ms: 18653.608
    sample_throughput: 23805.883
    sample_time_ms: 6796.303
    update_time_ms: 27.968
  timestamp: 1602789249
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     60 |          1544.55 | 9707520 |  269.529 |              318.838 |              107.323 |            806.646 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3430.259485318377
    time_step_min: 3113
  date: 2020-10-15_19-14-35
  done: false
  episode_len_mean: 806.4523261548578
  episode_reward_max: 318.838383838384
  episode_reward_mean: 269.86510261251334
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 12166
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3646986410021782
        entropy_coeff: 0.0005000000000000001
        kl: 0.005362459147969882
        model: {}
        policy_loss: -0.009549301854955653
        total_loss: 5.4394270579020185
        vf_explained_var: 0.9883525967597961
        vf_loss: 5.449024677276611
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.813333333333333
    gpu_util_percent0: 0.3226666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480070401000299
    mean_env_wait_ms: 1.2087516367149296
    mean_inference_ms: 4.395533603348328
    mean_raw_obs_processing_ms: 0.3839235018105495
  time_since_restore: 1570.0894465446472
  time_this_iter_s: 25.542323112487793
  time_total_s: 1570.0894465446472
  timers:
    learn_throughput: 8671.373
    learn_time_ms: 18658.176
    sample_throughput: 23812.949
    sample_time_ms: 6794.287
    update_time_ms: 29.698
  timestamp: 1602789275
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     61 |          1570.09 | 9869312 |  269.865 |              318.838 |              107.323 |            806.452 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3428.190158601057
    time_step_min: 3111
  date: 2020-10-15_19-15-01
  done: false
  episode_len_mean: 806.2169895436492
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 270.18828964034435
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 171
  episodes_total: 12337
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.37214011202255887
        entropy_coeff: 0.0005000000000000001
        kl: 0.005121894335995118
        model: {}
        policy_loss: -0.010580289565647641
        total_loss: 5.60819411277771
        vf_explained_var: 0.9875487685203552
        vf_loss: 5.618832627932231
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.662068965517236
    gpu_util_percent0: 0.3286206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8000000000000003
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479838100832335
    mean_env_wait_ms: 1.20876932675589
    mean_inference_ms: 4.393915915161117
    mean_raw_obs_processing_ms: 0.383849488704463
  time_since_restore: 1595.6937732696533
  time_this_iter_s: 25.604326725006104
  time_total_s: 1595.6937732696533
  timers:
    learn_throughput: 8673.163
    learn_time_ms: 18654.325
    sample_throughput: 23749.601
    sample_time_ms: 6812.409
    update_time_ms: 29.93
  timestamp: 1602789301
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     62 |          1595.69 | 10031104 |  270.188 |              319.747 |              107.323 |            806.217 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3425.127830940989
    time_step_min: 3111
  date: 2020-10-15_19-15-26
  done: false
  episode_len_mean: 805.8195040534097
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 270.6337456587813
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 245
  episodes_total: 12582
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.35922815650701523
        entropy_coeff: 0.0005000000000000001
        kl: 0.00536204210948199
        model: {}
        policy_loss: -0.011327358738829693
        total_loss: 6.121419548988342
        vf_explained_var: 0.9893788695335388
        vf_loss: 6.1327922741572065
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.151724137931037
    gpu_util_percent0: 0.3055172413793103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479509869771932
    mean_env_wait_ms: 1.2087927769297906
    mean_inference_ms: 4.39166798598538
    mean_raw_obs_processing_ms: 0.38374634723919016
  time_since_restore: 1621.1921033859253
  time_this_iter_s: 25.498330116271973
  time_total_s: 1621.1921033859253
  timers:
    learn_throughput: 8686.337
    learn_time_ms: 18626.033
    sample_throughput: 23720.212
    sample_time_ms: 6820.85
    update_time_ms: 28.709
  timestamp: 1602789326
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     63 |          1621.19 | 10192896 |  270.634 |              319.747 |              107.323 |             805.82 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3422.466368767639
    time_step_min: 3107
  date: 2020-10-15_19-15-53
  done: false
  episode_len_mean: 805.4814033442725
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 271.0310559888618
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 216
  episodes_total: 12798
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.34552522252003354
        entropy_coeff: 0.0005000000000000001
        kl: 0.004593537227871518
        model: {}
        policy_loss: -0.009283156353679564
        total_loss: 5.999730269114177
        vf_explained_var: 0.9880654811859131
        vf_loss: 6.009071469306946
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.813333333333333
    gpu_util_percent0: 0.36333333333333323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14792460713289304
    mean_env_wait_ms: 1.2088000694409307
    mean_inference_ms: 4.389750729062611
    mean_raw_obs_processing_ms: 0.3836556147713857
  time_since_restore: 1647.1129438877106
  time_this_iter_s: 25.92084050178528
  time_total_s: 1647.1129438877106
  timers:
    learn_throughput: 8677.9
    learn_time_ms: 18644.142
    sample_throughput: 23611.77
    sample_time_ms: 6852.176
    update_time_ms: 29.095
  timestamp: 1602789353
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     64 |          1647.11 | 10354688 |  271.031 |              319.747 |              107.323 |            805.481 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3420.6085778431525
    time_step_min: 3107
  date: 2020-10-15_19-16-19
  done: false
  episode_len_mean: 805.2426884790493
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 271.30751141322935
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 161
  episodes_total: 12959
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3530290946364403
        entropy_coeff: 0.0005000000000000001
        kl: 0.005374249963400264
        model: {}
        policy_loss: -0.009723703609779477
        total_loss: 5.924156467119853
        vf_explained_var: 0.9867007732391357
        vf_loss: 5.9339894851048784
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87666666666667
    gpu_util_percent0: 0.3026666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7966666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14790505919428673
    mean_env_wait_ms: 1.2088127037481136
    mean_inference_ms: 4.388364866120091
    mean_raw_obs_processing_ms: 0.3835928879666253
  time_since_restore: 1672.91681599617
  time_this_iter_s: 25.803872108459473
  time_total_s: 1672.91681599617
  timers:
    learn_throughput: 8664.324
    learn_time_ms: 18673.356
    sample_throughput: 23599.268
    sample_time_ms: 6855.806
    update_time_ms: 28.681
  timestamp: 1602789379
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     65 |          1672.92 | 10516480 |  271.308 |              319.747 |              107.323 |            805.243 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3418.0436701156423
    time_step_min: 3107
  date: 2020-10-15_19-16-45
  done: false
  episode_len_mean: 804.9158956468982
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 271.68127505909996
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 13186
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.34985073904196423
        entropy_coeff: 0.0005000000000000001
        kl: 0.004857385375847419
        model: {}
        policy_loss: -0.009454788290895522
        total_loss: 7.686990896860759
        vf_explained_var: 0.9862106442451477
        vf_loss: 7.69655970732371
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.500000000000004
    gpu_util_percent0: 0.3366666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787537188057215
    mean_env_wait_ms: 1.208828326570877
    mean_inference_ms: 4.386438715992201
    mean_raw_obs_processing_ms: 0.3835058457173365
  time_since_restore: 1698.663637638092
  time_this_iter_s: 25.746821641921997
  time_total_s: 1698.663637638092
  timers:
    learn_throughput: 8655.01
    learn_time_ms: 18693.451
    sample_throughput: 23608.593
    sample_time_ms: 6853.098
    update_time_ms: 31.091
  timestamp: 1602789405
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     66 |          1698.66 | 10678272 |  271.681 |              319.747 |              107.323 |            804.916 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3415.701389926767
    time_step_min: 3107
  date: 2020-10-15_19-17-11
  done: false
  episode_len_mean: 804.590137067938
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 272.05470602930365
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 238
  episodes_total: 13424
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3292933677633603
        entropy_coeff: 0.0005000000000000001
        kl: 0.004817910143174231
        model: {}
        policy_loss: -0.0093964245946457
        total_loss: 7.071157455444336
        vf_explained_var: 0.9875838160514832
        vf_loss: 7.080688397089641
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25172413793104
    gpu_util_percent0: 0.3386206896551725
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14784913918892278
    mean_env_wait_ms: 1.2088375549860113
    mean_inference_ms: 4.384523522505941
    mean_raw_obs_processing_ms: 0.3834158636468923
  time_since_restore: 1724.3234870433807
  time_this_iter_s: 25.659849405288696
  time_total_s: 1724.3234870433807
  timers:
    learn_throughput: 8659.102
    learn_time_ms: 18684.616
    sample_throughput: 23591.783
    sample_time_ms: 6857.981
    update_time_ms: 30.612
  timestamp: 1602789431
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     67 |          1724.32 | 10840064 |  272.055 |              319.747 |              107.323 |             804.59 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3413.870755831119
    time_step_min: 3107
  date: 2020-10-15_19-17-37
  done: false
  episode_len_mean: 804.3247976453274
  episode_reward_max: 319.74747474747494
  episode_reward_mean: 272.33647735634486
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 166
  episodes_total: 13590
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3318946411212285
        entropy_coeff: 0.0005000000000000001
        kl: 0.004911121679469943
        model: {}
        policy_loss: -0.010973521867223704
        total_loss: 5.2956951061884565
        vf_explained_var: 0.9877960085868835
        vf_loss: 5.306819200515747
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.240000000000002
    gpu_util_percent0: 0.36
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14783070622235348
    mean_env_wait_ms: 1.2088437821865898
    mean_inference_ms: 4.3831928543885565
    mean_raw_obs_processing_ms: 0.3833560354037227
  time_since_restore: 1750.1284041404724
  time_this_iter_s: 25.804917097091675
  time_total_s: 1750.1284041404724
  timers:
    learn_throughput: 8650.71
    learn_time_ms: 18702.743
    sample_throughput: 23594.84
    sample_time_ms: 6857.093
    update_time_ms: 32.53
  timestamp: 1602789457
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     68 |          1750.13 | 11001856 |  272.336 |              319.747 |              107.323 |            804.325 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3411.824821766332
    time_step_min: 3090
  date: 2020-10-15_19-18-03
  done: false
  episode_len_mean: 804.0359733101247
  episode_reward_max: 321.4141414141416
  episode_reward_mean: 272.6631414229324
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 13788
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.33505240579446155
        entropy_coeff: 0.0005000000000000001
        kl: 0.005111830464253823
        model: {}
        policy_loss: -0.008610475711369267
        total_loss: 6.609688838322957
        vf_explained_var: 0.9871423244476318
        vf_loss: 6.618458827336629
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.055172413793105
    gpu_util_percent0: 0.3703448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7896551724137932
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478061714929709
    mean_env_wait_ms: 1.2088525870558608
    mean_inference_ms: 4.381635143936912
    mean_raw_obs_processing_ms: 0.38328632879979807
  time_since_restore: 1775.7951588630676
  time_this_iter_s: 25.666754722595215
  time_total_s: 1775.7951588630676
  timers:
    learn_throughput: 8666.573
    learn_time_ms: 18668.51
    sample_throughput: 23518.113
    sample_time_ms: 6879.464
    update_time_ms: 32.85
  timestamp: 1602789483
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     69 |           1775.8 | 11163648 |  272.663 |              321.414 |              107.323 |            804.036 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3409.2248499571306
    time_step_min: 3090
  date: 2020-10-15_19-18-29
  done: false
  episode_len_mean: 803.721897706226
  episode_reward_max: 321.4141414141416
  episode_reward_mean: 273.07626413731265
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 14038
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3201984415451686
        entropy_coeff: 0.0005000000000000001
        kl: 0.00546536249263833
        model: {}
        policy_loss: -0.008231578317160407
        total_loss: 6.717953443527222
        vf_explained_var: 0.9885758757591248
        vf_loss: 6.726336717605591
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.80333333333333
    gpu_util_percent0: 0.3446666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.850000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778061927263195
    mean_env_wait_ms: 1.2088595774175508
    mean_inference_ms: 4.379791484464026
    mean_raw_obs_processing_ms: 0.38319962500912186
  time_since_restore: 1801.6131210327148
  time_this_iter_s: 25.817962169647217
  time_total_s: 1801.6131210327148
  timers:
    learn_throughput: 8674.844
    learn_time_ms: 18650.71
    sample_throughput: 23453.754
    sample_time_ms: 6898.341
    update_time_ms: 34.465
  timestamp: 1602789509
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     70 |          1801.61 | 11325440 |  273.076 |              321.414 |              107.323 |            803.722 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3407.4377909437158
    time_step_min: 3090
  date: 2020-10-15_19-18-55
  done: false
  episode_len_mean: 803.5146272855134
  episode_reward_max: 327.777777777778
  episode_reward_mean: 273.35449431019043
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 14220
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.31465202818314236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004968973381134371
        model: {}
        policy_loss: -0.008591292018536478
        total_loss: 5.156265695889791
        vf_explained_var: 0.9888489246368408
        vf_loss: 5.165006558100383
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.982758620689655
    gpu_util_percent0: 0.38999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14776204106593582
    mean_env_wait_ms: 1.208860423998168
    mean_inference_ms: 4.378462075883372
    mean_raw_obs_processing_ms: 0.38313713885991285
  time_since_restore: 1827.234985589981
  time_this_iter_s: 25.621864557266235
  time_total_s: 1827.234985589981
  timers:
    learn_throughput: 8666.362
    learn_time_ms: 18668.964
    sample_throughput: 23457.353
    sample_time_ms: 6897.283
    update_time_ms: 33.333
  timestamp: 1602789535
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     71 |          1827.23 | 11487232 |  273.354 |              327.778 |              107.323 |            803.515 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3405.5899561189663
    time_step_min: 3082
  date: 2020-10-15_19-19-21
  done: false
  episode_len_mean: 803.2679352732829
  episode_reward_max: 327.777777777778
  episode_reward_mean: 273.6328876654593
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 14399
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3225397790471713
        entropy_coeff: 0.0005000000000000001
        kl: 0.005117148510180414
        model: {}
        policy_loss: -0.010752269362759156
        total_loss: 5.90200936794281
        vf_explained_var: 0.987377405166626
        vf_loss: 5.91291864713033
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.369999999999997
    gpu_util_percent0: 0.2786666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477432331054233
    mean_env_wait_ms: 1.2088651985492305
    mean_inference_ms: 4.377185577322385
    mean_raw_obs_processing_ms: 0.3830788889477354
  time_since_restore: 1852.9574236869812
  time_this_iter_s: 25.722438097000122
  time_total_s: 1852.9574236869812
  timers:
    learn_throughput: 8654.596
    learn_time_ms: 18694.345
    sample_throughput: 23497.11
    sample_time_ms: 6885.613
    update_time_ms: 31.593
  timestamp: 1602789561
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     72 |          1852.96 | 11649024 |  273.633 |              327.778 |              107.323 |            803.268 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3403.018207954001
    time_step_min: 3082
  date: 2020-10-15_19-19-47
  done: false
  episode_len_mean: 802.948262917207
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.02298184906874
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 252
  episodes_total: 14651
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3073352202773094
        entropy_coeff: 0.0005000000000000001
        kl: 0.004935716240045925
        model: {}
        policy_loss: -0.008306973903624263
        total_loss: 5.957005699475606
        vf_explained_var: 0.9897264838218689
        vf_loss: 5.965462525685628
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.686666666666667
    gpu_util_percent0: 0.37833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771854323703235
    mean_env_wait_ms: 1.2088769489617848
    mean_inference_ms: 4.375446316634785
    mean_raw_obs_processing_ms: 0.38299955055083346
  time_since_restore: 1878.7684478759766
  time_this_iter_s: 25.81102418899536
  time_total_s: 1878.7684478759766
  timers:
    learn_throughput: 8634.472
    learn_time_ms: 18737.914
    sample_throughput: 23571.895
    sample_time_ms: 6863.767
    update_time_ms: 39.57
  timestamp: 1602789587
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     73 |          1878.77 | 11810816 |  274.023 |              327.778 |              107.323 |            802.948 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3400.8968870281587
    time_step_min: 3082
  date: 2020-10-15_19-20-13
  done: false
  episode_len_mean: 802.6835903306174
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.3427473849667
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 200
  episodes_total: 14851
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.29465169956286746
        entropy_coeff: 0.0005000000000000001
        kl: 0.004808453610166907
        model: {}
        policy_loss: -0.008224970602896065
        total_loss: 6.00279422601064
        vf_explained_var: 0.9875608086585999
        vf_loss: 6.011164704958598
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.120689655172416
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476990752183499
    mean_env_wait_ms: 1.208867830754734
    mean_inference_ms: 4.374092835377027
    mean_raw_obs_processing_ms: 0.38293369288839196
  time_since_restore: 1904.294893026352
  time_this_iter_s: 25.526445150375366
  time_total_s: 1904.294893026352
  timers:
    learn_throughput: 8639.529
    learn_time_ms: 18726.946
    sample_throughput: 23671.299
    sample_time_ms: 6834.944
    update_time_ms: 38.712
  timestamp: 1602789613
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     74 |          1904.29 | 11972608 |  274.343 |              327.778 |              107.323 |            802.684 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3399.1860185617948
    time_step_min: 3082
  date: 2020-10-15_19-20-39
  done: false
  episode_len_mean: 802.465876556362
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.5935182438943
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 168
  episodes_total: 15019
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.30256233861049014
        entropy_coeff: 0.0005000000000000001
        kl: 0.005048536811955273
        model: {}
        policy_loss: -0.009929008044612905
        total_loss: 5.197199583053589
        vf_explained_var: 0.9883285164833069
        vf_loss: 5.207279006640117
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.666666666666668
    gpu_util_percent0: 0.3153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476834136383374
    mean_env_wait_ms: 1.2088729147315618
    mean_inference_ms: 4.372985072438566
    mean_raw_obs_processing_ms: 0.3828825609321971
  time_since_restore: 1930.263643503189
  time_this_iter_s: 25.968750476837158
  time_total_s: 1930.263643503189
  timers:
    learn_throughput: 8639.346
    learn_time_ms: 18727.344
    sample_throughput: 23622.519
    sample_time_ms: 6849.058
    update_time_ms: 39.804
  timestamp: 1602789639
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     75 |          1930.26 | 12134400 |  274.594 |              327.778 |              107.323 |            802.466 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3396.8445306337103
    time_step_min: 3082
  date: 2020-10-15_19-21-05
  done: false
  episode_len_mean: 802.1712337747476
  episode_reward_max: 327.777777777778
  episode_reward_mean: 274.9597886561961
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 235
  episodes_total: 15254
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.29972270627816516
        entropy_coeff: 0.0005000000000000001
        kl: 0.005038650939241052
        model: {}
        policy_loss: -0.010396160088324299
        total_loss: 6.214753150939941
        vf_explained_var: 0.9886576533317566
        vf_loss: 6.225298086802165
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.43793103448276
    gpu_util_percent0: 0.3568965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476602849309539
    mean_env_wait_ms: 1.2088701518055054
    mean_inference_ms: 4.371464434208607
    mean_raw_obs_processing_ms: 0.3828124863470829
  time_since_restore: 1955.6715309619904
  time_this_iter_s: 25.40788745880127
  time_total_s: 1955.6715309619904
  timers:
    learn_throughput: 8662.184
    learn_time_ms: 18677.968
    sample_throughput: 23540.353
    sample_time_ms: 6872.964
    update_time_ms: 37.369
  timestamp: 1602789665
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     76 |          1955.67 | 12296192 |   274.96 |              327.778 |              107.323 |            802.171 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3394.59173628651
    time_step_min: 3082
  date: 2020-10-15_19-21-31
  done: false
  episode_len_mean: 801.9298585545437
  episode_reward_max: 327.777777777778
  episode_reward_mean: 275.29824499597794
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 229
  episodes_total: 15483
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.28374748677015305
        entropy_coeff: 0.0005000000000000001
        kl: 0.004746896796859801
        model: {}
        policy_loss: -0.00899387260627312
        total_loss: 5.213112831115723
        vf_explained_var: 0.9900777339935303
        vf_loss: 5.22224775950114
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.033333333333335
    gpu_util_percent0: 0.34833333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476407177517745
    mean_env_wait_ms: 1.2088654260354075
    mean_inference_ms: 4.370025012404728
    mean_raw_obs_processing_ms: 0.3827425358124314
  time_since_restore: 1981.6457796096802
  time_this_iter_s: 25.97424864768982
  time_total_s: 1981.6457796096802
  timers:
    learn_throughput: 8655.536
    learn_time_ms: 18692.315
    sample_throughput: 23487.938
    sample_time_ms: 6888.302
    update_time_ms: 38.867
  timestamp: 1602789691
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     77 |          1981.65 | 12457984 |  275.298 |              327.778 |              107.323 |             801.93 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3393.0229443055823
    time_step_min: 3082
  date: 2020-10-15_19-21-57
  done: false
  episode_len_mean: 801.7162671780121
  episode_reward_max: 327.777777777778
  episode_reward_mean: 275.5174564436309
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 162
  episodes_total: 15645
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.28466908385356265
        entropy_coeff: 0.0005000000000000001
        kl: 0.004842583555728197
        model: {}
        policy_loss: -0.008349898465288183
        total_loss: 5.032889048258464
        vf_explained_var: 0.9882912635803223
        vf_loss: 5.041380882263184
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.036666666666676
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476268571810303
    mean_env_wait_ms: 1.2088668673045906
    mean_inference_ms: 4.36903505966647
    mean_raw_obs_processing_ms: 0.38269614377721134
  time_since_restore: 2007.3563377857208
  time_this_iter_s: 25.71055817604065
  time_total_s: 2007.3563377857208
  timers:
    learn_throughput: 8661.388
    learn_time_ms: 18679.685
    sample_throughput: 23479.591
    sample_time_ms: 6890.75
    update_time_ms: 38.224
  timestamp: 1602789717
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     78 |          2007.36 | 12619776 |  275.517 |              327.778 |              107.323 |            801.716 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3391.2272870961624
    time_step_min: 3082
  date: 2020-10-15_19-22-23
  done: false
  episode_len_mean: 801.4931584589192
  episode_reward_max: 327.777777777778
  episode_reward_mean: 275.8049853475163
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 214
  episodes_total: 15859
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2888108814756076
        entropy_coeff: 0.0005000000000000001
        kl: 0.00441952309726427
        model: {}
        policy_loss: -0.009793649398488924
        total_loss: 6.388848265012105
        vf_explained_var: 0.9879357814788818
        vf_loss: 6.398785909016927
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.372413793103444
    gpu_util_percent0: 0.32068965517241377
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760661140111553
    mean_env_wait_ms: 1.2088689982142211
    mean_inference_ms: 4.367752067596855
    mean_raw_obs_processing_ms: 0.3826377095957587
  time_since_restore: 2032.858303785324
  time_this_iter_s: 25.50196599960327
  time_total_s: 2032.858303785324
  timers:
    learn_throughput: 8661.287
    learn_time_ms: 18679.902
    sample_throughput: 23543.238
    sample_time_ms: 6872.122
    update_time_ms: 40.11
  timestamp: 1602789743
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     79 |          2032.86 | 12781568 |  275.805 |              327.778 |              107.323 |            801.493 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3389.1448400348563
    time_step_min: 3071
  date: 2020-10-15_19-22-48
  done: false
  episode_len_mean: 801.2234914328284
  episode_reward_max: 327.777777777778
  episode_reward_mean: 276.122784211622
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 249
  episodes_total: 16108
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.26880109558502835
        entropy_coeff: 0.0005000000000000001
        kl: 0.00491958010631303
        model: {}
        policy_loss: -0.009118710830459046
        total_loss: 6.228283405303955
        vf_explained_var: 0.9892032742500305
        vf_loss: 6.237536231676738
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.59310344827586
    gpu_util_percent0: 0.3289655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475868095966105
    mean_env_wait_ms: 1.2088575999288227
    mean_inference_ms: 4.366292557023049
    mean_raw_obs_processing_ms: 0.38256391531433365
  time_since_restore: 2057.956801891327
  time_this_iter_s: 25.098498106002808
  time_total_s: 2057.956801891327
  timers:
    learn_throughput: 8687.747
    learn_time_ms: 18623.01
    sample_throughput: 23592.195
    sample_time_ms: 6857.861
    update_time_ms: 38.332
  timestamp: 1602789768
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     80 |          2057.96 | 12943360 |  276.123 |              327.778 |              107.323 |            801.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3387.7225405039117
    time_step_min: 3071
  date: 2020-10-15_19-23-14
  done: false
  episode_len_mean: 801.005222734255
  episode_reward_max: 327.777777777778
  episode_reward_mean: 276.3411907089325
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 167
  episodes_total: 16275
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2703153267502785
        entropy_coeff: 0.0005000000000000001
        kl: 0.006194658034170668
        model: {}
        policy_loss: -0.006865756008968067
        total_loss: 4.854857683181763
        vf_explained_var: 0.9887514710426331
        vf_loss: 4.861858447392781
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.466666666666665
    gpu_util_percent0: 0.29799999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757325105481175
    mean_env_wait_ms: 1.208851457103779
    mean_inference_ms: 4.365318545597294
    mean_raw_obs_processing_ms: 0.3825175349550161
  time_since_restore: 2083.6788518428802
  time_this_iter_s: 25.722049951553345
  time_total_s: 2083.6788518428802
  timers:
    learn_throughput: 8689.141
    learn_time_ms: 18620.022
    sample_throughput: 23564.043
    sample_time_ms: 6866.054
    update_time_ms: 40.58
  timestamp: 1602789794
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     81 |          2083.68 | 13105152 |  276.341 |              327.778 |              107.323 |            801.005 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3385.9176406135866
    time_step_min: 3071
  date: 2020-10-15_19-23-40
  done: false
  episode_len_mean: 800.7972070431086
  episode_reward_max: 327.777777777778
  episode_reward_mean: 276.6207061507608
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 195
  episodes_total: 16470
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.27451565364996594
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055320160851503415
        model: {}
        policy_loss: -0.009508050012906702
        total_loss: 4.978119134902954
        vf_explained_var: 0.9896482825279236
        vf_loss: 4.987764398256938
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.553333333333338
    gpu_util_percent0: 0.32933333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14755616191397522
    mean_env_wait_ms: 1.2088504020752064
    mean_inference_ms: 4.364199250807602
    mean_raw_obs_processing_ms: 0.3824655341071053
  time_since_restore: 2109.3730096817017
  time_this_iter_s: 25.69415783882141
  time_total_s: 2109.3730096817017
  timers:
    learn_throughput: 8693.682
    learn_time_ms: 18610.297
    sample_throughput: 23574.86
    sample_time_ms: 6862.904
    update_time_ms: 41.996
  timestamp: 1602789820
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     82 |          2109.37 | 13266944 |  276.621 |              327.778 |              107.323 |            800.797 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3383.7427132061894
    time_step_min: 3071
  date: 2020-10-15_19-24-06
  done: false
  episode_len_mean: 800.5460636515913
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 276.9474355906516
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 246
  episodes_total: 16716
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.26038045932849246
        entropy_coeff: 0.0005000000000000001
        kl: 0.004677352922347684
        model: {}
        policy_loss: -0.010525610453138748
        total_loss: 6.345296025276184
        vf_explained_var: 0.9889521598815918
        vf_loss: 6.35595182577769
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.26551724137931
    gpu_util_percent0: 0.34275862068965524
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753774969454903
    mean_env_wait_ms: 1.2088414925394864
    mean_inference_ms: 4.362879240758006
    mean_raw_obs_processing_ms: 0.38239858010806194
  time_since_restore: 2135.0039505958557
  time_this_iter_s: 25.630940914154053
  time_total_s: 2135.0039505958557
  timers:
    learn_throughput: 8701.967
    learn_time_ms: 18592.577
    sample_throughput: 23549.99
    sample_time_ms: 6870.152
    update_time_ms: 34.106
  timestamp: 1602789846
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     83 |             2135 | 13428736 |  276.947 |              328.687 |              107.323 |            800.546 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3382.0927957308036
    time_step_min: 3071
  date: 2020-10-15_19-24-32
  done: false
  episode_len_mean: 800.3411013189803
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.1866025249239
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 191
  episodes_total: 16907
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.253933717807134
        entropy_coeff: 0.0005000000000000001
        kl: 0.004451247902276616
        model: {}
        policy_loss: -0.00829052090436259
        total_loss: 4.614181836446126
        vf_explained_var: 0.9898722767829895
        vf_loss: 4.622599244117737
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.52333333333333
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752343706741025
    mean_env_wait_ms: 1.208832509018407
    mean_inference_ms: 4.361848871689951
    mean_raw_obs_processing_ms: 0.3823481772931836
  time_since_restore: 2160.667028427124
  time_this_iter_s: 25.66307783126831
  time_total_s: 2160.667028427124
  timers:
    learn_throughput: 8702.282
    learn_time_ms: 18591.904
    sample_throughput: 23505.194
    sample_time_ms: 6883.245
    update_time_ms: 34.688
  timestamp: 1602789872
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     84 |          2160.67 | 13590528 |  277.187 |              328.687 |              107.323 |            800.341 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3380.7096584907877
    time_step_min: 3071
  date: 2020-10-15_19-24-58
  done: false
  episode_len_mean: 800.1650081948021
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.39619621643726
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 177
  episodes_total: 17084
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2647946501771609
        entropy_coeff: 0.0005000000000000001
        kl: 0.004680490974957745
        model: {}
        policy_loss: -0.00973389969052126
        total_loss: 5.266422748565674
        vf_explained_var: 0.9886471629142761
        vf_loss: 5.276289105415344
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803333333333335
    gpu_util_percent0: 0.3326666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475102658028236
    mean_env_wait_ms: 1.2088291465561212
    mean_inference_ms: 4.360926135547365
    mean_raw_obs_processing_ms: 0.38230377851602426
  time_since_restore: 2186.4971306324005
  time_this_iter_s: 25.83010220527649
  time_total_s: 2186.4971306324005
  timers:
    learn_throughput: 8700.762
    learn_time_ms: 18595.154
    sample_throughput: 23591.263
    sample_time_ms: 6858.132
    update_time_ms: 41.125
  timestamp: 1602789898
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     85 |           2186.5 | 13752320 |  277.396 |              328.687 |              107.323 |            800.165 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3378.686061307114
    time_step_min: 3071
  date: 2020-10-15_19-25-24
  done: false
  episode_len_mean: 799.8880106162012
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.696302979017
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 17332
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.25041167934735614
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044287769900014
        model: {}
        policy_loss: -0.007105277982191183
        total_loss: 5.639120936393738
        vf_explained_var: 0.9900476336479187
        vf_loss: 5.64635153611501
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.14137931034483
    gpu_util_percent0: 0.3410344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14749084313509658
    mean_env_wait_ms: 1.2088211260593589
    mean_inference_ms: 4.359637647138838
    mean_raw_obs_processing_ms: 0.38223809712190593
  time_since_restore: 2211.883418083191
  time_this_iter_s: 25.386287450790405
  time_total_s: 2211.883418083191
  timers:
    learn_throughput: 8692.714
    learn_time_ms: 18612.369
    sample_throughput: 23669.485
    sample_time_ms: 6835.468
    update_time_ms: 42.495
  timestamp: 1602789924
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     86 |          2211.88 | 13914112 |  277.696 |              328.687 |              107.323 |            799.888 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3377.040349774247
    time_step_min: 3071
  date: 2020-10-15_19-25-50
  done: false
  episode_len_mean: 799.67426877245
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 277.9641935058435
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 207
  episodes_total: 17539
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.23861983915170035
        entropy_coeff: 0.0005000000000000001
        kl: 0.004112338880077004
        model: {}
        policy_loss: -0.008755873218130242
        total_loss: 4.376618842283885
        vf_explained_var: 0.9909425377845764
        vf_loss: 4.3854940533638
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.344827586206897
    gpu_util_percent0: 0.38827586206896547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747681396890477
    mean_env_wait_ms: 1.208810770131863
    mean_inference_ms: 4.358604317801075
    mean_raw_obs_processing_ms: 0.3821869495922735
  time_since_restore: 2237.5245594978333
  time_this_iter_s: 25.641141414642334
  time_total_s: 2237.5245594978333
  timers:
    learn_throughput: 8699.748
    learn_time_ms: 18597.321
    sample_throughput: 23741.243
    sample_time_ms: 6814.808
    update_time_ms: 42.634
  timestamp: 1602789950
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     87 |          2237.52 | 14075904 |  277.964 |              328.687 |              107.323 |            799.674 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3375.6159071610527
    time_step_min: 3071
  date: 2020-10-15_19-26-16
  done: false
  episode_len_mean: 799.488789744169
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.1795620404645
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 168
  episodes_total: 17707
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.25132378935813904
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046780900641654926
        model: {}
        policy_loss: -0.010096744537198296
        total_loss: 4.369649648666382
        vf_explained_var: 0.9898576736450195
        vf_loss: 4.37987208366394
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.773333333333333
    gpu_util_percent0: 0.35366666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746475260512773
    mean_env_wait_ms: 1.208806263330507
    mean_inference_ms: 4.357775294416345
    mean_raw_obs_processing_ms: 0.3821451814328261
  time_since_restore: 2263.153421640396
  time_this_iter_s: 25.628862142562866
  time_total_s: 2263.153421640396
  timers:
    learn_throughput: 8707.092
    learn_time_ms: 18581.635
    sample_throughput: 23716.052
    sample_time_ms: 6822.046
    update_time_ms: 42.015
  timestamp: 1602789976
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     88 |          2263.15 | 14237696 |   278.18 |              328.687 |              107.323 |            799.489 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3373.67232521778
    time_step_min: 3071
  date: 2020-10-15_19-26-42
  done: false
  episode_len_mean: 799.202061281337
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.483424777018
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 17950
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.24475285286704698
        entropy_coeff: 0.0005000000000000001
        kl: 0.004660970376183589
        model: {}
        policy_loss: -0.0100580982301229
        total_loss: 4.857591191927592
        vf_explained_var: 0.9910823702812195
        vf_loss: 4.867771665255229
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.568965517241384
    gpu_util_percent0: 0.34827586206896544
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744687418952382
    mean_env_wait_ms: 1.2087961329532229
    mean_inference_ms: 4.35660963134132
    mean_raw_obs_processing_ms: 0.3820864560190057
  time_since_restore: 2288.7243778705597
  time_this_iter_s: 25.570956230163574
  time_total_s: 2288.7243778705597
  timers:
    learn_throughput: 8702.713
    learn_time_ms: 18590.984
    sample_throughput: 23722.42
    sample_time_ms: 6820.215
    update_time_ms: 40.113
  timestamp: 1602790002
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     89 |          2288.72 | 14399488 |  278.483 |              328.687 |              107.323 |            799.202 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3371.87450353045
    time_step_min: 3071
  date: 2020-10-15_19-27-08
  done: false
  episode_len_mean: 798.9411667583929
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.75678913516
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 220
  episodes_total: 18170
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.22985550637046495
        entropy_coeff: 0.0005000000000000001
        kl: 0.004359815580149491
        model: {}
        policy_loss: -0.010557659346280465
        total_loss: 4.799871762593587
        vf_explained_var: 0.9905244708061218
        vf_loss: 4.8105442126592
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.156666666666663
    gpu_util_percent0: 0.314
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743265143934187
    mean_env_wait_ms: 1.208787991259863
    mean_inference_ms: 4.355556791454609
    mean_raw_obs_processing_ms: 0.3820327885891852
  time_since_restore: 2314.6390805244446
  time_this_iter_s: 25.914702653884888
  time_total_s: 2314.6390805244446
  timers:
    learn_throughput: 8671.914
    learn_time_ms: 18657.011
    sample_throughput: 23687.395
    sample_time_ms: 6830.3
    update_time_ms: 41.628
  timestamp: 1602790028
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     90 |          2314.64 | 14561280 |  278.757 |              328.687 |              107.323 |            798.941 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3370.5411951232845
    time_step_min: 3071
  date: 2020-10-15_19-27-34
  done: false
  episode_len_mean: 798.7363770250369
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 278.9538735415024
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 163
  episodes_total: 18333
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2376074567437172
        entropy_coeff: 0.0005000000000000001
        kl: 0.00426829334658881
        model: {}
        policy_loss: -0.007931897892073417
        total_loss: 4.145262360572815
        vf_explained_var: 0.9900648593902588
        vf_loss: 4.1533130804697675
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.733333333333334
    gpu_util_percent0: 0.32133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742215740366205
    mean_env_wait_ms: 1.208784382481578
    mean_inference_ms: 4.354802797524651
    mean_raw_obs_processing_ms: 0.3819945866019744
  time_since_restore: 2340.137617111206
  time_this_iter_s: 25.498536586761475
  time_total_s: 2340.137617111206
  timers:
    learn_throughput: 8683.089
    learn_time_ms: 18632.999
    sample_throughput: 23669.869
    sample_time_ms: 6835.357
    update_time_ms: 38.791
  timestamp: 1602790054
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     91 |          2340.14 | 14723072 |  278.954 |              328.687 |              107.323 |            798.736 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3368.685742050424
    time_step_min: 3071
  date: 2020-10-15_19-28-00
  done: false
  episode_len_mean: 798.4581739833019
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.2153721431933
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 18565
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.2376383182903131
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520108496459822
        model: {}
        policy_loss: -0.010269694961607456
        total_loss: 5.752100348472595
        vf_explained_var: 0.9893476963043213
        vf_loss: 5.762488961219788
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.010344827586206
    gpu_util_percent0: 0.3703448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740612374650736
    mean_env_wait_ms: 1.208776308674756
    mean_inference_ms: 4.353747087473777
    mean_raw_obs_processing_ms: 0.38194157348614793
  time_since_restore: 2365.594825029373
  time_this_iter_s: 25.457207918167114
  time_total_s: 2365.594825029373
  timers:
    learn_throughput: 8688.587
    learn_time_ms: 18621.209
    sample_throughput: 23679.886
    sample_time_ms: 6832.465
    update_time_ms: 37.341
  timestamp: 1602790080
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     92 |          2365.59 | 14884864 |  279.215 |              328.687 |              107.323 |            798.458 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3366.9289331982727
    time_step_min: 3059
  date: 2020-10-15_19-28-26
  done: false
  episode_len_mean: 798.2009149422842
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.47802134328003
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 234
  episodes_total: 18799
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.21689422676960626
        entropy_coeff: 0.0005000000000000001
        kl: 0.004144047600372384
        model: {}
        policy_loss: -0.008971595564313853
        total_loss: 5.403254389762878
        vf_explained_var: 0.9898805618286133
        vf_loss: 5.412334322929382
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000006
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473914047600598
    mean_env_wait_ms: 1.208767751896503
    mean_inference_ms: 4.352706825339006
    mean_raw_obs_processing_ms: 0.38188654677361467
  time_since_restore: 2391.2676832675934
  time_this_iter_s: 25.672858238220215
  time_total_s: 2391.2676832675934
  timers:
    learn_throughput: 8688.186
    learn_time_ms: 18622.069
    sample_throughput: 23670.724
    sample_time_ms: 6835.11
    update_time_ms: 36.998
  timestamp: 1602790106
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     93 |          2391.27 | 15046656 |  279.478 |              328.687 |              107.323 |            798.201 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3365.643325229891
    time_step_min: 3059
  date: 2020-10-15_19-28-52
  done: false
  episode_len_mean: 798.0363847289601
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.6674746835577
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 165
  episodes_total: 18964
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.22126816088954607
        entropy_coeff: 0.0005000000000000001
        kl: 0.004749035385126869
        model: {}
        policy_loss: -0.009598569934799647
        total_loss: 3.8700090646743774
        vf_explained_var: 0.990976095199585
        vf_loss: 3.8797181646029153
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44137931034483
    gpu_util_percent0: 0.36551724137931035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738127994735165
    mean_env_wait_ms: 1.2087627078649776
    mean_inference_ms: 4.351975891099278
    mean_raw_obs_processing_ms: 0.3818490300894538
  time_since_restore: 2416.7464752197266
  time_this_iter_s: 25.47879195213318
  time_total_s: 2416.7464752197266
  timers:
    learn_throughput: 8688.357
    learn_time_ms: 18621.703
    sample_throughput: 23730.05
    sample_time_ms: 6818.022
    update_time_ms: 35.27
  timestamp: 1602790132
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     94 |          2416.75 | 15208448 |  279.667 |              328.687 |              107.323 |            798.036 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3363.907727676472
    time_step_min: 3059
  date: 2020-10-15_19-29-18
  done: false
  episode_len_mean: 797.7913560294041
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 279.9184931005482
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 19181
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.23035281896591187
        entropy_coeff: 0.0005000000000000001
        kl: 0.004143829729097585
        model: {}
        policy_loss: -0.010221730219200253
        total_loss: 4.654616395632426
        vf_explained_var: 0.9907565712928772
        vf_loss: 4.664953231811523
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.470000000000006
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736734277177047
    mean_env_wait_ms: 1.2087557715927186
    mean_inference_ms: 4.351039971240571
    mean_raw_obs_processing_ms: 0.3818011878788889
  time_since_restore: 2442.292113304138
  time_this_iter_s: 25.54563808441162
  time_total_s: 2442.292113304138
  timers:
    learn_throughput: 8699.896
    learn_time_ms: 18597.004
    sample_throughput: 23717.471
    sample_time_ms: 6821.638
    update_time_ms: 27.237
  timestamp: 1602790158
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     95 |          2442.29 | 15370240 |  279.918 |              328.687 |              107.323 |            797.791 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3362.0694974718813
    time_step_min: 3059
  date: 2020-10-15_19-29-44
  done: false
  episode_len_mean: 797.5166803953872
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.1897813597257
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 19424
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21208161860704422
        entropy_coeff: 0.0005000000000000001
        kl: 0.004079505316137026
        model: {}
        policy_loss: -0.008522810928601151
        total_loss: 5.027996857961019
        vf_explained_var: 0.9906296730041504
        vf_loss: 5.0366257429122925
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09655172413793
    gpu_util_percent0: 0.39137931034482765
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735281970663827
    mean_env_wait_ms: 1.2087476700762536
    mean_inference_ms: 4.350026477778768
    mean_raw_obs_processing_ms: 0.38174693733318726
  time_since_restore: 2467.886288166046
  time_this_iter_s: 25.59417486190796
  time_total_s: 2467.886288166046
  timers:
    learn_throughput: 8693.223
    learn_time_ms: 18611.278
    sample_throughput: 23696.017
    sample_time_ms: 6827.814
    update_time_ms: 27.805
  timestamp: 1602790184
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     96 |          2467.89 | 15532032 |   280.19 |              328.687 |              107.323 |            797.517 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3360.7272262288375
    time_step_min: 3059
  date: 2020-10-15_19-30-10
  done: false
  episode_len_mean: 797.329964783341
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.3930129653602
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 169
  episodes_total: 19593
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.21102898692091307
        entropy_coeff: 0.0005000000000000001
        kl: 0.004033662747436513
        model: {}
        policy_loss: -0.009322172450993094
        total_loss: 4.123458504676819
        vf_explained_var: 0.9900892376899719
        vf_loss: 4.132886111736298
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380000000000003
    gpu_util_percent0: 0.3233333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734246011693075
    mean_env_wait_ms: 1.2087378670369342
    mean_inference_ms: 4.349303479472281
    mean_raw_obs_processing_ms: 0.38171003704909706
  time_since_restore: 2493.747995853424
  time_this_iter_s: 25.86170768737793
  time_total_s: 2493.747995853424
  timers:
    learn_throughput: 8688.342
    learn_time_ms: 18621.735
    sample_throughput: 23651.196
    sample_time_ms: 6840.753
    update_time_ms: 26.682
  timestamp: 1602790210
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     97 |          2493.75 | 15693824 |  280.393 |              328.687 |              107.323 |             797.33 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3359.103664709455
    time_step_min: 3059
  date: 2020-10-15_19-30-36
  done: false
  episode_len_mean: 797.149661581978
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.6225248749746
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 205
  episodes_total: 19798
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.22252398480971655
        entropy_coeff: 0.0005000000000000001
        kl: 0.00425146275665611
        model: {}
        policy_loss: -0.009861140764163187
        total_loss: 5.083629727363586
        vf_explained_var: 0.9898825287818909
        vf_loss: 5.093602140744527
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.303448275862067
    gpu_util_percent0: 0.3851724137931035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732949510189902
    mean_env_wait_ms: 1.208730578730437
    mean_inference_ms: 4.3484575630678215
    mean_raw_obs_processing_ms: 0.38166676145559564
  time_since_restore: 2519.324297428131
  time_this_iter_s: 25.57630157470703
  time_total_s: 2519.324297428131
  timers:
    learn_throughput: 8688.052
    learn_time_ms: 18622.357
    sample_throughput: 23671.522
    sample_time_ms: 6834.879
    update_time_ms: 25.836
  timestamp: 1602790236
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     98 |          2519.32 | 15855616 |  280.623 |              328.687 |              107.323 |             797.15 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3357.242201559688
    time_step_min: 3059
  date: 2020-10-15_19-31-03
  done: false
  episode_len_mean: 796.9059163922977
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 280.8911019805961
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 20046
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.20550085604190826
        entropy_coeff: 0.0005000000000000001
        kl: 0.005275413820830484
        model: {}
        policy_loss: -0.00833877339027822
        total_loss: 4.737703998883565
        vf_explained_var: 0.9914646744728088
        vf_loss: 4.746145447095235
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.050000000000004
    gpu_util_percent0: 0.33399999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731597171383295
    mean_env_wait_ms: 1.2087244332129903
    mean_inference_ms: 4.347494887212855
    mean_raw_obs_processing_ms: 0.38161556412614506
  time_since_restore: 2545.1497638225555
  time_this_iter_s: 25.82546639442444
  time_total_s: 2545.1497638225555
  timers:
    learn_throughput: 8680.335
    learn_time_ms: 18638.913
    sample_throughput: 23644.882
    sample_time_ms: 6842.58
    update_time_ms: 25.662
  timestamp: 1602790263
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |     99 |          2545.15 | 16017408 |  280.891 |              328.687 |              107.323 |            796.906 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3355.8999653173464
    time_step_min: 3059
  date: 2020-10-15_19-31-29
  done: false
  episode_len_mean: 796.7325092707046
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.09657764293115
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 20225
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2001587301492691
        entropy_coeff: 0.0005000000000000001
        kl: 0.004580363204392294
        model: {}
        policy_loss: -0.009310966458239514
        total_loss: 3.8330214420954385
        vf_explained_var: 0.9911468625068665
        vf_loss: 3.8424323201179504
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.796666666666667
    gpu_util_percent0: 0.30400000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730568132413954
    mean_env_wait_ms: 1.2087148996237387
    mean_inference_ms: 4.346771110563666
    mean_raw_obs_processing_ms: 0.38157777796425446
  time_since_restore: 2570.8299491405487
  time_this_iter_s: 25.680185317993164
  time_total_s: 2570.8299491405487
  timers:
    learn_throughput: 8685.144
    learn_time_ms: 18628.591
    sample_throughput: 23712.618
    sample_time_ms: 6823.034
    update_time_ms: 26.103
  timestamp: 1602790289
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    100 |          2570.83 | 16179200 |  281.097 |              328.687 |              107.323 |            796.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3354.3903803680982
    time_step_min: 3044
  date: 2020-10-15_19-31-55
  done: false
  episode_len_mean: 796.561835725131
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.3255936946978
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 192
  episodes_total: 20417
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21187574292222658
        entropy_coeff: 0.0005000000000000001
        kl: 0.004318523104302585
        model: {}
        policy_loss: -0.009248630599662041
        total_loss: 4.492278416951497
        vf_explained_var: 0.9903406500816345
        vf_loss: 4.501633008321126
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97241379310345
    gpu_util_percent0: 0.3572413793103449
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472943007173288
    mean_env_wait_ms: 1.2087086330630206
    mean_inference_ms: 4.346021054060833
    mean_raw_obs_processing_ms: 0.3815389147874252
  time_since_restore: 2596.284712076187
  time_this_iter_s: 25.454762935638428
  time_total_s: 2596.284712076187
  timers:
    learn_throughput: 8683.671
    learn_time_ms: 18631.751
    sample_throughput: 23742.962
    sample_time_ms: 6814.314
    update_time_ms: 26.446
  timestamp: 1602790315
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    101 |          2596.28 | 16340992 |  281.326 |              328.687 |              107.323 |            796.562 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3352.425870260836
    time_step_min: 3044
  date: 2020-10-15_19-32-21
  done: false
  episode_len_mean: 796.3256725372556
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.6141871589906
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 20668
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.1968988093237082
        entropy_coeff: 0.0005000000000000001
        kl: 0.004188684746623039
        model: {}
        policy_loss: -0.010462574491005702
        total_loss: 4.660730878512065
        vf_explained_var: 0.9915547370910645
        vf_loss: 4.671291907628377
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63
    gpu_util_percent0: 0.34700000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472803799336577
    mean_env_wait_ms: 1.2086959822670944
    mean_inference_ms: 4.345071595011017
    mean_raw_obs_processing_ms: 0.3814880721640989
  time_since_restore: 2622.0402250289917
  time_this_iter_s: 25.755512952804565
  time_total_s: 2622.0402250289917
  timers:
    learn_throughput: 8672.949
    learn_time_ms: 18654.785
    sample_throughput: 23723.279
    sample_time_ms: 6819.968
    update_time_ms: 26.467
  timestamp: 1602790341
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    102 |          2622.04 | 16502784 |  281.614 |              328.687 |              107.323 |            796.326 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3350.9843862599087
    time_step_min: 3044
  date: 2020-10-15_19-32-47
  done: false
  episode_len_mean: 796.1232679675888
  episode_reward_max: 328.68686868686893
  episode_reward_mean: 281.8326478090585
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 189
  episodes_total: 20857
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.18859607353806496
        entropy_coeff: 0.0005000000000000001
        kl: 0.004518361024868985
        model: {}
        policy_loss: -0.008264472465574121
        total_loss: 3.2326915661493936
        vf_explained_var: 0.9925307631492615
        vf_loss: 3.2410502632459006
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51
    gpu_util_percent0: 0.31533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472707535270836
    mean_env_wait_ms: 1.2086883345487325
    mean_inference_ms: 4.344372944598097
    mean_raw_obs_processing_ms: 0.38145122467746323
  time_since_restore: 2647.724894762039
  time_this_iter_s: 25.684669733047485
  time_total_s: 2647.724894762039
  timers:
    learn_throughput: 8674.217
    learn_time_ms: 18652.058
    sample_throughput: 23737.784
    sample_time_ms: 6815.801
    update_time_ms: 33.369
  timestamp: 1602790367
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    103 |          2647.72 | 16664576 |  281.833 |              328.687 |              107.323 |            796.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3349.5890084769976
    time_step_min: 3044
  date: 2020-10-15_19-33-13
  done: false
  episode_len_mean: 795.959077946768
  episode_reward_max: 329.2929292929293
  episode_reward_mean: 282.0518012827898
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 21040
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.19958882282177606
        entropy_coeff: 0.0005000000000000001
        kl: 0.004384696406001846
        model: {}
        policy_loss: -0.008615456722812572
        total_loss: 3.3687493602434793
        vf_explained_var: 0.9923567771911621
        vf_loss: 3.377464552720388
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51724137931035
    gpu_util_percent0: 0.37724137931034496
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726078799421682
    mean_env_wait_ms: 1.208683405243794
    mean_inference_ms: 4.343701792397884
    mean_raw_obs_processing_ms: 0.3814171153844188
  time_since_restore: 2673.45884513855
  time_this_iter_s: 25.73395037651062
  time_total_s: 2673.45884513855
  timers:
    learn_throughput: 8667.73
    learn_time_ms: 18666.017
    sample_throughput: 23708.577
    sample_time_ms: 6824.197
    update_time_ms: 35.18
  timestamp: 1602790393
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    104 |          2673.46 | 16826368 |  282.052 |              329.293 |              107.323 |            795.959 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3347.6731466227347
    time_step_min: 3044
  date: 2020-10-15_19-33-39
  done: false
  episode_len_mean: 795.7551087518203
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 282.34511223001846
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 21287
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19368390614787737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004042028042022139
        model: {}
        policy_loss: -0.0101562580451476
        total_loss: 5.010983943939209
        vf_explained_var: 0.9909693598747253
        vf_loss: 5.021237293879191
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.443333333333328
    gpu_util_percent0: 0.3113333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724765196419745
    mean_env_wait_ms: 1.208670766591511
    mean_inference_ms: 4.342818304257419
    mean_raw_obs_processing_ms: 0.38136898768950916
  time_since_restore: 2699.168268918991
  time_this_iter_s: 25.709423780441284
  time_total_s: 2699.168268918991
  timers:
    learn_throughput: 8663.569
    learn_time_ms: 18674.983
    sample_throughput: 23688.339
    sample_time_ms: 6830.027
    update_time_ms: 35.465
  timestamp: 1602790419
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    105 |          2699.17 | 16988160 |  282.345 |              330.354 |              107.323 |            795.755 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3346.148046255712
    time_step_min: 3044
  date: 2020-10-15_19-34-05
  done: false
  episode_len_mean: 795.6133190618019
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 282.572539900118
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 201
  episodes_total: 21488
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.17996865883469582
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036390256136655807
        model: {}
        policy_loss: -0.008919239606863508
        total_loss: 3.695371667544047
        vf_explained_var: 0.9921171069145203
        vf_loss: 3.7043808499972024
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.183333333333337
    gpu_util_percent0: 0.36200000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723773985186542
    mean_env_wait_ms: 1.2086571673659094
    mean_inference_ms: 4.342107020300608
    mean_raw_obs_processing_ms: 0.38133075819281065
  time_since_restore: 2724.8858392238617
  time_this_iter_s: 25.717570304870605
  time_total_s: 2724.8858392238617
  timers:
    learn_throughput: 8653.528
    learn_time_ms: 18696.651
    sample_throughput: 23718.836
    sample_time_ms: 6821.245
    update_time_ms: 35.141
  timestamp: 1602790445
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    106 |          2724.89 | 17149952 |  282.573 |              330.354 |              107.323 |            795.613 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3344.9089100666174
    time_step_min: 3044
  date: 2020-10-15_19-34-31
  done: false
  episode_len_mean: 795.4940899436698
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 282.77048115283407
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 170
  episodes_total: 21658
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.191970224181811
        entropy_coeff: 0.0005000000000000001
        kl: 0.004277789848856628
        model: {}
        policy_loss: -0.008316074284569671
        total_loss: 3.8877825935681662
        vf_explained_var: 0.99090975522995
        vf_loss: 3.8961947361628213
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.579310344827583
    gpu_util_percent0: 0.4024137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722904262272835
    mean_env_wait_ms: 1.208650203235449
    mean_inference_ms: 4.341504982479498
    mean_raw_obs_processing_ms: 0.3812996231857434
  time_since_restore: 2750.4334783554077
  time_this_iter_s: 25.54763913154602
  time_total_s: 2750.4334783554077
  timers:
    learn_throughput: 8662.103
    learn_time_ms: 18678.143
    sample_throughput: 23762.678
    sample_time_ms: 6808.66
    update_time_ms: 33.853
  timestamp: 1602790471
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    107 |          2750.43 | 17311744 |   282.77 |              330.354 |              107.323 |            795.494 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3343.0362255866075
    time_step_min: 3037
  date: 2020-10-15_19-34-58
  done: false
  episode_len_mean: 795.3190139237616
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.04181739790045
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 21905
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.1884261133770148
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038155772684452436
        model: {}
        policy_loss: -0.009960336741642095
        total_loss: 5.1879085302352905
        vf_explained_var: 0.9906181693077087
        vf_loss: 5.197963118553162
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.133333333333336
    gpu_util_percent0: 0.3583333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472161979745161
    mean_env_wait_ms: 1.2086361738535578
    mean_inference_ms: 4.340671856037645
    mean_raw_obs_processing_ms: 0.38125441278223576
  time_since_restore: 2776.210555791855
  time_this_iter_s: 25.777077436447144
  time_total_s: 2776.210555791855
  timers:
    learn_throughput: 8648.63
    learn_time_ms: 18707.241
    sample_throughput: 23810.596
    sample_time_ms: 6794.958
    update_time_ms: 37.486
  timestamp: 1602790498
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    108 |          2776.21 | 17473536 |  283.042 |              330.354 |              107.323 |            795.319 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3341.4933864830587
    time_step_min: 3037
  date: 2020-10-15_19-35-23
  done: false
  episode_len_mean: 795.1809838140881
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.2698903311074
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 213
  episodes_total: 22118
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.17300691331426302
        entropy_coeff: 0.0005000000000000001
        kl: 0.004342181685691078
        model: {}
        policy_loss: -0.008004099535658801
        total_loss: 3.749170660972595
        vf_explained_var: 0.9924371838569641
        vf_loss: 3.757261315981547
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.893103448275863
    gpu_util_percent0: 0.33724137931034487
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472061734492528
    mean_env_wait_ms: 1.208617454519361
    mean_inference_ms: 4.3399598602660765
    mean_raw_obs_processing_ms: 0.38121511460118096
  time_since_restore: 2801.75363945961
  time_this_iter_s: 25.543083667755127
  time_total_s: 2801.75363945961
  timers:
    learn_throughput: 8655.695
    learn_time_ms: 18691.972
    sample_throughput: 23863.853
    sample_time_ms: 6779.794
    update_time_ms: 38.541
  timestamp: 1602790523
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    109 |          2801.75 | 17635328 |   283.27 |              330.354 |              107.323 |            795.181 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3340.2776354236908
    time_step_min: 3037
  date: 2020-10-15_19-35-49
  done: false
  episode_len_mean: 795.0727778525597
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.4541742638391
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 169
  episodes_total: 22287
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.17893226817250252
        entropy_coeff: 0.0005000000000000001
        kl: 0.00458470363325129
        model: {}
        policy_loss: -0.007602003766805865
        total_loss: 3.63955157995224
        vf_explained_var: 0.9913334846496582
        vf_loss: 3.6472429831822715
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79
    gpu_util_percent0: 0.3166666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471982635878308
    mean_env_wait_ms: 1.2086072635246299
    mean_inference_ms: 4.33939617990102
    mean_raw_obs_processing_ms: 0.38118529870443696
  time_since_restore: 2827.1247725486755
  time_this_iter_s: 25.37113308906555
  time_total_s: 2827.1247725486755
  timers:
    learn_throughput: 8670.836
    learn_time_ms: 18659.332
    sample_throughput: 23854.365
    sample_time_ms: 6782.49
    update_time_ms: 44.298
  timestamp: 1602790549
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    110 |          2827.12 | 17797120 |  283.454 |              330.354 |              107.323 |            795.073 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3338.6874387964035
    time_step_min: 3037
  date: 2020-10-15_19-36-15
  done: false
  episode_len_mean: 794.925626443931
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.70362367230143
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 221
  episodes_total: 22508
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18554598838090897
        entropy_coeff: 0.0005000000000000001
        kl: 0.004358186890992026
        model: {}
        policy_loss: -0.009096708813255342
        total_loss: 4.001087586085002
        vf_explained_var: 0.9921403527259827
        vf_loss: 4.010276973247528
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.741379310344833
    gpu_util_percent0: 0.3513793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147187382385936
    mean_env_wait_ms: 1.208593203542014
    mean_inference_ms: 4.338677143397865
    mean_raw_obs_processing_ms: 0.38114928575162876
  time_since_restore: 2852.6881711483
  time_this_iter_s: 25.563398599624634
  time_total_s: 2852.6881711483
  timers:
    learn_throughput: 8662.3
    learn_time_ms: 18677.718
    sample_throughput: 23882.759
    sample_time_ms: 6774.427
    update_time_ms: 44.081
  timestamp: 1602790575
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    111 |          2852.69 | 17958912 |  283.704 |              330.354 |              107.323 |            794.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3337.065976657124
    time_step_min: 3037
  date: 2020-10-15_19-36-41
  done: false
  episode_len_mean: 794.7628698289884
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 283.95583522391445
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 22747
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.1673268067340056
        entropy_coeff: 0.0005000000000000001
        kl: 0.003918773960322142
        model: {}
        policy_loss: -0.00786621808720156
        total_loss: 3.714820146560669
        vf_explained_var: 0.9928756356239319
        vf_loss: 3.722770015398661
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.486666666666665
    gpu_util_percent0: 0.3486666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471768818340055
    mean_env_wait_ms: 1.208572238916915
    mean_inference_ms: 4.337927184760131
    mean_raw_obs_processing_ms: 0.38110557073240725
  time_since_restore: 2878.316607952118
  time_this_iter_s: 25.62843680381775
  time_total_s: 2878.316607952118
  timers:
    learn_throughput: 8667.809
    learn_time_ms: 18665.847
    sample_throughput: 23889.178
    sample_time_ms: 6772.606
    update_time_ms: 44.358
  timestamp: 1602790601
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    112 |          2878.32 | 18120704 |  283.956 |              330.354 |              107.323 |            794.763 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3335.922131864288
    time_step_min: 3037
  date: 2020-10-15_19-37-08
  done: false
  episode_len_mean: 794.6623898053592
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.1282952594814
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 167
  episodes_total: 22914
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1715995396176974
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037443676264956594
        model: {}
        policy_loss: -0.010066550826498618
        total_loss: 3.5865272084871926
        vf_explained_var: 0.9916996359825134
        vf_loss: 3.5966795285542807
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62666666666667
    gpu_util_percent0: 0.35700000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716908084533598
    mean_env_wait_ms: 1.208557162423406
    mean_inference_ms: 4.337386914085555
    mean_raw_obs_processing_ms: 0.381076377817301
  time_since_restore: 2904.050372838974
  time_this_iter_s: 25.73376488685608
  time_total_s: 2904.050372838974
  timers:
    learn_throughput: 8667.654
    learn_time_ms: 18666.181
    sample_throughput: 23866.245
    sample_time_ms: 6779.114
    update_time_ms: 38.22
  timestamp: 1602790628
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    113 |          2904.05 | 18282496 |  284.128 |              330.354 |              107.323 |            794.662 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3334.568723459572
    time_step_min: 3037
  date: 2020-10-15_19-37-34
  done: false
  episode_len_mean: 794.5371107266436
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.33656198664846
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 206
  episodes_total: 23120
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.17907493809858957
        entropy_coeff: 0.0005000000000000001
        kl: 0.003939100890420377
        model: {}
        policy_loss: -0.008535448170732707
        total_loss: 4.618942618370056
        vf_explained_var: 0.9906483292579651
        vf_loss: 4.627567529678345
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.996666666666666
    gpu_util_percent0: 0.33499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715897352899138
    mean_env_wait_ms: 1.2085395368686398
    mean_inference_ms: 4.3367415858535034
    mean_raw_obs_processing_ms: 0.38104186920698363
  time_since_restore: 2930.061899662018
  time_this_iter_s: 26.011526823043823
  time_total_s: 2930.061899662018
  timers:
    learn_throughput: 8659.555
    learn_time_ms: 18683.639
    sample_throughput: 23832.12
    sample_time_ms: 6788.821
    update_time_ms: 37.967
  timestamp: 1602790654
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    114 |          2930.06 | 18444288 |  284.337 |              330.354 |              107.323 |            794.537 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3332.8622567092516
    time_step_min: 3037
  date: 2020-10-15_19-38-00
  done: false
  episode_len_mean: 794.3871961656967
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.59424136953226
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 23368
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.17048650359114012
        entropy_coeff: 0.0005000000000000001
        kl: 0.005069626378826797
        model: {}
        policy_loss: -0.007811213474875937
        total_loss: 4.577751437822978
        vf_explained_var: 0.9916302561759949
        vf_loss: 4.585647861162822
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333333
    gpu_util_percent0: 0.3286666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471489915621937
    mean_env_wait_ms: 1.2085196069804642
    mean_inference_ms: 4.336014447254848
    mean_raw_obs_processing_ms: 0.3810012999909397
  time_since_restore: 2955.4950637817383
  time_this_iter_s: 25.43316411972046
  time_total_s: 2955.4950637817383
  timers:
    learn_throughput: 8666.62
    learn_time_ms: 18668.408
    sample_throughput: 23874.841
    sample_time_ms: 6776.673
    update_time_ms: 37.255
  timestamp: 1602790680
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    115 |           2955.5 | 18606080 |  284.594 |              330.354 |              107.323 |            794.387 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3331.6826092506703
    time_step_min: 3037
  date: 2020-10-15_19-38-26
  done: false
  episode_len_mean: 794.2877288365969
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.77253527502
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 23543
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.16459816321730614
        entropy_coeff: 0.0005000000000000001
        kl: 0.004231553291901946
        model: {}
        policy_loss: -0.00915714621078223
        total_loss: 3.058228353659312
        vf_explained_var: 0.992760181427002
        vf_loss: 3.06746776898702
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.16896551724138
    gpu_util_percent0: 0.3158620689655173
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714124015512856
    mean_env_wait_ms: 1.2085014087297732
    mean_inference_ms: 4.335472291343473
    mean_raw_obs_processing_ms: 0.38097191056896273
  time_since_restore: 2980.9989092350006
  time_this_iter_s: 25.50384545326233
  time_total_s: 2980.9989092350006
  timers:
    learn_throughput: 8678.321
    learn_time_ms: 18643.237
    sample_throughput: 23865.125
    sample_time_ms: 6779.432
    update_time_ms: 37.282
  timestamp: 1602790706
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    116 |             2981 | 18767872 |  284.773 |              330.354 |              107.323 |            794.288 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3330.438072329831
    time_step_min: 3037
  date: 2020-10-15_19-38-52
  done: false
  episode_len_mean: 794.161127258941
  episode_reward_max: 330.3535353535355
  episode_reward_mean: 284.95809861537145
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 196
  episodes_total: 23739
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.1774490475654602
        entropy_coeff: 0.0005000000000000001
        kl: 0.00433512757687519
        model: {}
        policy_loss: -0.007354454952292144
        total_loss: 3.6468114654223123
        vf_explained_var: 0.9922370910644531
        vf_loss: 3.6542545755704245
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666676
    gpu_util_percent0: 0.28300000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471324765229219
    mean_env_wait_ms: 1.208483307427207
    mean_inference_ms: 4.334894221973269
    mean_raw_obs_processing_ms: 0.38094143997739444
  time_since_restore: 3006.915451526642
  time_this_iter_s: 25.916542291641235
  time_total_s: 3006.915451526642
  timers:
    learn_throughput: 8670.556
    learn_time_ms: 18659.933
    sample_throughput: 23807.486
    sample_time_ms: 6795.846
    update_time_ms: 39.754
  timestamp: 1602790732
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    117 |          3006.92 | 18929664 |  284.958 |              330.354 |              107.323 |            794.161 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3328.9412477033575
    time_step_min: 3022
  date: 2020-10-15_19-39-19
  done: false
  episode_len_mean: 794.0208003334723
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.19025393577283
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 23990
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17093541473150253
        entropy_coeff: 0.0005000000000000001
        kl: 0.004781136366849144
        model: {}
        policy_loss: -0.010186252620769665
        total_loss: 4.849270820617676
        vf_explained_var: 0.9914939999580383
        vf_loss: 4.859542489051819
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60666666666667
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712141946137972
    mean_env_wait_ms: 1.20846083007074
    mean_inference_ms: 4.334170338591352
    mean_raw_obs_processing_ms: 0.3808999854681613
  time_since_restore: 3032.7489438056946
  time_this_iter_s: 25.833492279052734
  time_total_s: 3032.7489438056946
  timers:
    learn_throughput: 8671.702
    learn_time_ms: 18657.468
    sample_throughput: 23773.18
    sample_time_ms: 6805.652
    update_time_ms: 37.536
  timestamp: 1602790759
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    118 |          3032.75 | 19091456 |   285.19 |              331.717 |              107.323 |            794.021 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3327.818954085861
    time_step_min: 3022
  date: 2020-10-15_19-39-45
  done: false
  episode_len_mean: 793.9302970133201
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.3641340182665
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 24174
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.15821321805318198
        entropy_coeff: 0.0005000000000000001
        kl: 0.004321871092543006
        model: {}
        policy_loss: -0.009441652397072176
        total_loss: 2.875088612238566
        vf_explained_var: 0.993636429309845
        vf_loss: 2.8846094210942588
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.366666666666667
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471143131845792
    mean_env_wait_ms: 1.208440089250843
    mean_inference_ms: 4.333642639066219
    mean_raw_obs_processing_ms: 0.3808713040300615
  time_since_restore: 3058.7527253627777
  time_this_iter_s: 26.00378155708313
  time_total_s: 3058.7527253627777
  timers:
    learn_throughput: 8663.06
    learn_time_ms: 18676.081
    sample_throughput: 23681.345
    sample_time_ms: 6832.044
    update_time_ms: 38.474
  timestamp: 1602790785
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    119 |          3058.75 | 19253248 |  285.364 |              331.717 |              107.323 |             793.93 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3326.625138768965
    time_step_min: 3022
  date: 2020-10-15_19-40-12
  done: false
  episode_len_mean: 793.8232565775971
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.54715981387574
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 189
  episodes_total: 24363
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.1737185480693976
        entropy_coeff: 0.0005000000000000001
        kl: 0.003957886598072946
        model: {}
        policy_loss: -0.008577238186262548
        total_loss: 3.7827311952908835
        vf_explained_var: 0.991386890411377
        vf_loss: 3.7913953065872192
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.240000000000002
    gpu_util_percent0: 0.30266666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471063753752139
    mean_env_wait_ms: 1.2084225912175275
    mean_inference_ms: 4.333105933382145
    mean_raw_obs_processing_ms: 0.380843465502235
  time_since_restore: 3084.6223423480988
  time_this_iter_s: 25.869616985321045
  time_total_s: 3084.6223423480988
  timers:
    learn_throughput: 8639.275
    learn_time_ms: 18727.497
    sample_throughput: 23665.831
    sample_time_ms: 6836.523
    update_time_ms: 32.283
  timestamp: 1602790812
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    120 |          3084.62 | 19415040 |  285.547 |              331.717 |              107.323 |            793.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3325.165187296417
    time_step_min: 3022
  date: 2020-10-15_19-40-38
  done: false
  episode_len_mean: 793.6956751483619
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.76811731656863
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 24602
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.1705658994615078
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009722937422338873
        total_loss: .inf
        vf_explained_var: 0.9931064248085022
        vf_loss: 3.7914165258407593
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.563333333333336
    gpu_util_percent0: 0.3126666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709656114872519
    mean_env_wait_ms: 1.2083978520092455
    mean_inference_ms: 4.332466609533446
    mean_raw_obs_processing_ms: 0.3808081657269619
  time_since_restore: 3110.4305291175842
  time_this_iter_s: 25.808186769485474
  time_total_s: 3110.4305291175842
  timers:
    learn_throughput: 8630.837
    learn_time_ms: 18745.807
    sample_throughput: 23649.478
    sample_time_ms: 6841.25
    update_time_ms: 32.811
  timestamp: 1602790838
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    121 |          3110.43 | 19576832 |  285.768 |              331.717 |              107.323 |            793.696 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3323.9429817476985
    time_step_min: 3022
  date: 2020-10-15_19-41-04
  done: false
  episode_len_mean: 793.5747802950899
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 285.9527244549013
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 204
  episodes_total: 24806
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6645352591003756e-16
        cur_lr: 5.0e-05
        entropy: 0.1545626918474833
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009436539253025936
        total_loss: .inf
        vf_explained_var: 0.9927486777305603
        vf_loss: 3.441520075003306
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.756666666666668
    gpu_util_percent0: 0.3246666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708867705530732
    mean_env_wait_ms: 1.2083732840843053
    mean_inference_ms: 4.331896104938407
    mean_raw_obs_processing_ms: 0.3807753243227007
  time_since_restore: 3136.0935764312744
  time_this_iter_s: 25.663047313690186
  time_total_s: 3136.0935764312744
  timers:
    learn_throughput: 8628.451
    learn_time_ms: 18750.991
    sample_throughput: 23660.044
    sample_time_ms: 6838.195
    update_time_ms: 32.97
  timestamp: 1602790864
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    122 |          3136.09 | 19738624 |  285.953 |              331.717 |              107.323 |            793.575 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3322.9309916195516
    time_step_min: 3022
  date: 2020-10-15_19-41-30
  done: false
  episode_len_mean: 793.4654337296345
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.1000906143214
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 24981
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9968028886505646e-16
        cur_lr: 5.0e-05
        entropy: 0.16905799259742102
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009564494304261947
        total_loss: .inf
        vf_explained_var: 0.9925346970558167
        vf_loss: 3.3702242573102317
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.524137931034485
    gpu_util_percent0: 0.37551724137931036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470817421952257
    mean_env_wait_ms: 1.208358002929261
    mean_inference_ms: 4.331426203858515
    mean_raw_obs_processing_ms: 0.38075098856957285
  time_since_restore: 3161.6207962036133
  time_this_iter_s: 25.527219772338867
  time_total_s: 3161.6207962036133
  timers:
    learn_throughput: 8635.562
    learn_time_ms: 18735.55
    sample_throughput: 23666.09
    sample_time_ms: 6836.448
    update_time_ms: 32.374
  timestamp: 1602790890
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    123 |          3161.62 | 19900416 |    286.1 |              331.717 |              107.323 |            793.465 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3321.6161271102283
    time_step_min: 3022
  date: 2020-10-15_19-41-56
  done: false
  episode_len_mean: 793.3317603204188
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.2998846777646
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 236
  episodes_total: 25217
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.995204332975845e-16
        cur_lr: 5.0e-05
        entropy: 0.16849863901734352
        entropy_coeff: 0.0005000000000000001
        kl: 0.004204125434625894
        model: {}
        policy_loss: -0.008447214650611082
        total_loss: 3.7170286973317466
        vf_explained_var: 0.9931346774101257
        vf_loss: 3.725560188293457
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.886666666666667
    gpu_util_percent0: 0.35933333333333345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707236542246446
    mean_env_wait_ms: 1.2083304828832855
    mean_inference_ms: 4.330804675419254
    mean_raw_obs_processing_ms: 0.3807164776619652
  time_since_restore: 3187.3880887031555
  time_this_iter_s: 25.767292499542236
  time_total_s: 3187.3880887031555
  timers:
    learn_throughput: 8648.528
    learn_time_ms: 18707.462
    sample_throughput: 23659.663
    sample_time_ms: 6838.305
    update_time_ms: 32.962
  timestamp: 1602790916
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    124 |          3187.39 | 20062208 |    286.3 |              331.717 |              107.323 |            793.332 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3320.4379602283916
    time_step_min: 3022
  date: 2020-10-15_19-42-23
  done: false
  episode_len_mean: 793.1923575893384
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.479889114044
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 220
  episodes_total: 25437
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9976021664879224e-16
        cur_lr: 5.0e-05
        entropy: 0.14990362152457237
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050418119644746184
        model: {}
        policy_loss: -0.009882040584064574
        total_loss: 3.7525786558787027
        vf_explained_var: 0.9924448132514954
        vf_loss: 3.7625356316566467
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.243333333333332
    gpu_util_percent0: 0.37833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706425291568867
    mean_env_wait_ms: 1.2083036738120017
    mean_inference_ms: 4.3302361716597595
    mean_raw_obs_processing_ms: 0.3806835677981424
  time_since_restore: 3213.119304895401
  time_this_iter_s: 25.731216192245483
  time_total_s: 3213.119304895401
  timers:
    learn_throughput: 8645.764
    learn_time_ms: 18713.441
    sample_throughput: 23581.933
    sample_time_ms: 6860.846
    update_time_ms: 33.6
  timestamp: 1602790943
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    125 |          3213.12 | 20224000 |   286.48 |              331.717 |              107.323 |            793.192 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3319.4840223725896
    time_step_min: 3022
  date: 2020-10-15_19-42-49
  done: false
  episode_len_mean: 793.0779413487446
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.6257325096014
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 172
  episodes_total: 25609
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9976021664879224e-16
        cur_lr: 5.0e-05
        entropy: 0.1566551091770331
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008712213643472447
        total_loss: .inf
        vf_explained_var: 0.9940388798713684
        vf_loss: 2.5338725646336875
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50333333333333
    gpu_util_percent0: 0.3470000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705762460191313
    mean_env_wait_ms: 1.2082866076965757
    mean_inference_ms: 4.3297938079271105
    mean_raw_obs_processing_ms: 0.3806595620124086
  time_since_restore: 3239.029313802719
  time_this_iter_s: 25.910008907318115
  time_total_s: 3239.029313802719
  timers:
    learn_throughput: 8628.215
    learn_time_ms: 18751.503
    sample_throughput: 23575.116
    sample_time_ms: 6862.829
    update_time_ms: 33.672
  timestamp: 1602790969
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    126 |          3239.03 | 20385792 |  286.626 |              331.717 |              107.323 |            793.078 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3318.2404233870966
    time_step_min: 3022
  date: 2020-10-15_19-43-15
  done: false
  episode_len_mean: 792.9347758767516
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 286.8098848670963
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 225
  episodes_total: 25834
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.496403249731884e-16
        cur_lr: 5.0e-05
        entropy: 0.16044227530558905
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036605909505548575
        model: {}
        policy_loss: -0.0072889168513938785
        total_loss: 3.7698705395062766
        vf_explained_var: 0.9928343892097473
        vf_loss: 3.777239739894867
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22333333333334
    gpu_util_percent0: 0.325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704893034611619
    mean_env_wait_ms: 1.2082591003486252
    mean_inference_ms: 4.329223572517394
    mean_raw_obs_processing_ms: 0.38062823382812255
  time_since_restore: 3264.929313659668
  time_this_iter_s: 25.899999856948853
  time_total_s: 3264.929313659668
  timers:
    learn_throughput: 8628.374
    learn_time_ms: 18751.158
    sample_throughput: 23579.561
    sample_time_ms: 6861.536
    update_time_ms: 33.319
  timestamp: 1602790995
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    127 |          3264.93 | 20547584 |   286.81 |              331.717 |              107.323 |            792.935 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3316.9456250240173
    time_step_min: 3022
  date: 2020-10-15_19-43-41
  done: false
  episode_len_mean: 792.7985037406484
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 287.01088963682474
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 26065
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.248201624865942e-16
        cur_lr: 5.0e-05
        entropy: 0.14217434947689375
        entropy_coeff: 0.0005000000000000001
        kl: 0.003887091064825654
        model: {}
        policy_loss: -0.007579348202852998
        total_loss: 3.3648829261461892
        vf_explained_var: 0.9933257699012756
        vf_loss: 3.3725334207216897
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.920000000000005
    gpu_util_percent0: 0.30100000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470412655876578
    mean_env_wait_ms: 1.2082298647480323
    mean_inference_ms: 4.32865685422977
    mean_raw_obs_processing_ms: 0.3805954699952363
  time_since_restore: 3290.591001033783
  time_this_iter_s: 25.66168737411499
  time_total_s: 3290.591001033783
  timers:
    learn_throughput: 8635.509
    learn_time_ms: 18735.665
    sample_throughput: 23588.088
    sample_time_ms: 6859.055
    update_time_ms: 32.431
  timestamp: 1602791021
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    128 |          3290.59 | 20709376 |  287.011 |              331.717 |              107.323 |            792.799 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3316.034165521454
    time_step_min: 3022
  date: 2020-10-15_19-44-08
  done: false
  episode_len_mean: 792.6915923469777
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 287.15387929142787
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 173
  episodes_total: 26238
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.124100812432971e-16
        cur_lr: 5.0e-05
        entropy: 0.1430023598174254
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037861804788311324
        model: {}
        policy_loss: -0.008694825228303671
        total_loss: 3.032269318898519
        vf_explained_var: 0.9929423332214355
        vf_loss: 3.0410356521606445
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.613793103448273
    gpu_util_percent0: 0.40862068965517245
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703464264809316
    mean_env_wait_ms: 1.2082112177824813
    mean_inference_ms: 4.328232014126295
    mean_raw_obs_processing_ms: 0.3805711863731684
  time_since_restore: 3316.1989891529083
  time_this_iter_s: 25.607988119125366
  time_total_s: 3316.1989891529083
  timers:
    learn_throughput: 8646.273
    learn_time_ms: 18712.339
    sample_throughput: 23646.229
    sample_time_ms: 6842.19
    update_time_ms: 32.194
  timestamp: 1602791048
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    129 |           3316.2 | 20871168 |  287.154 |              331.717 |              107.323 |            792.692 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3314.972470463496
    time_step_min: 3022
  date: 2020-10-15_19-44-34
  done: false
  episode_len_mean: 792.5524763705104
  episode_reward_max: 331.71717171717165
  episode_reward_mean: 287.3213305073418
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 212
  episodes_total: 26450
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.620504062164855e-17
        cur_lr: 5.0e-05
        entropy: 0.15105008085568747
        entropy_coeff: 0.0005000000000000001
        kl: 0.004177333787083626
        model: {}
        policy_loss: -0.009296208234445658
        total_loss: 4.53856639067332
        vf_explained_var: 0.991170346736908
        vf_loss: 4.547938068707784
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.773333333333333
    gpu_util_percent0: 0.274
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702684118489795
    mean_env_wait_ms: 1.2081864996454637
    mean_inference_ms: 4.3277323326193855
    mean_raw_obs_processing_ms: 0.3805449726638476
  time_since_restore: 3342.0068831443787
  time_this_iter_s: 25.807893991470337
  time_total_s: 3342.0068831443787
  timers:
    learn_throughput: 8649.274
    learn_time_ms: 18705.848
    sample_throughput: 23646.801
    sample_time_ms: 6842.025
    update_time_ms: 31.472
  timestamp: 1602791074
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    130 |          3342.01 | 21032960 |  287.321 |              331.717 |              107.323 |            792.552 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3313.641835853942
    time_step_min: 3022
  date: 2020-10-15_19-45-00
  done: false
  episode_len_mean: 792.3798568698714
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 287.5304621773204
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 26689
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8102520310824274e-17
        cur_lr: 5.0e-05
        entropy: 0.13767210518320402
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010452359177482625
        total_loss: .inf
        vf_explained_var: 0.9935101866722107
        vf_loss: 3.3806007901827493
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.020000000000003
    gpu_util_percent0: 0.3469999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470193665007552
    mean_env_wait_ms: 1.2081572254387387
    mean_inference_ms: 4.327179380637197
    mean_raw_obs_processing_ms: 0.38051266211912355
  time_since_restore: 3367.9381635189056
  time_this_iter_s: 25.931280374526978
  time_total_s: 3367.9381635189056
  timers:
    learn_throughput: 8644.086
    learn_time_ms: 18717.074
    sample_throughput: 23652.416
    sample_time_ms: 6840.401
    update_time_ms: 32.88
  timestamp: 1602791100
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    131 |          3367.94 | 21194752 |   287.53 |              334.747 |              107.323 |             792.38 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3312.7147597211347
    time_step_min: 3022
  date: 2020-10-15_19-45-27
  done: false
  episode_len_mean: 792.2458589242509
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 287.66723065383025
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 26865
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2153780466236426e-17
        cur_lr: 5.0e-05
        entropy: 0.13703347370028496
        entropy_coeff: 0.0005000000000000001
        kl: 0.004063423994618158
        model: {}
        policy_loss: -0.009244664528523572
        total_loss: 3.463837484518687
        vf_explained_var: 0.9920132756233215
        vf_loss: 3.4731505711873374
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32333333333333
    gpu_util_percent0: 0.3393333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701291063701813
    mean_env_wait_ms: 1.2081337273609563
    mean_inference_ms: 4.326752065434235
    mean_raw_obs_processing_ms: 0.3804883133987606
  time_since_restore: 3393.613484144211
  time_this_iter_s: 25.675320625305176
  time_total_s: 3393.613484144211
  timers:
    learn_throughput: 8649.233
    learn_time_ms: 18705.936
    sample_throughput: 23612.555
    sample_time_ms: 6851.948
    update_time_ms: 32.733
  timestamp: 1602791127
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    132 |          3393.61 | 21356544 |  287.667 |              334.747 |              107.323 |            792.246 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3311.64306588244
    time_step_min: 3012
  date: 2020-10-15_19-45-53
  done: false
  episode_len_mean: 792.101458910434
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 287.84218360894255
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 210
  episodes_total: 27075
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1076890233118213e-17
        cur_lr: 5.0e-05
        entropy: 0.14753024900952974
        entropy_coeff: 0.0005000000000000001
        kl: 0.004406194901093841
        model: {}
        policy_loss: -0.008399242447922006
        total_loss: 4.08595625559489
        vf_explained_var: 0.9916926026344299
        vf_loss: 4.094429194927216
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.86206896551725
    gpu_util_percent0: 0.3403448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700484591899138
    mean_env_wait_ms: 1.208107459623124
    mean_inference_ms: 4.326265403154622
    mean_raw_obs_processing_ms: 0.3804618142906294
  time_since_restore: 3419.1777758598328
  time_this_iter_s: 25.56429171562195
  time_total_s: 3419.1777758598328
  timers:
    learn_throughput: 8645.128
    learn_time_ms: 18714.819
    sample_throughput: 23635.589
    sample_time_ms: 6845.27
    update_time_ms: 32.785
  timestamp: 1602791153
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    133 |          3419.18 | 21518336 |  287.842 |              334.747 |              107.323 |            792.101 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3310.260375421616
    time_step_min: 3012
  date: 2020-10-15_19-46-19
  done: false
  episode_len_mean: 791.9317666007761
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.04882783468327
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 27318
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0538445116559106e-17
        cur_lr: 5.0e-05
        entropy: 0.13398165752490362
        entropy_coeff: 0.0005000000000000001
        kl: 0.004139222340503086
        model: {}
        policy_loss: -0.008095535122871903
        total_loss: 3.1906695564587912
        vf_explained_var: 0.9938951134681702
        vf_loss: 3.1988320549329123
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.686666666666667
    gpu_util_percent0: 0.2926666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699784710104302
    mean_env_wait_ms: 1.2080766508716716
    mean_inference_ms: 4.32573411045697
    mean_raw_obs_processing_ms: 0.38043061896292485
  time_since_restore: 3444.958076477051
  time_this_iter_s: 25.780300617218018
  time_total_s: 3444.958076477051
  timers:
    learn_throughput: 8638.419
    learn_time_ms: 18729.353
    sample_throughput: 23681.932
    sample_time_ms: 6831.875
    update_time_ms: 32.259
  timestamp: 1602791179
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    134 |          3444.96 | 21680128 |  288.049 |              334.747 |              107.323 |            791.932 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3309.2249016465103
    time_step_min: 3012
  date: 2020-10-15_19-46-45
  done: false
  episode_len_mean: 791.8031206808758
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.2060309944575
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 27494
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.269222558279553e-18
        cur_lr: 5.0e-05
        entropy: 0.1274954273054997
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039248881706347065
        model: {}
        policy_loss: -0.007294766731016959
        total_loss: 2.3881035645802817
        vf_explained_var: 0.9942663311958313
        vf_loss: 2.3954621156056723
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.969999999999995
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699188018836867
    mean_env_wait_ms: 1.2080530065806452
    mean_inference_ms: 4.325326345607463
    mean_raw_obs_processing_ms: 0.3804071369011713
  time_since_restore: 3470.8450927734375
  time_this_iter_s: 25.88701629638672
  time_total_s: 3470.8450927734375
  timers:
    learn_throughput: 8628.683
    learn_time_ms: 18750.486
    sample_throughput: 23716.247
    sample_time_ms: 6821.99
    update_time_ms: 33.438
  timestamp: 1602791205
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    135 |          3470.85 | 21841920 |  288.206 |              334.747 |              107.323 |            791.803 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3308.1076283441794
    time_step_min: 3012
  date: 2020-10-15_19-47-12
  done: false
  episode_len_mean: 791.6535629196447
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.3783634482139
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 208
  episodes_total: 27702
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6346112791397766e-18
        cur_lr: 5.0e-05
        entropy: 0.13857279097040495
        entropy_coeff: 0.0005000000000000001
        kl: 0.003999483014922589
        model: {}
        policy_loss: -0.007809690810972825
        total_loss: 3.5393729408582053
        vf_explained_var: 0.992622435092926
        vf_loss: 3.5472520192464194
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.423333333333332
    gpu_util_percent0: 0.354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698413700751992
    mean_env_wait_ms: 1.2080251215153892
    mean_inference_ms: 4.324863430058365
    mean_raw_obs_processing_ms: 0.38038118697255013
  time_since_restore: 3496.5416820049286
  time_this_iter_s: 25.69658923149109
  time_total_s: 3496.5416820049286
  timers:
    learn_throughput: 8639.668
    learn_time_ms: 18726.644
    sample_throughput: 23706.982
    sample_time_ms: 6824.656
    update_time_ms: 32.13
  timestamp: 1602791232
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    136 |          3496.54 | 22003712 |  288.378 |              334.747 |              107.323 |            791.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3306.7343318880567
    time_step_min: 3012
  date: 2020-10-15_19-47-38
  done: false
  episode_len_mean: 791.4799813946831
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.5891202988416
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 27949
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3173056395698883e-18
        cur_lr: 5.0e-05
        entropy: 0.12484266608953476
        entropy_coeff: 0.0005000000000000001
        kl: 0.003520477873583635
        model: {}
        policy_loss: -0.009561487395937244
        total_loss: 3.2520604729652405
        vf_explained_var: 0.9937812685966492
        vf_loss: 3.26168429851532
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.723333333333336
    gpu_util_percent0: 0.325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469773723456218
    mean_env_wait_ms: 1.2079945833844874
    mean_inference_ms: 4.324353273513062
    mean_raw_obs_processing_ms: 0.38035146451560364
  time_since_restore: 3522.0697848796844
  time_this_iter_s: 25.52810287475586
  time_total_s: 3522.0697848796844
  timers:
    learn_throughput: 8647.94
    learn_time_ms: 18708.733
    sample_throughput: 23772.568
    sample_time_ms: 6805.828
    update_time_ms: 30.54
  timestamp: 1602791258
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    137 |          3522.07 | 22165504 |  288.589 |              334.747 |              107.323 |             791.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3305.7590357155573
    time_step_min: 3011
  date: 2020-10-15_19-48-04
  done: false
  episode_len_mean: 791.3540977777777
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.73748148148144
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 28125
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.586528197849442e-19
        cur_lr: 5.0e-05
        entropy: 0.11494629519681136
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032321646188696227
        model: {}
        policy_loss: -0.007082645606715232
        total_loss: 2.6156163613001504
        vf_explained_var: 0.9937495589256287
        vf_loss: 2.622756600379944
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.975862068965515
    gpu_util_percent0: 0.3241379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697138985398897
    mean_env_wait_ms: 1.2079679717954015
    mean_inference_ms: 4.323955502695155
    mean_raw_obs_processing_ms: 0.3803284385027909
  time_since_restore: 3547.502681493759
  time_this_iter_s: 25.432896614074707
  time_total_s: 3547.502681493759
  timers:
    learn_throughput: 8657.347
    learn_time_ms: 18688.404
    sample_throughput: 23781.606
    sample_time_ms: 6803.241
    update_time_ms: 30.079
  timestamp: 1602791284
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    138 |           3547.5 | 22327296 |  288.737 |              334.747 |              107.323 |            791.354 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3304.5782012302907
    time_step_min: 3011
  date: 2020-10-15_19-48-30
  done: false
  episode_len_mean: 791.2154758542785
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 288.9164145693021
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 203
  episodes_total: 28328
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.293264098924721e-19
        cur_lr: 5.0e-05
        entropy: 0.12652969484527907
        entropy_coeff: 0.0005000000000000001
        kl: 0.00361985428025946
        model: {}
        policy_loss: -0.010004464760034656
        total_loss: 2.798440714677175
        vf_explained_var: 0.9940226674079895
        vf_loss: 2.808508356412252
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.513333333333335
    gpu_util_percent0: 0.33499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696447431100493
    mean_env_wait_ms: 1.2079410739065706
    mean_inference_ms: 4.323526978009634
    mean_raw_obs_processing_ms: 0.3803045819181206
  time_since_restore: 3573.1897644996643
  time_this_iter_s: 25.68708300590515
  time_total_s: 3573.1897644996643
  timers:
    learn_throughput: 8653.401
    learn_time_ms: 18696.926
    sample_throughput: 23782.71
    sample_time_ms: 6802.925
    update_time_ms: 28.429
  timestamp: 1602791310
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    139 |          3573.19 | 22489088 |  288.916 |              334.747 |              107.323 |            791.215 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3303.149244840032
    time_step_min: 3011
  date: 2020-10-15_19-48-56
  done: false
  episode_len_mean: 791.0462927324259
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 289.1300421549905
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 28579
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6466320494623604e-19
        cur_lr: 5.0e-05
        entropy: 0.11726209583381812
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037438678555190563
        model: {}
        policy_loss: -0.0077141110474864645
        total_loss: 3.190114438533783
        vf_explained_var: 0.9940312504768372
        vf_loss: 3.197887142499288
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.459999999999997
    gpu_util_percent0: 0.3403333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695736382742375
    mean_env_wait_ms: 1.2079070726045567
    mean_inference_ms: 4.323028766654636
    mean_raw_obs_processing_ms: 0.3802747839970804
  time_since_restore: 3598.6741106510162
  time_this_iter_s: 25.48434615135193
  time_total_s: 3598.6741106510162
  timers:
    learn_throughput: 8663.137
    learn_time_ms: 18675.915
    sample_throughput: 23829.706
    sample_time_ms: 6789.509
    update_time_ms: 29.485
  timestamp: 1602791336
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    140 |          3598.67 | 22650880 |   289.13 |              334.747 |              107.323 |            791.046 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3302.096810140688
    time_step_min: 3011
  date: 2020-10-15_19-49-22
  done: false
  episode_len_mean: 790.9342096112387
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 289.2877555020263
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 28758
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.233160247311802e-20
        cur_lr: 5.0e-05
        entropy: 0.10977787648638089
        entropy_coeff: 0.0005000000000000001
        kl: 0.004931676938819389
        model: {}
        policy_loss: -0.009015165589516982
        total_loss: 2.122484505176544
        vf_explained_var: 0.9949579238891602
        vf_loss: 2.1315545439720154
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.956666666666667
    gpu_util_percent0: 0.3783333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695166368995974
    mean_env_wait_ms: 1.2078796258154119
    mean_inference_ms: 4.322641172299432
    mean_raw_obs_processing_ms: 0.38025245422607457
  time_since_restore: 3624.332355260849
  time_this_iter_s: 25.658244609832764
  time_total_s: 3624.332355260849
  timers:
    learn_throughput: 8675.242
    learn_time_ms: 18649.855
    sample_throughput: 23836.647
    sample_time_ms: 6787.532
    update_time_ms: 29.633
  timestamp: 1602791362
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    141 |          3624.33 | 22812672 |  289.288 |              334.747 |              107.323 |            790.934 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3300.9388919629273
    time_step_min: 3011
  date: 2020-10-15_19-49-49
  done: false
  episode_len_mean: 790.7940465501761
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 289.46351072015824
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 200
  episodes_total: 28958
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.116580123655901e-20
        cur_lr: 5.0e-05
        entropy: 0.12737267340222994
        entropy_coeff: 0.0005000000000000001
        kl: 0.00473406685826679
        model: {}
        policy_loss: -0.009174686424861042
        total_loss: 2.9874568382898965
        vf_explained_var: 0.9933413863182068
        vf_loss: 2.996695260206858
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.47586206896552
    gpu_util_percent0: 0.3831034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694504423003593
    mean_env_wait_ms: 1.207850133015465
    mean_inference_ms: 4.3222316051937355
    mean_raw_obs_processing_ms: 0.3802290237906243
  time_since_restore: 3650.103028535843
  time_this_iter_s: 25.770673274993896
  time_total_s: 3650.103028535843
  timers:
    learn_throughput: 8667.203
    learn_time_ms: 18667.153
    sample_throughput: 23866.145
    sample_time_ms: 6779.143
    update_time_ms: 29.789
  timestamp: 1602791389
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    142 |           3650.1 | 22974464 |  289.464 |              334.747 |              107.323 |            790.794 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3299.5096687924297
    time_step_min: 3011
  date: 2020-10-15_19-50-15
  done: false
  episode_len_mean: 790.6119213913996
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 289.6860760439231
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 29208
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0582900618279505e-20
        cur_lr: 5.0e-05
        entropy: 0.1208074763417244
        entropy_coeff: 0.0005000000000000001
        kl: 0.00525450073958685
        model: {}
        policy_loss: -0.008796717457395667
        total_loss: 2.7161702315012612
        vf_explained_var: 0.9947368502616882
        vf_loss: 2.7250274221102395
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.80666666666666
    gpu_util_percent0: 0.343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693795417354777
    mean_env_wait_ms: 1.2078151930327534
    mean_inference_ms: 4.321756616491531
    mean_raw_obs_processing_ms: 0.3802005647072787
  time_since_restore: 3675.927179813385
  time_this_iter_s: 25.824151277542114
  time_total_s: 3675.927179813385
  timers:
    learn_throughput: 8655.821
    learn_time_ms: 18691.7
    sample_throughput: 23867.022
    sample_time_ms: 6778.893
    update_time_ms: 31.16
  timestamp: 1602791415
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    143 |          3675.93 | 23136256 |  289.686 |              334.747 |              107.323 |            790.612 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3298.4341011312526
    time_step_min: 3011
  date: 2020-10-15_19-50-42
  done: false
  episode_len_mean: 790.4797890438924
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 289.84889555644907
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 29390
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0582900618279505e-20
        cur_lr: 5.0e-05
        entropy: 0.1105913296341896
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034540366614237428
        model: {}
        policy_loss: -0.009765300831835097
        total_loss: 2.016910046339035
        vf_explained_var: 0.9950759410858154
        vf_loss: 2.0267306168874106
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.136666666666663
    gpu_util_percent0: 0.388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469325950625103
    mean_env_wait_ms: 1.2077858497962342
    mean_inference_ms: 4.321380038856466
    mean_raw_obs_processing_ms: 0.3801790137205306
  time_since_restore: 3701.6511347293854
  time_this_iter_s: 25.723954916000366
  time_total_s: 3701.6511347293854
  timers:
    learn_throughput: 8660.915
    learn_time_ms: 18680.706
    sample_throughput: 23845.252
    sample_time_ms: 6785.082
    update_time_ms: 29.391
  timestamp: 1602791442
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    144 |          3701.65 | 23298048 |  289.849 |              334.747 |              107.323 |             790.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3297.289616191701
    time_step_min: 3011
  date: 2020-10-15_19-51-08
  done: false
  episode_len_mean: 790.337772069758
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 290.02937650125693
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 29588
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0291450309139752e-20
        cur_lr: 5.0e-05
        entropy: 0.1230356531838576
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008893049884742746
        total_loss: .inf
        vf_explained_var: 0.99349045753479
        vf_loss: 2.837773382663727
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.410000000000004
    gpu_util_percent0: 0.345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692624932702128
    mean_env_wait_ms: 1.2077555504522834
    mean_inference_ms: 4.320990377502929
    mean_raw_obs_processing_ms: 0.38015647006847014
  time_since_restore: 3727.3247706890106
  time_this_iter_s: 25.673635959625244
  time_total_s: 3727.3247706890106
  timers:
    learn_throughput: 8674.074
    learn_time_ms: 18652.365
    sample_throughput: 23841.007
    sample_time_ms: 6786.29
    update_time_ms: 35.38
  timestamp: 1602791468
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    145 |          3727.32 | 23459840 |  290.029 |              334.747 |              107.323 |            790.338 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3295.8130684297075
    time_step_min: 3011
  date: 2020-10-15_19-51-34
  done: false
  episode_len_mean: 790.1615335634572
  episode_reward_max: 334.7474747474752
  episode_reward_mean: 290.2547831612143
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 29839
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5437175463709622e-20
        cur_lr: 5.0e-05
        entropy: 0.1178820300847292
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006231627427041531
        total_loss: .inf
        vf_explained_var: 0.9946849942207336
        vf_loss: 2.7219404578208923
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.089999999999996
    gpu_util_percent0: 0.2983333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469193540781579
    mean_env_wait_ms: 1.207718638875358
    mean_inference_ms: 4.320532050875873
    mean_raw_obs_processing_ms: 0.38012869618803397
  time_since_restore: 3753.3983402252197
  time_this_iter_s: 26.073569536209106
  time_total_s: 3753.3983402252197
  timers:
    learn_throughput: 8666.07
    learn_time_ms: 18669.593
    sample_throughput: 23778.571
    sample_time_ms: 6804.109
    update_time_ms: 36.814
  timestamp: 1602791494
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    146 |           3753.4 | 23621632 |  290.255 |              334.747 |              107.323 |            790.162 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3294.7051034022684
    time_step_min: 3008
  date: 2020-10-15_19-52-01
  done: false
  episode_len_mean: 790.0268802877889
  episode_reward_max: 335.8080808080811
  episode_reward_mean: 290.4162570344037
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 30022
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.315576319556443e-20
        cur_lr: 5.0e-05
        entropy: 0.10519168712198734
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00757261308899615
        total_loss: .inf
        vf_explained_var: 0.9937124252319336
        vf_loss: 2.5897757013638816
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.94666666666667
    gpu_util_percent0: 0.31900000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146914354541396
    mean_env_wait_ms: 1.2076876807250396
    mean_inference_ms: 4.320171194794162
    mean_raw_obs_processing_ms: 0.38010811680819456
  time_since_restore: 3779.5946640968323
  time_this_iter_s: 26.19632387161255
  time_total_s: 3779.5946640968323
  timers:
    learn_throughput: 8653.34
    learn_time_ms: 18697.058
    sample_throughput: 23652.337
    sample_time_ms: 6840.423
    update_time_ms: 38.539
  timestamp: 1602791521
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    147 |          3779.59 | 23783424 |  290.416 |              335.808 |              107.323 |            790.027 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3293.5477831532903
    time_step_min: 3008
  date: 2020-10-15_19-52-27
  done: false
  episode_len_mean: 789.8738252812707
  episode_reward_max: 335.8080808080811
  episode_reward_mean: 290.5957740876668
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 30220
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.473364479334666e-20
        cur_lr: 5.0e-05
        entropy: 0.11740257715185483
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007038883496231089
        total_loss: .inf
        vf_explained_var: 0.9942283630371094
        vf_loss: 2.4738633235295615
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.583333333333332
    gpu_util_percent0: 0.28566666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690811862180725
    mean_env_wait_ms: 1.2076570103961473
    mean_inference_ms: 4.3198000696999586
    mean_raw_obs_processing_ms: 0.38008626130015266
  time_since_restore: 3805.3734061717987
  time_this_iter_s: 25.77874207496643
  time_total_s: 3805.3734061717987
  timers:
    learn_throughput: 8645.271
    learn_time_ms: 18714.509
    sample_throughput: 23596.23
    sample_time_ms: 6856.689
    update_time_ms: 38.541
  timestamp: 1602791547
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    148 |          3805.37 | 23945216 |  290.596 |              335.808 |              107.323 |            789.874 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3292.1260023662417
    time_step_min: 3006
  date: 2020-10-15_19-52-54
  done: false
  episode_len_mean: 789.6876599934361
  episode_reward_max: 335.8080808080811
  episode_reward_mean: 290.81056213596406
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 30470
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.210046719001998e-20
        cur_lr: 5.0e-05
        entropy: 0.11123882172008355
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007647786148784992
        total_loss: .inf
        vf_explained_var: 0.9939963221549988
        vf_loss: 3.107192556063334
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49666666666667
    gpu_util_percent0: 0.32866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469014979724348
    mean_env_wait_ms: 1.2076188572942785
    mean_inference_ms: 4.31936378550922
    mean_raw_obs_processing_ms: 0.38005924338422264
  time_since_restore: 3831.1531434059143
  time_this_iter_s: 25.7797372341156
  time_total_s: 3831.1531434059143
  timers:
    learn_throughput: 8651.183
    learn_time_ms: 18701.719
    sample_throughput: 23531.146
    sample_time_ms: 6875.653
    update_time_ms: 40.655
  timestamp: 1602791574
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    149 |          3831.15 | 24107008 |  290.811 |              335.808 |              107.323 |            789.688 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3291.060533795041
    time_step_min: 3006
  date: 2020-10-15_19-53-20
  done: false
  episode_len_mean: 789.5519851238051
  episode_reward_max: 335.8080808080811
  episode_reward_mean: 290.9756653739297
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 30653
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.815070078502998e-20
        cur_lr: 5.0e-05
        entropy: 0.09932537687321503
        entropy_coeff: 0.0005000000000000001
        kl: 0.004017101076897234
        model: {}
        policy_loss: -0.006858242481636505
        total_loss: 1.9671262005964916
        vf_explained_var: 0.9950775504112244
        vf_loss: 1.9740340411663055
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01666666666667
    gpu_util_percent0: 0.2956666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468965953031277
    mean_env_wait_ms: 1.2075874284772834
    mean_inference_ms: 4.319019084289533
    mean_raw_obs_processing_ms: 0.38003927850177255
  time_since_restore: 3857.0357410907745
  time_this_iter_s: 25.88259768486023
  time_total_s: 3857.0357410907745
  timers:
    learn_throughput: 8635.942
    learn_time_ms: 18734.725
    sample_throughput: 23506.064
    sample_time_ms: 6882.99
    update_time_ms: 38.767
  timestamp: 1602791600
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    150 |          3857.04 | 24268800 |  290.976 |              335.808 |              107.323 |            789.552 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3289.839385973453
    time_step_min: 3006
  date: 2020-10-15_19-53-47
  done: false
  episode_len_mean: 789.3938097553071
  episode_reward_max: 337.32323232323165
  episode_reward_mean: 291.1581591314211
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 202
  episodes_total: 30855
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.907535039251499e-20
        cur_lr: 5.0e-05
        entropy: 0.11092333309352398
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007509582967031747
        total_loss: .inf
        vf_explained_var: 0.9938502907752991
        vf_loss: 2.6425159573554993
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.18666666666667
    gpu_util_percent0: 0.295
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689044377789817
    mean_env_wait_ms: 1.2075557817924367
    mean_inference_ms: 4.3186536601743795
    mean_raw_obs_processing_ms: 0.3800182993708089
  time_since_restore: 3882.788875102997
  time_this_iter_s: 25.75313401222229
  time_total_s: 3882.788875102997
  timers:
    learn_throughput: 8636.589
    learn_time_ms: 18733.321
    sample_throughput: 23467.825
    sample_time_ms: 6894.205
    update_time_ms: 36.675
  timestamp: 1602791627
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    151 |          3882.79 | 24430592 |  291.158 |              337.323 |              107.323 |            789.394 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3288.3449441457683
    time_step_min: 2998
  date: 2020-10-15_19-54-13
  done: false
  episode_len_mean: 789.2206076193538
  episode_reward_max: 337.32323232323165
  episode_reward_mean: 291.3849441205171
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 31105
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.861302558877249e-20
        cur_lr: 5.0e-05
        entropy: 0.11103206252058347
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006378417805535719
        total_loss: .inf
        vf_explained_var: 0.9954283833503723
        vf_loss: 2.3021114667256675
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.473333333333333
    gpu_util_percent0: 0.35200000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468843202653166
    mean_env_wait_ms: 1.2075169965726225
    mean_inference_ms: 4.31824011092403
    mean_raw_obs_processing_ms: 0.3799923674023664
  time_since_restore: 3908.5845232009888
  time_this_iter_s: 25.795648097991943
  time_total_s: 3908.5845232009888
  timers:
    learn_throughput: 8637.869
    learn_time_ms: 18730.546
    sample_throughput: 23463.468
    sample_time_ms: 6895.485
    update_time_ms: 37.463
  timestamp: 1602791653
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    152 |          3908.58 | 24592384 |  291.385 |              337.323 |              107.323 |            789.221 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3287.255321191947
    time_step_min: 2998
  date: 2020-10-15_19-54-39
  done: false
  episode_len_mean: 789.0897874380694
  episode_reward_max: 337.32323232323165
  episode_reward_mean: 291.55067697915706
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 31285
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.791953838315871e-20
        cur_lr: 5.0e-05
        entropy: 0.09672314735750358
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007727806668602473
        total_loss: .inf
        vf_explained_var: 0.9944671988487244
        vf_loss: 2.1501700381437936
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.290000000000003
    gpu_util_percent0: 0.3366666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468795669261288
    mean_env_wait_ms: 1.2074854177564365
    mean_inference_ms: 4.317911676027275
    mean_raw_obs_processing_ms: 0.3799730652353594
  time_since_restore: 3934.4588742256165
  time_this_iter_s: 25.874351024627686
  time_total_s: 3934.4588742256165
  timers:
    learn_throughput: 8640.39
    learn_time_ms: 18725.08
    sample_throughput: 23434.088
    sample_time_ms: 6904.13
    update_time_ms: 36.064
  timestamp: 1602791679
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    153 |          3934.46 | 24754176 |  291.551 |              337.323 |              107.323 |             789.09 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3286.0484322330344
    time_step_min: 2981
  date: 2020-10-15_19-55-06
  done: false
  episode_len_mean: 788.9502032520326
  episode_reward_max: 337.9292929292926
  episode_reward_mean: 291.73482635039414
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 203
  episodes_total: 31488
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.318793075747381e-19
        cur_lr: 5.0e-05
        entropy: 0.10592580089966457
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008880749427286597
        total_loss: .inf
        vf_explained_var: 0.9946599006652832
        vf_loss: 2.2923084497451782
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.443333333333328
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687363487595578
    mean_env_wait_ms: 1.2074536716824402
    mean_inference_ms: 4.317562748900754
    mean_raw_obs_processing_ms: 0.3799528430609279
  time_since_restore: 3960.079684495926
  time_this_iter_s: 25.62081027030945
  time_total_s: 3960.079684495926
  timers:
    learn_throughput: 8649.393
    learn_time_ms: 18705.589
    sample_throughput: 23407.02
    sample_time_ms: 6912.114
    update_time_ms: 35.929
  timestamp: 1602791706
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    154 |          3960.08 | 24915968 |  291.735 |              337.929 |              107.323 |             788.95 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3284.4774405250205
    time_step_min: 2981
  date: 2020-10-15_19-55-32
  done: false
  episode_len_mean: 788.7841567935468
  episode_reward_max: 337.9292929292926
  episode_reward_mean: 291.9719010752852
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 31736
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9781896136210717e-19
        cur_lr: 5.0e-05
        entropy: 0.1079423613846302
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00826932019360053
        total_loss: .inf
        vf_explained_var: 0.9953904747962952
        vf_loss: 2.252188265323639
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.66666666666667
    gpu_util_percent0: 0.343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686771415220143
    mean_env_wait_ms: 1.207413777159053
    mean_inference_ms: 4.317161470779191
    mean_raw_obs_processing_ms: 0.3799279322674417
  time_since_restore: 3985.577880859375
  time_this_iter_s: 25.498196363449097
  time_total_s: 3985.577880859375
  timers:
    learn_throughput: 8648.889
    learn_time_ms: 18706.679
    sample_throughput: 23443.846
    sample_time_ms: 6901.257
    update_time_ms: 28.321
  timestamp: 1602791732
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    155 |          3985.58 | 25077760 |  291.972 |              337.929 |              107.323 |            788.784 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3283.2967843137253
    time_step_min: 2981
  date: 2020-10-15_19-55-58
  done: false
  episode_len_mean: 788.6719303192656
  episode_reward_max: 337.9292929292926
  episode_reward_mean: 292.15063977494657
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 31917
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9672844204316066e-19
        cur_lr: 5.0e-05
        entropy: 0.09686230433483918
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005736992956371978
        total_loss: .inf
        vf_explained_var: 0.9958160519599915
        vf_loss: 1.5762120286623638
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83666666666667
    gpu_util_percent0: 0.38199999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686317649021663
    mean_env_wait_ms: 1.2073817742386965
    mean_inference_ms: 4.3168455758025885
    mean_raw_obs_processing_ms: 0.3799089661506
  time_since_restore: 4011.407363176346
  time_this_iter_s: 25.829482316970825
  time_total_s: 4011.407363176346
  timers:
    learn_throughput: 8648.786
    learn_time_ms: 18706.903
    sample_throughput: 23527.553
    sample_time_ms: 6876.703
    update_time_ms: 27.557
  timestamp: 1602791758
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    156 |          4011.41 | 25239552 |  292.151 |              337.929 |              107.323 |            788.672 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3282.0614204651743
    time_step_min: 2981
  date: 2020-10-15_19-56-25
  done: false
  episode_len_mean: 788.5455224810064
  episode_reward_max: 338.08080808080854
  episode_reward_mean: 292.33482382675925
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 199
  episodes_total: 32116
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.450926630647411e-19
        cur_lr: 5.0e-05
        entropy: 0.09967237276335557
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006747314100114939
        total_loss: .inf
        vf_explained_var: 0.9943490624427795
        vf_loss: 2.405303478240967
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.273333333333337
    gpu_util_percent0: 0.2746666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685755139321524
    mean_env_wait_ms: 1.2073503396050962
    mean_inference_ms: 4.316512176956349
    mean_raw_obs_processing_ms: 0.379889613944837
  time_since_restore: 4037.073112010956
  time_this_iter_s: 25.665748834609985
  time_total_s: 4037.073112010956
  timers:
    learn_throughput: 8657.86
    learn_time_ms: 18687.297
    sample_throughput: 23640.928
    sample_time_ms: 6843.725
    update_time_ms: 26.697
  timestamp: 1602791785
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 19:56:26,101	WARNING util.py:136 -- The `process_trial` operation took 0.5379841327667236 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    157 |          4037.07 | 25401344 |  292.335 |              338.081 |              107.323 |            788.546 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3280.4994895907444
    time_step_min: 2981
  date: 2020-10-15_19-56-52
  done: false
  episode_len_mean: 788.3764404213908
  episode_reward_max: 338.08080808080854
  episode_reward_mean: 292.58618967955056
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 253
  episodes_total: 32369
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.676389945971116e-19
        cur_lr: 5.0e-05
        entropy: 0.1021429927398761
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004416724143084139
        total_loss: .inf
        vf_explained_var: 0.995029628276825
        vf_loss: 2.417747378349304
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14333333333334
    gpu_util_percent0: 0.31899999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468516517173329
    mean_env_wait_ms: 1.2073068649401426
    mean_inference_ms: 4.31611921411674
    mean_raw_obs_processing_ms: 0.37986452941505006
  time_since_restore: 4062.923909187317
  time_this_iter_s: 25.850797176361084
  time_total_s: 4062.923909187317
  timers:
    learn_throughput: 8648.75
    learn_time_ms: 18706.981
    sample_throughput: 23695.626
    sample_time_ms: 6827.927
    update_time_ms: 27.953
  timestamp: 1602791812
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    158 |          4062.92 | 25563136 |  292.586 |              338.081 |              107.323 |            788.376 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3279.4364290768144
    time_step_min: 2979
  date: 2020-10-15_19-57-18
  done: false
  episode_len_mean: 788.2621278687517
  episode_reward_max: 338.2323232323229
  episode_reward_mean: 292.7380241941364
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 32549
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0014584918956672e-18
        cur_lr: 5.0e-05
        entropy: 0.1013258130600055
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007982372420277292
        total_loss: .inf
        vf_explained_var: 0.9949772357940674
        vf_loss: 1.9787006676197052
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.416666666666668
    gpu_util_percent0: 0.3086666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684737305927728
    mean_env_wait_ms: 1.2072755950992975
    mean_inference_ms: 4.315818318306654
    mean_raw_obs_processing_ms: 0.37984631828973814
  time_since_restore: 4088.745746612549
  time_this_iter_s: 25.821837425231934
  time_total_s: 4088.745746612549
  timers:
    learn_throughput: 8636.015
    learn_time_ms: 18734.567
    sample_throughput: 23777.545
    sample_time_ms: 6804.403
    update_time_ms: 26.091
  timestamp: 1602791838
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    159 |          4088.75 | 25724928 |  292.738 |              338.232 |              107.323 |            788.262 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3278.3859793940505
    time_step_min: 2979
  date: 2020-10-15_19-57-44
  done: false
  episode_len_mean: 788.1336447742053
  episode_reward_max: 338.2323232323229
  episode_reward_mean: 292.898121084436
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 202
  episodes_total: 32751
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5021877378435013e-18
        cur_lr: 5.0e-05
        entropy: 0.10545332233111064
        entropy_coeff: 0.0005000000000000001
        kl: 0.003950495466900368
        model: {}
        policy_loss: -0.007360211015717748
        total_loss: 2.6061500310897827
        vf_explained_var: 0.9939563274383545
        vf_loss: 2.6135629415512085
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91
    gpu_util_percent0: 0.33833333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468417738777723
    mean_env_wait_ms: 1.2072430223260848
    mean_inference_ms: 4.3154943710990405
    mean_raw_obs_processing_ms: 0.37982724308795174
  time_since_restore: 4114.547283172607
  time_this_iter_s: 25.801536560058594
  time_total_s: 4114.547283172607
  timers:
    learn_throughput: 8644.55
    learn_time_ms: 18716.07
    sample_throughput: 23774.69
    sample_time_ms: 6805.22
    update_time_ms: 34.761
  timestamp: 1602791864
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    160 |          4114.55 | 25886720 |  292.898 |              338.232 |              107.323 |            788.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3276.851617013531
    time_step_min: 2977
  date: 2020-10-15_19-58-11
  done: false
  episode_len_mean: 787.9486122894194
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 293.1311417410072
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 253
  episodes_total: 33004
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.5109386892175065e-19
        cur_lr: 5.0e-05
        entropy: 0.09505638542274635
        entropy_coeff: 0.0005000000000000001
        kl: 0.003347692972359558
        model: {}
        policy_loss: -0.006737465246866729
        total_loss: 1.9355831642945607
        vf_explained_var: 0.9958307147026062
        vf_loss: 1.9423681596914928
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.456666666666667
    gpu_util_percent0: 0.3433333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683614097945855
    mean_env_wait_ms: 1.2071998863229505
    mean_inference_ms: 4.315111572173985
    mean_raw_obs_processing_ms: 0.37980239326592685
  time_since_restore: 4140.301951408386
  time_this_iter_s: 25.75466823577881
  time_total_s: 4140.301951408386
  timers:
    learn_throughput: 8645.952
    learn_time_ms: 18713.034
    sample_throughput: 23802.428
    sample_time_ms: 6797.29
    update_time_ms: 35.19
  timestamp: 1602791891
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    161 |           4140.3 | 26048512 |  293.131 |              338.535 |              107.323 |            787.949 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3275.7248423403034
    time_step_min: 2977
  date: 2020-10-15_19-58-37
  done: false
  episode_len_mean: 787.8079438266582
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 293.3112747582506
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 33183
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7554693446087532e-19
        cur_lr: 5.0e-05
        entropy: 0.08577884547412395
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006336007616482675
        total_loss: .inf
        vf_explained_var: 0.9968765377998352
        vf_loss: 1.0917055110136669
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36206896551724
    gpu_util_percent0: 0.3324137931034483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683201808941077
    mean_env_wait_ms: 1.2071686655549827
    mean_inference_ms: 4.3148251077275335
    mean_raw_obs_processing_ms: 0.3797848592651393
  time_since_restore: 4165.811462163925
  time_this_iter_s: 25.50951075553894
  time_total_s: 4165.811462163925
  timers:
    learn_throughput: 8654.071
    learn_time_ms: 18695.478
    sample_throughput: 23832.559
    sample_time_ms: 6788.696
    update_time_ms: 33.907
  timestamp: 1602791917
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 19:58:38,215	WARNING util.py:136 -- The `process_trial` operation took 0.5042881965637207 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    162 |          4165.81 | 26210304 |  293.311 |              338.535 |              107.323 |            787.808 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3274.4381766381766
    time_step_min: 2977
  date: 2020-10-15_19-59-03
  done: false
  episode_len_mean: 787.6510617905172
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 293.5111606676886
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 204
  episodes_total: 33387
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.633204016913128e-19
        cur_lr: 5.0e-05
        entropy: 0.09556969131032626
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006878918095026165
        total_loss: .inf
        vf_explained_var: 0.9951637387275696
        vf_loss: 1.9685743550459545
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53666666666667
    gpu_util_percent0: 0.3883333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682674209546226
    mean_env_wait_ms: 1.2071347834959552
    mean_inference_ms: 4.3145125578151715
    mean_raw_obs_processing_ms: 0.37976694044223985
  time_since_restore: 4190.984738349915
  time_this_iter_s: 25.17327618598938
  time_total_s: 4190.984738349915
  timers:
    learn_throughput: 8682.415
    learn_time_ms: 18634.447
    sample_throughput: 23862.836
    sample_time_ms: 6780.082
    update_time_ms: 33.92
  timestamp: 1602791943
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    163 |          4190.98 | 26372096 |  293.511 |              338.535 |              107.323 |            787.651 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3272.7363768935447
    time_step_min: 2977
  date: 2020-10-15_19-59-29
  done: false
  episode_len_mean: 787.4622952768779
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 293.76596119023964
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 256
  episodes_total: 33643
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.449806025369691e-19
        cur_lr: 5.0e-05
        entropy: 0.09203117651244004
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035511742850455144
        model: {}
        policy_loss: -0.006840375350899801
        total_loss: 1.7900392214457195
        vf_explained_var: 0.9959037899971008
        vf_loss: 1.7969256142775218
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.003448275862073
    gpu_util_percent0: 0.33689655172413796
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468209875512359
    mean_env_wait_ms: 1.2070896785686425
    mean_inference_ms: 4.314123802217644
    mean_raw_obs_processing_ms: 0.37974132496453966
  time_since_restore: 4216.423209428787
  time_this_iter_s: 25.43847107887268
  time_total_s: 4216.423209428787
  timers:
    learn_throughput: 8684.969
    learn_time_ms: 18628.968
    sample_throughput: 23910.276
    sample_time_ms: 6766.63
    update_time_ms: 33.707
  timestamp: 1602791969
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    164 |          4216.42 | 26533888 |  293.766 |              338.535 |              107.323 |            787.462 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3271.5773968141175
    time_step_min: 2977
  date: 2020-10-15_19-59-55
  done: false
  episode_len_mean: 787.3390702625976
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 293.9373463162497
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 173
  episodes_total: 33816
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2249030126848455e-19
        cur_lr: 5.0e-05
        entropy: 0.0845210583259662
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007521418442289966
        total_loss: .inf
        vf_explained_var: 0.9961356520652771
        vf_loss: 1.3544589777787526
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.053333333333327
    gpu_util_percent0: 0.3259999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681699145950494
    mean_env_wait_ms: 1.2070613188175747
    mean_inference_ms: 4.313864601881689
    mean_raw_obs_processing_ms: 0.3797259564546299
  time_since_restore: 4242.062420368195
  time_this_iter_s: 25.63921093940735
  time_total_s: 4242.062420368195
  timers:
    learn_throughput: 8681.641
    learn_time_ms: 18636.108
    sample_throughput: 23892.798
    sample_time_ms: 6771.58
    update_time_ms: 33.956
  timestamp: 1602791995
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    165 |          4242.06 | 26695680 |  293.937 |              338.535 |              107.323 |            787.339 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3270.160502545091
    time_step_min: 2977
  date: 2020-10-15_20-00-22
  done: false
  episode_len_mean: 787.1827852713861
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 294.1565839119396
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 213
  episodes_total: 34029
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.3373545190272695e-19
        cur_lr: 5.0e-05
        entropy: 0.08752713600794475
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007710177082723628
        total_loss: .inf
        vf_explained_var: 0.9960160255432129
        vf_loss: 1.5332024296124775
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.283333333333335
    gpu_util_percent0: 0.31666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468117366380995
    mean_env_wait_ms: 1.2070255387910678
    mean_inference_ms: 4.313559321035097
    mean_raw_obs_processing_ms: 0.3797081069634297
  time_since_restore: 4267.686792850494
  time_this_iter_s: 25.624372482299805
  time_total_s: 4267.686792850494
  timers:
    learn_throughput: 8696.236
    learn_time_ms: 18604.83
    sample_throughput: 23856.006
    sample_time_ms: 6782.024
    update_time_ms: 33.271
  timestamp: 1602792022
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    166 |          4267.69 | 26857472 |  294.157 |              338.535 |              107.323 |            787.183 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3268.483482781786
    time_step_min: 2977
  date: 2020-10-15_20-00-48
  done: false
  episode_len_mean: 787.0036465474489
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 294.41377808541375
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 34279
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.506031778540903e-19
        cur_lr: 5.0e-05
        entropy: 0.08426188429196675
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008286781634524232
        total_loss: .inf
        vf_explained_var: 0.9968149065971375
        vf_loss: 1.3580797215302784
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.389999999999993
    gpu_util_percent0: 0.282
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680665578714566
    mean_env_wait_ms: 1.2069812840581922
    mean_inference_ms: 4.313190760553574
    mean_raw_obs_processing_ms: 0.3796840944983966
  time_since_restore: 4293.324706077576
  time_this_iter_s: 25.6379132270813
  time_total_s: 4293.324706077576
  timers:
    learn_throughput: 8698.266
    learn_time_ms: 18600.49
    sample_throughput: 23883.535
    sample_time_ms: 6774.207
    update_time_ms: 33.048
  timestamp: 1602792048
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    167 |          4293.32 | 27019264 |  294.414 |              338.535 |              107.323 |            787.004 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3267.2717893574354
    time_step_min: 2977
  date: 2020-10-15_20-01-14
  done: false
  episode_len_mean: 786.8813967664219
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 294.59713385927427
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 172
  episodes_total: 34451
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4259047667811353e-18
        cur_lr: 5.0e-05
        entropy: 0.07425098928312461
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0048502285305100186
        total_loss: .inf
        vf_explained_var: 0.9968369007110596
        vf_loss: 1.0270089358091354
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.093103448275862
    gpu_util_percent0: 0.34827586206896555
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146802590857702
    mean_env_wait_ms: 1.2069537906220356
    mean_inference_ms: 4.312942411998623
    mean_raw_obs_processing_ms: 0.37966888900213924
  time_since_restore: 4319.103118896484
  time_this_iter_s: 25.77841281890869
  time_total_s: 4319.103118896484
  timers:
    learn_throughput: 8704.225
    learn_time_ms: 18587.755
    sample_throughput: 23858.302
    sample_time_ms: 6781.371
    update_time_ms: 32.075
  timestamp: 1602792074
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    168 |           4319.1 | 27181056 |  294.597 |              338.535 |              107.323 |            786.881 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3265.7848273073814
    time_step_min: 2977
  date: 2020-10-15_20-01-41
  done: false
  episode_len_mean: 786.7300259590425
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 294.82146675873236
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 219
  episodes_total: 34670
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.138857150171704e-18
        cur_lr: 5.0e-05
        entropy: 0.08875373564660549
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005408964539431811
        total_loss: .inf
        vf_explained_var: 0.996796190738678
        vf_loss: 1.2218366066614788
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.636666666666667
    gpu_util_percent0: 0.364
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467975874820191
    mean_env_wait_ms: 1.2069183143781999
    mean_inference_ms: 4.312641638046613
    mean_raw_obs_processing_ms: 0.37965132003667557
  time_since_restore: 4344.793364524841
  time_this_iter_s: 25.690245628356934
  time_total_s: 4344.793364524841
  timers:
    learn_throughput: 8712.357
    learn_time_ms: 18570.405
    sample_throughput: 23846.051
    sample_time_ms: 6784.855
    update_time_ms: 32.334
  timestamp: 1602792101
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    169 |          4344.79 | 27342848 |  294.821 |              338.535 |              107.323 |             786.73 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3264.2436408476956
    time_step_min: 2977
  date: 2020-10-15_20-02-07
  done: false
  episode_len_mean: 786.5584166356372
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 295.05343440997774
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 34913
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.208285725257555e-18
        cur_lr: 5.0e-05
        entropy: 0.09021621259550254
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009025743221476054
        total_loss: .inf
        vf_explained_var: 0.9962224364280701
        vf_loss: 1.5759497284889221
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.859999999999996
    gpu_util_percent0: 0.35233333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679259427985852
    mean_env_wait_ms: 1.2068736853073354
    mean_inference_ms: 4.312289182110495
    mean_raw_obs_processing_ms: 0.37962804311304665
  time_since_restore: 4370.421181678772
  time_this_iter_s: 25.627817153930664
  time_total_s: 4370.421181678772
  timers:
    learn_throughput: 8721.049
    learn_time_ms: 18551.896
    sample_throughput: 23810.957
    sample_time_ms: 6794.855
    update_time_ms: 23.347
  timestamp: 1602792127
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    170 |          4370.42 | 27504640 |  295.053 |              338.535 |              107.323 |            786.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3263.185390212584
    time_step_min: 2977
  date: 2020-10-15_20-02-33
  done: false
  episode_len_mean: 786.4312138398838
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 295.21221995656964
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 174
  episodes_total: 35087
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.812428587886333e-18
        cur_lr: 5.0e-05
        entropy: 0.07976365399857362
        entropy_coeff: 0.0005000000000000001
        kl: 0.00366802253605177
        model: {}
        policy_loss: -0.006971085036639124
        total_loss: 1.4701199034849803
        vf_explained_var: 0.9956750273704529
        vf_loss: 1.4771308600902557
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.833333333333336
    gpu_util_percent0: 0.35799999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678864299168545
    mean_env_wait_ms: 1.2068455718329487
    mean_inference_ms: 4.312046911379635
    mean_raw_obs_processing_ms: 0.379613338799992
  time_since_restore: 4396.087820768356
  time_this_iter_s: 25.66663908958435
  time_total_s: 4396.087820768356
  timers:
    learn_throughput: 8726.692
    learn_time_ms: 18539.901
    sample_throughput: 23770.79
    sample_time_ms: 6806.337
    update_time_ms: 23.072
  timestamp: 1602792153
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    171 |          4396.09 | 27666432 |  295.212 |              338.535 |              107.323 |            786.431 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3261.6971226080796
    time_step_min: 2977
  date: 2020-10-15_20-02-59
  done: false
  episode_len_mean: 786.2670385366821
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 295.4484548746518
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 230
  episodes_total: 35317
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4062142939431666e-18
        cur_lr: 5.0e-05
        entropy: 0.08528470806777477
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008969472439882034
        total_loss: .inf
        vf_explained_var: 0.9965043663978577
        vf_loss: 1.345561573902766
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.033333333333328
    gpu_util_percent0: 0.33933333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678393161918177
    mean_env_wait_ms: 1.2068077280552196
    mean_inference_ms: 4.311743869225438
    mean_raw_obs_processing_ms: 0.37959528786934876
  time_since_restore: 4421.792438030243
  time_this_iter_s: 25.704617261886597
  time_total_s: 4421.792438030243
  timers:
    learn_throughput: 8723.767
    learn_time_ms: 18546.116
    sample_throughput: 23759.124
    sample_time_ms: 6809.679
    update_time_ms: 22.917
  timestamp: 1602792179
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    172 |          4421.79 | 27828224 |  295.448 |              338.535 |              107.323 |            786.267 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3260.0833145561064
    time_step_min: 2977
  date: 2020-10-15_20-03-26
  done: false
  episode_len_mean: 786.1029370393293
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 295.68734949790485
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 229
  episodes_total: 35546
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6093214409147504e-18
        cur_lr: 5.0e-05
        entropy: 0.0793208722025156
        entropy_coeff: 0.0005000000000000001
        kl: 0.003940202567415933
        model: {}
        policy_loss: -0.007988071748210738
        total_loss: 0.8081584026416143
        vf_explained_var: 0.9978909492492676
        vf_loss: 0.8161861250797907
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.00344827586207
    gpu_util_percent0: 0.31172413793103443
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677897121565883
    mean_env_wait_ms: 1.206766639160088
    mean_inference_ms: 4.311423771401834
    mean_raw_obs_processing_ms: 0.3795735986494897
  time_since_restore: 4447.548758983612
  time_this_iter_s: 25.75632095336914
  time_total_s: 4447.548758983612
  timers:
    learn_throughput: 8699.482
    learn_time_ms: 18597.889
    sample_throughput: 23739.58
    sample_time_ms: 6815.285
    update_time_ms: 23.518
  timestamp: 1602792206
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: f76cb_00000
  
2020-10-15 20:03:27,081	WARNING util.py:136 -- The `process_trial` operation took 0.5042390823364258 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    173 |          4447.55 | 27990016 |  295.687 |              338.535 |              107.323 |            786.103 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3258.9033153042064
    time_step_min: 2977
  date: 2020-10-15_20-03-52
  done: false
  episode_len_mean: 785.9731280615815
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 295.86734242353555
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 35725
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8046607204573752e-18
        cur_lr: 5.0e-05
        entropy: 0.07456168097754319
        entropy_coeff: 0.0005000000000000001
        kl: 0.003188609960488975
        model: {}
        policy_loss: -0.008436275733401999
        total_loss: 0.9749392569065094
        vf_explained_var: 0.9969392418861389
        vf_loss: 0.9834128171205521
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.219999999999995
    gpu_util_percent0: 0.30799999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467750288966979
    mean_env_wait_ms: 1.2067374233514172
    mean_inference_ms: 4.311182217076962
    mean_raw_obs_processing_ms: 0.37955944595283553
  time_since_restore: 4473.405352592468
  time_this_iter_s: 25.8565936088562
  time_total_s: 4473.405352592468
  timers:
    learn_throughput: 8680.561
    learn_time_ms: 18638.426
    sample_throughput: 23740.589
    sample_time_ms: 6814.995
    update_time_ms: 24.322
  timestamp: 1602792232
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    174 |          4473.41 | 28151808 |  295.867 |              338.535 |              107.323 |            785.973 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3257.3448496659244
    time_step_min: 2977
  date: 2020-10-15_20-04-19
  done: false
  episode_len_mean: 785.8039597352762
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 296.1064260310687
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 237
  episodes_total: 35962
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.023303602286876e-19
        cur_lr: 5.0e-05
        entropy: 0.08310714984933536
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006980855236179195
        total_loss: .inf
        vf_explained_var: 0.9974549412727356
        vf_loss: 0.974156399567922
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82
    gpu_util_percent0: 0.3353333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677033488994087
    mean_env_wait_ms: 1.2066965739712843
    mean_inference_ms: 4.310876657162953
    mean_raw_obs_processing_ms: 0.3795404055676478
  time_since_restore: 4499.31022310257
  time_this_iter_s: 25.90487051010132
  time_total_s: 4499.31022310257
  timers:
    learn_throughput: 8670.906
    learn_time_ms: 18659.181
    sample_throughput: 23724.656
    sample_time_ms: 6819.572
    update_time_ms: 23.887
  timestamp: 1602792259
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:04:20,191	WARNING util.py:136 -- The `process_trial` operation took 0.5227911472320557 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    175 |          4499.31 | 28313600 |  296.106 |              338.535 |              107.323 |            785.804 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3255.981957550433
    time_step_min: 2977
  date: 2020-10-15_20-04-46
  done: false
  episode_len_mean: 785.6621797175156
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 296.32076311918206
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 36179
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.353495540343031e-18
        cur_lr: 5.0e-05
        entropy: 0.08304468045632045
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008309914371542012
        total_loss: .inf
        vf_explained_var: 0.9976362586021423
        vf_loss: 0.8570271333058676
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.913333333333334
    gpu_util_percent0: 0.30333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676572069377444
    mean_env_wait_ms: 1.206658833639937
    mean_inference_ms: 4.310585700733325
    mean_raw_obs_processing_ms: 0.37952045177437094
  time_since_restore: 4525.131468772888
  time_this_iter_s: 25.821245670318604
  time_total_s: 4525.131468772888
  timers:
    learn_throughput: 8660.833
    learn_time_ms: 18680.882
    sample_throughput: 23741.146
    sample_time_ms: 6814.835
    update_time_ms: 25.007
  timestamp: 1602792286
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    176 |          4525.13 | 28475392 |  296.321 |              338.535 |              107.323 |            785.662 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3254.7680405275187
    time_step_min: 2977
  date: 2020-10-15_20-05-12
  done: false
  episode_len_mean: 785.5511646453813
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 296.50724721015956
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 36363
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.030243310514547e-18
        cur_lr: 5.0e-05
        entropy: 0.07934969974060853
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006107747753655228
        total_loss: .inf
        vf_explained_var: 0.9974973797798157
        vf_loss: 0.8062567611535391
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90666666666667
    gpu_util_percent0: 0.323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676169257275154
    mean_env_wait_ms: 1.2066290300374505
    mean_inference_ms: 4.310343006988511
    mean_raw_obs_processing_ms: 0.379506103688514
  time_since_restore: 4551.009641170502
  time_this_iter_s: 25.878172397613525
  time_total_s: 4551.009641170502
  timers:
    learn_throughput: 8650.056
    learn_time_ms: 18704.157
    sample_throughput: 23710.882
    sample_time_ms: 6823.534
    update_time_ms: 24.245
  timestamp: 1602792312
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:05:13,333	WARNING util.py:136 -- The `process_trial` operation took 0.5207364559173584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    177 |          4551.01 | 28637184 |  296.507 |              338.535 |              107.323 |            785.551 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3253.184120559035
    time_step_min: 2977
  date: 2020-10-15_20-05-39
  done: false
  episode_len_mean: 785.4075399535583
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 296.74307478555534
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 242
  episodes_total: 36605
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0453649657718204e-18
        cur_lr: 5.0e-05
        entropy: 0.08954593911767006
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009820457567305615
        total_loss: .inf
        vf_explained_var: 0.9975438117980957
        vf_loss: 0.9755459676186243
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683870967741942
    gpu_util_percent0: 0.3629032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675720901913877
    mean_env_wait_ms: 1.2065872886325804
    mean_inference_ms: 4.31004796170208
    mean_raw_obs_processing_ms: 0.37948737910470426
  time_since_restore: 4576.983513832092
  time_this_iter_s: 25.973872661590576
  time_total_s: 4576.983513832092
  timers:
    learn_throughput: 8644.277
    learn_time_ms: 18716.66
    sample_throughput: 23729.328
    sample_time_ms: 6818.229
    update_time_ms: 26.209
  timestamp: 1602792339
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:05:40,106	WARNING util.py:136 -- The `process_trial` operation took 0.5440480709075928 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    178 |          4576.98 | 28798976 |  296.743 |              338.535 |              107.323 |            785.408 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3251.8891729446
    time_step_min: 2977
  date: 2020-10-15_20-06-05
  done: false
  episode_len_mean: 785.2913259623482
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 296.9405856670533
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 206
  episodes_total: 36811
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.56804744865773e-18
        cur_lr: 5.0e-05
        entropy: 0.07960606490572293
        entropy_coeff: 0.0005000000000000001
        kl: 0.003937847912311554
        model: {}
        policy_loss: -0.009116420902804142
        total_loss: 0.7009060382843018
        vf_explained_var: 0.9980149269104004
        vf_loss: 0.710062250494957
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.443333333333335
    gpu_util_percent0: 0.32500000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675286420365097
    mean_env_wait_ms: 1.2065507109303464
    mean_inference_ms: 4.309774558620679
    mean_raw_obs_processing_ms: 0.37946866811447905
  time_since_restore: 4602.5535364151
  time_this_iter_s: 25.570022583007812
  time_total_s: 4602.5535364151
  timers:
    learn_throughput: 8649.049
    learn_time_ms: 18706.334
    sample_throughput: 23742.839
    sample_time_ms: 6814.349
    update_time_ms: 27.615
  timestamp: 1602792365
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: f76cb_00000
  
2020-10-15 20:06:06,544	WARNING util.py:136 -- The `process_trial` operation took 0.5020127296447754 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    179 |          4602.55 | 28960768 |  296.941 |              338.535 |              107.323 |            785.291 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3250.67053634248
    time_step_min: 2977
  date: 2020-10-15_20-06-32
  done: false
  episode_len_mean: 785.1878040869283
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 297.12581949891387
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 185
  episodes_total: 36996
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.284023724328865e-18
        cur_lr: 5.0e-05
        entropy: 0.07662158831954002
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006729563814587891
        total_loss: .inf
        vf_explained_var: 0.9983401298522949
        vf_loss: 0.5200245355566343
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.913333333333338
    gpu_util_percent0: 0.33233333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674896143781768
    mean_env_wait_ms: 1.206520068521841
    mean_inference_ms: 4.309537969277321
    mean_raw_obs_processing_ms: 0.37945453650628236
  time_since_restore: 4628.1893537044525
  time_this_iter_s: 25.635817289352417
  time_total_s: 4628.1893537044525
  timers:
    learn_throughput: 8643.41
    learn_time_ms: 18718.537
    sample_throughput: 23788.67
    sample_time_ms: 6801.221
    update_time_ms: 28.307
  timestamp: 1602792392
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:06:33,110	WARNING util.py:136 -- The `process_trial` operation took 0.5244119167327881 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    180 |          4628.19 | 29122560 |  297.126 |              338.535 |              107.323 |            785.188 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3249.0354301075267
    time_step_min: 2977
  date: 2020-10-15_20-06-58
  done: false
  episode_len_mean: 785.0435529778207
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 297.36638985309844
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 246
  episodes_total: 37242
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.426035586493297e-18
        cur_lr: 5.0e-05
        entropy: 0.08256434338788192
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035670698077107468
        model: {}
        policy_loss: -0.008851428331884867
        total_loss: 0.530482051273187
        vf_explained_var: 0.9986119270324707
        vf_loss: 0.5393747637669245
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.37
    gpu_util_percent0: 0.3266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674421095881796
    mean_env_wait_ms: 1.2064769755791078
    mean_inference_ms: 4.30924291848451
    mean_raw_obs_processing_ms: 0.37943594922188
  time_since_restore: 4653.949322223663
  time_this_iter_s: 25.759968519210815
  time_total_s: 4653.949322223663
  timers:
    learn_throughput: 8638.486
    learn_time_ms: 18729.208
    sample_throughput: 23822.436
    sample_time_ms: 6791.581
    update_time_ms: 27.821
  timestamp: 1602792418
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: f76cb_00000
  
2020-10-15 20:06:59,611	WARNING util.py:136 -- The `process_trial` operation took 0.5335400104522705 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    181 |          4653.95 | 29284352 |  297.366 |              338.535 |              107.323 |            785.044 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3247.703285832687
    time_step_min: 2977
  date: 2020-10-15_20-07-25
  done: false
  episode_len_mean: 784.9229536653759
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 297.5632611331636
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 203
  episodes_total: 37445
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7130177932466485e-18
        cur_lr: 5.0e-05
        entropy: 0.07609941313664119
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036463728174567223
        model: {}
        policy_loss: -0.008898364137470102
        total_loss: 0.5987362017234167
        vf_explained_var: 0.9982283115386963
        vf_loss: 0.6076726168394089
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193333333333335
    gpu_util_percent0: 0.3303333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674038314794474
    mean_env_wait_ms: 1.2064414304036497
    mean_inference_ms: 4.308985029729493
    mean_raw_obs_processing_ms: 0.37941819835941676
  time_since_restore: 4679.764650344849
  time_this_iter_s: 25.815328121185303
  time_total_s: 4679.764650344849
  timers:
    learn_throughput: 8632.684
    learn_time_ms: 18741.796
    sample_throughput: 23801.414
    sample_time_ms: 6797.579
    update_time_ms: 28.185
  timestamp: 1602792445
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: f76cb_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    182 |          4679.76 | 29446144 |  297.563 |              338.535 |              107.323 |            784.923 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3246.5825219473263
    time_step_min: 2977
  date: 2020-10-15_20-07-51
  done: false
  episode_len_mean: 784.8080091411565
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 297.7362753813647
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 187
  episodes_total: 37632
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.565088966233243e-19
        cur_lr: 5.0e-05
        entropy: 0.07820229853192966
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008453350242537757
        total_loss: .inf
        vf_explained_var: 0.9971358180046082
        vf_loss: 0.9799072245756785
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.536666666666672
    gpu_util_percent0: 0.3603333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673638726542665
    mean_env_wait_ms: 1.2064105314521332
    mean_inference_ms: 4.308754571159586
    mean_raw_obs_processing_ms: 0.3794040252211939
  time_since_restore: 4705.562784194946
  time_this_iter_s: 25.798133850097656
  time_total_s: 4705.562784194946
  timers:
    learn_throughput: 8633.172
    learn_time_ms: 18740.735
    sample_throughput: 23785.882
    sample_time_ms: 6802.018
    update_time_ms: 27.401
  timestamp: 1602792471
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:07:52,632	WARNING util.py:136 -- The `process_trial` operation took 0.5134377479553223 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    183 |          4705.56 | 29607936 |  297.736 |              338.535 |              107.323 |            784.808 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3245.1582313608374
    time_step_min: 2977
  date: 2020-10-15_20-08-18
  done: false
  episode_len_mean: 784.6667282663217
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 297.9601247566347
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 37879
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2847633449349864e-18
        cur_lr: 5.0e-05
        entropy: 0.08569510094821453
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037604314275085926
        model: {}
        policy_loss: -0.008115123957395554
        total_loss: 1.0251929263273876
        vf_explained_var: 0.9974768161773682
        vf_loss: 1.0333508650461833
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.083333333333336
    gpu_util_percent0: 0.34299999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673168062521255
    mean_env_wait_ms: 1.2063673617841781
    mean_inference_ms: 4.308462885781076
    mean_raw_obs_processing_ms: 0.37938495153586116
  time_since_restore: 4731.155614376068
  time_this_iter_s: 25.592830181121826
  time_total_s: 4731.155614376068
  timers:
    learn_throughput: 8648.271
    learn_time_ms: 18708.016
    sample_throughput: 23765.373
    sample_time_ms: 6807.888
    update_time_ms: 27.313
  timestamp: 1602792498
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: f76cb_00000
  
2020-10-15 20:08:18,951	WARNING util.py:136 -- The `process_trial` operation took 0.5523700714111328 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    184 |          4731.16 | 29769728 |   297.96 |              338.535 |              107.323 |            784.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3243.9074273695282
    time_step_min: 2977
  date: 2020-10-15_20-08-44
  done: false
  episode_len_mean: 784.5582372560864
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 298.1490828127906
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 38077
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.423816724674932e-19
        cur_lr: 5.0e-05
        entropy: 0.07309601331750552
        entropy_coeff: 0.0005000000000000001
        kl: 0.002725440562547495
        model: {}
        policy_loss: -0.0061310995876435
        total_loss: 0.6439265459775925
        vf_explained_var: 0.9980595111846924
        vf_loss: 0.6500941862662634
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29666666666667
    gpu_util_percent0: 0.29933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672823169616167
    mean_env_wait_ms: 1.206332624368994
    mean_inference_ms: 4.308224971557856
    mean_raw_obs_processing_ms: 0.3793686995458447
  time_since_restore: 4757.06745839119
  time_this_iter_s: 25.91184401512146
  time_total_s: 4757.06745839119
  timers:
    learn_throughput: 8647.533
    learn_time_ms: 18709.614
    sample_throughput: 23769.847
    sample_time_ms: 6806.607
    update_time_ms: 27.427
  timestamp: 1602792524
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: f76cb_00000
  
2020-10-15 20:08:45,598	WARNING util.py:136 -- The `process_trial` operation took 0.5466077327728271 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    185 |          4757.07 | 29931520 |  298.149 |              338.535 |              107.323 |            784.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3242.681333054306
    time_step_min: 2977
  date: 2020-10-15_20-09-11
  done: false
  episode_len_mean: 784.4501175855762
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 298.3338348206391
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 193
  episodes_total: 38270
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.211908362337466e-19
        cur_lr: 5.0e-05
        entropy: 0.0773587841540575
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006477486274282758
        total_loss: .inf
        vf_explained_var: 0.9987533092498779
        vf_loss: 0.3946249907215436
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193548387096772
    gpu_util_percent0: 0.3135483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672416644546613
    mean_env_wait_ms: 1.2062996052418633
    mean_inference_ms: 4.307991192400249
    mean_raw_obs_processing_ms: 0.37935448032613045
  time_since_restore: 4782.898823022842
  time_this_iter_s: 25.831364631652832
  time_total_s: 4782.898823022842
  timers:
    learn_throughput: 8651.257
    learn_time_ms: 18701.56
    sample_throughput: 23736.174
    sample_time_ms: 6816.263
    update_time_ms: 25.871
  timestamp: 1602792551
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:09:12,305	WARNING util.py:136 -- The `process_trial` operation took 0.5290162563323975 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    186 |           4782.9 | 30093312 |  298.334 |              338.535 |              107.323 |             784.45 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3241.18582529823
    time_step_min: 2977
  date: 2020-10-15_20-09-38
  done: false
  episode_len_mean: 784.3134297359744
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 298.5562365785113
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 249
  episodes_total: 38519
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.817862543506199e-19
        cur_lr: 5.0e-05
        entropy: 0.08589933564265569
        entropy_coeff: 0.0005000000000000001
        kl: 0.003380662160149465
        model: {}
        policy_loss: -0.0074350312012635795
        total_loss: 0.9589120596647263
        vf_explained_var: 0.997611939907074
        vf_loss: 0.9663900136947632
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36666666666667
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671987079799767
    mean_env_wait_ms: 1.2062570344184547
    mean_inference_ms: 4.307717919501028
    mean_raw_obs_processing_ms: 0.37933536406206325
  time_since_restore: 4808.725128173828
  time_this_iter_s: 25.826305150985718
  time_total_s: 4808.725128173828
  timers:
    learn_throughput: 8663.414
    learn_time_ms: 18675.317
    sample_throughput: 23671.508
    sample_time_ms: 6834.883
    update_time_ms: 27.236
  timestamp: 1602792578
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: f76cb_00000
  
2020-10-15 20:09:38,877	WARNING util.py:136 -- The `process_trial` operation took 0.5440475940704346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    187 |          4808.73 | 30255104 |  298.556 |              338.535 |              107.323 |            784.313 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3240.191910832967
    time_step_min: 2977
  date: 2020-10-15_20-10-04
  done: false
  episode_len_mean: 784.2149776549301
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 298.71939017672787
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 192
  episodes_total: 38711
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4089312717530994e-19
        cur_lr: 5.0e-05
        entropy: 0.07324713530639808
        entropy_coeff: 0.0005000000000000001
        kl: 0.003845220955554396
        model: {}
        policy_loss: -0.006261440731274585
        total_loss: 0.8883844961722692
        vf_explained_var: 0.9975171685218811
        vf_loss: 0.8946825514237086
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65
    gpu_util_percent0: 0.36266666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671621312978445
    mean_env_wait_ms: 1.2062228062544311
    mean_inference_ms: 4.307485888701213
    mean_raw_obs_processing_ms: 0.3793201576710975
  time_since_restore: 4834.612402439117
  time_this_iter_s: 25.887274265289307
  time_total_s: 4834.612402439117
  timers:
    learn_throughput: 8672.785
    learn_time_ms: 18655.139
    sample_throughput: 23600.416
    sample_time_ms: 6855.472
    update_time_ms: 26.395
  timestamp: 1602792604
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: f76cb_00000
  
2020-10-15 20:10:05,457	WARNING util.py:136 -- The `process_trial` operation took 0.5104451179504395 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    188 |          4834.61 | 30416896 |  298.719 |              338.535 |              107.323 |            784.215 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3239.037106610741
    time_step_min: 2977
  date: 2020-10-15_20-10-31
  done: false
  episode_len_mean: 784.106521347968
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 298.89896575190767
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 192
  episodes_total: 38903
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2044656358765497e-19
        cur_lr: 5.0e-05
        entropy: 0.07353958239157994
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005474840969933818
        total_loss: .inf
        vf_explained_var: 0.9986533522605896
        vf_loss: 0.43537898113330203
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53225806451613
    gpu_util_percent0: 0.33225806451612905
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671237547779242
    mean_env_wait_ms: 1.206189986023447
    mean_inference_ms: 4.307262601851531
    mean_raw_obs_processing_ms: 0.3793063800859884
  time_since_restore: 4860.818251371384
  time_this_iter_s: 26.205848932266235
  time_total_s: 4860.818251371384
  timers:
    learn_throughput: 8656.671
    learn_time_ms: 18689.863
    sample_throughput: 23503.04
    sample_time_ms: 6883.875
    update_time_ms: 24.51
  timestamp: 1602792631
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:10:32,563	WARNING util.py:136 -- The `process_trial` operation took 0.5481700897216797 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    189 |          4860.82 | 30578688 |  298.899 |              338.535 |              107.323 |            784.107 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3237.714289366707
    time_step_min: 2977
  date: 2020-10-15_20-10-58
  done: false
  episode_len_mean: 783.9773975226664
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 299.07977102141325
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 252
  episodes_total: 39155
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8066984538148245e-19
        cur_lr: 5.0e-05
        entropy: 0.09730780745546024
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010108345159096643
        total_loss: .inf
        vf_explained_var: 0.9958932399749756
        vf_loss: 1.8442263801892598
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.590000000000007
    gpu_util_percent0: 0.3343333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670802718440898
    mean_env_wait_ms: 1.206147642162738
    mean_inference_ms: 4.306993732432695
    mean_raw_obs_processing_ms: 0.3792870938594748
  time_since_restore: 4886.636878728867
  time_this_iter_s: 25.81862735748291
  time_total_s: 4886.636878728867
  timers:
    learn_throughput: 8661.53
    learn_time_ms: 18679.379
    sample_throughput: 23438.467
    sample_time_ms: 6902.841
    update_time_ms: 23.554
  timestamp: 1602792658
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:10:59,121	WARNING util.py:136 -- The `process_trial` operation took 0.5524439811706543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    190 |          4886.64 | 30740480 |   299.08 |              338.535 |              107.323 |            783.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3237.0468944810564
    time_step_min: 2977
  date: 2020-10-15_20-11-24
  done: false
  episode_len_mean: 783.881732455583
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 299.1988846089957
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 188
  episodes_total: 39343
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7100476807222367e-19
        cur_lr: 5.0e-05
        entropy: 0.08006082226832707
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039964346021103365
        model: {}
        policy_loss: -0.00905364404025022
        total_loss: 1.6400530735651653
        vf_explained_var: 0.9955766201019287
        vf_loss: 1.6491467853387196
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34666666666666
    gpu_util_percent0: 0.3813333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467046897762222
    mean_env_wait_ms: 1.206114707605313
    mean_inference_ms: 4.306777765196044
    mean_raw_obs_processing_ms: 0.37927329618348393
  time_since_restore: 4912.403654336929
  time_this_iter_s: 25.766775608062744
  time_total_s: 4912.403654336929
  timers:
    learn_throughput: 8663.803
    learn_time_ms: 18674.478
    sample_throughput: 23400.327
    sample_time_ms: 6914.091
    update_time_ms: 25.92
  timestamp: 1602792684
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: f76cb_00000
  
2020-10-15 20:11:25,618	WARNING util.py:136 -- The `process_trial` operation took 0.5487508773803711 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    191 |           4912.4 | 30902272 |  299.199 |              338.535 |              107.323 |            783.882 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3235.978732561967
    time_step_min: 2977
  date: 2020-10-15_20-11-51
  done: false
  episode_len_mean: 783.7720731429728
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 299.3651058244244
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 196
  episodes_total: 39539
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3550238403611183e-19
        cur_lr: 5.0e-05
        entropy: 0.07554020422200362
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008680511098039764
        total_loss: .inf
        vf_explained_var: 0.9983561038970947
        vf_loss: 0.5452379286289215
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.406666666666663
    gpu_util_percent0: 0.2826666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670074781688533
    mean_env_wait_ms: 1.206082220076416
    mean_inference_ms: 4.306556408439387
    mean_raw_obs_processing_ms: 0.3792596654633674
  time_since_restore: 4938.070220708847
  time_this_iter_s: 25.666566371917725
  time_total_s: 4938.070220708847
  timers:
    learn_throughput: 8670.844
    learn_time_ms: 18659.315
    sample_throughput: 23432.717
    sample_time_ms: 6904.534
    update_time_ms: 26.793
  timestamp: 1602792711
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:11:52,023	WARNING util.py:136 -- The `process_trial` operation took 0.5549910068511963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    192 |          4938.07 | 31064064 |  299.365 |              338.535 |              107.323 |            783.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3234.5899919492804
    time_step_min: 2977
  date: 2020-10-15_20-12-17
  done: false
  episode_len_mean: 783.6311887408897
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 299.57820730552567
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 39790
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.032535760541677e-19
        cur_lr: 5.0e-05
        entropy: 0.07862192578613758
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007055249135495008
        total_loss: .inf
        vf_explained_var: 0.9982826709747314
        vf_loss: 0.6930640836556753
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.573333333333338
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669660959426215
    mean_env_wait_ms: 1.206039226328083
    mean_inference_ms: 4.306294852818776
    mean_raw_obs_processing_ms: 0.37924073065423763
  time_since_restore: 4963.800193786621
  time_this_iter_s: 25.729973077774048
  time_total_s: 4963.800193786621
  timers:
    learn_throughput: 8674.56
    learn_time_ms: 18651.32
    sample_throughput: 23442.165
    sample_time_ms: 6901.752
    update_time_ms: 28.875
  timestamp: 1602792737
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:12:18,500	WARNING util.py:136 -- The `process_trial` operation took 0.5452334880828857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    193 |           4963.8 | 31225856 |  299.578 |              338.535 |              107.323 |            783.631 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3233.593494102622
    time_step_min: 2977
  date: 2020-10-15_20-12-44
  done: false
  episode_len_mean: 783.5298811757349
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 299.73653230238597
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 185
  episodes_total: 39975
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.048803640812517e-19
        cur_lr: 5.0e-05
        entropy: 0.06895502718786399
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008266804180796802
        total_loss: .inf
        vf_explained_var: 0.9986752867698669
        vf_loss: 0.4201832339167595
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79666666666667
    gpu_util_percent0: 0.3063333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466934381607969
    mean_env_wait_ms: 1.2060079016052987
    mean_inference_ms: 4.30609271872018
    mean_raw_obs_processing_ms: 0.37922775760949445
  time_since_restore: 4989.721096038818
  time_this_iter_s: 25.920902252197266
  time_total_s: 4989.721096038818
  timers:
    learn_throughput: 8657.057
    learn_time_ms: 18689.031
    sample_throughput: 23473.066
    sample_time_ms: 6892.666
    update_time_ms: 30.949
  timestamp: 1602792764
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:12:45,192	WARNING util.py:136 -- The `process_trial` operation took 0.5857276916503906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    194 |          4989.72 | 31387648 |  299.737 |              338.535 |              107.323 |             783.53 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3232.553670204814
    time_step_min: 2977
  date: 2020-10-15_20-13-10
  done: false
  episode_len_mean: 783.4219434488251
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 299.90141483533057
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 201
  episodes_total: 40176
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.573205461218774e-19
        cur_lr: 5.0e-05
        entropy: 0.07439734227955341
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008366801678979149
        total_loss: .inf
        vf_explained_var: 0.9978317618370056
        vf_loss: 0.7500506639480591
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.646666666666665
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668938634414083
    mean_env_wait_ms: 1.2059752651313513
    mean_inference_ms: 4.305871537366919
    mean_raw_obs_processing_ms: 0.37921387247136357
  time_since_restore: 5015.406821489334
  time_this_iter_s: 25.685725450515747
  time_total_s: 5015.406821489334
  timers:
    learn_throughput: 8670.776
    learn_time_ms: 18659.461
    sample_throughput: 23453.947
    sample_time_ms: 6898.285
    update_time_ms: 31.318
  timestamp: 1602792790
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:13:11,617	WARNING util.py:136 -- The `process_trial` operation took 0.5538063049316406 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    195 |          5015.41 | 31549440 |  299.901 |              338.535 |              107.323 |            783.422 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3231.29201743092
    time_step_min: 2977
  date: 2020-10-15_20-13-37
  done: false
  episode_len_mean: 783.2852090032154
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 300.0954499234242
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 254
  episodes_total: 40430
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.859808191828162e-19
        cur_lr: 5.0e-05
        entropy: 0.08171592776974042
        entropy_coeff: 0.0005000000000000001
        kl: 0.004223060134487848
        model: {}
        policy_loss: -0.0077506942907348275
        total_loss: 1.31934987505277
        vf_explained_var: 0.9968662858009338
        vf_loss: 1.3271414041519165
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19
    gpu_util_percent0: 0.2783333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466855958554011
    mean_env_wait_ms: 1.2059312876261021
    mean_inference_ms: 4.305619562625448
    mean_raw_obs_processing_ms: 0.37919518320276735
  time_since_restore: 5041.424068212509
  time_this_iter_s: 26.01724672317505
  time_total_s: 5041.424068212509
  timers:
    learn_throughput: 8666.858
    learn_time_ms: 18667.896
    sample_throughput: 23423.527
    sample_time_ms: 6907.243
    update_time_ms: 31.691
  timestamp: 1602792817
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: f76cb_00000
  
2020-10-15 20:13:38,441	WARNING util.py:136 -- The `process_trial` operation took 0.5243544578552246 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    196 |          5041.42 | 31711232 |  300.095 |              338.535 |              107.323 |            783.285 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3230.3475238494343
    time_step_min: 2977
  date: 2020-10-15_20-14-04
  done: false
  episode_len_mean: 783.1885050112044
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 300.2423941451005
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 40609
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.429904095914081e-19
        cur_lr: 5.0e-05
        entropy: 0.07084418584903081
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0065058686983926846
        total_loss: .inf
        vf_explained_var: 0.9977386593818665
        vf_loss: 0.7244919786850611
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.416129032258066
    gpu_util_percent0: 0.3583870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466822937699824
    mean_env_wait_ms: 1.2059010799631782
    mean_inference_ms: 4.305423025519112
    mean_raw_obs_processing_ms: 0.37918289100088726
  time_since_restore: 5067.553804159164
  time_this_iter_s: 26.129735946655273
  time_total_s: 5067.553804159164
  timers:
    learn_throughput: 8650.492
    learn_time_ms: 18703.214
    sample_throughput: 23469.572
    sample_time_ms: 6893.692
    update_time_ms: 30.117
  timestamp: 1602792844
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:14:05,362	WARNING util.py:136 -- The `process_trial` operation took 0.5888199806213379 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    197 |          5067.55 | 31873024 |  300.242 |              338.535 |              107.323 |            783.189 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3229.2141455758288
    time_step_min: 2977
  date: 2020-10-15_20-14-31
  done: false
  episode_len_mean: 783.0778823068255
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 300.41129730347717
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 209
  episodes_total: 40818
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.144856143871122e-19
        cur_lr: 5.0e-05
        entropy: 0.08126163296401501
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008211487855684632
        total_loss: .inf
        vf_explained_var: 0.9970442652702332
        vf_loss: 1.077031781276067
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29666666666667
    gpu_util_percent0: 0.3179999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667834191172796
    mean_env_wait_ms: 1.2058673588434354
    mean_inference_ms: 4.3052113817487045
    mean_raw_obs_processing_ms: 0.3791691927261241
  time_since_restore: 5093.441494941711
  time_this_iter_s: 25.887690782546997
  time_total_s: 5093.441494941711
  timers:
    learn_throughput: 8642.844
    learn_time_ms: 18719.763
    sample_throughput: 23525.938
    sample_time_ms: 6877.175
    update_time_ms: 28.775
  timestamp: 1602792871
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:14:32,026	WARNING util.py:136 -- The `process_trial` operation took 0.5708365440368652 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    198 |          5093.44 | 32034816 |  300.411 |              338.535 |              107.323 |            783.078 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3227.9505947158036
    time_step_min: 2977
  date: 2020-10-15_20-14-57
  done: false
  episode_len_mean: 782.9449233016801
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 300.60930709579355
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 252
  episodes_total: 41070
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.717284215806682e-19
        cur_lr: 5.0e-05
        entropy: 0.07859416181842487
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006213841610588133
        total_loss: .inf
        vf_explained_var: 0.9978906512260437
        vf_loss: 0.8329341461261114
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.223333333333333
    gpu_util_percent0: 0.33166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667460072547137
    mean_env_wait_ms: 1.2058232892544247
    mean_inference_ms: 4.304955179086216
    mean_raw_obs_processing_ms: 0.3791505008697467
  time_since_restore: 5119.318827152252
  time_this_iter_s: 25.87733221054077
  time_total_s: 5119.318827152252
  timers:
    learn_throughput: 8649.212
    learn_time_ms: 18705.981
    sample_throughput: 23593.102
    sample_time_ms: 6857.598
    update_time_ms: 29.503
  timestamp: 1602792897
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:14:58,750	WARNING util.py:136 -- The `process_trial` operation took 0.5785160064697266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    199 |          5119.32 | 32196608 |  300.609 |              338.535 |              107.323 |            782.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3227.0216008931607
    time_step_min: 2977
  date: 2020-10-15_20-15-24
  done: false
  episode_len_mean: 782.8567791678789
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 300.7493688705501
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 174
  episodes_total: 41244
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1575926323710023e-18
        cur_lr: 5.0e-05
        entropy: 0.06764718641837437
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006863108040609707
        total_loss: .inf
        vf_explained_var: 0.9983331561088562
        vf_loss: 0.5147264500459036
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.638709677419353
    gpu_util_percent0: 0.3470967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466716140442813
    mean_env_wait_ms: 1.2057947851671802
    mean_inference_ms: 4.30477904412566
    mean_raw_obs_processing_ms: 0.3791393937577556
  time_since_restore: 5145.3820104599
  time_this_iter_s: 26.063183307647705
  time_total_s: 5145.3820104599
  timers:
    learn_throughput: 8629.515
    learn_time_ms: 18748.678
    sample_throughput: 23657.582
    sample_time_ms: 6838.907
    update_time_ms: 38.5
  timestamp: 1602792924
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:15:25,569	WARNING util.py:136 -- The `process_trial` operation took 0.5595216751098633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    200 |          5145.38 | 32358400 |  300.749 |              338.535 |              107.323 |            782.857 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3225.8981385354546
    time_step_min: 2977
  date: 2020-10-15_20-15-51
  done: false
  episode_len_mean: 782.7440245049564
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 300.918637424631
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 41461
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7363889485565036e-18
        cur_lr: 5.0e-05
        entropy: 0.07486619800329208
        entropy_coeff: 0.0005000000000000001
        kl: 0.002918972090507547
        model: {}
        policy_loss: -0.007308803537550072
        total_loss: 0.7289177527030309
        vf_explained_var: 0.9979598522186279
        vf_loss: 0.7362640152374903
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.190000000000005
    gpu_util_percent0: 0.31766666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666802585033428
    mean_env_wait_ms: 1.2057610901552416
    mean_inference_ms: 4.304573477186536
    mean_raw_obs_processing_ms: 0.3791252691250524
  time_since_restore: 5171.54504776001
  time_this_iter_s: 26.163037300109863
  time_total_s: 5171.54504776001
  timers:
    learn_throughput: 8621.383
    learn_time_ms: 18766.363
    sample_throughput: 23587.408
    sample_time_ms: 6859.253
    update_time_ms: 37.713
  timestamp: 1602792951
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: f76cb_00000
  
2020-10-15 20:15:52,599	WARNING util.py:136 -- The `process_trial` operation took 0.6050765514373779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    201 |          5171.55 | 32520192 |  300.919 |              338.535 |              107.323 |            782.744 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3224.6340310114733
    time_step_min: 2977
  date: 2020-10-15_20-16-18
  done: false
  episode_len_mean: 782.6162478419336
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 301.1147478525907
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 41704
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.681944742782518e-19
        cur_lr: 5.0e-05
        entropy: 0.07300748986502488
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007848201697925106
        total_loss: .inf
        vf_explained_var: 0.9984004497528076
        vf_loss: 0.5936932762463888
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.432258064516123
    gpu_util_percent0: 0.4003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466642451053231
    mean_env_wait_ms: 1.205718406214214
    mean_inference_ms: 4.3043299610264505
    mean_raw_obs_processing_ms: 0.3791077448533878
  time_since_restore: 5197.509340763092
  time_this_iter_s: 25.964293003082275
  time_total_s: 5197.509340763092
  timers:
    learn_throughput: 8611.995
    learn_time_ms: 18786.821
    sample_throughput: 23557.088
    sample_time_ms: 6868.082
    update_time_ms: 36.645
  timestamp: 1602792978
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:16:19,328	WARNING util.py:136 -- The `process_trial` operation took 0.5548052787780762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    202 |          5197.51 | 32681984 |  301.115 |              338.535 |              107.323 |            782.616 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3223.721084207759
    time_step_min: 2977
  date: 2020-10-15_20-16-45
  done: false
  episode_len_mean: 782.526684018243
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 301.2497319719316
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 41879
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3022917114173778e-18
        cur_lr: 5.0e-05
        entropy: 0.0699869046608607
        entropy_coeff: 0.0005000000000000001
        kl: 0.003992414625827223
        model: {}
        policy_loss: -0.0054902084860562654
        total_loss: 0.5307925144831339
        vf_explained_var: 0.9982759952545166
        vf_loss: 0.5363177135586739
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23333333333334
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666132107991037
    mean_env_wait_ms: 1.2056900098763195
    mean_inference_ms: 4.30415752119449
    mean_raw_obs_processing_ms: 0.37909680136751095
  time_since_restore: 5223.287529706955
  time_this_iter_s: 25.778188943862915
  time_total_s: 5223.287529706955
  timers:
    learn_throughput: 8608.011
    learn_time_ms: 18795.515
    sample_throughput: 23594.766
    sample_time_ms: 6857.114
    update_time_ms: 34.732
  timestamp: 1602793005
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: f76cb_00000
  
2020-10-15 20:16:45,848	WARNING util.py:136 -- The `process_trial` operation took 0.5462996959686279 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    203 |          5223.29 | 32843776 |   301.25 |              338.535 |              107.323 |            782.527 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3222.5571985545835
    time_step_min: 2977
  date: 2020-10-15_20-17-11
  done: false
  episode_len_mean: 782.4188476701657
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 301.42694459917664
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 42106
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.511458557086889e-19
        cur_lr: 5.0e-05
        entropy: 0.08073157692948978
        entropy_coeff: 0.0005000000000000001
        kl: 0.004883817746303976
        model: {}
        policy_loss: -0.006603321409784257
        total_loss: 0.5641804660360018
        vf_explained_var: 0.9984530806541443
        vf_loss: 0.5708241661389669
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.680000000000003
    gpu_util_percent0: 0.2843333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665798979328717
    mean_env_wait_ms: 1.205653682811502
    mean_inference_ms: 4.303949629141386
    mean_raw_obs_processing_ms: 0.379082438156872
  time_since_restore: 5249.178257703781
  time_this_iter_s: 25.890727996826172
  time_total_s: 5249.178257703781
  timers:
    learn_throughput: 8611.653
    learn_time_ms: 18787.566
    sample_throughput: 23571.705
    sample_time_ms: 6863.823
    update_time_ms: 32.724
  timestamp: 1602793031
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: f76cb_00000
  
2020-10-15 20:17:12,497	WARNING util.py:136 -- The `process_trial` operation took 0.5505340099334717 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    204 |          5249.18 | 33005568 |  301.427 |              338.535 |              107.323 |            782.419 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3221.4092349158313
    time_step_min: 2977
  date: 2020-10-15_20-17-38
  done: false
  episode_len_mean: 782.30539940479
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 301.59954330970913
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 42338
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2557292785434445e-19
        cur_lr: 5.0e-05
        entropy: 0.08001000930865605
        entropy_coeff: 0.0005000000000000001
        kl: 0.004211147160579761
        model: {}
        policy_loss: -0.008496789620645965
        total_loss: 0.7122013221184412
        vf_explained_var: 0.9980683326721191
        vf_loss: 0.7207381129264832
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17000000000001
    gpu_util_percent0: 0.33166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665419537057456
    mean_env_wait_ms: 1.2056135226585518
    mean_inference_ms: 4.30372202957692
    mean_raw_obs_processing_ms: 0.37906605212923833
  time_since_restore: 5275.172158479691
  time_this_iter_s: 25.993900775909424
  time_total_s: 5275.172158479691
  timers:
    learn_throughput: 8599.371
    learn_time_ms: 18814.399
    sample_throughput: 23562.128
    sample_time_ms: 6866.612
    update_time_ms: 34.064
  timestamp: 1602793058
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: f76cb_00000
  
2020-10-15 20:17:39,356	WARNING util.py:136 -- The `process_trial` operation took 0.58518385887146 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    205 |          5275.17 | 33167360 |    301.6 |              338.535 |              107.323 |            782.305 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3220.576258416914
    time_step_min: 2977
  date: 2020-10-15_20-18-04
  done: false
  episode_len_mean: 782.2154483018157
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 301.7289498142589
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 42516
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6278646392717222e-19
        cur_lr: 5.0e-05
        entropy: 0.07600015091399352
        entropy_coeff: 0.0005000000000000001
        kl: 0.003747561442044874
        model: {}
        policy_loss: -0.008625606424175203
        total_loss: 0.7714522033929825
        vf_explained_var: 0.9975247383117676
        vf_loss: 0.780115803082784
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.936666666666664
    gpu_util_percent0: 0.32300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665125325140235
    mean_env_wait_ms: 1.205585498167453
    mean_inference_ms: 4.303548700857739
    mean_raw_obs_processing_ms: 0.37905493678107977
  time_since_restore: 5300.721729040146
  time_this_iter_s: 25.549570560455322
  time_total_s: 5300.721729040146
  timers:
    learn_throughput: 8610.899
    learn_time_ms: 18789.212
    sample_throughput: 23638.519
    sample_time_ms: 6844.422
    update_time_ms: 33.777
  timestamp: 1602793084
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: f76cb_00000
  
2020-10-15 20:18:05,686	WARNING util.py:136 -- The `process_trial` operation took 0.5861005783081055 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    206 |          5300.72 | 33329152 |  301.729 |              338.535 |              107.323 |            782.215 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3219.472932471668
    time_step_min: 2977
  date: 2020-10-15_20-18-31
  done: false
  episode_len_mean: 782.0944561403509
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 301.892766259082
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 234
  episodes_total: 42750
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.139323196358611e-20
        cur_lr: 5.0e-05
        entropy: 0.08403218910098076
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035529894521459937
        model: {}
        policy_loss: -0.00738119909268183
        total_loss: 1.1351110090812047
        vf_explained_var: 0.9971505999565125
        vf_loss: 1.1425341814756393
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.946666666666665
    gpu_util_percent0: 0.34633333333333327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664770078341138
    mean_env_wait_ms: 1.2055474497264804
    mean_inference_ms: 4.303337167787287
    mean_raw_obs_processing_ms: 0.37904048915054483
  time_since_restore: 5326.314837217331
  time_this_iter_s: 25.59310817718506
  time_total_s: 5326.314837217331
  timers:
    learn_throughput: 8630.072
    learn_time_ms: 18747.468
    sample_throughput: 23653.691
    sample_time_ms: 6840.032
    update_time_ms: 33.788
  timestamp: 1602793111
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: f76cb_00000
  
2020-10-15 20:18:32,049	WARNING util.py:136 -- The `process_trial` operation took 0.5656850337982178 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    207 |          5326.31 | 33490944 |  301.893 |              338.535 |              107.323 |            782.094 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3218.4217527021992
    time_step_min: 2977
  date: 2020-10-15_20-18-57
  done: false
  episode_len_mean: 781.9848964393763
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 302.05160518379046
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 220
  episodes_total: 42970
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.0696615981793056e-20
        cur_lr: 5.0e-05
        entropy: 0.07753233301142852
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007744058811416228
        total_loss: .inf
        vf_explained_var: 0.9984344840049744
        vf_loss: 0.5680084774891535
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96
    gpu_util_percent0: 0.3026666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466443645299138
    mean_env_wait_ms: 1.205509908635856
    mean_inference_ms: 4.303132983321912
    mean_raw_obs_processing_ms: 0.3790251554716655
  time_since_restore: 5352.000344991684
  time_this_iter_s: 25.685507774353027
  time_total_s: 5352.000344991684
  timers:
    learn_throughput: 8638.666
    learn_time_ms: 18728.817
    sample_throughput: 23663.28
    sample_time_ms: 6837.26
    update_time_ms: 34.024
  timestamp: 1602793137
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:18:58,508	WARNING util.py:136 -- The `process_trial` operation took 0.5713412761688232 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    208 |             5352 | 33652736 |  302.052 |              338.535 |              107.323 |            781.985 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3217.542299751792
    time_step_min: 2977
  date: 2020-10-15_20-19-24
  done: false
  episode_len_mean: 781.8902922296123
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 302.18836062883713
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 43151
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.104492397268958e-20
        cur_lr: 5.0e-05
        entropy: 0.07390689104795456
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008115740201901644
        total_loss: .inf
        vf_explained_var: 0.9985975623130798
        vf_loss: 0.43082139641046524
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.936666666666667
    gpu_util_percent0: 0.30366666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664154056969206
    mean_env_wait_ms: 1.2054823345820047
    mean_inference_ms: 4.302966576245436
    mean_raw_obs_processing_ms: 0.37901445300035047
  time_since_restore: 5377.896859169006
  time_this_iter_s: 25.896514177322388
  time_total_s: 5377.896859169006
  timers:
    learn_throughput: 8638.637
    learn_time_ms: 18728.881
    sample_throughput: 23658.581
    sample_time_ms: 6838.618
    update_time_ms: 33.35
  timestamp: 1602793164
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:19:25,175	WARNING util.py:136 -- The `process_trial` operation took 0.5622823238372803 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    209 |           5377.9 | 33814528 |  302.188 |              338.535 |              107.323 |             781.89 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3216.441907134454
    time_step_min: 2977
  date: 2020-10-15_20-19-50
  done: false
  episode_len_mean: 781.770065675769
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 302.34011738539914
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 244
  episodes_total: 43395
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.156738595903436e-20
        cur_lr: 5.0e-05
        entropy: 0.10367085908850034
        entropy_coeff: 0.0005000000000000001
        kl: 0.004619994123155872
        model: {}
        policy_loss: -0.010465591753018089
        total_loss: 1.698946585257848
        vf_explained_var: 0.995725154876709
        vf_loss: 1.7094639937082927
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.336666666666666
    gpu_util_percent0: 0.35366666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663774605741564
    mean_env_wait_ms: 1.2054428579697094
    mean_inference_ms: 4.302751716257726
    mean_raw_obs_processing_ms: 0.37899919453829894
  time_since_restore: 5403.71183013916
  time_this_iter_s: 25.81497097015381
  time_total_s: 5403.71183013916
  timers:
    learn_throughput: 8650.543
    learn_time_ms: 18703.104
    sample_throughput: 23629.579
    sample_time_ms: 6847.011
    update_time_ms: 24.614
  timestamp: 1602793190
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: f76cb_00000
  
2020-10-15 20:19:51,894	WARNING util.py:136 -- The `process_trial` operation took 0.5843613147735596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    210 |          5403.71 | 33976320 |   302.34 |              338.535 |              107.323 |             781.77 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3215.7283704060787
    time_step_min: 2977
  date: 2020-10-15_20-20-17
  done: false
  episode_len_mean: 781.6651530787753
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 302.4456640710511
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 210
  episodes_total: 43605
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.578369297951718e-20
        cur_lr: 5.0e-05
        entropy: 0.09020292200148106
        entropy_coeff: 0.0005000000000000001
        kl: 0.004228242498356849
        model: {}
        policy_loss: -0.009615827390613655
        total_loss: 1.690534194310506
        vf_explained_var: 0.9955329895019531
        vf_loss: 1.700195183356603
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.554838709677416
    gpu_util_percent0: 0.3509677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663482682564488
    mean_env_wait_ms: 1.2054084597193724
    mean_inference_ms: 4.30255739566513
    mean_raw_obs_processing_ms: 0.3789855657963649
  time_since_restore: 5429.814975738525
  time_this_iter_s: 26.103145599365234
  time_total_s: 5429.814975738525
  timers:
    learn_throughput: 8642.288
    learn_time_ms: 18720.969
    sample_throughput: 23739.477
    sample_time_ms: 6815.314
    update_time_ms: 23.407
  timestamp: 1602793217
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: f76cb_00000
  
2020-10-15 20:20:18,871	WARNING util.py:136 -- The `process_trial` operation took 0.5954101085662842 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    211 |          5429.81 | 34138112 |  302.446 |              338.535 |              107.323 |            781.665 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3214.9258692024596
    time_step_min: 2977
  date: 2020-10-15_20-20-44
  done: false
  episode_len_mean: 781.5735458676837
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 302.56186635128836
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 43789
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.289184648975859e-20
        cur_lr: 5.0e-05
        entropy: 0.07912255388995011
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034304633736610413
        model: {}
        policy_loss: -0.00808720022905618
        total_loss: 0.9260821491479874
        vf_explained_var: 0.9971704483032227
        vf_loss: 0.934208924571673
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.063333333333333
    gpu_util_percent0: 0.3153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466317112739871
    mean_env_wait_ms: 1.2053806830207388
    mean_inference_ms: 4.302393791112765
    mean_raw_obs_processing_ms: 0.378974808059571
  time_since_restore: 5455.3310079574585
  time_this_iter_s: 25.516032218933105
  time_total_s: 5455.3310079574585
  timers:
    learn_throughput: 8656.623
    learn_time_ms: 18689.968
    sample_throughput: 23760.358
    sample_time_ms: 6809.325
    update_time_ms: 23.409
  timestamp: 1602793244
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: f76cb_00000
  
2020-10-15 20:20:45,356	WARNING util.py:136 -- The `process_trial` operation took 0.5975899696350098 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    212 |          5455.33 | 34299904 |  302.562 |              338.535 |              107.323 |            781.574 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3213.756238919951
    time_step_min: 2977
  date: 2020-10-15_20-21-10
  done: false
  episode_len_mean: 781.4536784741144
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 302.73697465114356
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 44040
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1445923244879295e-20
        cur_lr: 5.0e-05
        entropy: 0.08051398831109206
        entropy_coeff: 0.0005000000000000001
        kl: 0.003733660172050198
        model: {}
        policy_loss: -0.008104904166733226
        total_loss: 0.6107510874668757
        vf_explained_var: 0.9984102249145508
        vf_loss: 0.6188962360223135
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.13103448275862
    gpu_util_percent0: 0.3255172413793103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.879310344827587
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662833989458735
    mean_env_wait_ms: 1.2053397486761184
    mean_inference_ms: 4.302179334941015
    mean_raw_obs_processing_ms: 0.37895935210997334
  time_since_restore: 5480.908864736557
  time_this_iter_s: 25.57785677909851
  time_total_s: 5480.908864736557
  timers:
    learn_throughput: 8664.945
    learn_time_ms: 18672.017
    sample_throughput: 23743.32
    sample_time_ms: 6814.211
    update_time_ms: 23.828
  timestamp: 1602793270
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: f76cb_00000
  
2020-10-15 20:21:11,865	WARNING util.py:136 -- The `process_trial` operation took 0.6365313529968262 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    213 |          5480.91 | 34461696 |  302.737 |              338.535 |              107.323 |            781.454 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3212.8211457532016
    time_step_min: 2977
  date: 2020-10-15_20-21-37
  done: false
  episode_len_mean: 781.3621835443038
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 302.881293723857
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 200
  episodes_total: 44240
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.7229616224396474e-21
        cur_lr: 5.0e-05
        entropy: 0.07041349510351817
        entropy_coeff: 0.0005000000000000001
        kl: 0.004527619457803667
        model: {}
        policy_loss: -0.006219318398507312
        total_loss: 0.42054931322733563
        vf_explained_var: 0.9986973404884338
        vf_loss: 0.4268038496375084
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.78666666666667
    gpu_util_percent0: 0.31433333333333324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662531973847626
    mean_env_wait_ms: 1.2053078140782711
    mean_inference_ms: 4.301998670354716
    mean_raw_obs_processing_ms: 0.37894683366200627
  time_since_restore: 5506.503804683685
  time_this_iter_s: 25.594939947128296
  time_total_s: 5506.503804683685
  timers:
    learn_throughput: 8675.463
    learn_time_ms: 18649.379
    sample_throughput: 23770.229
    sample_time_ms: 6806.497
    update_time_ms: 25.129
  timestamp: 1602793297
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: f76cb_00000
  
2020-10-15 20:21:38,358	WARNING util.py:136 -- The `process_trial` operation took 0.6157817840576172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    214 |           5506.5 | 34623488 |  302.881 |              338.535 |              107.323 |            781.362 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3211.937478877999
    time_step_min: 2977
  date: 2020-10-15_20-22-04
  done: false
  episode_len_mean: 781.2813379251356
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 303.0149867914065
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 187
  episodes_total: 44427
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8614808112198237e-21
        cur_lr: 5.0e-05
        entropy: 0.07288581691682339
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009321590478066355
        total_loss: .inf
        vf_explained_var: 0.9982870221138
        vf_loss: 0.5506353924671809
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.45806451612903
    gpu_util_percent0: 0.3441935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466224235501207
    mean_env_wait_ms: 1.2052789953540552
    mean_inference_ms: 4.301834917689084
    mean_raw_obs_processing_ms: 0.3789360988913664
  time_since_restore: 5532.411023378372
  time_this_iter_s: 25.90721869468689
  time_total_s: 5532.411023378372
  timers:
    learn_throughput: 8671.584
    learn_time_ms: 18657.722
    sample_throughput: 23829.784
    sample_time_ms: 6789.487
    update_time_ms: 23.863
  timestamp: 1602793324
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:22:05,284	WARNING util.py:136 -- The `process_trial` operation took 0.6285743713378906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    215 |          5532.41 | 34785280 |  303.015 |              338.535 |              107.323 |            781.281 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3210.8832332646293
    time_step_min: 2977
  date: 2020-10-15_20-22-31
  done: false
  episode_len_mean: 781.1722771834012
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 303.17861230144683
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 44678
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2922212168297354e-21
        cur_lr: 5.0e-05
        entropy: 0.08179968843857448
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00947436728165485
        total_loss: .inf
        vf_explained_var: 0.9976388812065125
        vf_loss: 0.9546352575222651
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.91
    gpu_util_percent0: 0.292
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661889390101338
    mean_env_wait_ms: 1.20523900559591
    mean_inference_ms: 4.3016264328893214
    mean_raw_obs_processing_ms: 0.37892027807005124
  time_since_restore: 5558.372193336487
  time_this_iter_s: 25.961169958114624
  time_total_s: 5558.372193336487
  timers:
    learn_throughput: 8657.578
    learn_time_ms: 18687.905
    sample_throughput: 23796.252
    sample_time_ms: 6799.054
    update_time_ms: 23.728
  timestamp: 1602793351
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:22:32,202	WARNING util.py:136 -- The `process_trial` operation took 0.6390724182128906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    216 |          5558.37 | 34947072 |  303.179 |              338.535 |              107.323 |            781.172 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3210.0335712692395
    time_step_min: 2977
  date: 2020-10-15_20-22-58
  done: false
  episode_len_mean: 781.0957389909074
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 303.31289585100427
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 194
  episodes_total: 44872
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.4383318252446046e-21
        cur_lr: 5.0e-05
        entropy: 0.06785174707571666
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006719071321034183
        total_loss: .inf
        vf_explained_var: 0.9987294673919678
        vf_loss: 0.4511292154590289
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.27
    gpu_util_percent0: 0.3376666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661618498841233
    mean_env_wait_ms: 1.205208078985672
    mean_inference_ms: 4.301459983699702
    mean_raw_obs_processing_ms: 0.378908879381929
  time_since_restore: 5584.318402051926
  time_this_iter_s: 25.946208715438843
  time_total_s: 5584.318402051926
  timers:
    learn_throughput: 8649.097
    learn_time_ms: 18706.231
    sample_throughput: 23737.663
    sample_time_ms: 6815.835
    update_time_ms: 23.754
  timestamp: 1602793378
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:22:59,046	WARNING util.py:136 -- The `process_trial` operation took 0.6142475605010986 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    217 |          5584.32 | 35108864 |  303.313 |              338.535 |              107.323 |            781.096 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3209.1156195171784
    time_step_min: 2977
  date: 2020-10-15_20-23-24
  done: false
  episode_len_mean: 781.011338170361
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 303.44578380489986
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 197
  episodes_total: 45069
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.657497737866906e-21
        cur_lr: 5.0e-05
        entropy: 0.07609668870766957
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009190090136447301
        total_loss: .inf
        vf_explained_var: 0.9977755546569824
        vf_loss: 0.7494764029979706
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23333333333334
    gpu_util_percent0: 0.2843333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661310012090725
    mean_env_wait_ms: 1.2051779234080484
    mean_inference_ms: 4.301294463805601
    mean_raw_obs_processing_ms: 0.3788981458228249
  time_since_restore: 5609.966030359268
  time_this_iter_s: 25.64762830734253
  time_total_s: 5609.966030359268
  timers:
    learn_throughput: 8653.291
    learn_time_ms: 18697.164
    sample_throughput: 23719.939
    sample_time_ms: 6820.928
    update_time_ms: 23.74
  timestamp: 1602793404
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:23:25,612	WARNING util.py:136 -- The `process_trial` operation took 0.6290650367736816 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    218 |          5609.97 | 35270656 |  303.446 |              338.535 |              107.323 |            781.011 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3208.0503379423067
    time_step_min: 2977
  date: 2020-10-15_20-23-51
  done: false
  episode_len_mean: 780.9068982257922
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 303.6132977760659
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 45316
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4486246606800356e-20
        cur_lr: 5.0e-05
        entropy: 0.07834276060263316
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00897739787857669
        total_loss: .inf
        vf_explained_var: 0.9983122944831848
        vf_loss: 0.6746200770139694
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.860000000000003
    gpu_util_percent0: 0.31466666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660971060459058
    mean_env_wait_ms: 1.2051386771635426
    mean_inference_ms: 4.3010969142776005
    mean_raw_obs_processing_ms: 0.3788824481109943
  time_since_restore: 5635.574612855911
  time_this_iter_s: 25.608582496643066
  time_total_s: 5635.574612855911
  timers:
    learn_throughput: 8666.055
    learn_time_ms: 18669.626
    sample_throughput: 23725.159
    sample_time_ms: 6819.427
    update_time_ms: 23.44
  timestamp: 1602793431
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:23:52,069	WARNING util.py:136 -- The `process_trial` operation took 0.6325368881225586 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    219 |          5635.57 | 35432448 |  303.613 |              338.535 |              107.323 |            780.907 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3207.2083938234127
    time_step_min: 2977
  date: 2020-10-15_20-24-18
  done: false
  episode_len_mean: 780.8322125527426
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 303.74250482142094
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 188
  episodes_total: 45504
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1729369910200542e-20
        cur_lr: 5.0e-05
        entropy: 0.06776625104248524
        entropy_coeff: 0.0005000000000000001
        kl: 0.006189578678458929
        model: {}
        policy_loss: -0.010070612838414187
        total_loss: 0.3531825815637906
        vf_explained_var: 0.998934805393219
        vf_loss: 0.3632870689034462
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.903225806451612
    gpu_util_percent0: 0.33612903225806456
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660727667468482
    mean_env_wait_ms: 1.2051088630697797
    mean_inference_ms: 4.300938573069069
    mean_raw_obs_processing_ms: 0.3788715834719525
  time_since_restore: 5661.567013025284
  time_this_iter_s: 25.99240016937256
  time_total_s: 5661.567013025284
  timers:
    learn_throughput: 8662.289
    learn_time_ms: 18677.742
    sample_throughput: 23700.946
    sample_time_ms: 6826.394
    update_time_ms: 25.113
  timestamp: 1602793458
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: f76cb_00000
  
2020-10-15 20:24:19,040	WARNING util.py:136 -- The `process_trial` operation took 0.607607364654541 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    220 |          5661.57 | 35594240 |  303.743 |              338.535 |              107.323 |            780.832 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3206.319375396947
    time_step_min: 2977
  date: 2020-10-15_20-24-45
  done: false
  episode_len_mean: 780.7540642846203
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 303.87213933086196
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 199
  episodes_total: 45703
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1729369910200542e-20
        cur_lr: 5.0e-05
        entropy: 0.08108617613712947
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00969037803588435
        total_loss: .inf
        vf_explained_var: 0.9978234767913818
        vf_loss: 0.731140747666359
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.066666666666666
    gpu_util_percent0: 0.33399999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466040890960966
    mean_env_wait_ms: 1.205078187465367
    mean_inference_ms: 4.300773734990975
    mean_raw_obs_processing_ms: 0.378860740510839
  time_since_restore: 5687.565279245377
  time_this_iter_s: 25.998266220092773
  time_total_s: 5687.565279245377
  timers:
    learn_throughput: 8671.829
    learn_time_ms: 18657.195
    sample_throughput: 23644.384
    sample_time_ms: 6842.724
    update_time_ms: 27.074
  timestamp: 1602793485
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:24:45,924	WARNING util.py:136 -- The `process_trial` operation took 0.6694536209106445 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    221 |          5687.57 | 35756032 |  303.872 |              338.535 |              107.323 |            780.754 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3205.4176994620025
    time_step_min: 2977
  date: 2020-10-15_20-25-11
  done: false
  episode_len_mean: 780.6507300937915
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.013078140665
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 45953
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.259405486530081e-20
        cur_lr: 5.0e-05
        entropy: 0.08705782890319824
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031842339085415006
        model: {}
        policy_loss: -0.010584148432826623
        total_loss: 1.2652193009853363
        vf_explained_var: 0.9970522522926331
        vf_loss: 1.275846968094508
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.687096774193556
    gpu_util_percent0: 0.2974193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660095675256257
    mean_env_wait_ms: 1.2050389244430915
    mean_inference_ms: 4.300582208564198
    mean_raw_obs_processing_ms: 0.3788453431658311
  time_since_restore: 5713.3324909210205
  time_this_iter_s: 25.76721167564392
  time_total_s: 5713.3324909210205
  timers:
    learn_throughput: 8668.114
    learn_time_ms: 18665.19
    sample_throughput: 23588.41
    sample_time_ms: 6858.962
    update_time_ms: 26.723
  timestamp: 1602793511
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: f76cb_00000
  
2020-10-15 20:25:12,697	WARNING util.py:136 -- The `process_trial` operation took 0.6209628582000732 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    222 |          5713.33 | 35917824 |  304.013 |              338.535 |              107.323 |            780.651 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3204.685171927541
    time_step_min: 2977
  date: 2020-10-15_20-25-38
  done: false
  episode_len_mean: 780.5778659210612
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.13196927989827
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 46137
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6297027432650406e-20
        cur_lr: 5.0e-05
        entropy: 0.06820344055692355
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007961057582482075
        total_loss: .inf
        vf_explained_var: 0.9979787468910217
        vf_loss: 0.6465276132027308
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85333333333334
    gpu_util_percent0: 0.369
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465985185205494
    mean_env_wait_ms: 1.2050100769346064
    mean_inference_ms: 4.300431118052542
    mean_raw_obs_processing_ms: 0.378835039230005
  time_since_restore: 5739.198903799057
  time_this_iter_s: 25.8664128780365
  time_total_s: 5739.198903799057
  timers:
    learn_throughput: 8661.971
    learn_time_ms: 18678.428
    sample_throughput: 23536.533
    sample_time_ms: 6874.08
    update_time_ms: 26.398
  timestamp: 1602793538
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:25:39,400	WARNING util.py:136 -- The `process_trial` operation took 0.6217453479766846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    223 |           5739.2 | 36079616 |  304.132 |              338.535 |              107.323 |            780.578 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3203.788872329856
    time_step_min: 2977
  date: 2020-10-15_20-26-05
  done: false
  episode_len_mean: 780.4941196780389
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.26618420889156
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 204
  episodes_total: 46341
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.444554114897561e-20
        cur_lr: 5.0e-05
        entropy: 0.07156356237828732
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008024588780244812
        total_loss: .inf
        vf_explained_var: 0.9983634352684021
        vf_loss: 0.5629839797814687
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34
    gpu_util_percent0: 0.34766666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659538545301629
    mean_env_wait_ms: 1.204979235255986
    mean_inference_ms: 4.300267557909232
    mean_raw_obs_processing_ms: 0.37882421514630144
  time_since_restore: 5764.883387804031
  time_this_iter_s: 25.684484004974365
  time_total_s: 5764.883387804031
  timers:
    learn_throughput: 8661.561
    learn_time_ms: 18679.311
    sample_throughput: 23515.081
    sample_time_ms: 6880.35
    update_time_ms: 24.649
  timestamp: 1602793565
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:26:05,951	WARNING util.py:136 -- The `process_trial` operation took 0.6479637622833252 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    224 |          5764.88 | 36241408 |  304.266 |              338.535 |              107.323 |            780.494 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3202.719279023803
    time_step_min: 2977
  date: 2020-10-15_20-26-31
  done: false
  episode_len_mean: 780.395664305645
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.42472048235084
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 249
  episodes_total: 46590
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.666831172346341e-20
        cur_lr: 5.0e-05
        entropy: 0.07644341513514519
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009321062355411414
        total_loss: .inf
        vf_explained_var: 0.9982130527496338
        vf_loss: 0.7300372471412023
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.713333333333335
    gpu_util_percent0: 0.32266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465923697510773
    mean_env_wait_ms: 1.2049400313057834
    mean_inference_ms: 4.300079311240574
    mean_raw_obs_processing_ms: 0.3788089505202805
  time_since_restore: 5790.632885932922
  time_this_iter_s: 25.74949812889099
  time_total_s: 5790.632885932922
  timers:
    learn_throughput: 8676.762
    learn_time_ms: 18646.586
    sample_throughput: 23460.174
    sample_time_ms: 6896.454
    update_time_ms: 24.101
  timestamp: 1602793591
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:26:32,564	WARNING util.py:136 -- The `process_trial` operation took 0.646507740020752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    225 |          5790.63 | 36403200 |  304.425 |              338.535 |              107.323 |            780.396 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3201.9574131141926
    time_step_min: 2977
  date: 2020-10-15_20-26-58
  done: false
  episode_len_mean: 780.325892666239
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.5372713666492
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 46770
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.50024675851951e-20
        cur_lr: 5.0e-05
        entropy: 0.06639532931149006
        entropy_coeff: 0.0005000000000000001
        kl: 0.004271305399015546
        model: {}
        policy_loss: -0.00796218777880616
        total_loss: 0.5348829453190168
        vf_explained_var: 0.9982957243919373
        vf_loss: 0.5428783098856608
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.793333333333337
    gpu_util_percent0: 0.3696666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658999282349833
    mean_env_wait_ms: 1.2049121045727362
    mean_inference_ms: 4.299938516465011
    mean_raw_obs_processing_ms: 0.3787993762177687
  time_since_restore: 5816.175044775009
  time_this_iter_s: 25.542158842086792
  time_total_s: 5816.175044775009
  timers:
    learn_throughput: 8694.607
    learn_time_ms: 18608.318
    sample_throughput: 23466.717
    sample_time_ms: 6894.531
    update_time_ms: 23.999
  timestamp: 1602793618
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: f76cb_00000
  
2020-10-15 20:26:58,946	WARNING util.py:136 -- The `process_trial` operation took 0.6232085227966309 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    226 |          5816.18 | 36564992 |  304.537 |              338.535 |              107.323 |            780.326 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3201.2043546806426
    time_step_min: 2977
  date: 2020-10-15_20-27-24
  done: false
  episode_len_mean: 780.2442741592167
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.64743862636584
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 210
  episodes_total: 46980
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.750123379259755e-20
        cur_lr: 5.0e-05
        entropy: 0.08663439564406872
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01012156073799512
        total_loss: .inf
        vf_explained_var: 0.9961568713188171
        vf_loss: 1.491853505373001
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.143333333333334
    gpu_util_percent0: 0.332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658675049841233
    mean_env_wait_ms: 1.2048811825436663
    mean_inference_ms: 4.299774999850321
    mean_raw_obs_processing_ms: 0.37878856490759066
  time_since_restore: 5842.120772361755
  time_this_iter_s: 25.945727586746216
  time_total_s: 5842.120772361755
  timers:
    learn_throughput: 8689.628
    learn_time_ms: 18618.979
    sample_throughput: 23509.592
    sample_time_ms: 6881.957
    update_time_ms: 24.697
  timestamp: 1602793644
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:27:25,755	WARNING util.py:136 -- The `process_trial` operation took 0.6418392658233643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    227 |          5842.12 | 36726784 |  304.647 |              338.535 |              107.323 |            780.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3200.399775371379
    time_step_min: 2977
  date: 2020-10-15_20-27-51
  done: false
  episode_len_mean: 780.1403950795029
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.77250004223816
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 47231
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1251850688896326e-20
        cur_lr: 5.0e-05
        entropy: 0.0820947674413522
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008870561781805009
        total_loss: .inf
        vf_explained_var: 0.9958527684211731
        vf_loss: 1.7196864187717438
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.62666666666667
    gpu_util_percent0: 0.36933333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658386415202948
    mean_env_wait_ms: 1.2048415168589017
    mean_inference_ms: 4.299589466503899
    mean_raw_obs_processing_ms: 0.37877353762214333
  time_since_restore: 5867.64768743515
  time_this_iter_s: 25.526915073394775
  time_total_s: 5867.64768743515
  timers:
    learn_throughput: 8692.378
    learn_time_ms: 18613.088
    sample_throughput: 23535.378
    sample_time_ms: 6874.417
    update_time_ms: 24.208
  timestamp: 1602793671
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:27:52,186	WARNING util.py:136 -- The `process_trial` operation took 0.6836133003234863 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    228 |          5867.65 | 36888576 |  304.773 |              338.535 |              107.323 |             780.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3199.7722646847683
    time_step_min: 2977
  date: 2020-10-15_20-28-17
  done: false
  episode_len_mean: 780.0714074761623
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 304.8714765578321
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 173
  episodes_total: 47404
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.187777603334448e-20
        cur_lr: 5.0e-05
        entropy: 0.07021791053315003
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007939049265890693
        total_loss: .inf
        vf_explained_var: 0.9977430701255798
        vf_loss: 0.7279660552740097
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.72666666666667
    gpu_util_percent0: 0.3796666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658162436026037
    mean_env_wait_ms: 1.2048157881591364
    mean_inference_ms: 4.2994590553775485
    mean_raw_obs_processing_ms: 0.3787648082997516
  time_since_restore: 5893.32931470871
  time_this_iter_s: 25.68162727355957
  time_total_s: 5893.32931470871
  timers:
    learn_throughput: 8690.534
    learn_time_ms: 18617.037
    sample_throughput: 23531.447
    sample_time_ms: 6875.565
    update_time_ms: 24.977
  timestamp: 1602793697
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:28:18,753	WARNING util.py:136 -- The `process_trial` operation took 0.66943359375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    229 |          5893.33 | 37050368 |  304.871 |              338.535 |              107.323 |            780.071 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3198.8651619275793
    time_step_min: 2977
  date: 2020-10-15_20-28-44
  done: false
  episode_len_mean: 779.9811023622048
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.00879026485325
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 221
  episodes_total: 47625
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.281666405001673e-20
        cur_lr: 5.0e-05
        entropy: 0.07323024546106656
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007877243732461162
        total_loss: .inf
        vf_explained_var: 0.9979877471923828
        vf_loss: 0.7213732103506724
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.430000000000003
    gpu_util_percent0: 0.3646666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657859956926517
    mean_env_wait_ms: 1.2047841814059514
    mean_inference_ms: 4.299299208563112
    mean_raw_obs_processing_ms: 0.3787539511236652
  time_since_restore: 5918.898955821991
  time_this_iter_s: 25.56964111328125
  time_total_s: 5918.898955821991
  timers:
    learn_throughput: 8705.19
    learn_time_ms: 18585.693
    sample_throughput: 23564.315
    sample_time_ms: 6865.975
    update_time_ms: 22.993
  timestamp: 1602793724
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:28:45,212	WARNING util.py:136 -- The `process_trial` operation took 0.6720316410064697 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    230 |           5918.9 | 37212160 |  305.009 |              338.535 |              107.323 |            779.981 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3197.887813140396
    time_step_min: 2977
  date: 2020-10-15_20-29-10
  done: false
  episode_len_mean: 779.8870132040782
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.15769638555037
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 47864
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3922499607502507e-19
        cur_lr: 5.0e-05
        entropy: 0.07192121508220832
        entropy_coeff: 0.0005000000000000001
        kl: 0.003689350288671752
        model: {}
        policy_loss: -0.007919292562291957
        total_loss: 0.34137431532144547
        vf_explained_var: 0.9990909695625305
        vf_loss: 0.34932956099510193
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.493333333333332
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657566649741208
    mean_env_wait_ms: 1.2047470592514111
    mean_inference_ms: 4.299121572243241
    mean_raw_obs_processing_ms: 0.37873952765085955
  time_since_restore: 5944.46231842041
  time_this_iter_s: 25.56336259841919
  time_total_s: 5944.46231842041
  timers:
    learn_throughput: 8714.978
    learn_time_ms: 18564.82
    sample_throughput: 23630.343
    sample_time_ms: 6846.79
    update_time_ms: 20.766
  timestamp: 1602793750
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: f76cb_00000
  
2020-10-15 20:29:11,631	WARNING util.py:136 -- The `process_trial` operation took 0.6330101490020752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    231 |          5944.46 | 37373952 |  305.158 |              338.535 |              107.323 |            779.887 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3197.131077871755
    time_step_min: 2977
  date: 2020-10-15_20-29-37
  done: false
  episode_len_mean: 779.8173341103987
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.27489531902154
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 48044
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.961249803751253e-20
        cur_lr: 5.0e-05
        entropy: 0.06374884862452745
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004403252246750829
        total_loss: .inf
        vf_explained_var: 0.9992583394050598
        vf_loss: 0.2249704139928023
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.756666666666668
    gpu_util_percent0: 0.30566666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657341771926646
    mean_env_wait_ms: 1.2047214901468795
    mean_inference_ms: 4.298987993376076
    mean_raw_obs_processing_ms: 0.37873111796486003
  time_since_restore: 5970.145317077637
  time_this_iter_s: 25.682998657226562
  time_total_s: 5970.145317077637
  timers:
    learn_throughput: 8715.203
    learn_time_ms: 18564.34
    sample_throughput: 23662.871
    sample_time_ms: 6837.378
    update_time_ms: 20.769
  timestamp: 1602793777
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:29:38,215	WARNING util.py:136 -- The `process_trial` operation took 0.6771054267883301 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    232 |          5970.15 | 37535744 |  305.275 |              338.535 |              107.323 |            779.817 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3196.1614107109535
    time_step_min: 2977
  date: 2020-10-15_20-30-04
  done: false
  episode_len_mean: 779.7288753547532
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.41305165256443
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 229
  episodes_total: 48273
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.044187470562688e-19
        cur_lr: 5.0e-05
        entropy: 0.0759304544577996
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032975992459493377
        model: {}
        policy_loss: -0.008759496383845544
        total_loss: 0.9627392788728079
        vf_explained_var: 0.9973799586296082
        vf_loss: 0.9715367356936137
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.889999999999997
    gpu_util_percent0: 0.35866666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657045768913415
    mean_env_wait_ms: 1.2046885047513667
    mean_inference_ms: 4.298830912262275
    mean_raw_obs_processing_ms: 0.37871966313263533
  time_since_restore: 5996.029543638229
  time_this_iter_s: 25.88422656059265
  time_total_s: 5996.029543638229
  timers:
    learn_throughput: 8711.806
    learn_time_ms: 18571.581
    sample_throughput: 23681.49
    sample_time_ms: 6832.003
    update_time_ms: 20.938
  timestamp: 1602793804
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: f76cb_00000
  
2020-10-15 20:30:05,010	WARNING util.py:136 -- The `process_trial` operation took 0.6871697902679443 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    233 |          5996.03 | 37697536 |  305.413 |              338.535 |              107.323 |            779.729 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3195.2365298499763
    time_step_min: 2977
  date: 2020-10-15_20-30-30
  done: false
  episode_len_mean: 779.6492443454774
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.5486391095966
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 228
  episodes_total: 48501
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.22093735281344e-20
        cur_lr: 5.0e-05
        entropy: 0.06936541137595971
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0066919852591430145
        total_loss: .inf
        vf_explained_var: 0.9986454844474792
        vf_loss: 0.49417295306921005
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.710000000000004
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465677928004023
    mean_env_wait_ms: 1.2046532073598262
    mean_inference_ms: 4.298662808806728
    mean_raw_obs_processing_ms: 0.37870667110241124
  time_since_restore: 6021.888127326965
  time_this_iter_s: 25.858583688735962
  time_total_s: 6021.888127326965
  timers:
    learn_throughput: 8704.41
    learn_time_ms: 18587.361
    sample_throughput: 23683.819
    sample_time_ms: 6831.331
    update_time_ms: 23.166
  timestamp: 1602793830
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:30:31,796	WARNING util.py:136 -- The `process_trial` operation took 0.6380634307861328 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    234 |          6021.89 | 37859328 |  305.549 |              338.535 |              107.323 |            779.649 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3194.5827165062915
    time_step_min: 2977
  date: 2020-10-15_20-30-57
  done: false
  episode_len_mean: 779.5926085706069
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.6522049037148
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 177
  episodes_total: 48678
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.831406029220161e-20
        cur_lr: 5.0e-05
        entropy: 0.06790395639836788
        entropy_coeff: 0.0005000000000000001
        kl: 0.003926805627997965
        model: {}
        policy_loss: -0.009951978058476621
        total_loss: 0.5654335841536522
        vf_explained_var: 0.99822598695755
        vf_loss: 0.5754195203383764
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.710000000000004
    gpu_util_percent0: 0.3783333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656549909393826
    mean_env_wait_ms: 1.2046276733781824
    mean_inference_ms: 4.298533759455277
    mean_raw_obs_processing_ms: 0.37869812790791363
  time_since_restore: 6047.683708429337
  time_this_iter_s: 25.795581102371216
  time_total_s: 6047.683708429337
  timers:
    learn_throughput: 8696.795
    learn_time_ms: 18603.635
    sample_throughput: 23729.335
    sample_time_ms: 6818.227
    update_time_ms: 23.186
  timestamp: 1602793857
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: f76cb_00000
  
2020-10-15 20:30:58,545	WARNING util.py:136 -- The `process_trial` operation took 0.6638920307159424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    235 |          6047.68 | 38021120 |  305.652 |              338.535 |              107.323 |            779.593 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3193.681849805607
    time_step_min: 2977
  date: 2020-10-15_20-31-24
  done: false
  episode_len_mean: 779.5259445534838
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.7927781247212
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 234
  episodes_total: 48912
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9157030146100805e-20
        cur_lr: 5.0e-05
        entropy: 0.07530070779224236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004139095273179312
        model: {}
        policy_loss: -0.007990700857286962
        total_loss: 0.4389096101125081
        vf_explained_var: 0.9988871216773987
        vf_loss: 0.4469379484653473
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596774193548388
    gpu_util_percent0: 0.3658064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656245196697887
    mean_env_wait_ms: 1.2045933021978312
    mean_inference_ms: 4.298373118284235
    mean_raw_obs_processing_ms: 0.3786859490155627
  time_since_restore: 6073.684376716614
  time_this_iter_s: 26.00066828727722
  time_total_s: 6073.684376716614
  timers:
    learn_throughput: 8677.603
    learn_time_ms: 18644.78
    sample_throughput: 23758.544
    sample_time_ms: 6809.845
    update_time_ms: 23.464
  timestamp: 1602793884
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: f76cb_00000
  
2020-10-15 20:31:25,460	WARNING util.py:136 -- The `process_trial` operation took 0.665743350982666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    236 |          6073.68 | 38182912 |  305.793 |              338.535 |              107.323 |            779.526 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3192.7899250387027
    time_step_min: 2977
  date: 2020-10-15_20-31-51
  done: false
  episode_len_mean: 779.4620629299467
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.9259619025769
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 222
  episodes_total: 49134
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9578515073050402e-20
        cur_lr: 5.0e-05
        entropy: 0.07374487320582072
        entropy_coeff: 0.0005000000000000001
        kl: 0.01016502920538187
        model: {}
        policy_loss: -0.009104151557645915
        total_loss: 0.3760540559887886
        vf_explained_var: 0.9989228248596191
        vf_loss: 0.3851950839161873
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.130000000000006
    gpu_util_percent0: 0.35100000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655997981320176
    mean_env_wait_ms: 1.2045593007976934
    mean_inference_ms: 4.298215667185008
    mean_raw_obs_processing_ms: 0.37867396393698705
  time_since_restore: 6099.647498130798
  time_this_iter_s: 25.96312141418457
  time_total_s: 6099.647498130798
  timers:
    learn_throughput: 8687.349
    learn_time_ms: 18623.864
    sample_throughput: 23692.701
    sample_time_ms: 6828.77
    update_time_ms: 24.858
  timestamp: 1602793911
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: f76cb_00000
  
2020-10-15 20:31:52,335	WARNING util.py:136 -- The `process_trial` operation took 0.6884822845458984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    237 |          6099.65 | 38344704 |  305.926 |              338.535 |              107.323 |            779.462 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3192.2529076257943
    time_step_min: 2977
  date: 2020-10-15_20-32-18
  done: false
  episode_len_mean: 779.4187876452575
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.9949112901921
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 49309
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9578515073050402e-20
        cur_lr: 5.0e-05
        entropy: 0.12058206213017304
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014919525497437766
        total_loss: .inf
        vf_explained_var: 0.992730438709259
        vf_loss: 2.4394351641337075
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.083333333333336
    gpu_util_percent0: 0.3323333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655783248571658
    mean_env_wait_ms: 1.2045343050773674
    mean_inference_ms: 4.298094384903256
    mean_raw_obs_processing_ms: 0.37866587850971906
  time_since_restore: 6125.358743429184
  time_this_iter_s: 25.71124529838562
  time_total_s: 6125.358743429184
  timers:
    learn_throughput: 8680.937
    learn_time_ms: 18637.618
    sample_throughput: 23680.939
    sample_time_ms: 6832.162
    update_time_ms: 25.187
  timestamp: 1602793938
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:32:18,966	WARNING util.py:136 -- The `process_trial` operation took 0.6919090747833252 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    238 |          6125.36 | 38506496 |  305.995 |              338.535 |              107.323 |            779.419 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3192.5183434343435
    time_step_min: 2977
  date: 2020-10-15_20-32-45
  done: false
  episode_len_mean: 779.3859149812281
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.92570674652546
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 233
  episodes_total: 49542
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.93677726095756e-20
        cur_lr: 5.0e-05
        entropy: 0.16446123023827872
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01253332698252052
        total_loss: .inf
        vf_explained_var: 0.9858365058898926
        vf_loss: 7.847974856694539
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.74193548387097
    gpu_util_percent0: 0.34580645161290324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655475591381897
    mean_env_wait_ms: 1.204501612595217
    mean_inference_ms: 4.29793792739936
    mean_raw_obs_processing_ms: 0.3786541394565397
  time_since_restore: 6151.503280878067
  time_this_iter_s: 26.144537448883057
  time_total_s: 6151.503280878067
  timers:
    learn_throughput: 8664.319
    learn_time_ms: 18673.366
    sample_throughput: 23679.205
    sample_time_ms: 6832.662
    update_time_ms: 33.566
  timestamp: 1602793965
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:32:46,005	WARNING util.py:136 -- The `process_trial` operation took 0.6711721420288086 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    239 |           6151.5 | 38668288 |  305.926 |              338.535 |              107.323 |            779.386 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3192.7795386440885
    time_step_min: 2977
  date: 2020-10-15_20-33-11
  done: false
  episode_len_mean: 779.34027931277
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.8959270186037
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 223
  episodes_total: 49765
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.405165891436341e-20
        cur_lr: 5.0e-05
        entropy: 0.1329742781817913
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012091458036593394
        total_loss: .inf
        vf_explained_var: 0.9905039668083191
        vf_loss: 4.79087249437968
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333333
    gpu_util_percent0: 0.342
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146552316934347
    mean_env_wait_ms: 1.204469138942661
    mean_inference_ms: 4.297782030377418
    mean_raw_obs_processing_ms: 0.37864220756887446
  time_since_restore: 6177.40455198288
  time_this_iter_s: 25.901271104812622
  time_total_s: 6177.40455198288
  timers:
    learn_throughput: 8656.984
    learn_time_ms: 18689.189
    sample_throughput: 23628.944
    sample_time_ms: 6847.196
    update_time_ms: 35.218
  timestamp: 1602793991
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:33:12,816	WARNING util.py:136 -- The `process_trial` operation took 0.6794960498809814 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    240 |           6177.4 | 38830080 |  305.896 |              338.535 |              107.323 |             779.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3192.6750971982847
    time_step_min: 2977
  date: 2020-10-15_20-33-38
  done: false
  episode_len_mean: 779.2946335602724
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 305.91584042264856
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 175
  episodes_total: 49940
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.607748837154509e-20
        cur_lr: 5.0e-05
        entropy: 0.10702366381883621
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01170420446821178
        total_loss: .inf
        vf_explained_var: 0.9901077747344971
        vf_loss: 4.000960985819499
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.638709677419357
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655020911209926
    mean_env_wait_ms: 1.2044454291819664
    mean_inference_ms: 4.297662770990596
    mean_raw_obs_processing_ms: 0.3786342184672121
  time_since_restore: 6203.294654607773
  time_this_iter_s: 25.89010262489319
  time_total_s: 6203.294654607773
  timers:
    learn_throughput: 8644.816
    learn_time_ms: 18715.495
    sample_throughput: 23617.321
    sample_time_ms: 6850.565
    update_time_ms: 35.507
  timestamp: 1602794018
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:33:39,804	WARNING util.py:136 -- The `process_trial` operation took 0.6955728530883789 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    241 |          6203.29 | 38991872 |  305.916 |              338.535 |              107.323 |            779.295 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3192.084993916183
    time_step_min: 2977
  date: 2020-10-15_20-34-05
  done: false
  episode_len_mean: 779.2227005480817
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.0193625341608
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 235
  episodes_total: 50175
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.911623255731767e-20
        cur_lr: 5.0e-05
        entropy: 0.08725184760987759
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008812916581518948
        total_loss: .inf
        vf_explained_var: 0.995997428894043
        vf_loss: 1.6150248348712921
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.503333333333334
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654735279277614
    mean_env_wait_ms: 1.2044133642430197
    mean_inference_ms: 4.297508664541079
    mean_raw_obs_processing_ms: 0.3786227069941202
  time_since_restore: 6229.051193475723
  time_this_iter_s: 25.75653886795044
  time_total_s: 6229.051193475723
  timers:
    learn_throughput: 8641.505
    learn_time_ms: 18722.664
    sample_throughput: 23624.223
    sample_time_ms: 6848.564
    update_time_ms: 37.12
  timestamp: 1602794045
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:34:06,460	WARNING util.py:136 -- The `process_trial` operation took 0.6689441204071045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    242 |          6229.05 | 39153664 |  306.019 |              338.535 |              107.323 |            779.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3191.2929004071093
    time_step_min: 2977
  date: 2020-10-15_20-34-32
  done: false
  episode_len_mean: 779.1555251304641
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.1366958470953
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 222
  episodes_total: 50397
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.486743488359765e-19
        cur_lr: 5.0e-05
        entropy: 0.07778897136449814
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038687419146299362
        model: {}
        policy_loss: -0.006928653446569418
        total_loss: 1.0119088838497798
        vf_explained_var: 0.9972639679908752
        vf_loss: 1.0188764532407124
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.556666666666672
    gpu_util_percent0: 0.3226666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465448590671979
    mean_env_wait_ms: 1.204382039714222
    mean_inference_ms: 4.297359085956488
    mean_raw_obs_processing_ms: 0.3786113107305427
  time_since_restore: 6254.753123998642
  time_this_iter_s: 25.7019305229187
  time_total_s: 6254.753123998642
  timers:
    learn_throughput: 8647.121
    learn_time_ms: 18710.504
    sample_throughput: 23648.162
    sample_time_ms: 6841.631
    update_time_ms: 36.868
  timestamp: 1602794072
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: f76cb_00000
  
2020-10-15 20:34:33,076	WARNING util.py:136 -- The `process_trial` operation took 0.679706335067749 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    243 |          6254.75 | 39315456 |  306.137 |              338.535 |              107.323 |            779.156 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3190.64995844382
    time_step_min: 2977
  date: 2020-10-15_20-34-59
  done: false
  episode_len_mean: 779.097655014236
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.2355922400212
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 50576
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.433717441798825e-20
        cur_lr: 5.0e-05
        entropy: 0.07172470477720101
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008584694917468974
        total_loss: .inf
        vf_explained_var: 0.9978640675544739
        vf_loss: 0.6647564868132273
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77
    gpu_util_percent0: 0.314
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654271923486073
    mean_env_wait_ms: 1.2043583817454222
    mean_inference_ms: 4.297241088195502
    mean_raw_obs_processing_ms: 0.37860320200618053
  time_since_restore: 6280.691705942154
  time_this_iter_s: 25.938581943511963
  time_total_s: 6280.691705942154
  timers:
    learn_throughput: 8643.874
    learn_time_ms: 18717.532
    sample_throughput: 23646.752
    sample_time_ms: 6842.039
    update_time_ms: 36.502
  timestamp: 1602794099
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:35:00,006	WARNING util.py:136 -- The `process_trial` operation took 0.659919261932373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    244 |          6280.69 | 39477248 |  306.236 |              338.535 |              107.323 |            779.098 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3189.7662051170946
    time_step_min: 2977
  date: 2020-10-15_20-35-25
  done: false
  episode_len_mean: 779.0175348828055
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.36919347967705
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 237
  episodes_total: 50813
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1150576162698237e-19
        cur_lr: 5.0e-05
        entropy: 0.07579493088026841
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038489937433041632
        model: {}
        policy_loss: -0.005466839361664218
        total_loss: 0.47491788615783054
        vf_explained_var: 0.9987345337867737
        vf_loss: 0.48042261352141696
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612907
    gpu_util_percent0: 0.3412903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146539744021872
    mean_env_wait_ms: 1.2043251373434418
    mean_inference_ms: 4.297089079780069
    mean_raw_obs_processing_ms: 0.378592112151245
  time_since_restore: 6306.473150014877
  time_this_iter_s: 25.78144407272339
  time_total_s: 6306.473150014877
  timers:
    learn_throughput: 8648.686
    learn_time_ms: 18707.118
    sample_throughput: 23618.107
    sample_time_ms: 6850.337
    update_time_ms: 36.735
  timestamp: 1602794125
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: f76cb_00000
  
2020-10-15 20:35:26,838	WARNING util.py:136 -- The `process_trial` operation took 0.6434743404388428 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    245 |          6306.47 | 39639040 |  306.369 |              338.535 |              107.323 |            779.018 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3188.916764728956
    time_step_min: 2977
  date: 2020-10-15_20-35-52
  done: false
  episode_len_mean: 778.945071526553
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.49509201361053
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 51030
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.575288081349118e-20
        cur_lr: 5.0e-05
        entropy: 0.07029081632693608
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007110078804544173
        total_loss: .inf
        vf_explained_var: 0.9990184307098389
        vf_loss: 0.34464845558007556
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21
    gpu_util_percent0: 0.3243333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653751343071775
    mean_env_wait_ms: 1.2042950010294369
    mean_inference_ms: 4.296945793710816
    mean_raw_obs_processing_ms: 0.3785808844897238
  time_since_restore: 6332.361149549484
  time_this_iter_s: 25.887999534606934
  time_total_s: 6332.361149549484
  timers:
    learn_throughput: 8655.024
    learn_time_ms: 18693.421
    sample_throughput: 23592.518
    sample_time_ms: 6857.767
    update_time_ms: 38.294
  timestamp: 1602794152
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:35:53,674	WARNING util.py:136 -- The `process_trial` operation took 0.7045886516571045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    246 |          6332.36 | 39800832 |  306.495 |              338.535 |              107.323 |            778.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3188.2458375674196
    time_step_min: 2977
  date: 2020-10-15_20-36-19
  done: false
  episode_len_mean: 778.8876869605967
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.59540103656946
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 51214
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.362932122023676e-20
        cur_lr: 5.0e-05
        entropy: 0.0711720318843921
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00942096634147068
        total_loss: .inf
        vf_explained_var: 0.9983282685279846
        vf_loss: 0.5297558705012003
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.323333333333334
    gpu_util_percent0: 0.3406666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653543629220434
    mean_env_wait_ms: 1.2042712411066419
    mean_inference_ms: 4.2968330587278105
    mean_raw_obs_processing_ms: 0.37857334744438353
  time_since_restore: 6358.082862138748
  time_this_iter_s: 25.721712589263916
  time_total_s: 6358.082862138748
  timers:
    learn_throughput: 8655.782
    learn_time_ms: 18691.783
    sample_throughput: 23660.243
    sample_time_ms: 6838.138
    update_time_ms: 36.12
  timestamp: 1602794179
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:36:20,361	WARNING util.py:136 -- The `process_trial` operation took 0.7282352447509766 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    247 |          6358.08 | 39962624 |  306.595 |              338.535 |              107.323 |            778.888 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3187.4208407088254
    time_step_min: 2977
  date: 2020-10-15_20-36-46
  done: false
  episode_len_mean: 778.8166605119434
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.726129931607
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 237
  episodes_total: 51451
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2544398183035514e-19
        cur_lr: 5.0e-05
        entropy: 0.07807500598331292
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006372160627506673
        total_loss: .inf
        vf_explained_var: 0.9990641474723816
        vf_loss: 0.3532490183909734
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.073333333333334
    gpu_util_percent0: 0.3443333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653239090419581
    mean_env_wait_ms: 1.2042377765213854
    mean_inference_ms: 4.296679076145726
    mean_raw_obs_processing_ms: 0.3785617093197293
  time_since_restore: 6383.85156083107
  time_this_iter_s: 25.768698692321777
  time_total_s: 6383.85156083107
  timers:
    learn_throughput: 8649.801
    learn_time_ms: 18704.707
    sample_throughput: 23686.148
    sample_time_ms: 6830.659
    update_time_ms: 35.81
  timestamp: 1602794206
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:36:47,139	WARNING util.py:136 -- The `process_trial` operation took 0.7111406326293945 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    248 |          6383.85 | 40124416 |  306.726 |              338.535 |              107.323 |            778.817 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3186.65196334825
    time_step_min: 2977
  date: 2020-10-15_20-37-12
  done: false
  episode_len_mean: 778.7541761028202
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.83949418111195
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 212
  episodes_total: 51663
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8816597274553274e-19
        cur_lr: 5.0e-05
        entropy: 0.07478957250714302
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034198615273150304
        model: {}
        policy_loss: -0.00792238867143169
        total_loss: 0.593291607995828
        vf_explained_var: 0.9983728528022766
        vf_loss: 0.6012513836224874
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56451612903226
    gpu_util_percent0: 0.35516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465304369620528
    mean_env_wait_ms: 1.204208826300689
    mean_inference_ms: 4.296545169423705
    mean_raw_obs_processing_ms: 0.37855137414426826
  time_since_restore: 6409.620356321335
  time_this_iter_s: 25.768795490264893
  time_total_s: 6409.620356321335
  timers:
    learn_throughput: 8659.386
    learn_time_ms: 18684.003
    sample_throughput: 23708.827
    sample_time_ms: 6824.125
    update_time_ms: 27.243
  timestamp: 1602794232
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: f76cb_00000
  
2020-10-15 20:37:14,033	WARNING util.py:136 -- The `process_trial` operation took 0.7183454036712646 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    249 |          6409.62 | 40286208 |  306.839 |              338.535 |              107.323 |            778.754 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3185.989133162192
    time_step_min: 2977
  date: 2020-10-15_20-37-39
  done: false
  episode_len_mean: 778.6967078744865
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.93855489963573
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 188
  episodes_total: 51851
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.408298637276637e-20
        cur_lr: 5.0e-05
        entropy: 0.07557365422447522
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0077437896688934416
        total_loss: .inf
        vf_explained_var: 0.9986734390258789
        vf_loss: 0.45015661666790646
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.136666666666667
    gpu_util_percent0: 0.3626666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652805264307447
    mean_env_wait_ms: 1.2041839639076528
    mean_inference_ms: 4.296428452297679
    mean_raw_obs_processing_ms: 0.3785437300606075
  time_since_restore: 6435.247415065765
  time_this_iter_s: 25.627058744430542
  time_total_s: 6435.247415065765
  timers:
    learn_throughput: 8668.039
    learn_time_ms: 18665.353
    sample_throughput: 23767.776
    sample_time_ms: 6807.2
    update_time_ms: 26.253
  timestamp: 1602794259
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:37:40,625	WARNING util.py:136 -- The `process_trial` operation took 0.676612138748169 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    250 |          6435.25 | 40448000 |  306.939 |              338.535 |              107.323 |            778.697 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3185.1342196776113
    time_step_min: 2977
  date: 2020-10-15_20-38-06
  done: false
  episode_len_mean: 778.6188209095621
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.06401520726456
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 240
  episodes_total: 52091
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4112447955914956e-19
        cur_lr: 5.0e-05
        entropy: 0.0792290735989809
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008520802075508982
        total_loss: .inf
        vf_explained_var: 0.9987069964408875
        vf_loss: 0.49847019215424854
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.62666666666667
    gpu_util_percent0: 0.328
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465252966221443
    mean_env_wait_ms: 1.2041510407011875
    mean_inference_ms: 4.29628216393922
    mean_raw_obs_processing_ms: 0.3785318787131967
  time_since_restore: 6461.234031200409
  time_this_iter_s: 25.986616134643555
  time_total_s: 6461.234031200409
  timers:
    learn_throughput: 8663.267
    learn_time_ms: 18675.634
    sample_throughput: 23778.446
    sample_time_ms: 6804.145
    update_time_ms: 27.92
  timestamp: 1602794286
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:38:07,555	WARNING util.py:136 -- The `process_trial` operation took 0.6978309154510498 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    251 |          6461.23 | 40609792 |  307.064 |              338.535 |              107.323 |            778.619 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3184.431794695143
    time_step_min: 2977
  date: 2020-10-15_20-38-33
  done: false
  episode_len_mean: 778.5533310387028
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.1738872200667
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 205
  episodes_total: 52296
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1168671933872431e-19
        cur_lr: 5.0e-05
        entropy: 0.07174824612836044
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007992092203494394
        total_loss: .inf
        vf_explained_var: 0.9991335272789001
        vf_loss: 0.2874569122989972
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.286666666666665
    gpu_util_percent0: 0.32333333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465234211567278
    mean_env_wait_ms: 1.204123107632824
    mean_inference_ms: 4.29615497935776
    mean_raw_obs_processing_ms: 0.37852263961927707
  time_since_restore: 6487.012424707413
  time_this_iter_s: 25.778393507003784
  time_total_s: 6487.012424707413
  timers:
    learn_throughput: 8659.794
    learn_time_ms: 18683.123
    sample_throughput: 23798.436
    sample_time_ms: 6798.43
    update_time_ms: 26.68
  timestamp: 1602794313
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:38:34,282	WARNING util.py:136 -- The `process_trial` operation took 0.7088077068328857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    252 |          6487.01 | 40771584 |  307.174 |              338.535 |              107.323 |            778.553 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3183.7602776294716
    time_step_min: 2977
  date: 2020-10-15_20-39-00
  done: false
  episode_len_mean: 778.4949129291621
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.27476629650545
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 190
  episodes_total: 52486
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.175300790080865e-19
        cur_lr: 5.0e-05
        entropy: 0.0749078777929147
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006930598407052457
        total_loss: .inf
        vf_explained_var: 0.9988849759101868
        vf_loss: 0.4067942450443904
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.52258064516129
    gpu_util_percent0: 0.31580645161290327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652101376263757
    mean_env_wait_ms: 1.204097711944695
    mean_inference_ms: 4.296036238665996
    mean_raw_obs_processing_ms: 0.3785147962645749
  time_since_restore: 6512.866728067398
  time_this_iter_s: 25.85430335998535
  time_total_s: 6512.866728067398
  timers:
    learn_throughput: 8653.019
    learn_time_ms: 18697.753
    sample_throughput: 23807.85
    sample_time_ms: 6795.742
    update_time_ms: 28.806
  timestamp: 1602794340
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:39:01,273	WARNING util.py:136 -- The `process_trial` operation took 0.7120022773742676 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    253 |          6512.87 | 40933376 |  307.275 |              338.535 |              107.323 |            778.495 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3182.8811349402163
    time_step_min: 2977
  date: 2020-10-15_20-39-27
  done: false
  episode_len_mean: 778.4209967382235
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.40327974426816
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 246
  episodes_total: 52732
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.762951185121297e-19
        cur_lr: 5.0e-05
        entropy: 0.07806690782308578
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008381410695922872
        total_loss: .inf
        vf_explained_var: 0.9986740946769714
        vf_loss: 0.539829875032107
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.910000000000004
    gpu_util_percent0: 0.3780000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651831894855546
    mean_env_wait_ms: 1.2040642423832428
    mean_inference_ms: 4.29588974176537
    mean_raw_obs_processing_ms: 0.3785026765745503
  time_since_restore: 6538.631947994232
  time_this_iter_s: 25.765219926834106
  time_total_s: 6538.631947994232
  timers:
    learn_throughput: 8667.552
    learn_time_ms: 18666.402
    sample_throughput: 23788.24
    sample_time_ms: 6801.344
    update_time_ms: 27.571
  timestamp: 1602794367
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:39:28,193	WARNING util.py:136 -- The `process_trial` operation took 0.7845938205718994 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    254 |          6538.63 | 41095168 |  307.403 |              338.535 |              107.323 |            778.421 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3182.1913666616247
    time_step_min: 2977
  date: 2020-10-15_20-39-54
  done: false
  episode_len_mean: 778.3616096731532
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.5055972534718
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 198
  episodes_total: 52930
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.144426777681947e-19
        cur_lr: 5.0e-05
        entropy: 0.06797104390958945
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008431720446120986
        total_loss: .inf
        vf_explained_var: 0.9988293647766113
        vf_loss: 0.399315503736337
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43225806451613
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651645872706312
    mean_env_wait_ms: 1.2040380674541218
    mean_inference_ms: 4.295773448329573
    mean_raw_obs_processing_ms: 0.378494546442028
  time_since_restore: 6564.6285037994385
  time_this_iter_s: 25.9965558052063
  time_total_s: 6564.6285037994385
  timers:
    learn_throughput: 8654.452
    learn_time_ms: 18694.657
    sample_throughput: 23812.361
    sample_time_ms: 6794.454
    update_time_ms: 27.29
  timestamp: 1602794394
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:39:55,351	WARNING util.py:136 -- The `process_trial` operation took 0.7476279735565186 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    255 |          6564.63 | 41256960 |  307.506 |              338.535 |              107.323 |            778.362 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3181.501309270388
    time_step_min: 2977
  date: 2020-10-15_20-40-21
  done: false
  episode_len_mean: 778.3025129411765
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.6070882947119
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 195
  episodes_total: 53125
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.071664016652292e-18
        cur_lr: 5.0e-05
        entropy: 0.07636726647615433
        entropy_coeff: 0.0005000000000000001
        kl: 0.012967311078682542
        model: {}
        policy_loss: -0.00908166698839826
        total_loss: 0.3142879381775856
        vf_explained_var: 0.9990211129188538
        vf_loss: 0.3234077903131644
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.540000000000006
    gpu_util_percent0: 0.3123333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465139246251538
    mean_env_wait_ms: 1.2040116458803178
    mean_inference_ms: 4.29565239705713
    mean_raw_obs_processing_ms: 0.37848659599462847
  time_since_restore: 6590.470329761505
  time_this_iter_s: 25.84182596206665
  time_total_s: 6590.470329761505
  timers:
    learn_throughput: 8657.664
    learn_time_ms: 18687.721
    sample_throughput: 23793.837
    sample_time_ms: 6799.744
    update_time_ms: 25.944
  timestamp: 1602794421
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: f76cb_00000
  
2020-10-15 20:40:22,166	WARNING util.py:136 -- The `process_trial` operation took 0.7270069122314453 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    256 |          6590.47 | 41418752 |  307.607 |              338.535 |              107.323 |            778.303 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3180.901239429225
    time_step_min: 2977
  date: 2020-10-15_20-40-47
  done: false
  episode_len_mean: 778.2324583590954
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.6610095105402
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 248
  episodes_total: 53373
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.071664016652292e-18
        cur_lr: 5.0e-05
        entropy: 0.13014830648899078
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01780702379376938
        total_loss: .inf
        vf_explained_var: 0.9921877980232239
        vf_loss: 3.4901257157325745
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.663333333333334
    gpu_util_percent0: 0.359
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651165188805715
    mean_env_wait_ms: 1.203979290617267
    mean_inference_ms: 4.295513792991343
    mean_raw_obs_processing_ms: 0.3784746501146349
  time_since_restore: 6616.209138393402
  time_this_iter_s: 25.738808631896973
  time_total_s: 6616.209138393402
  timers:
    learn_throughput: 8658.067
    learn_time_ms: 18686.85
    sample_throughput: 23795.471
    sample_time_ms: 6799.277
    update_time_ms: 28.111
  timestamp: 1602794447
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:40:48,891	WARNING util.py:136 -- The `process_trial` operation took 0.7408702373504639 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    257 |          6616.21 | 41580544 |  307.661 |              338.535 |              107.323 |            778.232 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3181.2280269058297
    time_step_min: 2977
  date: 2020-10-15_20-41-14
  done: false
  episode_len_mean: 778.1819200179232
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.6021161542613
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 189
  episodes_total: 53562
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6074960249784377e-18
        cur_lr: 5.0e-05
        entropy: 0.14852150529623032
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010962046896262715
        total_loss: .inf
        vf_explained_var: 0.9886415600776672
        vf_loss: 5.078034679094951
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706666666666674
    gpu_util_percent0: 0.325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650972031791146
    mean_env_wait_ms: 1.2039555190002287
    mean_inference_ms: 4.295401691336388
    mean_raw_obs_processing_ms: 0.3784670659095386
  time_since_restore: 6642.084903240204
  time_this_iter_s: 25.875764846801758
  time_total_s: 6642.084903240204
  timers:
    learn_throughput: 8655.212
    learn_time_ms: 18693.014
    sample_throughput: 23787.108
    sample_time_ms: 6801.668
    update_time_ms: 28.481
  timestamp: 1602794474
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:41:15,853	WARNING util.py:136 -- The `process_trial` operation took 0.7462558746337891 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    258 |          6642.08 | 41742336 |  307.602 |              338.535 |              107.323 |            778.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3181.655074190606
    time_step_min: 2977
  date: 2020-10-15_20-41-41
  done: false
  episode_len_mean: 778.1382010975723
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.53731529789576
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 193
  episodes_total: 53755
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4112440374676563e-18
        cur_lr: 5.0e-05
        entropy: 0.15667657802502313
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01303709321655333
        total_loss: .inf
        vf_explained_var: 0.9879671931266785
        vf_loss: 5.86124320824941
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.870967741935484
    gpu_util_percent0: 0.34838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650731849810503
    mean_env_wait_ms: 1.2039315777360946
    mean_inference_ms: 4.295284104199752
    mean_raw_obs_processing_ms: 0.37845940039632264
  time_since_restore: 6667.8041269779205
  time_this_iter_s: 25.719223737716675
  time_total_s: 6667.8041269779205
  timers:
    learn_throughput: 8659.796
    learn_time_ms: 18683.118
    sample_throughput: 23782.514
    sample_time_ms: 6802.982
    update_time_ms: 30.188
  timestamp: 1602794501
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:41:42,721	WARNING util.py:136 -- The `process_trial` operation took 0.700087308883667 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    259 |           6667.8 | 41904128 |  307.537 |              338.535 |              107.323 |            778.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3182.1402390438247
    time_step_min: 2977
  date: 2020-10-15_20-42-08
  done: false
  episode_len_mean: 778.0872664654581
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.4609080790687
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 252
  episodes_total: 54007
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6168660562014854e-18
        cur_lr: 5.0e-05
        entropy: 0.15399499610066414
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045864232039699955
        model: {}
        policy_loss: -0.011384651258898279
        total_loss: 5.857638756434123
        vf_explained_var: 0.9894563555717468
        vf_loss: 5.869100292523702
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.953333333333337
    gpu_util_percent0: 0.36933333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650494243799622
    mean_env_wait_ms: 1.2039012824577096
    mean_inference_ms: 4.29514580182558
    mean_raw_obs_processing_ms: 0.3784477519006688
  time_since_restore: 6693.714605808258
  time_this_iter_s: 25.910478830337524
  time_total_s: 6693.714605808258
  timers:
    learn_throughput: 8659.296
    learn_time_ms: 18684.198
    sample_throughput: 23667.436
    sample_time_ms: 6836.06
    update_time_ms: 31.833
  timestamp: 1602794528
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: f76cb_00000
  
2020-10-15 20:42:09,599	WARNING util.py:136 -- The `process_trial` operation took 0.7213804721832275 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    260 |          6693.71 | 42065920 |  307.461 |              338.535 |              107.323 |            778.087 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3182.4856330329444
    time_step_min: 2977
  date: 2020-10-15_20-42-35
  done: false
  episode_len_mean: 778.0483263829944
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.40685725767094
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 187
  episodes_total: 54194
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8084330281007427e-18
        cur_lr: 5.0e-05
        entropy: 0.14644153540333113
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01237689913250506
        total_loss: .inf
        vf_explained_var: 0.9893390536308289
        vf_loss: 4.8760643402735395
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.469999999999995
    gpu_util_percent0: 0.36500000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650312645895125
    mean_env_wait_ms: 1.2038796192062684
    mean_inference_ms: 4.295036783993251
    mean_raw_obs_processing_ms: 0.37844038707536537
  time_since_restore: 6719.4470183849335
  time_this_iter_s: 25.732412576675415
  time_total_s: 6719.4470183849335
  timers:
    learn_throughput: 8676.367
    learn_time_ms: 18647.435
    sample_throughput: 23633.553
    sample_time_ms: 6845.86
    update_time_ms: 31.935
  timestamp: 1602794555
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:42:36,390	WARNING util.py:136 -- The `process_trial` operation took 0.7089114189147949 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    261 |          6719.45 | 42227712 |  307.407 |              338.535 |              107.323 |            778.048 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3182.8415735919184
    time_step_min: 2977
  date: 2020-10-15_20-43-02
  done: false
  episode_len_mean: 778.0047436062439
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.35207059656864
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 195
  episodes_total: 54389
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.712649542151114e-18
        cur_lr: 5.0e-05
        entropy: 0.15240937223037085
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011319675996977216
        total_loss: .inf
        vf_explained_var: 0.9870679378509521
        vf_loss: 6.1244252522786455
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.212903225806453
    gpu_util_percent0: 0.3435483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650077487539032
    mean_env_wait_ms: 1.2038574074271526
    mean_inference_ms: 4.294922375895806
    mean_raw_obs_processing_ms: 0.3784330287002346
  time_since_restore: 6745.409341573715
  time_this_iter_s: 25.96232318878174
  time_total_s: 6745.409341573715
  timers:
    learn_throughput: 8676.061
    learn_time_ms: 18648.094
    sample_throughput: 23577.297
    sample_time_ms: 6862.195
    update_time_ms: 32.903
  timestamp: 1602794582
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:43:03,361	WARNING util.py:136 -- The `process_trial` operation took 0.7560334205627441 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    262 |          6745.41 | 42389504 |  307.352 |              338.535 |              107.323 |            778.005 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3183.339633699634
    time_step_min: 2977
  date: 2020-10-15_20-43-29
  done: false
  episode_len_mean: 777.9500567329161
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.27665088349175
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 253
  episodes_total: 54642
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.068974313226672e-18
        cur_lr: 5.0e-05
        entropy: 0.14966768895586333
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01223768941902866
        total_loss: .inf
        vf_explained_var: 0.9879195094108582
        vf_loss: 6.897783517837524
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82333333333333
    gpu_util_percent0: 0.3196666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649847997844623
    mean_env_wait_ms: 1.203829817895604
    mean_inference_ms: 4.294784482275207
    mean_raw_obs_processing_ms: 0.3784213108294294
  time_since_restore: 6771.224378824234
  time_this_iter_s: 25.8150372505188
  time_total_s: 6771.224378824234
  timers:
    learn_throughput: 8682.087
    learn_time_ms: 18635.151
    sample_throughput: 23546.662
    sample_time_ms: 6871.122
    update_time_ms: 32.617
  timestamp: 1602794609
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:43:30,146	WARNING util.py:136 -- The `process_trial` operation took 0.7250356674194336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    263 |          6771.22 | 42551296 |  307.277 |              338.535 |              107.323 |             777.95 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3183.6444034754672
    time_step_min: 2977
  date: 2020-10-15_20-43-56
  done: false
  episode_len_mean: 777.916827782439
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.22978793884937
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 54826
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103461469840008e-18
        cur_lr: 5.0e-05
        entropy: 0.1399148367345333
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014325960558683922
        total_loss: .inf
        vf_explained_var: 0.9893584847450256
        vf_loss: 4.801400383313497
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.103225806451615
    gpu_util_percent0: 0.2932258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649666478134382
    mean_env_wait_ms: 1.2038098091621716
    mean_inference_ms: 4.294679227029955
    mean_raw_obs_processing_ms: 0.3784144378062483
  time_since_restore: 6797.139917850494
  time_this_iter_s: 25.915539026260376
  time_total_s: 6797.139917850494
  timers:
    learn_throughput: 8667.676
    learn_time_ms: 18666.133
    sample_throughput: 23587.613
    sample_time_ms: 6859.194
    update_time_ms: 34.017
  timestamp: 1602794636
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:43:57,225	WARNING util.py:136 -- The `process_trial` operation took 0.7316222190856934 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    264 |          6797.14 | 42713088 |   307.23 |              338.535 |              107.323 |            777.917 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3183.9890323754094
    time_step_min: 2977
  date: 2020-10-15_20-44-23
  done: false
  episode_len_mean: 777.8872632765076
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.18081270705676
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 196
  episodes_total: 55022
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.155192204760009e-18
        cur_lr: 5.0e-05
        entropy: 0.1478911911447843
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010722911518920833
        total_loss: .inf
        vf_explained_var: 0.9885303378105164
        vf_loss: 5.421724319458008
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.290322580645164
    gpu_util_percent0: 0.3064516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649427653552477
    mean_env_wait_ms: 1.2037893084810418
    mean_inference_ms: 4.294565485368638
    mean_raw_obs_processing_ms: 0.37840709815645657
  time_since_restore: 6823.1183252334595
  time_this_iter_s: 25.978407382965088
  time_total_s: 6823.1183252334595
  timers:
    learn_throughput: 8672.5
    learn_time_ms: 18655.75
    sample_throughput: 23562.872
    sample_time_ms: 6866.395
    update_time_ms: 34.687
  timestamp: 1602794663
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:44:24,418	WARNING util.py:136 -- The `process_trial` operation took 0.7739467620849609 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    265 |          6823.12 | 42874880 |  307.181 |              338.535 |              107.323 |            777.887 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3184.416868866114
    time_step_min: 2977
  date: 2020-10-15_20-44-50
  done: false
  episode_len_mean: 777.8543641890425
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.125627703539
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 246
  episodes_total: 55268
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3732788307140015e-17
        cur_lr: 5.0e-05
        entropy: 0.14692681158582369
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011280365327062706
        total_loss: .inf
        vf_explained_var: 0.9899353981018066
        vf_loss: 5.606996456782023
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.240000000000006
    gpu_util_percent0: 0.3483333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464920843828726
    mean_env_wait_ms: 1.2037635232857298
    mean_inference_ms: 4.29443041024099
    mean_raw_obs_processing_ms: 0.37839594234693774
  time_since_restore: 6849.16225194931
  time_this_iter_s: 26.04392671585083
  time_total_s: 6849.16225194931
  timers:
    learn_throughput: 8661.183
    learn_time_ms: 18680.127
    sample_throughput: 23581.455
    sample_time_ms: 6860.985
    update_time_ms: 34.807
  timestamp: 1602794690
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:44:51,525	WARNING util.py:136 -- The `process_trial` operation took 0.715806245803833 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    266 |          6849.16 | 43036672 |  307.126 |              338.535 |              107.323 |            777.854 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3184.67612826389
    time_step_min: 2977
  date: 2020-10-15_20-45-17
  done: false
  episode_len_mean: 777.8271515894625
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.0852022633519
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 191
  episodes_total: 55459
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0599182460710022e-17
        cur_lr: 5.0e-05
        entropy: 0.13782811413208643
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011215082350342223
        total_loss: .inf
        vf_explained_var: 0.9901741147041321
        vf_loss: 4.582033395767212
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.025806451612905
    gpu_util_percent0: 0.28903225806451616
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649032732341818
    mean_env_wait_ms: 1.2037449642529194
    mean_inference_ms: 4.294327567921494
    mean_raw_obs_processing_ms: 0.3783889709104104
  time_since_restore: 6875.241285324097
  time_this_iter_s: 26.079033374786377
  time_total_s: 6875.241285324097
  timers:
    learn_throughput: 8648.588
    learn_time_ms: 18707.331
    sample_throughput: 23567.96
    sample_time_ms: 6864.913
    update_time_ms: 34.984
  timestamp: 1602794717
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:45:18,737	WARNING util.py:136 -- The `process_trial` operation took 0.8102619647979736 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    267 |          6875.24 | 43198464 |  307.085 |              338.535 |              107.323 |            777.827 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3184.9414652567975
    time_step_min: 2977
  date: 2020-10-15_20-45-44
  done: false
  episode_len_mean: 777.8058400718778
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.0424160744916
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 191
  episodes_total: 55650
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0898773691065033e-17
        cur_lr: 5.0e-05
        entropy: 0.1462759350736936
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01404391931525121
        total_loss: .inf
        vf_explained_var: 0.9896497130393982
        vf_loss: 4.961002031962077
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.567741935483873
    gpu_util_percent0: 0.33741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648808637123428
    mean_env_wait_ms: 1.2037268196053559
    mean_inference_ms: 4.294223045776312
    mean_raw_obs_processing_ms: 0.37838216659056767
  time_since_restore: 6900.95204782486
  time_this_iter_s: 25.71076250076294
  time_total_s: 6900.95204782486
  timers:
    learn_throughput: 8656.622
    learn_time_ms: 18689.969
    sample_throughput: 23568.486
    sample_time_ms: 6864.76
    update_time_ms: 36.397
  timestamp: 1602794744
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:45:45,636	WARNING util.py:136 -- The `process_trial` operation took 0.7574398517608643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    268 |          6900.95 | 43360256 |  307.042 |              338.535 |              107.323 |            777.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3185.279525233176
    time_step_min: 2977
  date: 2020-10-15_20-46-11
  done: false
  episode_len_mean: 777.7720792114632
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.99496801614833
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 251
  episodes_total: 55901
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6348160536597556e-17
        cur_lr: 5.0e-05
        entropy: 0.14279536406199136
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011486068581386158
        total_loss: .inf
        vf_explained_var: 0.9909279346466064
        vf_loss: 5.132247606913249
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77
    gpu_util_percent0: 0.3443333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648577467013318
    mean_env_wait_ms: 1.2037025218383561
    mean_inference_ms: 4.294085001575583
    mean_raw_obs_processing_ms: 0.37837118226466443
  time_since_restore: 6927.0212326049805
  time_this_iter_s: 26.06918478012085
  time_total_s: 6927.0212326049805
  timers:
    learn_throughput: 8644.347
    learn_time_ms: 18716.51
    sample_throughput: 23538.422
    sample_time_ms: 6873.528
    update_time_ms: 34.565
  timestamp: 1602794771
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:46:12,948	WARNING util.py:136 -- The `process_trial` operation took 0.7331852912902832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    269 |          6927.02 | 43522048 |  306.995 |              338.535 |              107.323 |            777.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3185.409541821296
    time_step_min: 2977
  date: 2020-10-15_20-46-38
  done: false
  episode_len_mean: 777.7466571581388
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.97866974253134
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 189
  episodes_total: 56090
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.952224080489634e-17
        cur_lr: 5.0e-05
        entropy: 0.11915760301053524
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011416686000302434
        total_loss: .inf
        vf_explained_var: 0.9909549355506897
        vf_loss: 4.112828075885773
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.783870967741933
    gpu_util_percent0: 0.3070967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648406278222648
    mean_env_wait_ms: 1.2036846968172266
    mean_inference_ms: 4.2939842372290125
    mean_raw_obs_processing_ms: 0.37836424350278847
  time_since_restore: 6952.80847454071
  time_this_iter_s: 25.78724193572998
  time_total_s: 6952.80847454071
  timers:
    learn_throughput: 8637.804
    learn_time_ms: 18730.687
    sample_throughput: 23660.315
    sample_time_ms: 6838.117
    update_time_ms: 32.837
  timestamp: 1602794798
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:46:39,862	WARNING util.py:136 -- The `process_trial` operation took 0.7599709033966064 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    270 |          6952.81 | 43683840 |  306.979 |              338.535 |              107.323 |            777.747 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3185.336047793464
    time_step_min: 2977
  date: 2020-10-15_20-47-05
  done: false
  episode_len_mean: 777.7036813303959
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 306.99891926155163
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 194
  episodes_total: 56284
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0428336120734449e-16
        cur_lr: 5.0e-05
        entropy: 0.10467610942820708
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011500090806900213
        total_loss: .inf
        vf_explained_var: 0.9936556816101074
        vf_loss: 2.643422802289327
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.436666666666667
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648175848101927
    mean_env_wait_ms: 1.2036675819200653
    mean_inference_ms: 4.2938768575114405
    mean_raw_obs_processing_ms: 0.37835739283254005
  time_since_restore: 6978.766433477402
  time_this_iter_s: 25.957958936691284
  time_total_s: 6978.766433477402
  timers:
    learn_throughput: 8624.363
    learn_time_ms: 18759.878
    sample_throughput: 23684.53
    sample_time_ms: 6831.126
    update_time_ms: 31.852
  timestamp: 1602794825
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:47:06,924	WARNING util.py:136 -- The `process_trial` operation took 0.7219173908233643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    271 |          6978.77 | 43845632 |  306.999 |              338.535 |              107.323 |            777.704 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3184.899833616539
    time_step_min: 2977
  date: 2020-10-15_20-47-32
  done: false
  episode_len_mean: 777.6362092751777
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.07928269214483
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 254
  episodes_total: 56538
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5642504181101674e-16
        cur_lr: 5.0e-05
        entropy: 0.0877623005459706
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010409547210050126
        total_loss: .inf
        vf_explained_var: 0.9962994456291199
        vf_loss: 1.648138831059138
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.58709677419355
    gpu_util_percent0: 0.34193548387096784
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647958232814215
    mean_env_wait_ms: 1.2036428726905455
    mean_inference_ms: 4.293744126789761
    mean_raw_obs_processing_ms: 0.37834642404485025
  time_since_restore: 7004.768778800964
  time_this_iter_s: 26.002345323562622
  time_total_s: 7004.768778800964
  timers:
    learn_throughput: 8618.046
    learn_time_ms: 18773.629
    sample_throughput: 23722.344
    sample_time_ms: 6820.237
    update_time_ms: 32.472
  timestamp: 1602794852
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:47:33,977	WARNING util.py:136 -- The `process_trial` operation took 0.7463126182556152 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    272 |          7004.77 | 44007424 |  307.079 |              338.535 |              107.323 |            777.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3184.3418782308004
    time_step_min: 2977
  date: 2020-10-15_20-47-59
  done: false
  episode_len_mean: 777.5855473088518
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.16086699550203
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 185
  episodes_total: 56723
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3463756271652516e-16
        cur_lr: 5.0e-05
        entropy: 0.06620465343197186
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009239629326960616
        total_loss: .inf
        vf_explained_var: 0.9975953102111816
        vf_loss: 0.7820113251606623
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43548387096775
    gpu_util_percent0: 0.3641935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647802901385165
    mean_env_wait_ms: 1.2036261126726224
    mean_inference_ms: 4.293648258959883
    mean_raw_obs_processing_ms: 0.3783401546626786
  time_since_restore: 7030.686520814896
  time_this_iter_s: 25.917742013931274
  time_total_s: 7030.686520814896
  timers:
    learn_throughput: 8609.31
    learn_time_ms: 18792.679
    sample_throughput: 23756.266
    sample_time_ms: 6810.498
    update_time_ms: 32.223
  timestamp: 1602794879
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:48:01,103	WARNING util.py:136 -- The `process_trial` operation took 0.7873969078063965 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    273 |          7030.69 | 44169216 |  307.161 |              338.535 |              107.323 |            777.586 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3183.811192376666
    time_step_min: 2977
  date: 2020-10-15_20-48-27
  done: false
  episode_len_mean: 777.5328882642305
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.2443656523066
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 197
  episodes_total: 56920
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5195634407478766e-16
        cur_lr: 5.0e-05
        entropy: 0.0764366773267587
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006730879928606252
        total_loss: .inf
        vf_explained_var: 0.9978902339935303
        vf_loss: 0.7429852038621902
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896666666666672
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647572003382128
    mean_env_wait_ms: 1.2036085125794045
    mean_inference_ms: 4.293540902822499
    mean_raw_obs_processing_ms: 0.37833343940199976
  time_since_restore: 7056.70014333725
  time_this_iter_s: 26.013622522354126
  time_total_s: 7056.70014333725
  timers:
    learn_throughput: 8610.911
    learn_time_ms: 18789.185
    sample_throughput: 23700.477
    sample_time_ms: 6826.529
    update_time_ms: 30.835
  timestamp: 1602794907
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:48:28,239	WARNING util.py:136 -- The `process_trial` operation took 0.7510900497436523 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    274 |           7056.7 | 44331008 |  307.244 |              338.535 |              107.323 |            777.533 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3183.160537692752
    time_step_min: 2977
  date: 2020-10-15_20-48-54
  done: false
  episode_len_mean: 777.4626847398339
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.35018837257576
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 255
  episodes_total: 57175
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.279345161121816e-16
        cur_lr: 5.0e-05
        entropy: 0.07772881040970485
        entropy_coeff: 0.0005000000000000001
        kl: 0.003608963277656585
        model: {}
        policy_loss: -0.009322371852552655
        total_loss: 0.9752630392710367
        vf_explained_var: 0.9976792335510254
        vf_loss: 0.9846242616573969
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4741935483871
    gpu_util_percent0: 0.3309677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647365202688814
    mean_env_wait_ms: 1.2035841994209773
    mean_inference_ms: 4.293414553353311
    mean_raw_obs_processing_ms: 0.37832280820604286
  time_since_restore: 7082.796196937561
  time_this_iter_s: 26.09605360031128
  time_total_s: 7082.796196937561
  timers:
    learn_throughput: 8606.884
    learn_time_ms: 18797.976
    sample_throughput: 23704.189
    sample_time_ms: 6825.46
    update_time_ms: 32.12
  timestamp: 1602794934
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: f76cb_00000
  
2020-10-15 20:48:55,360	WARNING util.py:136 -- The `process_trial` operation took 0.755141019821167 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    275 |           7082.8 | 44492800 |   307.35 |              338.535 |              107.323 |            777.463 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3182.641291830966
    time_step_min: 2977
  date: 2020-10-15_20-49-21
  done: false
  episode_len_mean: 777.412912336983
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.43040718222045
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 57356
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.639672580560908e-16
        cur_lr: 5.0e-05
        entropy: 0.06446288526058197
        entropy_coeff: 0.0005000000000000001
        kl: 0.003629195135241995
        model: {}
        policy_loss: -0.008455388810640821
        total_loss: 0.640124668677648
        vf_explained_var: 0.9979961514472961
        vf_loss: 0.6486123154560725
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.486666666666668
    gpu_util_percent0: 0.35500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647201075324823
    mean_env_wait_ms: 1.2035673140797325
    mean_inference_ms: 4.293318587761276
    mean_raw_obs_processing_ms: 0.37831651406714273
  time_since_restore: 7108.448920965195
  time_this_iter_s: 25.652724027633667
  time_total_s: 7108.448920965195
  timers:
    learn_throughput: 8620.013
    learn_time_ms: 18769.345
    sample_throughput: 23745.36
    sample_time_ms: 6813.626
    update_time_ms: 31.589
  timestamp: 1602794961
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: f76cb_00000
  
2020-10-15 20:49:22,121	WARNING util.py:136 -- The `process_trial` operation took 0.7457499504089355 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    276 |          7108.45 | 44654592 |   307.43 |              338.535 |              107.323 |            777.413 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3182.0028340925687
    time_step_min: 2977
  date: 2020-10-15_20-49-48
  done: false
  episode_len_mean: 777.3597713531169
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.5252095280416
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 200
  episodes_total: 57556
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.319836290280454e-16
        cur_lr: 5.0e-05
        entropy: 0.0666519080599149
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00860542599063289
        total_loss: .inf
        vf_explained_var: 0.9987034201622009
        vf_loss: 0.44752614945173264
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.72903225806452
    gpu_util_percent0: 0.3548387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646984440250418
    mean_env_wait_ms: 1.2035494789497825
    mean_inference_ms: 4.293214807484746
    mean_raw_obs_processing_ms: 0.3783100431178285
  time_since_restore: 7134.389147520065
  time_this_iter_s: 25.940226554870605
  time_total_s: 7134.389147520065
  timers:
    learn_throughput: 8622.754
    learn_time_ms: 18763.379
    sample_throughput: 23762.924
    sample_time_ms: 6808.59
    update_time_ms: 29.669
  timestamp: 1602794988
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:49:49,118	WARNING util.py:136 -- The `process_trial` operation took 0.7928400039672852 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    277 |          7134.39 | 44816384 |  307.525 |              338.535 |              107.323 |             777.36 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3181.2279077013623
    time_step_min: 2977
  date: 2020-10-15_20-50-14
  done: false
  episode_len_mean: 777.2925913753438
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.64150560979886
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 255
  episodes_total: 57811
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9797544354206807e-16
        cur_lr: 5.0e-05
        entropy: 0.06999784087141354
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007904879506289339
        total_loss: .inf
        vf_explained_var: 0.9986665844917297
        vf_loss: 0.5380827635526657
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.620000000000005
    gpu_util_percent0: 0.3503333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646758452869368
    mean_env_wait_ms: 1.2035257037401343
    mean_inference_ms: 4.293084941373102
    mean_raw_obs_processing_ms: 0.37829908990082445
  time_since_restore: 7160.185968637466
  time_this_iter_s: 25.796821117401123
  time_total_s: 7160.185968637466
  timers:
    learn_throughput: 8622.743
    learn_time_ms: 18763.404
    sample_throughput: 23738.578
    sample_time_ms: 6815.573
    update_time_ms: 29.468
  timestamp: 1602795014
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:50:15,940	WARNING util.py:136 -- The `process_trial` operation took 0.7527132034301758 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    278 |          7160.19 | 44978176 |  307.642 |              338.535 |              107.323 |            777.293 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3180.6766644807235
    time_step_min: 2977
  date: 2020-10-15_20-50-41
  done: false
  episode_len_mean: 777.2435331447886
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.72464766308315
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 177
  episodes_total: 57988
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.969631653131021e-16
        cur_lr: 5.0e-05
        entropy: 0.0633130306378007
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006798344858301182
        total_loss: .inf
        vf_explained_var: 0.9987709522247314
        vf_loss: 0.38503208259741467
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.286666666666672
    gpu_util_percent0: 0.32399999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464661873102014
    mean_env_wait_ms: 1.2035088979434958
    mean_inference_ms: 4.29299871171418
    mean_raw_obs_processing_ms: 0.37829342706977787
  time_since_restore: 7186.033166408539
  time_this_iter_s: 25.847197771072388
  time_total_s: 7186.033166408539
  timers:
    learn_throughput: 8629.188
    learn_time_ms: 18749.389
    sample_throughput: 23779.738
    sample_time_ms: 6803.776
    update_time_ms: 31.49
  timestamp: 1602795041
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:50:42,945	WARNING util.py:136 -- The `process_trial` operation took 0.7663652896881104 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    279 |          7186.03 | 45139968 |  307.725 |              338.535 |              107.323 |            777.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3180.215424297137
    time_step_min: 2977
  date: 2020-10-15_20-51-09
  done: false
  episode_len_mean: 777.1926731618469
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.7811571043181
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 209
  episodes_total: 58197
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.454447479696531e-16
        cur_lr: 5.0e-05
        entropy: 0.1281051319092512
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013889348595209109
        total_loss: .inf
        vf_explained_var: 0.9928295612335205
        vf_loss: 2.6933273673057556
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.303225806451614
    gpu_util_percent0: 0.3283870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646398149425305
    mean_env_wait_ms: 1.2034908967864628
    mean_inference_ms: 4.2928986027282106
    mean_raw_obs_processing_ms: 0.3782868836851735
  time_since_restore: 7212.10862326622
  time_this_iter_s: 26.075456857681274
  time_total_s: 7212.10862326622
  timers:
    learn_throughput: 8615.52
    learn_time_ms: 18779.133
    sample_throughput: 23755.735
    sample_time_ms: 6810.65
    update_time_ms: 31.972
  timestamp: 1602795069
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:51:10,098	WARNING util.py:136 -- The `process_trial` operation took 0.7855453491210938 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    280 |          7212.11 | 45301760 |  307.781 |              338.535 |              107.323 |            777.193 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3180.580960534201
    time_step_min: 2977
  date: 2020-10-15_20-51-36
  done: false
  episode_len_mean: 777.1599568840146
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.7180033434418
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 250
  episodes_total: 58447
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.681671219544798e-16
        cur_lr: 5.0e-05
        entropy: 0.16088897859056792
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012124423888356736
        total_loss: .inf
        vf_explained_var: 0.9884515404701233
        vf_loss: 6.167559583981832
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.700000000000003
    gpu_util_percent0: 0.357741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646193816026873
    mean_env_wait_ms: 1.2034687713193029
    mean_inference_ms: 4.292770775319455
    mean_raw_obs_processing_ms: 0.3782764913816366
  time_since_restore: 7238.147034406662
  time_this_iter_s: 26.038411140441895
  time_total_s: 7238.147034406662
  timers:
    learn_throughput: 8615.799
    learn_time_ms: 18778.526
    sample_throughput: 23767.41
    sample_time_ms: 6807.305
    update_time_ms: 31.297
  timestamp: 1602795096
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:51:37,211	WARNING util.py:136 -- The `process_trial` operation took 0.7926335334777832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    281 |          7238.15 | 45463552 |  307.718 |              338.535 |              107.323 |             777.16 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3180.865139384421
    time_step_min: 2977
  date: 2020-10-15_20-52-03
  done: false
  episode_len_mean: 777.132273417376
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.6707083802664
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 174
  episodes_total: 58621
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0022506829317198e-15
        cur_lr: 5.0e-05
        entropy: 0.14339621240894
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012233456998122469
        total_loss: .inf
        vf_explained_var: 0.9892923831939697
        vf_loss: 4.765914638837178
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53225806451613
    gpu_util_percent0: 0.325483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646042818652288
    mean_env_wait_ms: 1.203453107018386
    mean_inference_ms: 4.29268542640139
    mean_raw_obs_processing_ms: 0.37827092311269866
  time_since_restore: 7264.19578742981
  time_this_iter_s: 26.048753023147583
  time_total_s: 7264.19578742981
  timers:
    learn_throughput: 8615.167
    learn_time_ms: 18779.903
    sample_throughput: 23755.236
    sample_time_ms: 6810.793
    update_time_ms: 29.909
  timestamp: 1602795123
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:52:04,494	WARNING util.py:136 -- The `process_trial` operation took 0.7746870517730713 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    282 |           7264.2 | 45625344 |  307.671 |              338.535 |              107.323 |            777.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3181.0509100187105
    time_step_min: 2977
  date: 2020-10-15_20-52-30
  done: false
  episode_len_mean: 777.0948123470221
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.6581768184978
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 211
  episodes_total: 58832
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5033760243975794e-15
        cur_lr: 5.0e-05
        entropy: 0.12195662098626296
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012275732783564308
        total_loss: .inf
        vf_explained_var: 0.9917623996734619
        vf_loss: 3.824673851331075
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.300000000000004
    gpu_util_percent0: 0.33966666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645833645094883
    mean_env_wait_ms: 1.2034360704987357
    mean_inference_ms: 4.29258748680332
    mean_raw_obs_processing_ms: 0.37826444345447774
  time_since_restore: 7290.244131803513
  time_this_iter_s: 26.048344373703003
  time_total_s: 7290.244131803513
  timers:
    learn_throughput: 8614.201
    learn_time_ms: 18782.01
    sample_throughput: 23718.82
    sample_time_ms: 6821.25
    update_time_ms: 28.989
  timestamp: 1602795150
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:52:31,718	WARNING util.py:136 -- The `process_trial` operation took 0.7795913219451904 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    283 |          7290.24 | 45787136 |  307.658 |              338.535 |              107.323 |            777.095 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3180.774700611481
    time_step_min: 2977
  date: 2020-10-15_20-52-57
  done: false
  episode_len_mean: 777.0446182230572
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.7095563362258
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 247
  episodes_total: 59079
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.255064036596369e-15
        cur_lr: 5.0e-05
        entropy: 0.09279324300587177
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010940508019605963
        total_loss: .inf
        vf_explained_var: 0.9944779872894287
        vf_loss: 2.5146931211153665
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.97096774193549
    gpu_util_percent0: 0.3232258064516128
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645614016495595
    mean_env_wait_ms: 1.2034153146437634
    mean_inference_ms: 4.292458134547121
    mean_raw_obs_processing_ms: 0.3782541922176085
  time_since_restore: 7316.348960161209
  time_this_iter_s: 26.104828357696533
  time_total_s: 7316.348960161209
  timers:
    learn_throughput: 8606.957
    learn_time_ms: 18797.816
    sample_throughput: 23750.243
    sample_time_ms: 6812.225
    update_time_ms: 30.68
  timestamp: 1602795177
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:52:58,922	WARNING util.py:136 -- The `process_trial` operation took 0.8212604522705078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    284 |          7316.35 | 45948928 |   307.71 |              338.535 |              107.323 |            777.045 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3180.3411075270633
    time_step_min: 2977
  date: 2020-10-15_20-53-25
  done: false
  episode_len_mean: 777.00300396591
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.76909164209815
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 59255
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3825960548945535e-15
        cur_lr: 5.0e-05
        entropy: 0.0691839437931776
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010037754332491508
        total_loss: .inf
        vf_explained_var: 0.9975318908691406
        vf_loss: 0.8296967446804047
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.83548387096775
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645476483211
    mean_env_wait_ms: 1.2034000033715677
    mean_inference_ms: 4.292376578922758
    mean_raw_obs_processing_ms: 0.37824888111738264
  time_since_restore: 7342.443197011948
  time_this_iter_s: 26.094236850738525
  time_total_s: 7342.443197011948
  timers:
    learn_throughput: 8619.781
    learn_time_ms: 18769.851
    sample_throughput: 23682.418
    sample_time_ms: 6831.735
    update_time_ms: 30.642
  timestamp: 1602795205
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:53:26,143	WARNING util.py:136 -- The `process_trial` operation took 0.7854921817779541 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    285 |          7342.44 | 46110720 |  307.769 |              338.535 |              107.323 |            777.003 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.7280070000675
    time_step_min: 2977
  date: 2020-10-15_20-53-52
  done: false
  episode_len_mean: 776.9497225491845
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8641696942521
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 215
  episodes_total: 59470
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.073894082341831e-15
        cur_lr: 5.0e-05
        entropy: 0.065873712922136
        entropy_coeff: 0.0005000000000000001
        kl: 0.00338829462028419
        model: {}
        policy_loss: -0.007490160933230072
        total_loss: 0.49526110539833706
        vf_explained_var: 0.9985530972480774
        vf_loss: 0.5027842000126839
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.099999999999998
    gpu_util_percent0: 0.36233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645272932605644
    mean_env_wait_ms: 1.203384137537353
    mean_inference_ms: 4.2922845289963485
    mean_raw_obs_processing_ms: 0.37824255520170874
  time_since_restore: 7368.3500893116
  time_this_iter_s: 25.9068922996521
  time_total_s: 7368.3500893116
  timers:
    learn_throughput: 8616.371
    learn_time_ms: 18777.278
    sample_throughput: 23625.766
    sample_time_ms: 6848.116
    update_time_ms: 30.67
  timestamp: 1602795232
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: f76cb_00000
  
2020-10-15 20:53:53,132	WARNING util.py:136 -- The `process_trial` operation took 0.7970583438873291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    286 |          7368.35 | 46272512 |  307.864 |              338.535 |              107.323 |             776.95 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.9995977945737
    time_step_min: 2977
  date: 2020-10-15_20-54-18
  done: false
  episode_len_mean: 776.8919163331268
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.97435950786144
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 59713
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5369470411709154e-15
        cur_lr: 5.0e-05
        entropy: 0.06552333819369476
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035444036669408283
        model: {}
        policy_loss: -0.009319852561626854
        total_loss: 0.5108127122124037
        vf_explained_var: 0.998629093170166
        vf_loss: 0.520165334145228
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870967
    gpu_util_percent0: 0.3003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464507440881999
    mean_env_wait_ms: 1.2033625641873356
    mean_inference_ms: 4.292156309254823
    mean_raw_obs_processing_ms: 0.37823260526081054
  time_since_restore: 7394.134185791016
  time_this_iter_s: 25.784096479415894
  time_total_s: 7394.134185791016
  timers:
    learn_throughput: 8621.3
    learn_time_ms: 18766.543
    sample_throughput: 23647.222
    sample_time_ms: 6841.903
    update_time_ms: 30.353
  timestamp: 1602795258
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: f76cb_00000
  
2020-10-15 20:54:20,156	WARNING util.py:136 -- The `process_trial` operation took 0.7815530300140381 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    287 |          7394.13 | 46434304 |  307.974 |              338.535 |              107.323 |            776.892 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.512039032867
    time_step_min: 2977
  date: 2020-10-15_20-54-45
  done: false
  episode_len_mean: 776.8514084389453
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.0516328945924
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 59889
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2684735205854577e-15
        cur_lr: 5.0e-05
        entropy: 0.05954739358276129
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006055206584278494
        total_loss: .inf
        vf_explained_var: 0.9988150000572205
        vf_loss: 0.3718194415171941
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644912840637256
    mean_env_wait_ms: 1.2033467139155967
    mean_inference_ms: 4.292072369385848
    mean_raw_obs_processing_ms: 0.3782269409491267
  time_since_restore: 7419.827448606491
  time_this_iter_s: 25.693262815475464
  time_total_s: 7419.827448606491
  timers:
    learn_throughput: 8623.591
    learn_time_ms: 18761.557
    sample_throughput: 23656.691
    sample_time_ms: 6839.165
    update_time_ms: 29.424
  timestamp: 1602795285
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:54:46,976	WARNING util.py:136 -- The `process_trial` operation took 0.8521575927734375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    288 |          7419.83 | 46596096 |  308.052 |              338.535 |              107.323 |            776.851 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.88214630573
    time_step_min: 2977
  date: 2020-10-15_20-55-12
  done: false
  episode_len_mean: 776.8007752711785
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.14831283487706
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 219
  episodes_total: 60108
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9027102808781864e-15
        cur_lr: 5.0e-05
        entropy: 0.06399341144909461
        entropy_coeff: 0.0005000000000000001
        kl: 0.004015318467281759
        model: {}
        policy_loss: -0.0049529812337520225
        total_loss: 0.6015821620821953
        vf_explained_var: 0.9985210299491882
        vf_loss: 0.6065671344598135
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.050000000000004
    gpu_util_percent0: 0.3096666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464471780984694
    mean_env_wait_ms: 1.2033306920364364
    mean_inference_ms: 4.291978731655476
    mean_raw_obs_processing_ms: 0.3782206834147079
  time_since_restore: 7445.411740779877
  time_this_iter_s: 25.58429217338562
  time_total_s: 7445.411740779877
  timers:
    learn_throughput: 8633.969
    learn_time_ms: 18739.007
    sample_throughput: 23659.393
    sample_time_ms: 6838.383
    update_time_ms: 27.115
  timestamp: 1602795312
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: f76cb_00000
  
2020-10-15 20:55:13,683	WARNING util.py:136 -- The `process_trial` operation took 0.8340239524841309 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    289 |          7445.41 | 46757888 |  308.148 |              338.535 |              107.323 |            776.801 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.1876596026927
    time_step_min: 2977
  date: 2020-10-15_20-55-39
  done: false
  episode_len_mean: 776.747365281368
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.25502489600717
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 240
  episodes_total: 60348
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.513551404390932e-16
        cur_lr: 5.0e-05
        entropy: 0.06478607282042503
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006693636042958436
        total_loss: .inf
        vf_explained_var: 0.9987381100654602
        vf_loss: 0.513336847225825
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644527823765527
    mean_env_wait_ms: 1.203309565953891
    mean_inference_ms: 4.291862032268288
    mean_raw_obs_processing_ms: 0.3782115294809778
  time_since_restore: 7471.338567018509
  time_this_iter_s: 25.926826238632202
  time_total_s: 7471.338567018509
  timers:
    learn_throughput: 8640.74
    learn_time_ms: 18724.323
    sample_throughput: 23697.603
    sample_time_ms: 6827.357
    update_time_ms: 28.452
  timestamp: 1602795339
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:55:40,714	WARNING util.py:136 -- The `process_trial` operation took 0.8121640682220459 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    290 |          7471.34 | 46919680 |  308.255 |              338.535 |              107.323 |            776.747 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3176.770150290164
    time_step_min: 2977
  date: 2020-10-15_20-56-06
  done: false
  episode_len_mean: 776.7091449814127
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.31218004080455
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 177
  episodes_total: 60525
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4270327106586398e-15
        cur_lr: 5.0e-05
        entropy: 0.10586750755707423
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014450198669995492
        total_loss: .inf
        vf_explained_var: 0.9953705668449402
        vf_loss: 1.5401740471522014
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0
    gpu_util_percent0: 0.3153333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644362955222823
    mean_env_wait_ms: 1.2032941736679517
    mean_inference_ms: 4.291777927447181
    mean_raw_obs_processing_ms: 0.3782058073739943
  time_since_restore: 7497.408697605133
  time_this_iter_s: 26.070130586624146
  time_total_s: 7497.408697605133
  timers:
    learn_throughput: 8637.863
    learn_time_ms: 18730.559
    sample_throughput: 23672.739
    sample_time_ms: 6834.528
    update_time_ms: 28.419
  timestamp: 1602795366
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:56:08,013	WARNING util.py:136 -- The `process_trial` operation took 0.8330326080322266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    291 |          7497.41 | 47081472 |  308.312 |              338.535 |              107.323 |            776.709 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3176.917677100494
    time_step_min: 2977
  date: 2020-10-15_20-56-33
  done: false
  episode_len_mean: 776.6890948602285
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.26726735266135
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 60742
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1405490659879594e-15
        cur_lr: 5.0e-05
        entropy: 0.15311014900604883
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01179631584091112
        total_loss: .inf
        vf_explained_var: 0.9898192286491394
        vf_loss: 5.1318925221761065
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.061290322580643
    gpu_util_percent0: 0.3116129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464417780911396
    mean_env_wait_ms: 1.2032780441324287
    mean_inference_ms: 4.291684782826449
    mean_raw_obs_processing_ms: 0.378199278540658
  time_since_restore: 7523.24714756012
  time_this_iter_s: 25.838449954986572
  time_total_s: 7523.24714756012
  timers:
    learn_throughput: 8649.345
    learn_time_ms: 18705.694
    sample_throughput: 23668.12
    sample_time_ms: 6835.862
    update_time_ms: 29.333
  timestamp: 1602795393
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:56:34,973	WARNING util.py:136 -- The `process_trial` operation took 0.829791784286499 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    292 |          7523.25 | 47243264 |  308.267 |              338.535 |              107.323 |            776.689 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.22077261389
    time_step_min: 2977
  date: 2020-10-15_20-57-00
  done: false
  episode_len_mean: 776.6666010692381
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.22048090866355
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 236
  episodes_total: 60978
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.210823598981939e-15
        cur_lr: 5.0e-05
        entropy: 0.14366716518998146
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011368390987627208
        total_loss: .inf
        vf_explained_var: 0.9908742308616638
        vf_loss: 4.80695641040802
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82333333333333
    gpu_util_percent0: 0.30700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643980460334596
    mean_env_wait_ms: 1.2032589597766383
    mean_inference_ms: 4.291569747218026
    mean_raw_obs_processing_ms: 0.37819045000273005
  time_since_restore: 7549.068448781967
  time_this_iter_s: 25.821301221847534
  time_total_s: 7549.068448781967
  timers:
    learn_throughput: 8659.432
    learn_time_ms: 18683.904
    sample_throughput: 23677.272
    sample_time_ms: 6833.22
    update_time_ms: 30.269
  timestamp: 1602795420
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:57:02,003	WARNING util.py:136 -- The `process_trial` operation took 0.80922532081604 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    293 |          7549.07 | 47405056 |   308.22 |              338.535 |              107.323 |            776.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.436692083647
    time_step_min: 2977
  date: 2020-10-15_20-57-27
  done: false
  episode_len_mean: 776.6486689776964
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.1833823221423
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 61156
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.816235398472909e-15
        cur_lr: 5.0e-05
        entropy: 0.1385636404156685
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013011718556905786
        total_loss: .inf
        vf_explained_var: 0.9897763133049011
        vf_loss: 4.631884773572286
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.461290322580645
    gpu_util_percent0: 0.32387096774193547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643824259559812
    mean_env_wait_ms: 1.2032442832322847
    mean_inference_ms: 4.291485826934547
    mean_raw_obs_processing_ms: 0.37818478520831406
  time_since_restore: 7574.940354585648
  time_this_iter_s: 25.87190580368042
  time_total_s: 7574.940354585648
  timers:
    learn_throughput: 8679.308
    learn_time_ms: 18641.118
    sample_throughput: 23613.339
    sample_time_ms: 6851.72
    update_time_ms: 29.26
  timestamp: 1602795447
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:57:29,006	WARNING util.py:136 -- The `process_trial` operation took 0.8471543788909912 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    294 |          7574.94 | 47566848 |  308.183 |              338.535 |              107.323 |            776.649 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.644577429191
    time_step_min: 2977
  date: 2020-10-15_20-57-55
  done: false
  episode_len_mean: 776.6287539311379
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.15032793018423
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 213
  episodes_total: 61369
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.224353097709363e-15
        cur_lr: 5.0e-05
        entropy: 0.14182372267047563
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012821668550410928
        total_loss: .inf
        vf_explained_var: 0.9910843968391418
        vf_loss: 4.335281093915303
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.461290322580652
    gpu_util_percent0: 0.35935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643645737608682
    mean_env_wait_ms: 1.2032291466521605
    mean_inference_ms: 4.291396614441943
    mean_raw_obs_processing_ms: 0.37817875385864913
  time_since_restore: 7601.148034334183
  time_this_iter_s: 26.207679748535156
  time_total_s: 7601.148034334183
  timers:
    learn_throughput: 8663.75
    learn_time_ms: 18674.592
    sample_throughput: 23690.159
    sample_time_ms: 6829.502
    update_time_ms: 29.52
  timestamp: 1602795475
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:57:56,372	WARNING util.py:136 -- The `process_trial` operation took 0.850623607635498 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    295 |          7601.15 | 47728640 |   308.15 |              338.535 |              107.323 |            776.629 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.8908668604936
    time_step_min: 2977
  date: 2020-10-15_20-58-22
  done: false
  episode_len_mean: 776.608239055982
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.1108337018188
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 240
  episodes_total: 61609
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0836529646564048e-14
        cur_lr: 5.0e-05
        entropy: 0.13273081431786218
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011324394814437255
        total_loss: .inf
        vf_explained_var: 0.9915825724601746
        vf_loss: 4.356938203175862
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.846666666666668
    gpu_util_percent0: 0.356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643450025086244
    mean_env_wait_ms: 1.203211680984067
    mean_inference_ms: 4.291284055783809
    mean_raw_obs_processing_ms: 0.3781700208392957
  time_since_restore: 7626.859783887863
  time_this_iter_s: 25.71174955368042
  time_total_s: 7626.859783887863
  timers:
    learn_throughput: 8668.233
    learn_time_ms: 18664.934
    sample_throughput: 23722.098
    sample_time_ms: 6820.307
    update_time_ms: 29.729
  timestamp: 1602795502
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:58:23,179	WARNING util.py:136 -- The `process_trial` operation took 0.8147180080413818 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    296 |          7626.86 | 47890432 |  308.111 |              338.535 |              107.323 |            776.608 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.0675736961452
    time_step_min: 2977
  date: 2020-10-15_20-58-49
  done: false
  episode_len_mean: 776.5876954452754
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.0844180368314
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 173
  episodes_total: 61782
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.625479446984607e-14
        cur_lr: 5.0e-05
        entropy: 0.12759790693720183
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012679040102132907
        total_loss: .inf
        vf_explained_var: 0.9915440678596497
        vf_loss: 3.570532242457072
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.477419354838712
    gpu_util_percent0: 0.3480645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643317655860594
    mean_env_wait_ms: 1.2031991739063366
    mean_inference_ms: 4.291209640631588
    mean_raw_obs_processing_ms: 0.37816501643626593
  time_since_restore: 7652.8146295547485
  time_this_iter_s: 25.954845666885376
  time_total_s: 7652.8146295547485
  timers:
    learn_throughput: 8669.069
    learn_time_ms: 18663.133
    sample_throughput: 23702.663
    sample_time_ms: 6825.9
    update_time_ms: 31.938
  timestamp: 1602795529
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:58:50,289	WARNING util.py:136 -- The `process_trial` operation took 0.8646690845489502 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    297 |          7652.81 | 48052224 |  308.084 |              338.535 |              107.323 |            776.588 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.2960942543577
    time_step_min: 2977
  date: 2020-10-15_20-59-16
  done: false
  episode_len_mean: 776.5524337924583
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.0522801643088
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 220
  episodes_total: 62002
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4382191704769108e-14
        cur_lr: 5.0e-05
        entropy: 0.1337639888127645
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01299071020912379
        total_loss: .inf
        vf_explained_var: 0.9893553853034973
        vf_loss: 5.149569153785706
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01666666666667
    gpu_util_percent0: 0.3483333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146431157295454
    mean_env_wait_ms: 1.2031850645669395
    mean_inference_ms: 4.2911190030692685
    mean_raw_obs_processing_ms: 0.37815908974404694
  time_since_restore: 7678.666669368744
  time_this_iter_s: 25.85203981399536
  time_total_s: 7678.666669368744
  timers:
    learn_throughput: 8666.978
    learn_time_ms: 18667.638
    sample_throughput: 23684.516
    sample_time_ms: 6831.13
    update_time_ms: 33.66
  timestamp: 1602795556
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:59:17,297	WARNING util.py:136 -- The `process_trial` operation took 0.8479392528533936 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    298 |          7678.67 | 48214016 |  308.052 |              338.535 |              107.323 |            776.552 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.5211659351435
    time_step_min: 2977
  date: 2020-10-15_20-59-43
  done: false
  episode_len_mean: 776.5094873154352
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.01902721240464
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 62241
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6573287557153655e-14
        cur_lr: 5.0e-05
        entropy: 0.12361636757850647
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01028628320879458
        total_loss: .inf
        vf_explained_var: 0.9914877414703369
        vf_loss: 4.290789107481639
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.22903225806452
    gpu_util_percent0: 0.3319354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464293165551577
    mean_env_wait_ms: 1.2031678153063514
    mean_inference_ms: 4.291002896343318
    mean_raw_obs_processing_ms: 0.37815028825793
  time_since_restore: 7704.477923631668
  time_this_iter_s: 25.811254262924194
  time_total_s: 7704.477923631668
  timers:
    learn_throughput: 8667.86
    learn_time_ms: 18665.738
    sample_throughput: 23635.863
    sample_time_ms: 6845.191
    update_time_ms: 33.981
  timestamp: 1602795583
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 20:59:44,263	WARNING util.py:136 -- The `process_trial` operation took 0.8177089691162109 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    299 |          7704.48 | 48375808 |  308.019 |              338.535 |              107.323 |            776.509 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.678156312625
    time_step_min: 2977
  date: 2020-10-15_21-00-10
  done: false
  episode_len_mean: 776.4841149045933
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9980266318924
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 62417
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.485993133573049e-14
        cur_lr: 5.0e-05
        entropy: 0.12437219483156998
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012890200305264443
        total_loss: .inf
        vf_explained_var: 0.9916799068450928
        vf_loss: 3.541650036970774
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17
    gpu_util_percent0: 0.28566666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642793878862725
    mean_env_wait_ms: 1.203156279294445
    mean_inference_ms: 4.290927290239343
    mean_raw_obs_processing_ms: 0.3781452834560024
  time_since_restore: 7730.503713130951
  time_this_iter_s: 26.025789499282837
  time_total_s: 7730.503713130951
  timers:
    learn_throughput: 8672.321
    learn_time_ms: 18656.137
    sample_throughput: 23533.866
    sample_time_ms: 6874.859
    update_time_ms: 31.96
  timestamp: 1602795610
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:00:11,475	WARNING util.py:136 -- The `process_trial` operation took 0.8339245319366455 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    300 |           7730.5 | 48537600 |  307.998 |              338.535 |              107.323 |            776.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.820807770961
    time_step_min: 2977
  date: 2020-10-15_21-00-37
  done: false
  episode_len_mean: 776.4501229364243
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.97480021016753
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 62634
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.228989700359574e-14
        cur_lr: 5.0e-05
        entropy: 0.13210072492559752
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011345691358049711
        total_loss: .inf
        vf_explained_var: 0.9917410016059875
        vf_loss: 3.9260562459627786
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.364516129032257
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642605733939615
    mean_env_wait_ms: 1.2031435863369102
    mean_inference_ms: 4.2908375471191205
    mean_raw_obs_processing_ms: 0.3781396406857342
  time_since_restore: 7756.627762794495
  time_this_iter_s: 26.1240496635437
  time_total_s: 7756.627762794495
  timers:
    learn_throughput: 8670.138
    learn_time_ms: 18660.833
    sample_throughput: 23540.572
    sample_time_ms: 6872.9
    update_time_ms: 34.847
  timestamp: 1602795637
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:00:38,759	WARNING util.py:136 -- The `process_trial` operation took 0.8612563610076904 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    301 |          7756.63 | 48699392 |  307.975 |              338.535 |              107.323 |             776.45 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.9731183651384
    time_step_min: 2977
  date: 2020-10-15_21-01-04
  done: false
  episode_len_mean: 776.4203235092965
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9517142702453
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 62873
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2343484550539359e-13
        cur_lr: 5.0e-05
        entropy: 0.12384984456002712
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01129031297750771
        total_loss: .inf
        vf_explained_var: 0.9928252696990967
        vf_loss: 3.6278991301854453
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10333333333334
    gpu_util_percent0: 0.3203333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642424704296017
    mean_env_wait_ms: 1.2031275202476697
    mean_inference_ms: 4.290726966086495
    mean_raw_obs_processing_ms: 0.3781309559147956
  time_since_restore: 7782.312237262726
  time_this_iter_s: 25.6844744682312
  time_total_s: 7782.312237262726
  timers:
    learn_throughput: 8673.896
    learn_time_ms: 18652.749
    sample_throughput: 23561.099
    sample_time_ms: 6866.912
    update_time_ms: 33.718
  timestamp: 1602795664
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:01:05,716	WARNING util.py:136 -- The `process_trial` operation took 0.8857378959655762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    302 |          7782.31 | 48861184 |  307.952 |              338.535 |              107.323 |             776.42 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.074499658768
    time_step_min: 2977
  date: 2020-10-15_21-01-31
  done: false
  episode_len_mean: 776.400751796222
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9374659856507
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 63049
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8515226825809037e-13
        cur_lr: 5.0e-05
        entropy: 0.1243243037412564
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009567460355659326
        total_loss: .inf
        vf_explained_var: 0.9911046028137207
        vf_loss: 3.782901187737783
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.78709677419355
    gpu_util_percent0: 0.29967741935483866
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642296556933077
    mean_env_wait_ms: 1.2031168983010512
    mean_inference_ms: 4.290654941230412
    mean_raw_obs_processing_ms: 0.3781262155497295
  time_since_restore: 7808.430683851242
  time_this_iter_s: 26.118446588516235
  time_total_s: 7808.430683851242
  timers:
    learn_throughput: 8665.501
    learn_time_ms: 18670.819
    sample_throughput: 23528.619
    sample_time_ms: 6876.392
    update_time_ms: 34.181
  timestamp: 1602795691
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:01:33,095	WARNING util.py:136 -- The `process_trial` operation took 0.8458845615386963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    303 |          7808.43 | 49022976 |  307.937 |              338.535 |              107.323 |            776.401 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.1866369287713
    time_step_min: 2977
  date: 2020-10-15_21-01-59
  done: false
  episode_len_mean: 776.3769937244116
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9214298818795
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 212
  episodes_total: 63261
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7772840238713556e-13
        cur_lr: 5.0e-05
        entropy: 0.1315849063297113
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011290841484272581
        total_loss: .inf
        vf_explained_var: 0.9920246005058289
        vf_loss: 3.855225841204325
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.529032258064518
    gpu_util_percent0: 0.3112903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642111680293257
    mean_env_wait_ms: 1.2031050152027816
    mean_inference_ms: 4.290569377024225
    mean_raw_obs_processing_ms: 0.3781204536392091
  time_since_restore: 7834.63720536232
  time_this_iter_s: 26.20652151107788
  time_total_s: 7834.63720536232
  timers:
    learn_throughput: 8649.282
    learn_time_ms: 18705.829
    sample_throughput: 23536.266
    sample_time_ms: 6874.158
    update_time_ms: 34.889
  timestamp: 1602795719
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:02:00,489	WARNING util.py:136 -- The `process_trial` operation took 0.8753917217254639 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    304 |          7834.64 | 49184768 |  307.921 |              338.535 |              107.323 |            776.377 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.296665721219
    time_step_min: 2977
  date: 2020-10-15_21-02-26
  done: false
  episode_len_mean: 776.3533320735702
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.90445316735
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 63504
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.165926035807034e-13
        cur_lr: 5.0e-05
        entropy: 0.12246929667890072
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013158831580464417
        total_loss: .inf
        vf_explained_var: 0.994056224822998
        vf_loss: 3.009273588657379
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43225806451613
    gpu_util_percent0: 0.29
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146419302487108
    mean_env_wait_ms: 1.2030899238204795
    mean_inference_ms: 4.290457653380777
    mean_raw_obs_processing_ms: 0.37811200653010013
  time_since_restore: 7860.533227682114
  time_this_iter_s: 25.8960223197937
  time_total_s: 7860.533227682114
  timers:
    learn_throughput: 8664.771
    learn_time_ms: 18672.392
    sample_throughput: 23532.732
    sample_time_ms: 6875.19
    update_time_ms: 32.853
  timestamp: 1602795746
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:02:27,545	WARNING util.py:136 -- The `process_trial` operation took 0.8195281028747559 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    305 |          7860.53 | 49346560 |  307.904 |              338.535 |              107.323 |            776.353 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.358465068041
    time_step_min: 2977
  date: 2020-10-15_21-02-53
  done: false
  episode_len_mean: 776.338489321608
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.89250149104106
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 63680
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.24888905371055e-13
        cur_lr: 5.0e-05
        entropy: 0.11906127880016963
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010083964773608992
        total_loss: .inf
        vf_explained_var: 0.9939079284667969
        vf_loss: 2.6149773399035134
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.700000000000003
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641803735470302
    mean_env_wait_ms: 1.2030794258036073
    mean_inference_ms: 4.290384780723537
    mean_raw_obs_processing_ms: 0.37810732197633024
  time_since_restore: 7886.404277563095
  time_this_iter_s: 25.871049880981445
  time_total_s: 7886.404277563095
  timers:
    learn_throughput: 8659.926
    learn_time_ms: 18682.839
    sample_throughput: 23517.131
    sample_time_ms: 6879.751
    update_time_ms: 32.361
  timestamp: 1602795773
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:02:54,566	WARNING util.py:136 -- The `process_trial` operation took 0.8482906818389893 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    306 |           7886.4 | 49508352 |  307.893 |              338.535 |              107.323 |            776.338 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.4756080372103
    time_step_min: 2977
  date: 2020-10-15_21-03-20
  done: false
  episode_len_mean: 776.3182721652712
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8733797004397
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 215
  episodes_total: 63895
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.373333580565828e-13
        cur_lr: 5.0e-05
        entropy: 0.1285107253740231
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011192427579468736
        total_loss: .inf
        vf_explained_var: 0.9922218918800354
        vf_loss: 3.823323448499044
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.590322580645168
    gpu_util_percent0: 0.3309677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641624253833335
    mean_env_wait_ms: 1.203068940519603
    mean_inference_ms: 4.290299436290129
    mean_raw_obs_processing_ms: 0.37810176615508395
  time_since_restore: 7912.463929176331
  time_this_iter_s: 26.059651613235474
  time_total_s: 7912.463929176331
  timers:
    learn_throughput: 8654.419
    learn_time_ms: 18694.728
    sample_throughput: 23522.588
    sample_time_ms: 6878.155
    update_time_ms: 31.758
  timestamp: 1602795800
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:03:21,805	WARNING util.py:136 -- The `process_trial` operation took 0.8735265731811523 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    307 |          7912.46 | 49670144 |  307.873 |              338.535 |              107.323 |            776.318 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.569710113271
    time_step_min: 2977
  date: 2020-10-15_21-03-47
  done: false
  episode_len_mean: 776.2910066109517
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8612108045656
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 241
  episodes_total: 64136
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4060000370848742e-12
        cur_lr: 5.0e-05
        entropy: 0.11996723338961601
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011024738370906562
        total_loss: .inf
        vf_explained_var: 0.9932493567466736
        vf_loss: 3.40405927101771
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05
    gpu_util_percent0: 0.32166666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641443258748513
    mean_env_wait_ms: 1.2030545271951834
    mean_inference_ms: 4.290191355432754
    mean_raw_obs_processing_ms: 0.37809341853943373
  time_since_restore: 7938.110687494278
  time_this_iter_s: 25.646758317947388
  time_total_s: 7938.110687494278
  timers:
    learn_throughput: 8659.56
    learn_time_ms: 18683.628
    sample_throughput: 23550.177
    sample_time_ms: 6870.097
    update_time_ms: 30.926
  timestamp: 1602795827
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:03:48,708	WARNING util.py:136 -- The `process_trial` operation took 0.8536415100097656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    308 |          7938.11 | 49831936 |  307.861 |              338.535 |              107.323 |            776.291 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.592217450834
    time_step_min: 2977
  date: 2020-10-15_21-04-14
  done: false
  episode_len_mean: 776.2663494728986
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.853792142905
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 64314
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1090000556273114e-12
        cur_lr: 5.0e-05
        entropy: 0.11393272380034129
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010827277631809315
        total_loss: .inf
        vf_explained_var: 0.9919564127922058
        vf_loss: 3.392876625061035
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.483333333333334
    gpu_util_percent0: 0.2896666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641316130613918
    mean_env_wait_ms: 1.2030450925747944
    mean_inference_ms: 4.29012059969487
    mean_raw_obs_processing_ms: 0.37808894180685787
  time_since_restore: 7963.81608915329
  time_this_iter_s: 25.70540165901184
  time_total_s: 7963.81608915329
  timers:
    learn_throughput: 8655.911
    learn_time_ms: 18691.504
    sample_throughput: 23588.471
    sample_time_ms: 6858.944
    update_time_ms: 31.97
  timestamp: 1602795854
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:04:15,646	WARNING util.py:136 -- The `process_trial` operation took 0.8347182273864746 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    309 |          7963.82 | 49993728 |  307.854 |              338.535 |              107.323 |            776.266 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.6548398101913
    time_step_min: 2977
  date: 2020-10-15_21-04-41
  done: false
  episode_len_mean: 776.2395549218944
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8466532107588
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 214
  episodes_total: 64528
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.163500083440967e-12
        cur_lr: 5.0e-05
        entropy: 0.12230549256006877
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010282248277993252
        total_loss: .inf
        vf_explained_var: 0.9923510551452637
        vf_loss: 3.58940980831782
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.422580645161293
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464113817254029
    mean_env_wait_ms: 1.2030350233919274
    mean_inference_ms: 4.2900387802188975
    mean_raw_obs_processing_ms: 0.37808355922886805
  time_since_restore: 7989.814611673355
  time_this_iter_s: 25.998522520065308
  time_total_s: 7989.814611673355
  timers:
    learn_throughput: 8652.135
    learn_time_ms: 18699.661
    sample_throughput: 23662.19
    sample_time_ms: 6837.575
    update_time_ms: 41.335
  timestamp: 1602795881
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:04:42,841	WARNING util.py:136 -- The `process_trial` operation took 0.8866279125213623 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    310 |          7989.81 | 50155520 |  307.847 |              338.535 |              107.323 |             776.24 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.684835843955
    time_step_min: 2977
  date: 2020-10-15_21-05-08
  done: false
  episode_len_mean: 776.2023870180802
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8417834372256
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 239
  episodes_total: 64767
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.74525012516145e-12
        cur_lr: 5.0e-05
        entropy: 0.11450735541681449
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00990412593819201
        total_loss: .inf
        vf_explained_var: 0.9932207465171814
        vf_loss: 3.395776013533274
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64
    gpu_util_percent0: 0.35066666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640961030178476
    mean_env_wait_ms: 1.2030214275589632
    mean_inference_ms: 4.2899292665151485
    mean_raw_obs_processing_ms: 0.37807536425993776
  time_since_restore: 8015.483463525772
  time_this_iter_s: 25.668851852416992
  time_total_s: 8015.483463525772
  timers:
    learn_throughput: 8670.064
    learn_time_ms: 18660.993
    sample_throughput: 23677.619
    sample_time_ms: 6833.12
    update_time_ms: 37.86
  timestamp: 1602795908
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:05:09,768	WARNING util.py:136 -- The `process_trial` operation took 0.8628847599029541 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    311 |          8015.48 | 50317312 |  307.842 |              338.535 |              107.323 |            776.202 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.7069889987984
    time_step_min: 2977
  date: 2020-10-15_21-05-35
  done: false
  episode_len_mean: 776.1785538309929
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8396072700396
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 177
  episodes_total: 64944
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.117875187742176e-12
        cur_lr: 5.0e-05
        entropy: 0.10938517811397712
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011721241035653899
        total_loss: .inf
        vf_explained_var: 0.9926490187644958
        vf_loss: 3.016260822614034
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.783870967741933
    gpu_util_percent0: 0.3216129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640840881194792
    mean_env_wait_ms: 1.2030126392625176
    mean_inference_ms: 4.289858699130693
    mean_raw_obs_processing_ms: 0.37807077676595235
  time_since_restore: 8041.322569847107
  time_this_iter_s: 25.83910632133484
  time_total_s: 8041.322569847107
  timers:
    learn_throughput: 8674.184
    learn_time_ms: 18652.129
    sample_throughput: 23601.933
    sample_time_ms: 6855.032
    update_time_ms: 38.528
  timestamp: 1602795935
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:05:36,800	WARNING util.py:136 -- The `process_trial` operation took 0.8871452808380127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    312 |          8041.32 | 50479104 |   307.84 |              338.535 |              107.323 |            776.179 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.699924750833
    time_step_min: 2977
  date: 2020-10-15_21-06-02
  done: false
  episode_len_mean: 776.1422366825765
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8402054895709
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 215
  episodes_total: 65159
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.067681278161326e-11
        cur_lr: 5.0e-05
        entropy: 0.11762756668031216
        entropy_coeff: 0.0005000000000000001
        kl: 0.004324741351107757
        model: {}
        policy_loss: -0.010916552195946375
        total_loss: 2.8980425000190735
        vf_explained_var: 0.9936590194702148
        vf_loss: 2.9090179602305093
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.374193548387098
    gpu_util_percent0: 0.3106451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464065666802396
    mean_env_wait_ms: 1.2030037177126138
    mean_inference_ms: 4.289780214340005
    mean_raw_obs_processing_ms: 0.3780656578834657
  time_since_restore: 8067.489428520203
  time_this_iter_s: 26.166858673095703
  time_total_s: 8067.489428520203
  timers:
    learn_throughput: 8678.183
    learn_time_ms: 18643.533
    sample_throughput: 23581.669
    sample_time_ms: 6860.922
    update_time_ms: 36.809
  timestamp: 1602795962
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: f76cb_00000
  
2020-10-15 21:06:04,181	WARNING util.py:136 -- The `process_trial` operation took 0.9026241302490234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    313 |          8067.49 | 50640896 |   307.84 |              338.535 |              107.323 |            776.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.6902386780907
    time_step_min: 2977
  date: 2020-10-15_21-06-30
  done: false
  episode_len_mean: 776.1040793859515
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.841053110846
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 243
  episodes_total: 65402
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.33840639080663e-12
        cur_lr: 5.0e-05
        entropy: 0.10951673177381356
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010325620154617354
        total_loss: .inf
        vf_explained_var: 0.9929057955741882
        vf_loss: 3.4646220803260803
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83548387096775
    gpu_util_percent0: 0.2651612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464048539792931
    mean_env_wait_ms: 1.2029906412955802
    mean_inference_ms: 4.2896691555744875
    mean_raw_obs_processing_ms: 0.3780572325995593
  time_since_restore: 8093.68460726738
  time_this_iter_s: 26.195178747177124
  time_total_s: 8093.68460726738
  timers:
    learn_throughput: 8677.093
    learn_time_ms: 18645.876
    sample_throughput: 23594.197
    sample_time_ms: 6857.279
    update_time_ms: 35.731
  timestamp: 1602795990
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:06:31,549	WARNING util.py:136 -- The `process_trial` operation took 0.8719875812530518 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    314 |          8093.68 | 50802688 |  307.841 |              338.535 |              107.323 |            776.104 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.6683807373047
    time_step_min: 2977
  date: 2020-10-15_21-06-57
  done: false
  episode_len_mean: 776.0747354295648
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.84348717588523
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 65578
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.007609586209944e-12
        cur_lr: 5.0e-05
        entropy: 0.10663836884001891
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01223723492391097
        total_loss: .inf
        vf_explained_var: 0.9948489665985107
        vf_loss: 2.0817775626977286
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.074193548387097
    gpu_util_percent0: 0.3261290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640365620994464
    mean_env_wait_ms: 1.2029825927311621
    mean_inference_ms: 4.289604179161802
    mean_raw_obs_processing_ms: 0.37805311172114664
  time_since_restore: 8120.017418861389
  time_this_iter_s: 26.3328115940094
  time_total_s: 8120.017418861389
  timers:
    learn_throughput: 8663.523
    learn_time_ms: 18675.082
    sample_throughput: 23523.42
    sample_time_ms: 6877.911
    update_time_ms: 37.78
  timestamp: 1602796017
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:06:59,157	WARNING util.py:136 -- The `process_trial` operation took 0.8522701263427734 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    315 |          8120.02 | 50964480 |  307.843 |              338.535 |              107.323 |            776.075 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.6439652025792
    time_step_min: 2977
  date: 2020-10-15_21-07-25
  done: false
  episode_len_mean: 776.0384381554549
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.84703434625925
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 216
  episodes_total: 65794
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2011414379314917e-11
        cur_lr: 5.0e-05
        entropy: 0.11646147196491559
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011337483272654936
        total_loss: .inf
        vf_explained_var: 0.9930436611175537
        vf_loss: 3.2393924395243325
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.299999999999997
    gpu_util_percent0: 0.3071875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640188025409698
    mean_env_wait_ms: 1.2029733458678822
    mean_inference_ms: 4.289520665133885
    mean_raw_obs_processing_ms: 0.3780477221406837
  time_since_restore: 8146.343011379242
  time_this_iter_s: 26.325592517852783
  time_total_s: 8146.343011379242
  timers:
    learn_throughput: 8656.881
    learn_time_ms: 18689.411
    sample_throughput: 23454.018
    sample_time_ms: 6898.264
    update_time_ms: 38.14
  timestamp: 1602796045
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:07:26,803	WARNING util.py:136 -- The `process_trial` operation took 0.9006497859954834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    316 |          8146.34 | 51126272 |  307.847 |              338.535 |              107.323 |            776.038 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.6308795005457
    time_step_min: 2977
  date: 2020-10-15_21-07-52
  done: false
  episode_len_mean: 775.9997879880062
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.84948096832886
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 240
  episodes_total: 66034
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8017121568972376e-11
        cur_lr: 5.0e-05
        entropy: 0.1074967843790849
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010270860681581931
        total_loss: .inf
        vf_explained_var: 0.9937066435813904
        vf_loss: 3.04278697570165
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.251612903225812
    gpu_util_percent0: 0.3277419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464001861728599
    mean_env_wait_ms: 1.2029620919926318
    mean_inference_ms: 4.289417190732755
    mean_raw_obs_processing_ms: 0.37803971602443776
  time_since_restore: 8172.529903411865
  time_this_iter_s: 26.18689203262329
  time_total_s: 8172.529903411865
  timers:
    learn_throughput: 8654.584
    learn_time_ms: 18694.371
    sample_throughput: 23420.663
    sample_time_ms: 6908.088
    update_time_ms: 37.613
  timestamp: 1602796072
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:07:54,304	WARNING util.py:136 -- The `process_trial` operation took 0.9301197528839111 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    317 |          8172.53 | 51288064 |  307.849 |              338.535 |              107.323 |                776 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.601484101076
    time_step_min: 2977
  date: 2020-10-15_21-08-20
  done: false
  episode_len_mean: 775.9722096360066
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8529884252584
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 66210
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.702568235345856e-11
        cur_lr: 5.0e-05
        entropy: 0.10584458398322265
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012329516968748067
        total_loss: .inf
        vf_explained_var: 0.9943013191223145
        vf_loss: 2.333364168802897
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.996666666666663
    gpu_util_percent0: 0.36933333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639896690012072
    mean_env_wait_ms: 1.202954367681981
    mean_inference_ms: 4.2893517140244715
    mean_raw_obs_processing_ms: 0.37803555465036487
  time_since_restore: 8198.249243497849
  time_this_iter_s: 25.719340085983276
  time_total_s: 8198.249243497849
  timers:
    learn_throughput: 8651.992
    learn_time_ms: 18699.97
    sample_throughput: 23408.509
    sample_time_ms: 6911.675
    update_time_ms: 36.474
  timestamp: 1602796100
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:08:21,273	WARNING util.py:136 -- The `process_trial` operation took 0.9324052333831787 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    318 |          8198.25 | 51449856 |  307.853 |              338.535 |              107.323 |            775.972 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.5589395993375
    time_step_min: 2977
  date: 2020-10-15_21-08-47
  done: false
  episode_len_mean: 775.9335260115607
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8602658631109
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 222
  episodes_total: 66432
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.053852353018785e-11
        cur_lr: 5.0e-05
        entropy: 0.11358258003989856
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009502712792406479
        total_loss: .inf
        vf_explained_var: 0.9933786988258362
        vf_loss: 3.085898995399475
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993548387096777
    gpu_util_percent0: 0.30290322580645157
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463973061568538
    mean_env_wait_ms: 1.2029457004643216
    mean_inference_ms: 4.289267683120661
    mean_raw_obs_processing_ms: 0.3780301500415166
  time_since_restore: 8224.096616268158
  time_this_iter_s: 25.84737277030945
  time_total_s: 8224.096616268158
  timers:
    learn_throughput: 8650.445
    learn_time_ms: 18703.315
    sample_throughput: 23402.514
    sample_time_ms: 6913.445
    update_time_ms: 35.289
  timestamp: 1602796127
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:08:48,316	WARNING util.py:136 -- The `process_trial` operation took 0.8818163871765137 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    319 |           8224.1 | 51611648 |   307.86 |              338.535 |              107.323 |            775.934 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.515715057637
    time_step_min: 2977
  date: 2020-10-15_21-09-14
  done: false
  episode_len_mean: 775.8973389733898
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.86612411578665
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 234
  episodes_total: 66666
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.080778529528175e-11
        cur_lr: 5.0e-05
        entropy: 0.10715594949821632
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012676320203657573
        total_loss: .inf
        vf_explained_var: 0.9940745234489441
        vf_loss: 2.877126455307007
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.487096774193553
    gpu_util_percent0: 0.32064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639559005633299
    mean_env_wait_ms: 1.2029356291674553
    mean_inference_ms: 4.289169611136641
    mean_raw_obs_processing_ms: 0.37802255034280446
  time_since_restore: 8250.14354467392
  time_this_iter_s: 26.04692840576172
  time_total_s: 8250.14354467392
  timers:
    learn_throughput: 8644.98
    learn_time_ms: 18715.139
    sample_throughput: 23400.179
    sample_time_ms: 6914.135
    update_time_ms: 25.843
  timestamp: 1602796154
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:09:15,742	WARNING util.py:136 -- The `process_trial` operation took 0.8544797897338867 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    320 |          8250.14 | 51773440 |  307.866 |              338.535 |              107.323 |            775.897 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.4803227399966
    time_step_min: 2977
  date: 2020-10-15_21-09-41
  done: false
  episode_len_mean: 775.8771187074575
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8709467326417
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 66845
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.121167794292265e-11
        cur_lr: 5.0e-05
        entropy: 0.10708155234654744
        entropy_coeff: 0.0005000000000000001
        kl: 0.004037787051250537
        model: {}
        policy_loss: -0.010208337082682798
        total_loss: 3.1772650678952536
        vf_explained_var: 0.9924141764640808
        vf_loss: 3.187527040640513
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.446666666666665
    gpu_util_percent0: 0.2803333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639426972340072
    mean_env_wait_ms: 1.2029283263450379
    mean_inference_ms: 4.289101454041424
    mean_raw_obs_processing_ms: 0.3780182098366028
  time_since_restore: 8276.19769024849
  time_this_iter_s: 26.054145574569702
  time_total_s: 8276.19769024849
  timers:
    learn_throughput: 8629.437
    learn_time_ms: 18748.847
    sample_throughput: 23397.037
    sample_time_ms: 6915.064
    update_time_ms: 26.832
  timestamp: 1602796181
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: f76cb_00000
  
2020-10-15 21:09:43,134	WARNING util.py:136 -- The `process_trial` operation took 0.9016008377075195 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    321 |           8276.2 | 51935232 |  307.871 |              338.535 |              107.323 |            775.877 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.4066575154425
    time_step_min: 2977
  date: 2020-10-15_21-10-08
  done: false
  episode_len_mean: 775.8565251103423
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.88144853641995
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 219
  episodes_total: 67064
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5605838971461325e-11
        cur_lr: 5.0e-05
        entropy: 0.11626929913957913
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010833992797415704
        total_loss: .inf
        vf_explained_var: 0.9939078688621521
        vf_loss: 2.8969037532806396
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32903225806452
    gpu_util_percent0: 0.2796774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639272065769743
    mean_env_wait_ms: 1.20291962906534
    mean_inference_ms: 4.289022754029313
    mean_raw_obs_processing_ms: 0.3780128520509154
  time_since_restore: 8301.833909273148
  time_this_iter_s: 25.636219024658203
  time_total_s: 8301.833909273148
  timers:
    learn_throughput: 8626.972
    learn_time_ms: 18754.205
    sample_throughput: 23480.836
    sample_time_ms: 6890.385
    update_time_ms: 25.992
  timestamp: 1602796208
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:10:10,203	WARNING util.py:136 -- The `process_trial` operation took 0.945392370223999 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    322 |          8301.83 | 52097024 |  307.881 |              338.535 |              107.323 |            775.857 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.3577227310575
    time_step_min: 2977
  date: 2020-10-15_21-10-36
  done: false
  episode_len_mean: 775.8399952450295
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.8864261504162
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 234
  episodes_total: 67298
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.840875845719198e-11
        cur_lr: 5.0e-05
        entropy: 0.10707305868466695
        entropy_coeff: 0.0005000000000000001
        kl: 0.00457076351934423
        model: {}
        policy_loss: -0.010287801541077593
        total_loss: 3.619813402493795
        vf_explained_var: 0.9926348328590393
        vf_loss: 3.6301547288894653
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79354838709678
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639107671972698
    mean_env_wait_ms: 1.2029102339329532
    mean_inference_ms: 4.288926396890134
    mean_raw_obs_processing_ms: 0.37800561081794304
  time_since_restore: 8327.70896267891
  time_this_iter_s: 25.87505340576172
  time_total_s: 8327.70896267891
  timers:
    learn_throughput: 8627.818
    learn_time_ms: 18752.367
    sample_throughput: 23546.896
    sample_time_ms: 6871.054
    update_time_ms: 25.941
  timestamp: 1602796236
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: f76cb_00000
  
2020-10-15 21:10:37,464	WARNING util.py:136 -- The `process_trial` operation took 0.8850893974304199 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    323 |          8327.71 | 52258816 |  307.886 |              338.535 |              107.323 |             775.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.326072127647
    time_step_min: 2977
  date: 2020-10-15_21-11-03
  done: false
  episode_len_mean: 775.8269361866089
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.89366291026096
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 67478
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.420437922859599e-11
        cur_lr: 5.0e-05
        entropy: 0.10338783822953701
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010324454516497402
        total_loss: .inf
        vf_explained_var: 0.9944581389427185
        vf_loss: 2.32762477795283
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.561290322580653
    gpu_util_percent0: 0.31225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638974866724996
    mean_env_wait_ms: 1.2029029937584457
    mean_inference_ms: 4.2888581482241195
    mean_raw_obs_processing_ms: 0.3780012040575217
  time_since_restore: 8353.93885087967
  time_this_iter_s: 26.229888200759888
  time_total_s: 8353.93885087967
  timers:
    learn_throughput: 8625.486
    learn_time_ms: 18757.437
    sample_throughput: 23588.054
    sample_time_ms: 6859.065
    update_time_ms: 27.028
  timestamp: 1602796263
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:11:04,952	WARNING util.py:136 -- The `process_trial` operation took 0.8862650394439697 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    324 |          8353.94 | 52420608 |  307.894 |              338.535 |              107.323 |            775.827 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.2821138211384
    time_step_min: 2977
  date: 2020-10-15_21-11-31
  done: false
  episode_len_mean: 775.8066241210188
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.89909151790914
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 214
  episodes_total: 67692
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.1306568842893985e-11
        cur_lr: 5.0e-05
        entropy: 0.11114504995445411
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011574996709138455
        total_loss: .inf
        vf_explained_var: 0.9941046237945557
        vf_loss: 2.807263414065043
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.38387096774194
    gpu_util_percent0: 0.2770967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638829813018986
    mean_env_wait_ms: 1.2028951204946694
    mean_inference_ms: 4.288780382073413
    mean_raw_obs_processing_ms: 0.37799627616992754
  time_since_restore: 8380.002394914627
  time_this_iter_s: 26.063544034957886
  time_total_s: 8380.002394914627
  timers:
    learn_throughput: 8630.32
    learn_time_ms: 18746.93
    sample_throughput: 23645.828
    sample_time_ms: 6842.306
    update_time_ms: 25.999
  timestamp: 1602796291
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:11:32,433	WARNING util.py:136 -- The `process_trial` operation took 0.8956577777862549 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    325 |             8380 | 52582400 |  307.899 |              338.535 |              107.323 |            775.807 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.253789385302
    time_step_min: 2977
  date: 2020-10-15_21-11-58
  done: false
  episode_len_mean: 775.7851874751578
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9032006234674
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 237
  episodes_total: 67929
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.695985326434098e-11
        cur_lr: 5.0e-05
        entropy: 0.10326694883406162
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010647964741413793
        total_loss: .inf
        vf_explained_var: 0.9942073822021484
        vf_loss: 2.940619468688965
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.62
    gpu_util_percent0: 0.31866666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638665816702254
    mean_env_wait_ms: 1.2028853721013864
    mean_inference_ms: 4.288687279324749
    mean_raw_obs_processing_ms: 0.37798895746403555
  time_since_restore: 8406.01159787178
  time_this_iter_s: 26.00920295715332
  time_total_s: 8406.01159787178
  timers:
    learn_throughput: 8633.365
    learn_time_ms: 18740.318
    sample_throughput: 23704.557
    sample_time_ms: 6825.354
    update_time_ms: 26.385
  timestamp: 1602796318
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:11:59,786	WARNING util.py:136 -- The `process_trial` operation took 0.9229445457458496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    326 |          8406.01 | 52744192 |  307.903 |              338.535 |              107.323 |            775.785 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.2246121297603
    time_step_min: 2977
  date: 2020-10-15_21-12-25
  done: false
  episode_len_mean: 775.7743223798196
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.90825620312006
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 177
  episodes_total: 68106
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1543977989651149e-10
        cur_lr: 5.0e-05
        entropy: 0.10070351573328178
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009923055224741498
        total_loss: .inf
        vf_explained_var: 0.9949072003364563
        vf_loss: 2.1762847304344177
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.445161290322584
    gpu_util_percent0: 0.295483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146385486418507
    mean_env_wait_ms: 1.2028789066157892
    mean_inference_ms: 4.288624202223288
    mean_raw_obs_processing_ms: 0.37798498433539546
  time_since_restore: 8432.196724176407
  time_this_iter_s: 26.185126304626465
  time_total_s: 8432.196724176407
  timers:
    learn_throughput: 8629.421
    learn_time_ms: 18748.883
    sample_throughput: 23719.413
    sample_time_ms: 6821.079
    update_time_ms: 27.177
  timestamp: 1602796345
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:12:27,282	WARNING util.py:136 -- The `process_trial` operation took 0.8937780857086182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    327 |           8432.2 | 52905984 |  307.908 |              338.535 |              107.323 |            775.774 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.209797891037
    time_step_min: 2977
  date: 2020-10-15_21-12-53
  done: false
  episode_len_mean: 775.7527443576008
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.91277134212066
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 216
  episodes_total: 68322
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7315966984476726e-10
        cur_lr: 5.0e-05
        entropy: 0.10665585783620675
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011108756434017172
        total_loss: .inf
        vf_explained_var: 0.9931485056877136
        vf_loss: 3.3516603112220764
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89032258064516
    gpu_util_percent0: 0.3280645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463840296570364
    mean_env_wait_ms: 1.2028711279702466
    mean_inference_ms: 4.288548533680325
    mean_raw_obs_processing_ms: 0.37798031207360494
  time_since_restore: 8458.074959039688
  time_this_iter_s: 25.87823486328125
  time_total_s: 8458.074959039688
  timers:
    learn_throughput: 8630.974
    learn_time_ms: 18745.509
    sample_throughput: 23702.307
    sample_time_ms: 6826.002
    update_time_ms: 28.985
  timestamp: 1602796373
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:12:54,407	WARNING util.py:136 -- The `process_trial` operation took 0.927778959274292 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    328 |          8458.07 | 53067776 |  307.913 |              338.535 |              107.323 |            775.753 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.1675326191657
    time_step_min: 2977
  date: 2020-10-15_21-13-20
  done: false
  episode_len_mean: 775.7254667444574
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.92210170550317
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 238
  episodes_total: 68560
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.597395047671509e-10
        cur_lr: 5.0e-05
        entropy: 0.1063020583242178
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010577927215005426
        total_loss: .inf
        vf_explained_var: 0.9952960014343262
        vf_loss: 2.2904398441314697
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.680000000000003
    gpu_util_percent0: 0.34800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463823338343702
    mean_env_wait_ms: 1.2028615957098003
    mean_inference_ms: 4.288452681398857
    mean_raw_obs_processing_ms: 0.3779727284806171
  time_since_restore: 8483.823214292526
  time_this_iter_s: 25.748255252838135
  time_total_s: 8483.823214292526
  timers:
    learn_throughput: 8630.724
    learn_time_ms: 18746.051
    sample_throughput: 23711.109
    sample_time_ms: 6823.468
    update_time_ms: 29.251
  timestamp: 1602796400
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:13:21,513	WARNING util.py:136 -- The `process_trial` operation took 0.9444241523742676 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    329 |          8483.82 | 53229568 |  307.922 |              338.535 |              107.323 |            775.725 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.1483762027456
    time_step_min: 2977
  date: 2020-10-15_21-13-47
  done: false
  episode_len_mean: 775.704170849154
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.92481617995514
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 179
  episodes_total: 68739
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8960925715072636e-10
        cur_lr: 5.0e-05
        entropy: 0.10814633779227734
        entropy_coeff: 0.0005000000000000001
        kl: 0.00475211344504108
        model: {}
        policy_loss: -0.012340272354776971
        total_loss: 2.746539751688639
        vf_explained_var: 0.9933843016624451
        vf_loss: 2.7589340607325235
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.761290322580646
    gpu_util_percent0: 0.31967741935483873
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638117138357704
    mean_env_wait_ms: 1.2028555342720508
    mean_inference_ms: 4.288391321657632
    mean_raw_obs_processing_ms: 0.3779689015441608
  time_since_restore: 8509.529175519943
  time_this_iter_s: 25.705961227416992
  time_total_s: 8509.529175519943
  timers:
    learn_throughput: 8649.553
    learn_time_ms: 18705.243
    sample_throughput: 23718.315
    sample_time_ms: 6821.395
    update_time_ms: 29.307
  timestamp: 1602796427
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: f76cb_00000
  
2020-10-15 21:13:48,542	WARNING util.py:136 -- The `process_trial` operation took 0.9827637672424316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    330 |          8509.53 | 53391360 |  307.925 |              338.535 |              107.323 |            775.704 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.171167380106
    time_step_min: 2977
  date: 2020-10-15_21-14-14
  done: false
  episode_len_mean: 775.6813666487811
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9218948186566
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 218
  episodes_total: 68957
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9480462857536318e-10
        cur_lr: 5.0e-05
        entropy: 0.1169800932208697
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010753679647071598
        total_loss: .inf
        vf_explained_var: 0.9933013319969177
        vf_loss: 3.171863615512848
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.283333333333335
    gpu_util_percent0: 0.37366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637957204499855
    mean_env_wait_ms: 1.2028479460968224
    mean_inference_ms: 4.288312351854407
    mean_raw_obs_processing_ms: 0.3779636297160915
  time_since_restore: 8535.120804309845
  time_this_iter_s: 25.591628789901733
  time_total_s: 8535.120804309845
  timers:
    learn_throughput: 8668.492
    learn_time_ms: 18664.376
    sample_throughput: 23728.999
    sample_time_ms: 6818.324
    update_time_ms: 28.697
  timestamp: 1602796454
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:14:15,589	WARNING util.py:136 -- The `process_trial` operation took 0.9037699699401855 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    331 |          8535.12 | 53553152 |  307.922 |              338.535 |              107.323 |            775.681 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.1272595806217
    time_step_min: 2977
  date: 2020-10-15_21-14-41
  done: false
  episode_len_mean: 775.6493669788415
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9285177477165
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 235
  episodes_total: 69192
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9220694286304476e-10
        cur_lr: 5.0e-05
        entropy: 0.10556539831062157
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011573613446671516
        total_loss: .inf
        vf_explained_var: 0.9949010014533997
        vf_loss: 2.4704308112462363
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.52258064516129
    gpu_util_percent0: 0.32161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637804117251435
    mean_env_wait_ms: 1.2028390411793302
    mean_inference_ms: 4.288222500545466
    mean_raw_obs_processing_ms: 0.37795680326188724
  time_since_restore: 8561.006189346313
  time_this_iter_s: 25.885385036468506
  time_total_s: 8561.006189346313
  timers:
    learn_throughput: 8657.925
    learn_time_ms: 18687.156
    sample_throughput: 23730.129
    sample_time_ms: 6817.999
    update_time_ms: 29.255
  timestamp: 1602796481
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:14:42,750	WARNING util.py:136 -- The `process_trial` operation took 0.9257142543792725 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    332 |          8561.01 | 53714944 |  307.929 |              338.535 |              107.323 |            775.649 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3179.0665676205863
    time_step_min: 2977
  date: 2020-10-15_21-15-08
  done: false
  episode_len_mean: 775.627057805968
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9361184280458
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 69370
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.383104142945671e-10
        cur_lr: 5.0e-05
        entropy: 0.09140297397971153
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01109040891363596
        total_loss: .inf
        vf_explained_var: 0.9951527714729309
        vf_loss: 2.015359709660212
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.258064516129032
    gpu_util_percent0: 0.3583870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637697999892837
    mean_env_wait_ms: 1.2028330552944486
    mean_inference_ms: 4.288163139272362
    mean_raw_obs_processing_ms: 0.37795315165153587
  time_since_restore: 8587.000437736511
  time_this_iter_s: 25.994248390197754
  time_total_s: 8587.000437736511
  timers:
    learn_throughput: 8655.588
    learn_time_ms: 18692.202
    sample_throughput: 23743.375
    sample_time_ms: 6814.196
    update_time_ms: 29.056
  timestamp: 1602796508
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:15:10,013	WARNING util.py:136 -- The `process_trial` operation took 0.9302034378051758 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    333 |             8587 | 53876736 |  307.936 |              338.535 |              107.323 |            775.627 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.978475916607
    time_step_min: 2977
  date: 2020-10-15_21-15-35
  done: false
  episode_len_mean: 775.598143464766
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9521686865204
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 222
  episodes_total: 69592
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.574656214418507e-10
        cur_lr: 5.0e-05
        entropy: 0.09396169272561868
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008741230980376713
        total_loss: .inf
        vf_explained_var: 0.9955387115478516
        vf_loss: 2.1036268870035806
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05161290322581
    gpu_util_percent0: 0.3051612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463752993642572
    mean_env_wait_ms: 1.2028254804774734
    mean_inference_ms: 4.288080965217116
    mean_raw_obs_processing_ms: 0.3779476686387366
  time_since_restore: 8612.941058158875
  time_this_iter_s: 25.94062042236328
  time_total_s: 8612.941058158875
  timers:
    learn_throughput: 8670.002
    learn_time_ms: 18661.126
    sample_throughput: 23737.183
    sample_time_ms: 6815.973
    update_time_ms: 27.911
  timestamp: 1602796535
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:15:37,300	WARNING util.py:136 -- The `process_trial` operation took 0.9645133018493652 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    334 |          8612.94 | 54038528 |  307.952 |              338.535 |              107.323 |            775.598 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.854495951852
    time_step_min: 2977
  date: 2020-10-15_21-16-03
  done: false
  episode_len_mean: 775.5680753863119
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9729636288704
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 235
  episodes_total: 69827
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.86198432162776e-10
        cur_lr: 5.0e-05
        entropy: 0.08632456262906392
        entropy_coeff: 0.0005000000000000001
        kl: 0.005202104647954305
        model: {}
        policy_loss: -0.009117045595000187
        total_loss: 1.8289310435454051
        vf_explained_var: 0.996213436126709
        vf_loss: 1.8380911946296692
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.770967741935486
    gpu_util_percent0: 0.2948387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637389100653053
    mean_env_wait_ms: 1.2028171515521524
    mean_inference_ms: 4.2879971481987695
    mean_raw_obs_processing_ms: 0.3779413745932272
  time_since_restore: 8639.344844818115
  time_this_iter_s: 26.403786659240723
  time_total_s: 8639.344844818115
  timers:
    learn_throughput: 8660.267
    learn_time_ms: 18682.103
    sample_throughput: 23698.763
    sample_time_ms: 6827.023
    update_time_ms: 29.121
  timestamp: 1602796563
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: f76cb_00000
  
2020-10-15 21:16:05,140	WARNING util.py:136 -- The `process_trial` operation took 0.9964461326599121 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    335 |          8639.34 | 54200320 |  307.973 |              338.535 |              107.323 |            775.568 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.7270908077357
    time_step_min: 2977
  date: 2020-10-15_21-16-30
  done: false
  episode_len_mean: 775.5432052912018
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 307.9925304788525
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 176
  episodes_total: 70003
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.86198432162776e-10
        cur_lr: 5.0e-05
        entropy: 0.0833053607493639
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010444202155667881
        total_loss: .inf
        vf_explained_var: 0.9973907470703125
        vf_loss: 1.0527797043323517
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.538709677419355
    gpu_util_percent0: 0.3054838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463727748792748
    mean_env_wait_ms: 1.202810814599365
    mean_inference_ms: 4.287937237491522
    mean_raw_obs_processing_ms: 0.3779375064206518
  time_since_restore: 8665.185196638107
  time_this_iter_s: 25.840351819992065
  time_total_s: 8665.185196638107
  timers:
    learn_throughput: 8670.777
    learn_time_ms: 18659.459
    sample_throughput: 23712.606
    sample_time_ms: 6823.037
    update_time_ms: 37.193
  timestamp: 1602796590
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:16:32,281	WARNING util.py:136 -- The `process_trial` operation took 0.9757442474365234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    336 |          8665.19 | 54362112 |  307.993 |              338.535 |              107.323 |            775.543 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.594583042444
    time_step_min: 2977
  date: 2020-10-15_21-16-58
  done: false
  episode_len_mean: 775.5136766862692
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.0075132276502
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 226
  episodes_total: 70229
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.479297648244164e-09
        cur_lr: 5.0e-05
        entropy: 0.10961077051858108
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009435971888403097
        total_loss: .inf
        vf_explained_var: 0.995269775390625
        vf_loss: 2.270235021909078
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.206451612903223
    gpu_util_percent0: 0.3080645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637121011660487
    mean_env_wait_ms: 1.2028039346965853
    mean_inference_ms: 4.287860359839814
    mean_raw_obs_processing_ms: 0.377932305026043
  time_since_restore: 8691.356511831284
  time_this_iter_s: 26.17131519317627
  time_total_s: 8691.356511831284
  timers:
    learn_throughput: 8667.778
    learn_time_ms: 18665.914
    sample_throughput: 23739.032
    sample_time_ms: 6815.442
    update_time_ms: 37.328
  timestamp: 1602796618
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:16:59,702	WARNING util.py:136 -- The `process_trial` operation took 0.9157001972198486 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    337 |          8691.36 | 54523904 |  308.008 |              338.535 |              107.323 |            775.514 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.5852505112475
    time_step_min: 2977
  date: 2020-10-15_21-17-25
  done: false
  episode_len_mean: 775.4848562264044
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.008162180435
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 229
  episodes_total: 70458
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2189464723662455e-09
        cur_lr: 5.0e-05
        entropy: 0.1027417288472255
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01193809910910204
        total_loss: .inf
        vf_explained_var: 0.9956185817718506
        vf_loss: 2.0893094638983407
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.516129032258064
    gpu_util_percent0: 0.2906451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146369762852338
    mean_env_wait_ms: 1.2027950316632188
    mean_inference_ms: 4.2877749671116945
    mean_raw_obs_processing_ms: 0.3779259990115249
  time_since_restore: 8717.364199399948
  time_this_iter_s: 26.00768756866455
  time_total_s: 8717.364199399948
  timers:
    learn_throughput: 8660.468
    learn_time_ms: 18681.669
    sample_throughput: 23745.219
    sample_time_ms: 6813.666
    update_time_ms: 35.375
  timestamp: 1602796645
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:17:27,023	WARNING util.py:136 -- The `process_trial` operation took 0.9784018993377686 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    338 |          8717.36 | 54685696 |  308.008 |              338.535 |              107.323 |            775.485 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.5384026971133
    time_step_min: 2977
  date: 2020-10-15_21-17-53
  done: false
  episode_len_mean: 775.4602044283369
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.0162367488236
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 70636
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.328419708549368e-09
        cur_lr: 5.0e-05
        entropy: 0.08940505236387253
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009502147081851339
        total_loss: .inf
        vf_explained_var: 0.995342493057251
        vf_loss: 1.9036986728509266
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.186666666666667
    gpu_util_percent0: 0.35566666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463686985397332
    mean_env_wait_ms: 1.2027892228814698
    mean_inference_ms: 4.2877170840719945
    mean_raw_obs_processing_ms: 0.3779223092487446
  time_since_restore: 8743.514351844788
  time_this_iter_s: 26.150152444839478
  time_total_s: 8743.514351844788
  timers:
    learn_throughput: 8647.352
    learn_time_ms: 18710.005
    sample_throughput: 23718.787
    sample_time_ms: 6821.259
    update_time_ms: 36.375
  timestamp: 1602796673
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:17:54,592	WARNING util.py:136 -- The `process_trial` operation took 0.9859790802001953 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    339 |          8743.51 | 54847488 |  308.016 |              338.535 |              107.323 |             775.46 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.404639866706
    time_step_min: 2977
  date: 2020-10-15_21-18-20
  done: false
  episode_len_mean: 775.4288274557949
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.03486796332146
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 70863
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.992629562824053e-09
        cur_lr: 5.0e-05
        entropy: 0.08689817103246848
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010996641833723212
        total_loss: .inf
        vf_explained_var: 0.996232807636261
        vf_loss: 1.8178191184997559
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.096774193548395
    gpu_util_percent0: 0.29161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463670972191828
    mean_env_wait_ms: 1.2027824748160671
    mean_inference_ms: 4.287639551347019
    mean_raw_obs_processing_ms: 0.3779172162269934
  time_since_restore: 8769.391113042831
  time_this_iter_s: 25.876761198043823
  time_total_s: 8769.391113042831
  timers:
    learn_throughput: 8643.013
    learn_time_ms: 18719.399
    sample_throughput: 23698.278
    sample_time_ms: 6827.163
    update_time_ms: 36.407
  timestamp: 1602796700
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:18:21,780	WARNING util.py:136 -- The `process_trial` operation took 0.9812040328979492 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    340 |          8769.39 | 55009280 |  308.035 |              338.535 |              107.323 |            775.429 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.242723229366
    time_step_min: 2977
  date: 2020-10-15_21-18-47
  done: false
  episode_len_mean: 775.3962723308482
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.0608909463179
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 71090
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.488944344236078e-09
        cur_lr: 5.0e-05
        entropy: 0.07801501887540023
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007957047523329189
        total_loss: .inf
        vf_explained_var: 0.997500479221344
        vf_loss: 1.1587884426116943
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.13225806451613
    gpu_util_percent0: 0.3032258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636571332590548
    mean_env_wait_ms: 1.2027737211377272
    mean_inference_ms: 4.287556858414901
    mean_raw_obs_processing_ms: 0.37791104130286857
  time_since_restore: 8795.407595157623
  time_this_iter_s: 26.01648211479187
  time_total_s: 8795.407595157623
  timers:
    learn_throughput: 8632.602
    learn_time_ms: 18741.974
    sample_throughput: 23634.281
    sample_time_ms: 6845.649
    update_time_ms: 36.67
  timestamp: 1602796727
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:18:49,253	WARNING util.py:136 -- The `process_trial` operation took 0.9539120197296143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    341 |          8795.41 | 55171072 |  308.061 |              338.535 |              107.323 |            775.396 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3178.0834784073677
    time_step_min: 2977
  date: 2020-10-15_21-19-15
  done: false
  episode_len_mean: 775.3668163322576
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.0872645353493
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 71270
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1233416516354121e-08
        cur_lr: 5.0e-05
        entropy: 0.07620761853953202
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009828278668768084
        total_loss: .inf
        vf_explained_var: 0.9974586963653564
        vf_loss: 0.9979084531466166
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.048387096774192
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463646297633194
    mean_env_wait_ms: 1.20276741845956
    mean_inference_ms: 4.287497709342598
    mean_raw_obs_processing_ms: 0.3779072560397431
  time_since_restore: 8821.753827810287
  time_this_iter_s: 26.346232652664185
  time_total_s: 8821.753827810287
  timers:
    learn_throughput: 8626.143
    learn_time_ms: 18756.008
    sample_throughput: 23566.249
    sample_time_ms: 6865.412
    update_time_ms: 38.339
  timestamp: 1602796755
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:19:16,926	WARNING util.py:136 -- The `process_trial` operation took 0.9700560569763184 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    342 |          8821.75 | 55332864 |  308.087 |              338.535 |              107.323 |            775.367 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.8691697336935
    time_step_min: 2977
  date: 2020-10-15_21-19-42
  done: false
  episode_len_mean: 775.327352065006
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.1171987564206
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 71501
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.685012477453118e-08
        cur_lr: 5.0e-05
        entropy: 0.08634592530628045
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006708821883269896
        total_loss: .inf
        vf_explained_var: 0.9971718192100525
        vf_loss: 1.3395250737667084
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.474193548387095
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463631543612603
    mean_env_wait_ms: 1.2027606682463425
    mean_inference_ms: 4.287423561293362
    mean_raw_obs_processing_ms: 0.3779024354337733
  time_since_restore: 8847.67065000534
  time_this_iter_s: 25.9168221950531
  time_total_s: 8847.67065000534
  timers:
    learn_throughput: 8627.084
    learn_time_ms: 18753.962
    sample_throughput: 23556.227
    sample_time_ms: 6868.332
    update_time_ms: 38.406
  timestamp: 1602796782
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:19:44,300	WARNING util.py:136 -- The `process_trial` operation took 0.941370964050293 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    343 |          8847.67 | 55494656 |  308.117 |              338.535 |              107.323 |            775.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.655864176002
    time_step_min: 2977
  date: 2020-10-15_21-20-10
  done: false
  episode_len_mean: 775.2896142102254
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.1474364688954
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 222
  episodes_total: 71723
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5275187161796766e-08
        cur_lr: 5.0e-05
        entropy: 0.07980594349404176
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0091118124400964
        total_loss: .inf
        vf_explained_var: 0.9976001381874084
        vf_loss: 1.0608505705992382
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.361290322580647
    gpu_util_percent0: 0.31548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636171331007677
    mean_env_wait_ms: 1.2027525169879476
    mean_inference_ms: 4.287343280726948
    mean_raw_obs_processing_ms: 0.37789637968533096
  time_since_restore: 8873.77543258667
  time_this_iter_s: 26.104782581329346
  time_total_s: 8873.77543258667
  timers:
    learn_throughput: 8615.545
    learn_time_ms: 18779.08
    sample_throughput: 23586.455
    sample_time_ms: 6859.53
    update_time_ms: 37.859
  timestamp: 1602796810
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:20:11,711	WARNING util.py:136 -- The `process_trial` operation took 0.9381310939788818 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    344 |          8873.78 | 55656448 |  308.147 |              338.535 |              107.323 |             775.29 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.4792458081124
    time_step_min: 2977
  date: 2020-10-15_21-20-37
  done: false
  episode_len_mean: 775.2611428650896
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.1736616024655
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 184
  episodes_total: 71907
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7912780742695144e-08
        cur_lr: 5.0e-05
        entropy: 0.07564437637726466
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009100866901765889
        total_loss: .inf
        vf_explained_var: 0.9974427819252014
        vf_loss: 1.0031635860602062
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.653333333333332
    gpu_util_percent0: 0.35966666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636047401985755
    mean_env_wait_ms: 1.2027457661901835
    mean_inference_ms: 4.287280949058373
    mean_raw_obs_processing_ms: 0.3778923749401818
  time_since_restore: 8899.594470977783
  time_this_iter_s: 25.81903839111328
  time_total_s: 8899.594470977783
  timers:
    learn_throughput: 8636.713
    learn_time_ms: 18733.052
    sample_throughput: 23620.072
    sample_time_ms: 6849.767
    update_time_ms: 35.499
  timestamp: 1602796837
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:20:38,900	WARNING util.py:136 -- The `process_trial` operation took 0.9566168785095215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    345 |          8899.59 | 55818240 |  308.174 |              338.535 |              107.323 |            775.261 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.2524931689254
    time_step_min: 2977
  date: 2020-10-15_21-21-04
  done: false
  episode_len_mean: 775.2238872177325
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.21013542738274
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 72139
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6869171114042725e-08
        cur_lr: 5.0e-05
        entropy: 0.08902463565270106
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007497139124704215
        total_loss: .inf
        vf_explained_var: 0.9969334006309509
        vf_loss: 1.4241866966088612
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56129032258065
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635906202610036
    mean_env_wait_ms: 1.2027394765874202
    mean_inference_ms: 4.287208667201129
    mean_raw_obs_processing_ms: 0.37788745642097166
  time_since_restore: 8925.642545938492
  time_this_iter_s: 26.048074960708618
  time_total_s: 8925.642545938492
  timers:
    learn_throughput: 8621.719
    learn_time_ms: 18765.63
    sample_throughput: 23632.813
    sample_time_ms: 6846.074
    update_time_ms: 26.861
  timestamp: 1602796864
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:21:06,279	WARNING util.py:136 -- The `process_trial` operation took 0.9889962673187256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    346 |          8925.64 | 55980032 |   308.21 |              338.535 |              107.323 |            775.224 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.1225765411955
    time_step_min: 2977
  date: 2020-10-15_21-21-32
  done: false
  episode_len_mean: 775.1935706783128
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.22535222868294
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 72356
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.530375667106408e-08
        cur_lr: 5.0e-05
        entropy: 0.10508572620650132
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009345139648454884
        total_loss: .inf
        vf_explained_var: 0.9954531788825989
        vf_loss: 1.9909452497959137
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96451612903226
    gpu_util_percent0: 0.29193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635775922935765
    mean_env_wait_ms: 1.2027308803816965
    mean_inference_ms: 4.287133754623717
    mean_raw_obs_processing_ms: 0.37788207544073815
  time_since_restore: 8951.931759119034
  time_this_iter_s: 26.289213180541992
  time_total_s: 8951.931759119034
  timers:
    learn_throughput: 8621.125
    learn_time_ms: 18766.925
    sample_throughput: 23591.709
    sample_time_ms: 6858.003
    update_time_ms: 24.745
  timestamp: 1602796892
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:21:33,982	WARNING util.py:136 -- The `process_trial` operation took 0.9828159809112549 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    347 |          8951.93 | 56141824 |  308.225 |              338.535 |              107.323 |            775.194 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.161027352856
    time_step_min: 2977
  date: 2020-10-15_21-21-59
  done: false
  episode_len_mean: 775.1730379519983
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.2176435636643
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 72539
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.279556350065961e-07
        cur_lr: 5.0e-05
        entropy: 0.11089069085816543
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012576178628175208
        total_loss: .inf
        vf_explained_var: 0.9914297461509705
        vf_loss: 3.6144938468933105
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.999999999999996
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635654902507153
    mean_env_wait_ms: 1.2027246333983923
    mean_inference_ms: 4.2870725956889695
    mean_raw_obs_processing_ms: 0.37787828364943266
  time_since_restore: 8977.707872867584
  time_this_iter_s: 25.776113748550415
  time_total_s: 8977.707872867584
  timers:
    learn_throughput: 8628.521
    learn_time_ms: 18750.837
    sample_throughput: 23585.9
    sample_time_ms: 6859.691
    update_time_ms: 24.642
  timestamp: 1602796919
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:22:01,113	WARNING util.py:136 -- The `process_trial` operation took 1.0103936195373535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    348 |          8977.71 | 56303616 |  308.218 |              338.535 |              107.323 |            775.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3177.12859892754
    time_step_min: 2977
  date: 2020-10-15_21-22-27
  done: false
  episode_len_mean: 775.1447809597098
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.231504985545
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 233
  episodes_total: 72772
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9193345250989417e-07
        cur_lr: 5.0e-05
        entropy: 0.09641014722486337
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010879339592065662
        total_loss: .inf
        vf_explained_var: 0.9954883456230164
        vf_loss: 2.16268253326416
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.651612903225807
    gpu_util_percent0: 0.3161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463550753792365
    mean_env_wait_ms: 1.202718180253005
    mean_inference_ms: 4.287002202311669
    mean_raw_obs_processing_ms: 0.3778732608636003
  time_since_restore: 9003.800149917603
  time_this_iter_s: 26.09227705001831
  time_total_s: 9003.800149917603
  timers:
    learn_throughput: 8635.473
    learn_time_ms: 18735.742
    sample_throughput: 23548.84
    sample_time_ms: 6870.487
    update_time_ms: 23.662
  timestamp: 1602796947
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:22:28,496	WARNING util.py:136 -- The `process_trial` operation took 0.9481613636016846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    349 |           9003.8 | 56465408 |  308.232 |              338.535 |              107.323 |            775.145 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3176.9245743426645
    time_step_min: 2977
  date: 2020-10-15_21-22-54
  done: false
  episode_len_mean: 775.1175809722146
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.2649956849141
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 216
  episodes_total: 72988
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.879001787648413e-07
        cur_lr: 5.0e-05
        entropy: 0.07684255329271157
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008677522593643516
        total_loss: .inf
        vf_explained_var: 0.9973866939544678
        vf_loss: 1.1042288144429524
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612907
    gpu_util_percent0: 0.29258064516129034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635386476335563
    mean_env_wait_ms: 1.2027100586965147
    mean_inference_ms: 4.286927202573609
    mean_raw_obs_processing_ms: 0.37786794606474944
  time_since_restore: 9029.704835653305
  time_this_iter_s: 25.904685735702515
  time_total_s: 9029.704835653305
  timers:
    learn_throughput: 8629.122
    learn_time_ms: 18749.532
    sample_throughput: 23593.555
    sample_time_ms: 6857.466
    update_time_ms: 24.489
  timestamp: 1602796974
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:22:55,843	WARNING util.py:136 -- The `process_trial` operation took 1.0331089496612549 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    350 |           9029.7 | 56627200 |  308.265 |              338.535 |              107.323 |            775.118 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3176.7218476938015
    time_step_min: 2977
  date: 2020-10-15_21-23-21
  done: false
  episode_len_mean: 775.0941356548359
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.29701602541934
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 73171
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.31850268147262e-07
        cur_lr: 5.0e-05
        entropy: 0.07280312230189641
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009153850142107936
        total_loss: .inf
        vf_explained_var: 0.9979096055030823
        vf_loss: 0.8007145076990128
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89666666666666
    gpu_util_percent0: 0.31566666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146352688235929
    mean_env_wait_ms: 1.2027033997665604
    mean_inference_ms: 4.2868671009705315
    mean_raw_obs_processing_ms: 0.37786421305830215
  time_since_restore: 9055.497303962708
  time_this_iter_s: 25.792468309402466
  time_total_s: 9055.497303962708
  timers:
    learn_throughput: 8636.283
    learn_time_ms: 18733.985
    sample_throughput: 23644.856
    sample_time_ms: 6842.588
    update_time_ms: 26.401
  timestamp: 1602797001
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:23:23,086	WARNING util.py:136 -- The `process_trial` operation took 0.9608581066131592 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    351 |           9055.5 | 56788992 |  308.297 |              338.535 |              107.323 |            775.094 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3176.4433858106727
    time_step_min: 2977
  date: 2020-10-15_21-23-49
  done: false
  episode_len_mean: 775.0654297274102
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.34042744664345
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 236
  episodes_total: 73407
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.477754022208928e-07
        cur_lr: 5.0e-05
        entropy: 0.07950274211664994
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008088007278274745
        total_loss: .inf
        vf_explained_var: 0.9978416562080383
        vf_loss: 1.0428708146015804
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.341935483870966
    gpu_util_percent0: 0.37741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463512088557614
    mean_env_wait_ms: 1.2026972517806709
    mean_inference_ms: 4.286798137903635
    mean_raw_obs_processing_ms: 0.377859253590552
  time_since_restore: 9081.550338745117
  time_this_iter_s: 26.053034782409668
  time_total_s: 9081.550338745117
  timers:
    learn_throughput: 8644.935
    learn_time_ms: 18715.236
    sample_throughput: 23650.554
    sample_time_ms: 6840.939
    update_time_ms: 25.261
  timestamp: 1602797029
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:23:50,554	WARNING util.py:136 -- The `process_trial` operation took 0.9783713817596436 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    352 |          9081.55 | 56950784 |   308.34 |              338.535 |              107.323 |            775.065 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3176.170345202501
    time_step_min: 2977
  date: 2020-10-15_21-24-16
  done: false
  episode_len_mean: 775.0420118986173
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.38163219217796
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 215
  episodes_total: 73622
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.716631033313394e-07
        cur_lr: 5.0e-05
        entropy: 0.07043810747563839
        entropy_coeff: 0.0005000000000000001
        kl: 0.00497370147301505
        model: {}
        policy_loss: -0.007492895975398521
        total_loss: 0.7942467282215754
        vf_explained_var: 0.9980353713035583
        vf_loss: 0.801774819691976
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.906451612903226
    gpu_util_percent0: 0.3109677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146349965268786
    mean_env_wait_ms: 1.2026887820590886
    mean_inference_ms: 4.286724057634149
    mean_raw_obs_processing_ms: 0.3778541792880884
  time_since_restore: 9107.566271781921
  time_this_iter_s: 26.0159330368042
  time_total_s: 9107.566271781921
  timers:
    learn_throughput: 8643.166
    learn_time_ms: 18719.067
    sample_throughput: 23635.398
    sample_time_ms: 6845.326
    update_time_ms: 26.501
  timestamp: 1602797056
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: f76cb_00000
  
2020-10-15 21:24:18,056	WARNING util.py:136 -- The `process_trial` operation took 1.040637731552124 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    353 |          9107.57 | 57112576 |  308.382 |              338.535 |              107.323 |            775.042 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3175.9048154876496
    time_step_min: 2977
  date: 2020-10-15_21-24-44
  done: false
  episode_len_mean: 775.0267465178039
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.4191078855325
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 73804
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.858315516656697e-07
        cur_lr: 5.0e-05
        entropy: 0.07159980572760105
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007693659145540248
        total_loss: .inf
        vf_explained_var: 0.9986427426338196
        vf_loss: 0.5066889747977257
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55806451612903
    gpu_util_percent0: 0.34548387096774197
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634881421105192
    mean_env_wait_ms: 1.2026821031360746
    mean_inference_ms: 4.2866642683924505
    mean_raw_obs_processing_ms: 0.3778503757549653
  time_since_restore: 9133.57228422165
  time_this_iter_s: 26.006012439727783
  time_total_s: 9133.57228422165
  timers:
    learn_throughput: 8652.952
    learn_time_ms: 18697.896
    sample_throughput: 23568.806
    sample_time_ms: 6864.667
    update_time_ms: 26.039
  timestamp: 1602797084
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:24:45,456	WARNING util.py:136 -- The `process_trial` operation took 0.9506628513336182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    354 |          9133.57 | 57274368 |  308.419 |              338.535 |              107.323 |            775.027 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3175.63029934455
    time_step_min: 2977
  date: 2020-10-15_21-25-11
  done: false
  episode_len_mean: 775.008900954928
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.4569058632027
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 233
  episodes_total: 74037
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.287473274985046e-07
        cur_lr: 5.0e-05
        entropy: 0.08806024305522442
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011186258071878305
        total_loss: .inf
        vf_explained_var: 0.9970136284828186
        vf_loss: 1.3552862703800201
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.74375
    gpu_util_percent0: 0.2925
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634745184119363
    mean_env_wait_ms: 1.2026756300528056
    mean_inference_ms: 4.286598394275682
    mean_raw_obs_processing_ms: 0.37784578678473985
  time_since_restore: 9159.812305927277
  time_this_iter_s: 26.24002170562744
  time_total_s: 9159.812305927277
  timers:
    learn_throughput: 8640.325
    learn_time_ms: 18725.221
    sample_throughput: 23558.094
    sample_time_ms: 6867.788
    update_time_ms: 27.491
  timestamp: 1602797111
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:25:13,010	WARNING util.py:136 -- The `process_trial` operation took 0.966547966003418 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    355 |          9159.81 | 57436160 |  308.457 |              338.535 |              107.323 |            775.009 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3175.4470166549886
    time_step_min: 2977
  date: 2020-10-15_21-25-38
  done: false
  episode_len_mean: 774.9951921781992
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.4877895228853
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 217
  episodes_total: 74254
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0931209912477567e-06
        cur_lr: 5.0e-05
        entropy: 0.08062241412699223
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010211767551178733
        total_loss: .inf
        vf_explained_var: 0.9977375864982605
        vf_loss: 0.9771159142255783
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.633333333333333
    gpu_util_percent0: 0.3776666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634628678588713
    mean_env_wait_ms: 1.2026670233869285
    mean_inference_ms: 4.286525118993608
    mean_raw_obs_processing_ms: 0.3778406581953942
  time_since_restore: 9185.511120319366
  time_this_iter_s: 25.698814392089844
  time_total_s: 9185.511120319366
  timers:
    learn_throughput: 8658.996
    learn_time_ms: 18684.845
    sample_throughput: 23546.875
    sample_time_ms: 6871.06
    update_time_ms: 28.11
  timestamp: 1602797138
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:25:40,078	WARNING util.py:136 -- The `process_trial` operation took 1.0109336376190186 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    356 |          9185.51 | 57597952 |  308.488 |              338.535 |              107.323 |            774.995 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3175.2196712056243
    time_step_min: 2977
  date: 2020-10-15_21-26-05
  done: false
  episode_len_mean: 774.9823738832539
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.5199099207294
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 74435
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6396814868716354e-06
        cur_lr: 5.0e-05
        entropy: 0.07199124371012051
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009474269832329204
        total_loss: .inf
        vf_explained_var: 0.9978703856468201
        vf_loss: 0.8007353494564692
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.119354838709683
    gpu_util_percent0: 0.335483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634512794176147
    mean_env_wait_ms: 1.2026602913637332
    mean_inference_ms: 4.28646743059856
    mean_raw_obs_processing_ms: 0.3778371220002978
  time_since_restore: 9211.224931955338
  time_this_iter_s: 25.71381163597107
  time_total_s: 9211.224931955338
  timers:
    learn_throughput: 8688.042
    learn_time_ms: 18622.377
    sample_throughput: 23563.012
    sample_time_ms: 6866.355
    update_time_ms: 27.837
  timestamp: 1602797165
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:26:07,165	WARNING util.py:136 -- The `process_trial` operation took 1.0177476406097412 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    357 |          9211.22 | 57759744 |   308.52 |              338.535 |              107.323 |            774.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3174.9051996783705
    time_step_min: 2977
  date: 2020-10-15_21-26-33
  done: false
  episode_len_mean: 774.9643995606868
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.56545078980855
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 74662
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4595222303074525e-06
        cur_lr: 5.0e-05
        entropy: 0.07202370713154475
        entropy_coeff: 0.0005000000000000001
        kl: 0.004643294455793996
        model: {}
        policy_loss: -0.00810135476058349
        total_loss: 0.7987816681464514
        vf_explained_var: 0.9981780052185059
        vf_loss: 0.8069190482298533
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.732258064516135
    gpu_util_percent0: 0.30645161290322587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634389349046908
    mean_env_wait_ms: 1.2026535706046788
    mean_inference_ms: 4.286402358196195
    mean_raw_obs_processing_ms: 0.3778326925274334
  time_since_restore: 9237.341730117798
  time_this_iter_s: 26.116798162460327
  time_total_s: 9237.341730117798
  timers:
    learn_throughput: 8673.226
    learn_time_ms: 18654.188
    sample_throughput: 23594.364
    sample_time_ms: 6857.231
    update_time_ms: 28.065
  timestamp: 1602797193
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: f76cb_00000
  
2020-10-15 21:26:34,648	WARNING util.py:136 -- The `process_trial` operation took 1.0100905895233154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    358 |          9237.34 | 57921536 |  308.565 |              338.535 |              107.323 |            774.964 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3174.5900807011917
    time_step_min: 2977
  date: 2020-10-15_21-27-00
  done: false
  episode_len_mean: 774.9447693827951
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.61306762035866
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 224
  episodes_total: 74886
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2297611151537263e-06
        cur_lr: 5.0e-05
        entropy: 0.06528957777967055
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008576643203317266
        total_loss: .inf
        vf_explained_var: 0.9987244009971619
        vf_loss: 0.5189017628630003
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.480645161290322
    gpu_util_percent0: 0.38322580645161297
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634258761782928
    mean_env_wait_ms: 1.2026446813165461
    mean_inference_ms: 4.286329569320062
    mean_raw_obs_processing_ms: 0.37782750769140255
  time_since_restore: 9263.514941692352
  time_this_iter_s: 26.173211574554443
  time_total_s: 9263.514941692352
  timers:
    learn_throughput: 8663.956
    learn_time_ms: 18674.149
    sample_throughput: 23651.758
    sample_time_ms: 6840.591
    update_time_ms: 31.831
  timestamp: 1602797220
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:27:02,156	WARNING util.py:136 -- The `process_trial` operation took 0.9832749366760254 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    359 |          9263.51 | 58083328 |  308.613 |              338.535 |              107.323 |            774.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3174.3214971209213
    time_step_min: 2977
  date: 2020-10-15_21-27-28
  done: false
  episode_len_mean: 774.929608611089
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.65441038149055
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 75066
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.84464167273059e-06
        cur_lr: 5.0e-05
        entropy: 0.07425684481859207
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008363190630916506
        total_loss: .inf
        vf_explained_var: 0.9987289309501648
        vf_loss: 0.4596491927901904
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870963
    gpu_util_percent0: 0.27419354838709675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634148319975715
    mean_env_wait_ms: 1.202637849235741
    mean_inference_ms: 4.2862740302804045
    mean_raw_obs_processing_ms: 0.3778241273926061
  time_since_restore: 9289.410040855408
  time_this_iter_s: 25.89509916305542
  time_total_s: 9289.410040855408
  timers:
    learn_throughput: 8667.741
    learn_time_ms: 18665.994
    sample_throughput: 23629.431
    sample_time_ms: 6847.054
    update_time_ms: 31.202
  timestamp: 1602797248
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:27:29,527	WARNING util.py:136 -- The `process_trial` operation took 1.079540491104126 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    360 |          9289.41 | 58245120 |  308.654 |              338.535 |              107.323 |             774.93 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3174.005899940203
    time_step_min: 2977
  date: 2020-10-15_21-27-55
  done: false
  episode_len_mean: 774.9157204138279
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.6961088634462
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 75297
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.766962509095884e-06
        cur_lr: 5.0e-05
        entropy: 0.08866365005572636
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010623813179942468
        total_loss: .inf
        vf_explained_var: 0.9966588020324707
        vf_loss: 1.4937582314014435
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.28709677419355
    gpu_util_percent0: 0.32741935483870976
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634020101545642
    mean_env_wait_ms: 1.202630537269094
    mean_inference_ms: 4.286209585105462
    mean_raw_obs_processing_ms: 0.3778197258091026
  time_since_restore: 9315.713702917099
  time_this_iter_s: 26.303662061691284
  time_total_s: 9315.713702917099
  timers:
    learn_throughput: 8649.649
    learn_time_ms: 18705.037
    sample_throughput: 23603.62
    sample_time_ms: 6854.542
    update_time_ms: 29.318
  timestamp: 1602797275
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:27:57,236	WARNING util.py:136 -- The `process_trial` operation took 1.05088210105896 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    361 |          9315.71 | 58406912 |  308.696 |              338.535 |              107.323 |            774.916 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.8262712987253
    time_step_min: 2977
  date: 2020-10-15_21-28-23
  done: false
  episode_len_mean: 774.9065628476085
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.7268120582915
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 219
  episodes_total: 75516
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.150443763643826e-06
        cur_lr: 5.0e-05
        entropy: 0.08362288835148017
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009669720602687448
        total_loss: .inf
        vf_explained_var: 0.9958312511444092
        vf_loss: 1.8156406382719676
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.258064516129036
    gpu_util_percent0: 0.3132258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633904697632055
    mean_env_wait_ms: 1.2026215326211491
    mean_inference_ms: 4.286138488654684
    mean_raw_obs_processing_ms: 0.37781461517284176
  time_since_restore: 9341.914402246475
  time_this_iter_s: 26.20069932937622
  time_total_s: 9341.914402246475
  timers:
    learn_throughput: 8639.327
    learn_time_ms: 18727.384
    sample_throughput: 23665.645
    sample_time_ms: 6836.577
    update_time_ms: 29.926
  timestamp: 1602797303
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:28:24,861	WARNING util.py:136 -- The `process_trial` operation took 1.0409915447235107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    362 |          9341.91 | 58568704 |  308.727 |              338.535 |              107.323 |            774.907 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.602786443187
    time_step_min: 2977
  date: 2020-10-15_21-28-51
  done: false
  episode_len_mean: 774.8969931566571
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.75898120369277
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 75694
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.22566564546574e-06
        cur_lr: 5.0e-05
        entropy: 0.07401369636257489
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01112860675493721
        total_loss: .inf
        vf_explained_var: 0.9976094365119934
        vf_loss: 0.896838923295339
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.225806451612904
    gpu_util_percent0: 0.28483870967741937
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633802522081343
    mean_env_wait_ms: 1.202614491164963
    mean_inference_ms: 4.286085872847812
    mean_raw_obs_processing_ms: 0.3778113589474672
  time_since_restore: 9368.162816762924
  time_this_iter_s: 26.248414516448975
  time_total_s: 9368.162816762924
  timers:
    learn_throughput: 8632.643
    learn_time_ms: 18741.885
    sample_throughput: 23670.792
    sample_time_ms: 6835.09
    update_time_ms: 30.197
  timestamp: 1602797331
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:28:52,609	WARNING util.py:136 -- The `process_trial` operation took 0.9964404106140137 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    363 |          9368.16 | 58730496 |  308.759 |              338.535 |              107.323 |            774.897 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.283594480977
    time_step_min: 2977
  date: 2020-10-15_21-29-18
  done: false
  episode_len_mean: 774.8886532762594
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.80756993710565
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 75925
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.338498468198612e-06
        cur_lr: 5.0e-05
        entropy: 0.0727032454063495
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008881112797401633
        total_loss: .inf
        vf_explained_var: 0.9981456398963928
        vf_loss: 0.8023712287346522
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.687096774193545
    gpu_util_percent0: 0.2948387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463366700974736
    mean_env_wait_ms: 1.2026068803047492
    mean_inference_ms: 4.28602583863239
    mean_raw_obs_processing_ms: 0.37780747790548763
  time_since_restore: 9394.078911066055
  time_this_iter_s: 25.916094303131104
  time_total_s: 9394.078911066055
  timers:
    learn_throughput: 8632.683
    learn_time_ms: 18741.798
    sample_throughput: 23739.473
    sample_time_ms: 6815.315
    update_time_ms: 30.245
  timestamp: 1602797358
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:29:19,881	WARNING util.py:136 -- The `process_trial` operation took 0.9903357028961182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    364 |          9394.08 | 58892288 |  308.808 |              338.535 |              107.323 |            774.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.0060575002626
    time_step_min: 2977
  date: 2020-10-15_21-29-45
  done: false
  episode_len_mean: 774.8785622357051
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.85258701585235
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 221
  episodes_total: 76146
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4007747702297914e-05
        cur_lr: 5.0e-05
        entropy: 0.06772977920869987
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008458035973793207
        total_loss: .inf
        vf_explained_var: 0.9984660744667053
        vf_loss: 0.6481882532437643
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.545161290322582
    gpu_util_percent0: 0.35096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633542594971305
    mean_env_wait_ms: 1.2025965492380617
    mean_inference_ms: 4.285949638773949
    mean_raw_obs_processing_ms: 0.3778020332898832
  time_since_restore: 9420.145091056824
  time_this_iter_s: 26.066179990768433
  time_total_s: 9420.145091056824
  timers:
    learn_throughput: 8634.093
    learn_time_ms: 18738.736
    sample_throughput: 23798.152
    sample_time_ms: 6798.511
    update_time_ms: 29.305
  timestamp: 1602797385
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:29:47,356	WARNING util.py:136 -- The `process_trial` operation took 1.02504301071167 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    365 |          9420.15 | 59054080 |  308.853 |              338.535 |              107.323 |            774.879 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3172.735239368675
    time_step_min: 2977
  date: 2020-10-15_21-30-13
  done: false
  episode_len_mean: 774.8721143515971
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.895092343131
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 76326
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1011621553446874e-05
        cur_lr: 5.0e-05
        entropy: 0.0628882364059488
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006203108389551441
        total_loss: .inf
        vf_explained_var: 0.9993219375610352
        vf_loss: 0.26585161437590915
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096777
    gpu_util_percent0: 0.29516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633451561418032
    mean_env_wait_ms: 1.2025892630227824
    mean_inference_ms: 4.285899163791239
    mean_raw_obs_processing_ms: 0.3777989185251666
  time_since_restore: 9446.096771717072
  time_this_iter_s: 25.951680660247803
  time_total_s: 9446.096771717072
  timers:
    learn_throughput: 8623.076
    learn_time_ms: 18762.679
    sample_throughput: 23825.644
    sample_time_ms: 6790.666
    update_time_ms: 29.135
  timestamp: 1602797413
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:30:14,761	WARNING util.py:136 -- The `process_trial` operation took 1.0668485164642334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    366 |           9446.1 | 59215872 |  308.895 |              338.535 |              107.323 |            774.872 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3172.5303759084018
    time_step_min: 2977
  date: 2020-10-15_21-30-41
  done: false
  episode_len_mean: 774.8608621815806
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.91279153388876
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 224
  episodes_total: 76550
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.151743233017031e-05
        cur_lr: 5.0e-05
        entropy: 0.10862706849972407
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008862041596633693
        total_loss: .inf
        vf_explained_var: 0.992420494556427
        vf_loss: 3.4468021194140115
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.638709677419357
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633323152089212
    mean_env_wait_ms: 1.2025817818907718
    mean_inference_ms: 4.285840676172391
    mean_raw_obs_processing_ms: 0.3777951387112333
  time_since_restore: 9472.342187404633
  time_this_iter_s: 26.245415687561035
  time_total_s: 9472.342187404633
  timers:
    learn_throughput: 8595.775
    learn_time_ms: 18822.271
    sample_throughput: 23831.317
    sample_time_ms: 6789.05
    update_time_ms: 31.485
  timestamp: 1602797441
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:30:42,590	WARNING util.py:136 -- The `process_trial` operation took 1.092881202697754 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    367 |          9472.34 | 59377664 |  308.913 |              338.535 |              107.323 |            774.861 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3172.7753626021345
    time_step_min: 2977
  date: 2020-10-15_21-31-08
  done: false
  episode_len_mean: 774.8445147761757
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.87584081348
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 229
  episodes_total: 76779
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.727614849525546e-05
        cur_lr: 5.0e-05
        entropy: 0.1213717187444369
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013742945739068091
        total_loss: .inf
        vf_explained_var: 0.9907835125923157
        vf_loss: 4.556034286816915
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.716129032258063
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633197405634205
    mean_env_wait_ms: 1.2025711568094433
    mean_inference_ms: 4.285765939417946
    mean_raw_obs_processing_ms: 0.3777898192032876
  time_since_restore: 9498.361287117004
  time_this_iter_s: 26.019099712371826
  time_total_s: 9498.361287117004
  timers:
    learn_throughput: 8600.922
    learn_time_ms: 18811.007
    sample_throughput: 23830.588
    sample_time_ms: 6789.257
    update_time_ms: 31.489
  timestamp: 1602797468
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:31:10,045	WARNING util.py:136 -- The `process_trial` operation took 1.0583865642547607 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    368 |          9498.36 | 59539456 |  308.876 |              338.535 |              107.323 |            774.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3172.9271143470064
    time_step_min: 2977
  date: 2020-10-15_21-31-35
  done: false
  episode_len_mean: 774.8315682783892
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.85346309752146
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 76957
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.09142227428832e-05
        cur_lr: 5.0e-05
        entropy: 0.12044126043717067
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014217125436213488
        total_loss: .inf
        vf_explained_var: 0.9910943508148193
        vf_loss: 3.7478023171424866
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.956666666666663
    gpu_util_percent0: 0.38899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633107527187908
    mean_env_wait_ms: 1.2025644838145422
    mean_inference_ms: 4.285717590480644
    mean_raw_obs_processing_ms: 0.3777869232573287
  time_since_restore: 9523.572088479996
  time_this_iter_s: 25.210801362991333
  time_total_s: 9523.572088479996
  timers:
    learn_throughput: 8647.665
    learn_time_ms: 18709.328
    sample_throughput: 23831.528
    sample_time_ms: 6788.99
    update_time_ms: 27.044
  timestamp: 1602797495
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:31:36,646	WARNING util.py:136 -- The `process_trial` operation took 1.0300207138061523 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    369 |          9523.57 | 59701248 |  308.853 |              338.535 |              107.323 |            774.832 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.1199051026756
    time_step_min: 2977
  date: 2020-10-15_21-32-02
  done: false
  episode_len_mean: 774.8175516338853
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.82659487669986
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 221
  episodes_total: 77178
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0001063713341143248
        cur_lr: 5.0e-05
        entropy: 0.12511632281045118
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010249301907606423
        total_loss: .inf
        vf_explained_var: 0.9901289939880371
        vf_loss: 4.800772587458293
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.658064516129034
    gpu_util_percent0: 0.3016129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632984904862087
    mean_env_wait_ms: 1.2025574078849026
    mean_inference_ms: 4.285660623474252
    mean_raw_obs_processing_ms: 0.37778309843074165
  time_since_restore: 9549.550334692001
  time_this_iter_s: 25.978246212005615
  time_total_s: 9549.550334692001
  timers:
    learn_throughput: 8644.52
    learn_time_ms: 18716.134
    sample_throughput: 23829.437
    sample_time_ms: 6789.585
    update_time_ms: 27.569
  timestamp: 1602797522
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:32:04,059	WARNING util.py:136 -- The `process_trial` operation took 1.0324039459228516 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    370 |          9549.55 | 59863040 |  308.827 |              338.535 |              107.323 |            774.818 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.2531796091407
    time_step_min: 2977
  date: 2020-10-15_21-32-30
  done: false
  episode_len_mean: 774.8106058648754
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.80793401003973
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 77410
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0001595570011714872
        cur_lr: 5.0e-05
        entropy: 0.11927577791114648
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011733200071224322
        total_loss: .inf
        vf_explained_var: 0.9929163455963135
        vf_loss: 3.357752561569214
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.490322580645163
    gpu_util_percent0: 0.32516129032258073
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632856872334732
    mean_env_wait_ms: 1.2025473509263878
    mean_inference_ms: 4.285584004325856
    mean_raw_obs_processing_ms: 0.3777775454276349
  time_since_restore: 9575.494240760803
  time_this_iter_s: 25.94390606880188
  time_total_s: 9575.494240760803
  timers:
    learn_throughput: 8656.808
    learn_time_ms: 18689.567
    sample_throughput: 23868.473
    sample_time_ms: 6778.481
    update_time_ms: 28.572
  timestamp: 1602797550
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:32:31,442	WARNING util.py:136 -- The `process_trial` operation took 1.0184516906738281 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    371 |          9575.49 | 60024832 |  308.808 |              338.535 |              107.323 |            774.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.3486446754187
    time_step_min: 2977
  date: 2020-10-15_21-32-57
  done: false
  episode_len_mean: 774.8017219157601
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.7936018430425
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 77588
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00023933550175723073
        cur_lr: 5.0e-05
        entropy: 0.1125436828782161
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011686059112738198
        total_loss: .inf
        vf_explained_var: 0.9920729994773865
        vf_loss: 3.327504058678945
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.01935483870968
    gpu_util_percent0: 0.31225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632769608670715
    mean_env_wait_ms: 1.2025406654778692
    mean_inference_ms: 4.285536260580057
    mean_raw_obs_processing_ms: 0.377774812175605
  time_since_restore: 9601.457788228989
  time_this_iter_s: 25.963547468185425
  time_total_s: 9601.457788228989
  timers:
    learn_throughput: 8667.062
    learn_time_ms: 18667.457
    sample_throughput: 23839.777
    sample_time_ms: 6786.641
    update_time_ms: 26.844
  timestamp: 1602797577
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:32:59,013	WARNING util.py:136 -- The `process_trial` operation took 1.0620763301849365 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    372 |          9601.46 | 60186624 |  308.794 |              338.535 |              107.323 |            774.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.3723301657515
    time_step_min: 2977
  date: 2020-10-15_21-33-25
  done: false
  episode_len_mean: 774.7873382256552
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.7983168055525
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 221
  episodes_total: 77809
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003590032526358462
        cur_lr: 5.0e-05
        entropy: 0.09789621892074744
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010156978135152409
        total_loss: .inf
        vf_explained_var: 0.9933924674987793
        vf_loss: 3.131852130095164
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.770967741935486
    gpu_util_percent0: 0.2764516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463264219858774
    mean_env_wait_ms: 1.2025341900476694
    mean_inference_ms: 4.285478044385838
    mean_raw_obs_processing_ms: 0.37777092671096646
  time_since_restore: 9627.527490854263
  time_this_iter_s: 26.069702625274658
  time_total_s: 9627.527490854263
  timers:
    learn_throughput: 8677.896
    learn_time_ms: 18644.15
    sample_throughput: 23822.663
    sample_time_ms: 6791.516
    update_time_ms: 25.258
  timestamp: 1602797605
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:33:26,476	WARNING util.py:136 -- The `process_trial` operation took 1.0249691009521484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    373 |          9627.53 | 60348416 |  308.798 |              338.535 |              107.323 |            774.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.205051605872
    time_step_min: 2977
  date: 2020-10-15_21-33-52
  done: false
  episode_len_mean: 774.7753501544139
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.82548009148206
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 228
  episodes_total: 78037
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0005385048789537693
        cur_lr: 5.0e-05
        entropy: 0.07917133718729019
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007965838332893327
        total_loss: .inf
        vf_explained_var: 0.9972869753837585
        vf_loss: 1.2111038168271382
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.041935483870965
    gpu_util_percent0: 0.29870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632509116651923
    mean_env_wait_ms: 1.202523931567522
    mean_inference_ms: 4.285404661696921
    mean_raw_obs_processing_ms: 0.3777655596894418
  time_since_restore: 9653.57752108574
  time_this_iter_s: 26.05003023147583
  time_total_s: 9653.57752108574
  timers:
    learn_throughput: 8670.091
    learn_time_ms: 18660.935
    sample_throughput: 23837.957
    sample_time_ms: 6787.159
    update_time_ms: 27.441
  timestamp: 1602797632
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:33:53,964	WARNING util.py:136 -- The `process_trial` operation took 1.0248165130615234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    374 |          9653.58 | 60510208 |  308.825 |              338.535 |              107.323 |            774.775 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3173.0142367417943
    time_step_min: 2977
  date: 2020-10-15_21-34-19
  done: false
  episode_len_mean: 774.7669138327793
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.85906947253153
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 78220
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.000807757318430654
        cur_lr: 5.0e-05
        entropy: 0.06750292920817931
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009556719984781617
        total_loss: .inf
        vf_explained_var: 0.9978017210960388
        vf_loss: 0.8180047273635864
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.287096774193554
    gpu_util_percent0: 0.32870967741935475
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632432008435955
    mean_env_wait_ms: 1.2025172628875271
    mean_inference_ms: 4.285358207168812
    mean_raw_obs_processing_ms: 0.3777628056841106
  time_since_restore: 9679.498670816422
  time_this_iter_s: 25.921149730682373
  time_total_s: 9679.498670816422
  timers:
    learn_throughput: 8675.23
    learn_time_ms: 18649.88
    sample_throughput: 23812.977
    sample_time_ms: 6794.279
    update_time_ms: 26.94
  timestamp: 1602797659
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:34:21,474	WARNING util.py:136 -- The `process_trial` operation took 1.0550975799560547 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    375 |           9679.5 | 60672000 |  308.859 |              338.535 |              107.323 |            774.767 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3172.733640329617
    time_step_min: 2977
  date: 2020-10-15_21-34-47
  done: false
  episode_len_mean: 774.7592432046509
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.8996053399517
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 216
  episodes_total: 78436
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0012116359776459808
        cur_lr: 5.0e-05
        entropy: 0.06505702404926221
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007202916810153208
        total_loss: .inf
        vf_explained_var: 0.9979670643806458
        vf_loss: 0.8530245870351791
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.267741935483873
    gpu_util_percent0: 0.3383870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632309660041204
    mean_env_wait_ms: 1.2025099640304295
    mean_inference_ms: 4.285302167349871
    mean_raw_obs_processing_ms: 0.37775892987543086
  time_since_restore: 9705.498104572296
  time_this_iter_s: 25.999433755874634
  time_total_s: 9705.498104572296
  timers:
    learn_throughput: 8674.443
    learn_time_ms: 18651.573
    sample_throughput: 23804.086
    sample_time_ms: 6796.816
    update_time_ms: 26.466
  timestamp: 1602797687
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:34:48,973	WARNING util.py:136 -- The `process_trial` operation took 1.0769457817077637 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    376 |           9705.5 | 60833792 |    308.9 |              338.535 |              107.323 |            774.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3172.429456279809
    time_step_min: 2977
  date: 2020-10-15_21-35-14
  done: false
  episode_len_mean: 774.7561239147292
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.94722197504814
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 78667
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0018174539664689712
        cur_lr: 5.0e-05
        entropy: 0.06388080057998498
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008021497167646885
        total_loss: .inf
        vf_explained_var: 0.998732328414917
        vf_loss: 0.5445319712162018
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.02258064516129
    gpu_util_percent0: 0.3906451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632184059885392
    mean_env_wait_ms: 1.2024996563753552
    mean_inference_ms: 4.285232370226509
    mean_raw_obs_processing_ms: 0.37775388216458394
  time_since_restore: 9731.31842136383
  time_this_iter_s: 25.820316791534424
  time_total_s: 9731.31842136383
  timers:
    learn_throughput: 8690.837
    learn_time_ms: 18616.39
    sample_throughput: 23827.251
    sample_time_ms: 6790.208
    update_time_ms: 25.82
  timestamp: 1602797714
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:35:16,472	WARNING util.py:136 -- The `process_trial` operation took 1.1315531730651855 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    377 |          9731.32 | 60995584 |  308.947 |              338.535 |              107.323 |            774.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3172.173971525011
    time_step_min: 2977
  date: 2020-10-15_21-35-42
  done: false
  episode_len_mean: 774.755466213474
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 308.9843685946314
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 78848
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.002726180949703457
        cur_lr: 5.0e-05
        entropy: 0.06212543789297342
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008949422820781669
        total_loss: .inf
        vf_explained_var: 0.9986775517463684
        vf_loss: 0.49997291217247647
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.903225806451612
    gpu_util_percent0: 0.37064516129032254
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463210695990728
    mean_env_wait_ms: 1.2024921079617823
    mean_inference_ms: 4.285183718586694
    mean_raw_obs_processing_ms: 0.37775094847474655
  time_since_restore: 9757.355298757553
  time_this_iter_s: 26.036877393722534
  time_total_s: 9757.355298757553
  timers:
    learn_throughput: 8685.013
    learn_time_ms: 18628.872
    sample_throughput: 23835.911
    sample_time_ms: 6787.742
    update_time_ms: 26.96
  timestamp: 1602797742
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:35:43,989	WARNING util.py:136 -- The `process_trial` operation took 1.0495209693908691 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    378 |          9757.36 | 61157376 |  308.984 |              338.535 |              107.323 |            774.755 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3171.91410926486
    time_step_min: 2977
  date: 2020-10-15_21-36-09
  done: false
  episode_len_mean: 774.7550246012572
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.02555998507233
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 213
  episodes_total: 79061
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.004089271424555187
        cur_lr: 5.0e-05
        entropy: 0.06214488670229912
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035812147155714533
        model: {}
        policy_loss: -0.009523460334700454
        total_loss: 0.5959659417470297
        vf_explained_var: 0.9985805153846741
        vf_loss: 0.6055058290561041
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53548387096774
    gpu_util_percent0: 0.29032258064516125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631991412844858
    mean_env_wait_ms: 1.2024844159401467
    mean_inference_ms: 4.285128284937183
    mean_raw_obs_processing_ms: 0.3777471697281371
  time_since_restore: 9783.36717414856
  time_this_iter_s: 26.01187539100647
  time_total_s: 9783.36717414856
  timers:
    learn_throughput: 8646.173
    learn_time_ms: 18712.557
    sample_throughput: 23851.844
    sample_time_ms: 6783.207
    update_time_ms: 35.87
  timestamp: 1602797769
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: f76cb_00000
  
2020-10-15 21:36:11,430	WARNING util.py:136 -- The `process_trial` operation took 1.067739486694336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    379 |          9783.37 | 61319168 |  309.026 |              338.535 |              107.323 |            774.755 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3171.6009312067836
    time_step_min: 2977
  date: 2020-10-15_21-36-37
  done: false
  episode_len_mean: 774.7586702650894
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.0725774913103
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 233
  episodes_total: 79294
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0020446357122775936
        cur_lr: 5.0e-05
        entropy: 0.06515240172545116
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007071050524245948
        total_loss: .inf
        vf_explained_var: 0.998799741268158
        vf_loss: 0.5302311132351557
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612907
    gpu_util_percent0: 0.2970967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631858373377957
    mean_env_wait_ms: 1.2024741271149528
    mean_inference_ms: 4.285063927688747
    mean_raw_obs_processing_ms: 0.37774243741206237
  time_since_restore: 9809.501490831375
  time_this_iter_s: 26.13431668281555
  time_total_s: 9809.501490831375
  timers:
    learn_throughput: 8642.714
    learn_time_ms: 18720.045
    sample_throughput: 23799.349
    sample_time_ms: 6798.169
    update_time_ms: 36.709
  timestamp: 1602797797
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:36:39,105	WARNING util.py:136 -- The `process_trial` operation took 1.0660672187805176 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    380 |           9809.5 | 61480960 |  309.073 |              338.535 |              107.323 |            774.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3171.3706850970616
    time_step_min: 2977
  date: 2020-10-15_21-37-05
  done: false
  episode_len_mean: 774.7600281846092
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.1050261790486
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 79476
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0030669535684163898
        cur_lr: 5.0e-05
        entropy: 0.07144106427828471
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010243866204594573
        total_loss: .inf
        vf_explained_var: 0.9973306059837341
        vf_loss: 1.031837393840154
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.521875
    gpu_util_percent0: 0.268125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631778989742186
    mean_env_wait_ms: 1.2024653175760116
    mean_inference_ms: 4.285012670770649
    mean_raw_obs_processing_ms: 0.3777392704514977
  time_since_restore: 9835.726968050003
  time_this_iter_s: 26.22547721862793
  time_total_s: 9835.726968050003
  timers:
    learn_throughput: 8633.959
    learn_time_ms: 18739.028
    sample_throughput: 23763.779
    sample_time_ms: 6808.345
    update_time_ms: 35.34
  timestamp: 1602797825
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:37:06,883	WARNING util.py:136 -- The `process_trial` operation took 1.105591058731079 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    381 |          9835.73 | 61642752 |  309.105 |              338.535 |              107.323 |             774.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3171.1359532927363
    time_step_min: 2977
  date: 2020-10-15_21-37-33
  done: false
  episode_len_mean: 774.7591828027156
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.1385240460372
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 211
  episodes_total: 79687
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.004600430352624585
        cur_lr: 5.0e-05
        entropy: 0.0712108351290226
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008377890063760182
        total_loss: .inf
        vf_explained_var: 0.9976799488067627
        vf_loss: 0.9760010043780009
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.048387096774196
    gpu_util_percent0: 0.3161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631668189849087
    mean_env_wait_ms: 1.202457576984904
    mean_inference_ms: 4.284958020405935
    mean_raw_obs_processing_ms: 0.3777356822608651
  time_since_restore: 9861.86786198616
  time_this_iter_s: 26.140893936157227
  time_total_s: 9861.86786198616
  timers:
    learn_throughput: 8631.261
    learn_time_ms: 18744.886
    sample_throughput: 23769.056
    sample_time_ms: 6806.833
    update_time_ms: 37.753
  timestamp: 1602797853
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:37:34,476	WARNING util.py:136 -- The `process_trial` operation took 1.0831801891326904 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    382 |          9861.87 | 61804544 |  309.139 |              338.535 |              107.323 |            774.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3170.8569407581754
    time_step_min: 2977
  date: 2020-10-15_21-38-00
  done: false
  episode_len_mean: 774.7633949798543
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.181468454661
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 79918
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006900645528936877
        cur_lr: 5.0e-05
        entropy: 0.06092086837937435
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008838637584024886
        total_loss: .inf
        vf_explained_var: 0.9983358979225159
        vf_loss: 0.7578457742929459
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.490322580645163
    gpu_util_percent0: 0.2951612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631545008363359
    mean_env_wait_ms: 1.2024472042319732
    mean_inference_ms: 4.284899793842026
    mean_raw_obs_processing_ms: 0.37773151095237745
  time_since_restore: 9887.987097978592
  time_this_iter_s: 26.11923599243164
  time_total_s: 9887.987097978592
  timers:
    learn_throughput: 8624.862
    learn_time_ms: 18758.792
    sample_throughput: 23768.564
    sample_time_ms: 6806.974
    update_time_ms: 37.718
  timestamp: 1602797880
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:38:02,254	WARNING util.py:136 -- The `process_trial` operation took 1.1020474433898926 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    383 |          9887.99 | 61966336 |  309.181 |              338.535 |              107.323 |            774.763 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3170.600564555413
    time_step_min: 2977
  date: 2020-10-15_21-38-28
  done: false
  episode_len_mean: 774.7689033143998
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.2190622283001
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 187
  episodes_total: 80105
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.010350968293405316
        cur_lr: 5.0e-05
        entropy: 0.054966456877688565
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038643732550553977
        model: {}
        policy_loss: -0.009228356553649064
        total_loss: 0.43269894023736316
        vf_explained_var: 0.9988662600517273
        vf_loss: 0.44191478937864304
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.522580645161288
    gpu_util_percent0: 0.3035483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631464334632865
    mean_env_wait_ms: 1.2024375333730144
    mean_inference_ms: 4.284846443994063
    mean_raw_obs_processing_ms: 0.3777279407195121
  time_since_restore: 9914.290840625763
  time_this_iter_s: 26.30374264717102
  time_total_s: 9914.290840625763
  timers:
    learn_throughput: 8623.038
    learn_time_ms: 18762.76
    sample_throughput: 23703.142
    sample_time_ms: 6825.762
    update_time_ms: 37.248
  timestamp: 1602797908
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: f76cb_00000
  
2020-10-15 21:38:30,008	WARNING util.py:136 -- The `process_trial` operation took 1.0679380893707275 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    384 |          9914.29 | 62128128 |  309.219 |              338.535 |              107.323 |            774.769 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3170.3261863530465
    time_step_min: 2977
  date: 2020-10-15_21-38-56
  done: false
  episode_len_mean: 774.7675727502522
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.2585054368914
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 204
  episodes_total: 80309
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.005175484146702658
        cur_lr: 5.0e-05
        entropy: 0.06261810412009557
        entropy_coeff: 0.0005000000000000001
        kl: 0.003869273040133218
        model: {}
        policy_loss: -0.01081286269860963
        total_loss: 0.8135245790084203
        vf_explained_var: 0.9980081915855408
        vf_loss: 0.8243487179279327
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.24193548387097
    gpu_util_percent0: 0.2929032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631356557870556
    mean_env_wait_ms: 1.2024295198599821
    mean_inference_ms: 4.284795253044611
    mean_raw_obs_processing_ms: 0.3777246688412777
  time_since_restore: 9940.610094547272
  time_this_iter_s: 26.31925392150879
  time_total_s: 9940.610094547272
  timers:
    learn_throughput: 8611.297
    learn_time_ms: 18788.343
    sample_throughput: 23661.621
    sample_time_ms: 6837.74
    update_time_ms: 38.84
  timestamp: 1602797936
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: f76cb_00000
  
2020-10-15 21:38:57,790	WARNING util.py:136 -- The `process_trial` operation took 1.0919454097747803 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    385 |          9940.61 | 62289920 |  309.259 |              338.535 |              107.323 |            774.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3170.0656497273394
    time_step_min: 2977
  date: 2020-10-15_21-39-23
  done: false
  episode_len_mean: 774.7636600658017
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.2983431684779
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 236
  episodes_total: 80545
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.002587742073351329
        cur_lr: 5.0e-05
        entropy: 0.06002697069197893
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008558526072495928
        total_loss: .inf
        vf_explained_var: 0.9974489212036133
        vf_loss: 1.1938496728738148
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.36774193548387
    gpu_util_percent0: 0.3122580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631234119991293
    mean_env_wait_ms: 1.202418765161727
    mean_inference_ms: 4.284735235380392
    mean_raw_obs_processing_ms: 0.3777203439604127
  time_since_restore: 9966.620722770691
  time_this_iter_s: 26.01062822341919
  time_total_s: 9966.620722770691
  timers:
    learn_throughput: 8610.793
    learn_time_ms: 18789.443
    sample_throughput: 23661.19
    sample_time_ms: 6837.864
    update_time_ms: 39.126
  timestamp: 1602797963
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:39:25,286	WARNING util.py:136 -- The `process_trial` operation took 1.1085424423217773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    386 |          9966.62 | 62451712 |  309.298 |              338.535 |              107.323 |            774.764 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3169.8323894265923
    time_step_min: 2977
  date: 2020-10-15_21-39-51
  done: false
  episode_len_mean: 774.7665324828141
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.33420161858885
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 190
  episodes_total: 80735
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0038816131100269935
        cur_lr: 5.0e-05
        entropy: 0.052941023372113705
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033606450112226107
        model: {}
        policy_loss: -0.007720563386101276
        total_loss: 0.7107740491628647
        vf_explained_var: 0.9981730580329895
        vf_loss: 0.7185080349445343
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.674193548387095
    gpu_util_percent0: 0.3296774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631157580702148
    mean_env_wait_ms: 1.2024086052718497
    mean_inference_ms: 4.284683541768624
    mean_raw_obs_processing_ms: 0.37771682449090066
  time_since_restore: 9992.488152265549
  time_this_iter_s: 25.867429494857788
  time_total_s: 9992.488152265549
  timers:
    learn_throughput: 8615.744
    learn_time_ms: 18778.646
    sample_throughput: 23643.714
    sample_time_ms: 6842.918
    update_time_ms: 46.332
  timestamp: 1602797991
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: f76cb_00000
  
2020-10-15 21:39:52,640	WARNING util.py:136 -- The `process_trial` operation took 1.112123727798462 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    387 |          9992.49 | 62613504 |  309.334 |              338.535 |              107.323 |            774.767 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3169.582741995302
    time_step_min: 2977
  date: 2020-10-15_21-40-18
  done: false
  episode_len_mean: 774.7694854939949
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.3708117352041
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 197
  episodes_total: 80932
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0019408065550134967
        cur_lr: 5.0e-05
        entropy: 0.05595665300885836
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009728778211865574
        total_loss: .inf
        vf_explained_var: 0.9985139966011047
        vf_loss: 0.5950236668189367
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.022580645161288
    gpu_util_percent0: 0.3038709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631050470406556
    mean_env_wait_ms: 1.2023995132306304
    mean_inference_ms: 4.284629029206858
    mean_raw_obs_processing_ms: 0.3777131715230225
  time_since_restore: 10018.422108650208
  time_this_iter_s: 25.933956384658813
  time_total_s: 10018.422108650208
  timers:
    learn_throughput: 8636.406
    learn_time_ms: 18733.718
    sample_throughput: 23561.645
    sample_time_ms: 6866.753
    update_time_ms: 45.402
  timestamp: 1602798018
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:40:20,109	WARNING util.py:136 -- The `process_trial` operation took 1.1562621593475342 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    388 |          10018.4 | 62775296 |  309.371 |              338.535 |              107.323 |            774.769 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3169.273168807475
    time_step_min: 2977
  date: 2020-10-15_21-40-46
  done: false
  episode_len_mean: 774.7707234734611
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.41641477611864
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 232
  episodes_total: 81164
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.002911209832520245
        cur_lr: 5.0e-05
        entropy: 0.05590997108568748
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033569945759760835
        model: {}
        policy_loss: -0.00865817287073393
        total_loss: 0.6501539001862208
        vf_explained_var: 0.998542308807373
        vf_loss: 0.6588302801052729
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.903225806451616
    gpu_util_percent0: 0.31419354838709684
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630917672002933
    mean_env_wait_ms: 1.2023886258792535
    mean_inference_ms: 4.28457441534229
    mean_raw_obs_processing_ms: 0.37770948493769485
  time_since_restore: 10044.482516288757
  time_this_iter_s: 26.060407638549805
  time_total_s: 10044.482516288757
  timers:
    learn_throughput: 8636.269
    learn_time_ms: 18734.015
    sample_throughput: 23553.889
    sample_time_ms: 6869.014
    update_time_ms: 36.923
  timestamp: 1602798046
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: f76cb_00000
  
2020-10-15 21:40:47,728	WARNING util.py:136 -- The `process_trial` operation took 1.1852643489837646 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    389 |          10044.5 | 62937088 |  309.416 |              338.535 |              107.323 |            774.771 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3169.046124712566
    time_step_min: 2977
  date: 2020-10-15_21-41-13
  done: false
  episode_len_mean: 774.7727647022675
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.452632389153
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 201
  episodes_total: 81365
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0014556049162601226
        cur_lr: 5.0e-05
        entropy: 0.06317108310759068
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010911734658293426
        total_loss: .inf
        vf_explained_var: 0.9983115196228027
        vf_loss: 0.6894636104504267
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.058064516129033
    gpu_util_percent0: 0.3332258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630841760572566
    mean_env_wait_ms: 1.2023779086738375
    mean_inference_ms: 4.284520780446438
    mean_raw_obs_processing_ms: 0.37770565594661576
  time_since_restore: 10070.343762159348
  time_this_iter_s: 25.86124587059021
  time_total_s: 10070.343762159348
  timers:
    learn_throughput: 8649.882
    learn_time_ms: 18704.533
    sample_throughput: 23586.634
    sample_time_ms: 6859.478
    update_time_ms: 37.394
  timestamp: 1602798073
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:41:15,117	WARNING util.py:136 -- The `process_trial` operation took 1.1413896083831787 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    390 |          10070.3 | 63098880 |  309.453 |              338.535 |              107.323 |            774.773 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3168.8049857689666
    time_step_min: 2977
  date: 2020-10-15_21-41-41
  done: false
  episode_len_mean: 774.7765039115187
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.4885386716566
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 189
  episodes_total: 81554
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.002183407374390184
        cur_lr: 5.0e-05
        entropy: 0.06718372305234273
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010055756400106475
        total_loss: .inf
        vf_explained_var: 0.9980484843254089
        vf_loss: 0.7770649343729019
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.72258064516129
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630744816415597
    mean_env_wait_ms: 1.2023688125246887
    mean_inference_ms: 4.284471542889678
    mean_raw_obs_processing_ms: 0.377702616407629
  time_since_restore: 10096.452201604843
  time_this_iter_s: 26.108439445495605
  time_total_s: 10096.452201604843
  timers:
    learn_throughput: 8651.263
    learn_time_ms: 18701.546
    sample_throughput: 23602.449
    sample_time_ms: 6854.882
    update_time_ms: 39.662
  timestamp: 1602798101
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:41:42,873	WARNING util.py:136 -- The `process_trial` operation took 1.1649224758148193 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    391 |          10096.5 | 63260672 |  309.489 |              338.535 |              107.323 |            774.777 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3168.559165698208
    time_step_min: 2977
  date: 2020-10-15_21-42-08
  done: false
  episode_len_mean: 774.7779353687016
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.52059013107834
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 233
  episodes_total: 81787
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0032751110615852758
        cur_lr: 5.0e-05
        entropy: 0.0769198772807916
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01253956566991595
        total_loss: .inf
        vf_explained_var: 0.9971099495887756
        vf_loss: 1.3489553332328796
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.258064516129036
    gpu_util_percent0: 0.3009677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630621370196645
    mean_env_wait_ms: 1.2023577293425822
    mean_inference_ms: 4.284418693779972
    mean_raw_obs_processing_ms: 0.37769872947637506
  time_since_restore: 10122.354127168655
  time_this_iter_s: 25.901925563812256
  time_total_s: 10122.354127168655
  timers:
    learn_throughput: 8654.26
    learn_time_ms: 18695.071
    sample_throughput: 23618.49
    sample_time_ms: 6850.226
    update_time_ms: 37.691
  timestamp: 1602798128
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:42:10,389	WARNING util.py:136 -- The `process_trial` operation took 1.133836269378662 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    392 |          10122.4 | 63422464 |  309.521 |              338.535 |              107.323 |            774.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3168.375373689798
    time_step_min: 2977
  date: 2020-10-15_21-42-36
  done: false
  episode_len_mean: 774.7829867674858
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.54700243486144
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 208
  episodes_total: 81995
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.004912666592377914
        cur_lr: 5.0e-05
        entropy: 0.06934275788565476
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012028875712227697
        total_loss: .inf
        vf_explained_var: 0.9967061877250671
        vf_loss: 1.465241809686025
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.29375
    gpu_util_percent0: 0.30937499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630542220900086
    mean_env_wait_ms: 1.2023459242615084
    mean_inference_ms: 4.284363122696269
    mean_raw_obs_processing_ms: 0.3776947562244211
  time_since_restore: 10148.209266901016
  time_this_iter_s: 25.85513973236084
  time_total_s: 10148.209266901016
  timers:
    learn_throughput: 8661.299
    learn_time_ms: 18679.877
    sample_throughput: 23660.568
    sample_time_ms: 6838.044
    update_time_ms: 37.816
  timestamp: 1602798156
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:42:37,933	WARNING util.py:136 -- The `process_trial` operation took 1.1141786575317383 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    393 |          10148.2 | 63584256 |  309.547 |              338.535 |              107.323 |            774.783 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3168.2073583481456
    time_step_min: 2977
  date: 2020-10-15_21-43-04
  done: false
  episode_len_mean: 774.7880871258213
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.5739605841821
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 185
  episodes_total: 82180
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.007368999888566871
        cur_lr: 5.0e-05
        entropy: 0.06428004149347544
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010199971271504182
        total_loss: .inf
        vf_explained_var: 0.9969877600669861
        vf_loss: 1.2737026313940685
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.296774193548387
    gpu_util_percent0: 0.34838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630456280032061
    mean_env_wait_ms: 1.2023364505196443
    mean_inference_ms: 4.284318872616773
    mean_raw_obs_processing_ms: 0.377692060765298
  time_since_restore: 10174.269379615784
  time_this_iter_s: 26.060112714767456
  time_total_s: 10174.269379615784
  timers:
    learn_throughput: 8669.337
    learn_time_ms: 18662.558
    sample_throughput: 23695.687
    sample_time_ms: 6827.909
    update_time_ms: 38.353
  timestamp: 1602798184
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:43:05,624	WARNING util.py:136 -- The `process_trial` operation took 1.146331548690796 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    394 |          10174.3 | 63746048 |  309.574 |              338.535 |              107.323 |            774.788 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3167.9476380677197
    time_step_min: 2977
  date: 2020-10-15_21-43-31
  done: false
  episode_len_mean: 774.7935833808593
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.61640405707345
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 231
  episodes_total: 82411
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.011053499832850306
        cur_lr: 5.0e-05
        entropy: 0.06015354891618093
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010217267139523756
        total_loss: .inf
        vf_explained_var: 0.9985019564628601
        vf_loss: 0.6551031321287155
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.71612903225806
    gpu_util_percent0: 0.3519354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630331318594567
    mean_env_wait_ms: 1.202325679675851
    mean_inference_ms: 4.284263939755699
    mean_raw_obs_processing_ms: 0.37768795151449713
  time_since_restore: 10200.504055261612
  time_this_iter_s: 26.234675645828247
  time_total_s: 10200.504055261612
  timers:
    learn_throughput: 8683.207
    learn_time_ms: 18632.748
    sample_throughput: 23623.978
    sample_time_ms: 6848.635
    update_time_ms: 36.767
  timestamp: 1602798211
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:43:33,465	WARNING util.py:136 -- The `process_trial` operation took 1.1047563552856445 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    395 |          10200.5 | 63907840 |  309.616 |              338.535 |              107.323 |            774.794 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3167.678481043314
    time_step_min: 2977
  date: 2020-10-15_21-43-59
  done: false
  episode_len_mean: 774.8009924357035
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.65823805376004
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 214
  episodes_total: 82625
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.016580249749275463
        cur_lr: 5.0e-05
        entropy: 0.055398261485000454
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007200796928373165
        total_loss: .inf
        vf_explained_var: 0.9986658096313477
        vf_loss: 0.5564729819695154
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.10322580645162
    gpu_util_percent0: 0.3067741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630243339759452
    mean_env_wait_ms: 1.2023128412640445
    mean_inference_ms: 4.284208310919292
    mean_raw_obs_processing_ms: 0.3776840981237391
  time_since_restore: 10226.575345039368
  time_this_iter_s: 26.071289777755737
  time_total_s: 10226.575345039368
  timers:
    learn_throughput: 8690.209
    learn_time_ms: 18617.734
    sample_throughput: 23532.305
    sample_time_ms: 6875.315
    update_time_ms: 38.808
  timestamp: 1602798239
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:44:01,017	WARNING util.py:136 -- The `process_trial` operation took 1.0939264297485352 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    396 |          10226.6 | 64069632 |  309.658 |              338.535 |              107.323 |            774.801 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3167.4461064459615
    time_step_min: 2977
  date: 2020-10-15_21-44-27
  done: false
  episode_len_mean: 774.8060671199295
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.6934425711582
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 182
  episodes_total: 82807
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.02487037462391319
        cur_lr: 5.0e-05
        entropy: 0.05550685245543718
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009247785957995802
        total_loss: .inf
        vf_explained_var: 0.9986634254455566
        vf_loss: 0.5085144390662512
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.035483870967745
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630158567350052
    mean_env_wait_ms: 1.202302965318427
    mean_inference_ms: 4.284165394836214
    mean_raw_obs_processing_ms: 0.37768143541317917
  time_since_restore: 10252.795334815979
  time_this_iter_s: 26.219989776611328
  time_total_s: 10252.795334815979
  timers:
    learn_throughput: 8670.4
    learn_time_ms: 18660.269
    sample_throughput: 23531.144
    sample_time_ms: 6875.654
    update_time_ms: 32.423
  timestamp: 1602798267
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:44:28,889	WARNING util.py:136 -- The `process_trial` operation took 1.1686522960662842 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    397 |          10252.8 | 64231424 |  309.693 |              338.535 |              107.323 |            774.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3167.1887381308143
    time_step_min: 2977
  date: 2020-10-15_21-44-55
  done: false
  episode_len_mean: 774.8123449355654
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.73413710755636
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 223
  episodes_total: 83030
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.03730556193586978
        cur_lr: 5.0e-05
        entropy: 0.06033893736700217
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0076461573141083745
        total_loss: .inf
        vf_explained_var: 0.9987063407897949
        vf_loss: 0.5984309663375219
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.7
    gpu_util_percent0: 0.28625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8812499999999996
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630057521141668
    mean_env_wait_ms: 1.2022924894763922
    mean_inference_ms: 4.284115715610161
    mean_raw_obs_processing_ms: 0.37767770068703993
  time_since_restore: 10279.04100060463
  time_this_iter_s: 26.245665788650513
  time_total_s: 10279.04100060463
  timers:
    learn_throughput: 8644.946
    learn_time_ms: 18715.213
    sample_throughput: 23585.884
    sample_time_ms: 6859.696
    update_time_ms: 32.139
  timestamp: 1602798295
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:44:56,796	WARNING util.py:136 -- The `process_trial` operation took 1.0984580516815186 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    398 |            10279 | 64393216 |  309.734 |              338.535 |              107.323 |            774.812 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3166.9287971831322
    time_step_min: 2977
  date: 2020-10-15_21-45-22
  done: false
  episode_len_mean: 774.8185454327067
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.77332268089583
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 225
  episodes_total: 83255
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.055958342903804664
        cur_lr: 5.0e-05
        entropy: 0.06018572207540274
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008674757632737359
        total_loss: .inf
        vf_explained_var: 0.9983170032501221
        vf_loss: 0.7325330724318823
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73225806451613
    gpu_util_percent0: 0.3032258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462994680609533
    mean_env_wait_ms: 1.2022782256287305
    mean_inference_ms: 4.284056930933522
    mean_raw_obs_processing_ms: 0.377673696134204
  time_since_restore: 10305.164138317108
  time_this_iter_s: 26.123137712478638
  time_total_s: 10305.164138317108
  timers:
    learn_throughput: 8639.043
    learn_time_ms: 18728.0
    sample_throughput: 23589.212
    sample_time_ms: 6858.729
    update_time_ms: 35.45
  timestamp: 1602798322
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:45:24,407	WARNING util.py:136 -- The `process_trial` operation took 1.101233959197998 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    399 |          10305.2 | 64555008 |  309.773 |              338.535 |              107.323 |            774.819 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3166.7173196148265
    time_step_min: 2977
  date: 2020-10-15_21-45-50
  done: false
  episode_len_mean: 774.8218930159529
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.80259609507027
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 178
  episodes_total: 83433
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.08393751435570702
        cur_lr: 5.0e-05
        entropy: 0.05903715050468842
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007694882374683705
        total_loss: .inf
        vf_explained_var: 0.9982709288597107
        vf_loss: 0.662033682068189
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.31935483870968
    gpu_util_percent0: 0.3254838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629871828310265
    mean_env_wait_ms: 1.202268516019349
    mean_inference_ms: 4.284016083300731
    mean_raw_obs_processing_ms: 0.3776710391087968
  time_since_restore: 10331.239981174469
  time_this_iter_s: 26.07584285736084
  time_total_s: 10331.239981174469
  timers:
    learn_throughput: 8619.688
    learn_time_ms: 18770.052
    sample_throughput: 23639.515
    sample_time_ms: 6844.134
    update_time_ms: 36.761
  timestamp: 1602798350
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:45:52,001	WARNING util.py:136 -- The `process_trial` operation took 1.1311163902282715 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    400 |          10331.2 | 64716800 |  309.803 |              338.535 |              107.323 |            774.822 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3166.488775131864
    time_step_min: 2977
  date: 2020-10-15_21-46-17
  done: false
  episode_len_mean: 774.8306655031022
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.83986618766846
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 218
  episodes_total: 83651
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.12590627153356054
        cur_lr: 5.0e-05
        entropy: 0.06029406158874432
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00995855056802005
        total_loss: .inf
        vf_explained_var: 0.9985665678977966
        vf_loss: 0.6436113168795904
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55483870967742
    gpu_util_percent0: 0.34774193548387095
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629770936312514
    mean_env_wait_ms: 1.2022572480336433
    mean_inference_ms: 4.283969582078556
    mean_raw_obs_processing_ms: 0.377667824955447
  time_since_restore: 10357.211733579636
  time_this_iter_s: 25.971752405166626
  time_total_s: 10357.211733579636
  timers:
    learn_throughput: 8625.346
    learn_time_ms: 18757.74
    sample_throughput: 23641.4
    sample_time_ms: 6843.588
    update_time_ms: 34.784
  timestamp: 1602798377
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:46:19,577	WARNING util.py:136 -- The `process_trial` operation took 1.1189055442810059 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    401 |          10357.2 | 64878592 |   309.84 |              338.535 |              107.323 |            774.831 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3166.214146836047
    time_step_min: 2977
  date: 2020-10-15_21-46-45
  done: false
  episode_len_mean: 774.8429009144343
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.8826040728469
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 226
  episodes_total: 83877
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.18885940730034081
        cur_lr: 5.0e-05
        entropy: 0.05619690474122763
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008445362184526553
        total_loss: .inf
        vf_explained_var: 0.999164342880249
        vf_loss: 0.3877246826887131
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.36451612903226
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629642133008064
    mean_env_wait_ms: 1.2022434317232547
    mean_inference_ms: 4.283911503659242
    mean_raw_obs_processing_ms: 0.37766354826185317
  time_since_restore: 10383.066567897797
  time_this_iter_s: 25.85483431816101
  time_total_s: 10383.066567897797
  timers:
    learn_throughput: 8631.939
    learn_time_ms: 18743.413
    sample_throughput: 23618.222
    sample_time_ms: 6850.304
    update_time_ms: 34.81
  timestamp: 1602798405
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:46:46,940	WARNING util.py:136 -- The `process_trial` operation took 1.116222620010376 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    402 |          10383.1 | 65040384 |  309.883 |              338.535 |              107.323 |            774.843 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3165.970089742674
    time_step_min: 2977
  date: 2020-10-15_21-47-13
  done: false
  episode_len_mean: 774.8515940994528
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.91806357652183
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 183
  episodes_total: 84060
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.28328911095051124
        cur_lr: 5.0e-05
        entropy: 0.05374131506929795
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007015028128080303
        total_loss: .inf
        vf_explained_var: 0.9988930225372314
        vf_loss: 0.42181497315565747
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.412903225806453
    gpu_util_percent0: 0.2864516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629589215878086
    mean_env_wait_ms: 1.2022323070809908
    mean_inference_ms: 4.283867692054796
    mean_raw_obs_processing_ms: 0.3776608149813855
  time_since_restore: 10409.27265906334
  time_this_iter_s: 26.206091165542603
  time_total_s: 10409.27265906334
  timers:
    learn_throughput: 8622.361
    learn_time_ms: 18764.234
    sample_throughput: 23578.519
    sample_time_ms: 6861.839
    update_time_ms: 36.001
  timestamp: 1602798433
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:47:14,774	WARNING util.py:136 -- The `process_trial` operation took 1.141495704650879 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    403 |          10409.3 | 65202176 |  309.918 |              338.535 |              107.323 |            774.852 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3165.72242009783
    time_step_min: 2977
  date: 2020-10-15_21-47-40
  done: false
  episode_len_mean: 774.8635576124362
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.9582456821688
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 210
  episodes_total: 84270
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4249336664257668
        cur_lr: 5.0e-05
        entropy: 0.05608158931136131
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007222731642589982
        total_loss: .inf
        vf_explained_var: 0.9989355206489563
        vf_loss: 0.4380682681997617
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.467741935483872
    gpu_util_percent0: 0.29806451612903223
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629492954469708
    mean_env_wait_ms: 1.2022207137249035
    mean_inference_ms: 4.28382353538345
    mean_raw_obs_processing_ms: 0.3776576656780743
  time_since_restore: 10435.107688903809
  time_this_iter_s: 25.83502984046936
  time_total_s: 10435.107688903809
  timers:
    learn_throughput: 8631.622
    learn_time_ms: 18744.102
    sample_throughput: 23574.201
    sample_time_ms: 6863.096
    update_time_ms: 33.691
  timestamp: 1602798460
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:47:42,140	WARNING util.py:136 -- The `process_trial` operation took 1.1307106018066406 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    404 |          10435.1 | 65363968 |  309.958 |              338.535 |              107.323 |            774.864 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3165.476668048073
    time_step_min: 2977
  date: 2020-10-15_21-48-08
  done: false
  episode_len_mean: 774.8721374723362
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 309.9995331852676
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 84497
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.6374004996386501
        cur_lr: 5.0e-05
        entropy: 0.05610069756706556
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008330100031647211
        total_loss: .inf
        vf_explained_var: 0.9981128573417664
        vf_loss: 0.9287776152292887
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354845
    gpu_util_percent0: 0.26999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462937967064071
    mean_env_wait_ms: 1.2022075175592382
    mean_inference_ms: 4.283770295360786
    mean_raw_obs_processing_ms: 0.37765395084713516
  time_since_restore: 10461.05359864235
  time_this_iter_s: 25.94590973854065
  time_total_s: 10461.05359864235
  timers:
    learn_throughput: 8632.071
    learn_time_ms: 18743.127
    sample_throughput: 23706.154
    sample_time_ms: 6824.894
    update_time_ms: 34.144
  timestamp: 1602798488
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:48:09,657	WARNING util.py:136 -- The `process_trial` operation took 1.1361446380615234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    405 |          10461.1 | 65525760 |      310 |              338.535 |              107.323 |            774.872 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3165.2631902275357
    time_step_min: 2977
  date: 2020-10-15_21-48-35
  done: false
  episode_len_mean: 774.8800892688456
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.0349452631358
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 191
  episodes_total: 84688
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.9561007494579754
        cur_lr: 5.0e-05
        entropy: 0.05517150554805994
        entropy_coeff: 0.0005000000000000001
        kl: 0.0009356134348005677
        model: {}
        policy_loss: -0.007653584160531561
        total_loss: 0.5369318351149559
        vf_explained_var: 0.9986253380775452
        vf_loss: 0.5437184820572535
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53548387096774
    gpu_util_percent0: 0.314516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629304622113765
    mean_env_wait_ms: 1.2021944039826156
    mean_inference_ms: 4.283720345682464
    mean_raw_obs_processing_ms: 0.3776507205367847
  time_since_restore: 10487.2770113945
  time_this_iter_s: 26.22341275215149
  time_total_s: 10487.2770113945
  timers:
    learn_throughput: 8614.959
    learn_time_ms: 18780.357
    sample_throughput: 23784.037
    sample_time_ms: 6802.546
    update_time_ms: 31.96
  timestamp: 1602798515
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: f76cb_00000
  
2020-10-15 21:48:37,551	WARNING util.py:136 -- The `process_trial` operation took 1.1713318824768066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    406 |          10487.3 | 65687552 |  310.035 |              338.535 |              107.323 |             774.88 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3165.0032764859097
    time_step_min: 2977
  date: 2020-10-15_21-49-03
  done: false
  episode_len_mean: 774.8903391487707
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.0738474759255
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 201
  episodes_total: 84889
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.4780503747289877
        cur_lr: 5.0e-05
        entropy: 0.05588153284043074
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007001535734161735
        total_loss: .inf
        vf_explained_var: 0.9991037249565125
        vf_loss: 0.37843306610981625
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.461290322580645
    gpu_util_percent0: 0.3483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629212825216745
    mean_env_wait_ms: 1.2021823676398151
    mean_inference_ms: 4.2836755113567655
    mean_raw_obs_processing_ms: 0.3776477445553924
  time_since_restore: 10513.135402917862
  time_this_iter_s: 25.858391523361206
  time_total_s: 10513.135402917862
  timers:
    learn_throughput: 8633.85
    learn_time_ms: 18739.265
    sample_throughput: 23769.095
    sample_time_ms: 6806.822
    update_time_ms: 30.904
  timestamp: 1602798543
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:49:05,047	WARNING util.py:136 -- The `process_trial` operation took 1.1372621059417725 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    407 |          10513.1 | 65849344 |  310.074 |              338.535 |              107.323 |             774.89 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3164.7208547852456
    time_step_min: 2977
  date: 2020-10-15_21-49-30
  done: false
  episode_len_mean: 774.9033671695098
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.11845687952416
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 227
  episodes_total: 85116
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.7170755620934814
        cur_lr: 5.0e-05
        entropy: 0.054293304992218815
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006947438543041547
        total_loss: .inf
        vf_explained_var: 0.9990599751472473
        vf_loss: 0.4141966054836909
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.422580645161293
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629119538954322
    mean_env_wait_ms: 1.2021690298118672
    mean_inference_ms: 4.283629542701756
    mean_raw_obs_processing_ms: 0.3776443618785538
  time_since_restore: 10539.02957701683
  time_this_iter_s: 25.894174098968506
  time_total_s: 10539.02957701683
  timers:
    learn_throughput: 8656.42
    learn_time_ms: 18690.404
    sample_throughput: 23723.333
    sample_time_ms: 6819.952
    update_time_ms: 32.48
  timestamp: 1602798570
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:49:32,475	WARNING util.py:136 -- The `process_trial` operation took 1.1159298419952393 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    408 |            10539 | 66011136 |  310.118 |              338.535 |              107.323 |            774.903 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3164.4616647512726
    time_step_min: 2977
  date: 2020-10-15_21-49-58
  done: false
  episode_len_mean: 774.9126424117399
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.1544039958875
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 200
  episodes_total: 85316
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0756133431402224
        cur_lr: 5.0e-05
        entropy: 0.05476959173878034
        entropy_coeff: 0.0005000000000000001
        kl: 0.0006648259877692908
        model: {}
        policy_loss: -0.004563175059350518
        total_loss: 0.932056133945783
        vf_explained_var: 0.9977715015411377
        vf_loss: 0.9359315931797028
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.461290322580652
    gpu_util_percent0: 0.3003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629038909377792
    mean_env_wait_ms: 1.2021556770311457
    mean_inference_ms: 4.283579858829171
    mean_raw_obs_processing_ms: 0.37764082638095064
  time_since_restore: 10564.865290164948
  time_this_iter_s: 25.835713148117065
  time_total_s: 10564.865290164948
  timers:
    learn_throughput: 8678.183
    learn_time_ms: 18643.534
    sample_throughput: 23690.453
    sample_time_ms: 6829.418
    update_time_ms: 30.616
  timestamp: 1602798598
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: f76cb_00000
  
2020-10-15 21:49:59,903	WARNING util.py:136 -- The `process_trial` operation took 1.1922581195831299 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    409 |          10564.9 | 66172928 |  310.154 |              338.535 |              107.323 |            774.913 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3164.217267685395
    time_step_min: 2977
  date: 2020-10-15_21-50-26
  done: false
  episode_len_mean: 774.9209781540908
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.1894854896914
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 192
  episodes_total: 85508
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.5378066715701112
        cur_lr: 5.0e-05
        entropy: 0.0560455567513903
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007575985937971079
        total_loss: .inf
        vf_explained_var: 0.9989312291145325
        vf_loss: 0.4188356747229894
    num_steps_sampled: 66334720
    num_steps_trained: 66334720
  iterations_since_restore: 410
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.564516129032263
    gpu_util_percent0: 0.3035483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628952638493134
    mean_env_wait_ms: 1.202143034017715
    mean_inference_ms: 4.283535193515738
    mean_raw_obs_processing_ms: 0.37763799159000566
  time_since_restore: 10590.981765270233
  time_this_iter_s: 26.116475105285645
  time_total_s: 10590.981765270233
  timers:
    learn_throughput: 8680.241
    learn_time_ms: 18639.114
    sample_throughput: 23645.348
    sample_time_ms: 6842.445
    update_time_ms: 27.231
  timestamp: 1602798626
  timesteps_since_restore: 0
  timesteps_total: 66334720
  training_iteration: 410
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:50:27,638	WARNING util.py:136 -- The `process_trial` operation took 1.1267685890197754 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    410 |            10591 | 66334720 |  310.189 |              338.535 |              107.323 |            774.921 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3163.944769884242
    time_step_min: 2977
  date: 2020-10-15_21-50-53
  done: false
  episode_len_mean: 774.9309057827335
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.2317478359608
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 230
  episodes_total: 85738
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.8067100073551665
        cur_lr: 5.0e-05
        entropy: 0.05476503477742275
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008834070235025138
        total_loss: .inf
        vf_explained_var: 0.999023973941803
        vf_loss: 0.45602427671353024
    num_steps_sampled: 66496512
    num_steps_trained: 66496512
  iterations_since_restore: 411
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.128124999999997
    gpu_util_percent0: 0.33625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628843345176024
    mean_env_wait_ms: 1.2021291791365651
    mean_inference_ms: 4.283491683207625
    mean_raw_obs_processing_ms: 0.3776347331485384
  time_since_restore: 10617.108139276505
  time_this_iter_s: 26.126374006271362
  time_total_s: 10617.108139276505
  timers:
    learn_throughput: 8673.245
    learn_time_ms: 18654.149
    sample_throughput: 23639.812
    sample_time_ms: 6844.048
    update_time_ms: 26.976
  timestamp: 1602798653
  timesteps_since_restore: 0
  timesteps_total: 66496512
  training_iteration: 411
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:50:55,476	WARNING util.py:136 -- The `process_trial` operation took 1.1408276557922363 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    411 |          10617.1 | 66496512 |  310.232 |              338.535 |              107.323 |            774.931 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3163.6729643210524
    time_step_min: 2977
  date: 2020-10-15_21-51-21
  done: false
  episode_len_mean: 774.9429881205859
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.27107614946635
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 209
  episodes_total: 85947
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.21006501103275
        cur_lr: 5.0e-05
        entropy: 0.05254031872997681
        entropy_coeff: 0.0005000000000000001
        kl: 0.0006787303670231873
        model: {}
        policy_loss: -0.006713012553518638
        total_loss: 0.3000497842828433
        vf_explained_var: 0.9992459416389465
        vf_loss: 0.3059677680333455
    num_steps_sampled: 66658304
    num_steps_trained: 66658304
  iterations_since_restore: 412
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.50967741935484
    gpu_util_percent0: 0.33258064516129027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628768626174082
    mean_env_wait_ms: 1.202115257414567
    mean_inference_ms: 4.2834397740819625
    mean_raw_obs_processing_ms: 0.37763105070961067
  time_since_restore: 10643.259969234467
  time_this_iter_s: 26.151829957962036
  time_total_s: 10643.259969234467
  timers:
    learn_throughput: 8656.633
    learn_time_ms: 18689.946
    sample_throughput: 23689.219
    sample_time_ms: 6829.773
    update_time_ms: 35.205
  timestamp: 1602798681
  timesteps_since_restore: 0
  timesteps_total: 66658304
  training_iteration: 412
  trial_id: f76cb_00000
  
2020-10-15 21:51:23,233	WARNING util.py:136 -- The `process_trial` operation took 1.1635122299194336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    412 |          10643.3 | 66658304 |  310.271 |              338.535 |              107.323 |            774.943 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3163.439362962619
    time_step_min: 2977
  date: 2020-10-15_21-51-49
  done: false
  episode_len_mean: 774.953511053316
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.30596345209483
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 181
  episodes_total: 86128
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.605032505516375
        cur_lr: 5.0e-05
        entropy: 0.05217980934927861
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006074655839862923
        total_loss: .inf
        vf_explained_var: 0.9993636012077332
        vf_loss: 0.2647644467651844
    num_steps_sampled: 66820096
    num_steps_trained: 66820096
  iterations_since_restore: 413
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.280645161290327
    gpu_util_percent0: 0.3632258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628689152924115
    mean_env_wait_ms: 1.2021028678849268
    mean_inference_ms: 4.283399839143129
    mean_raw_obs_processing_ms: 0.3776284021525714
  time_since_restore: 10669.375406742096
  time_this_iter_s: 26.115437507629395
  time_total_s: 10669.375406742096
  timers:
    learn_throughput: 8672.433
    learn_time_ms: 18655.896
    sample_throughput: 23601.516
    sample_time_ms: 6855.153
    update_time_ms: 33.869
  timestamp: 1602798709
  timesteps_since_restore: 0
  timesteps_total: 66820096
  training_iteration: 413
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:51:51,000	WARNING util.py:136 -- The `process_trial` operation took 1.1891393661499023 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    413 |          10669.4 | 66820096 |  310.306 |              338.535 |              107.323 |            774.954 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3163.15104504588
    time_step_min: 2977
  date: 2020-10-15_21-52-16
  done: false
  episode_len_mean: 774.9682006623897
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.3496980832715
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 226
  episodes_total: 86354
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.9075487582745622
        cur_lr: 5.0e-05
        entropy: 0.053671265641848244
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005160168152845775
        total_loss: .inf
        vf_explained_var: 0.9992150664329529
        vf_loss: 0.34783601264158887
    num_steps_sampled: 66981888
    num_steps_trained: 66981888
  iterations_since_restore: 414
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.719354838709684
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628587087941702
    mean_env_wait_ms: 1.202089623131454
    mean_inference_ms: 4.283357238381402
    mean_raw_obs_processing_ms: 0.3776253236504288
  time_since_restore: 10695.137958049774
  time_this_iter_s: 25.762551307678223
  time_total_s: 10695.137958049774
  timers:
    learn_throughput: 8675.563
    learn_time_ms: 18649.164
    sample_throughput: 23576.179
    sample_time_ms: 6862.52
    update_time_ms: 34.139
  timestamp: 1602798736
  timesteps_since_restore: 0
  timesteps_total: 66981888
  training_iteration: 414
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:52:18,391	WARNING util.py:136 -- The `process_trial` operation took 1.157557725906372 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    414 |          10695.1 | 66981888 |   310.35 |              338.535 |              107.323 |            774.968 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3162.8817764731716
    time_step_min: 2977
  date: 2020-10-15_21-52-44
  done: false
  episode_len_mean: 774.9807330229979
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.3905036293887
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 219
  episodes_total: 86573
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.361323137411844
        cur_lr: 5.0e-05
        entropy: 0.05278081074357033
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006845470517873764
        total_loss: .inf
        vf_explained_var: 0.9992368221282959
        vf_loss: 0.33657270421584445
    num_steps_sampled: 67143680
    num_steps_trained: 67143680
  iterations_since_restore: 415
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.332258064516132
    gpu_util_percent0: 0.3367741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628497500564558
    mean_env_wait_ms: 1.2020737834921205
    mean_inference_ms: 4.283303228601426
    mean_raw_obs_processing_ms: 0.3776213190377746
  time_since_restore: 10721.209134101868
  time_this_iter_s: 26.071176052093506
  time_total_s: 10721.209134101868
  timers:
    learn_throughput: 8672.271
    learn_time_ms: 18656.244
    sample_throughput: 23532.384
    sample_time_ms: 6875.292
    update_time_ms: 35.904
  timestamp: 1602798764
  timesteps_since_restore: 0
  timesteps_total: 67143680
  training_iteration: 415
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:52:46,137	WARNING util.py:136 -- The `process_trial` operation took 1.1727824211120605 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    415 |          10721.2 | 67143680 |  310.391 |              338.535 |              107.323 |            774.981 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3162.658255584643
    time_step_min: 2977
  date: 2020-10-15_21-53-12
  done: false
  episode_len_mean: 774.9933028252625
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.42502707384614
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 180
  episodes_total: 86753
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.041984706117766
        cur_lr: 5.0e-05
        entropy: 0.05278152786195278
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004165542855237921
        total_loss: .inf
        vf_explained_var: 0.9994731545448303
        vf_loss: 0.20886039485534033
    num_steps_sampled: 67305472
    num_steps_trained: 67305472
  iterations_since_restore: 416
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.46129032258065
    gpu_util_percent0: 0.2806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462843072391203
    mean_env_wait_ms: 1.2020612968797069
    mean_inference_ms: 4.2832639392246525
    mean_raw_obs_processing_ms: 0.37761883216189984
  time_since_restore: 10747.188124418259
  time_this_iter_s: 25.97899031639099
  time_total_s: 10747.188124418259
  timers:
    learn_throughput: 8683.892
    learn_time_ms: 18631.277
    sample_throughput: 23532.812
    sample_time_ms: 6875.166
    update_time_ms: 35.921
  timestamp: 1602798792
  timesteps_since_restore: 0
  timesteps_total: 67305472
  training_iteration: 416
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:53:13,739	WARNING util.py:136 -- The `process_trial` operation took 1.1289432048797607 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    416 |          10747.2 | 67305472 |  310.425 |              338.535 |              107.323 |            774.993 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3162.394800414126
    time_step_min: 2977
  date: 2020-10-15_21-53-39
  done: false
  episode_len_mean: 775.0059904337028
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.462917474427
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 219
  episodes_total: 86972
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.062977059176648
        cur_lr: 5.0e-05
        entropy: 0.05673113899926344
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006877473148051649
        total_loss: .inf
        vf_explained_var: 0.998654842376709
        vf_loss: 0.6090040157238642
    num_steps_sampled: 67467264
    num_steps_trained: 67467264
  iterations_since_restore: 417
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.758064516129036
    gpu_util_percent0: 0.3416129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628329199642606
    mean_env_wait_ms: 1.2020481393971076
    mean_inference_ms: 4.283223174732196
    mean_raw_obs_processing_ms: 0.37761568297260406
  time_since_restore: 10773.093995809555
  time_this_iter_s: 25.905871391296387
  time_total_s: 10773.093995809555
  timers:
    learn_throughput: 8677.343
    learn_time_ms: 18645.339
    sample_throughput: 23562.113
    sample_time_ms: 6866.617
    update_time_ms: 34.724
  timestamp: 1602798819
  timesteps_since_restore: 0
  timesteps_total: 67467264
  training_iteration: 417
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:53:41,308	WARNING util.py:136 -- The `process_trial` operation took 1.1524286270141602 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    417 |          10773.1 | 67467264 |  310.463 |              338.535 |              107.323 |            775.006 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3162.135405551157
    time_step_min: 2977
  date: 2020-10-15_21-54-07
  done: false
  episode_len_mean: 775.0206892597053
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.5029479380073
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 223
  episodes_total: 87195
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.594465588764973
        cur_lr: 5.0e-05
        entropy: 0.052482371839384236
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004912568588527695
        total_loss: .inf
        vf_explained_var: 0.9990894198417664
        vf_loss: 0.4024953693151474
    num_steps_sampled: 67629056
    num_steps_trained: 67629056
  iterations_since_restore: 418
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.2741935483871
    gpu_util_percent0: 0.32903225806451625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628222859833329
    mean_env_wait_ms: 1.2020315906269479
    mean_inference_ms: 4.283169918241802
    mean_raw_obs_processing_ms: 0.37761202316499504
  time_since_restore: 10799.080777168274
  time_this_iter_s: 25.986781358718872
  time_total_s: 10799.080777168274
  timers:
    learn_throughput: 8668.758
    learn_time_ms: 18663.804
    sample_throughput: 23594.951
    sample_time_ms: 6857.06
    update_time_ms: 34.473
  timestamp: 1602798847
  timesteps_since_restore: 0
  timesteps_total: 67629056
  training_iteration: 418
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:54:08,949	WARNING util.py:136 -- The `process_trial` operation took 1.1482396125793457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | RUNNING  | 172.17.0.4:38736 |    418 |          10799.1 | 67629056 |  310.503 |              338.535 |              107.323 |            775.021 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_f76cb_00000:
  custom_metrics:
    time_step_max: 4503
    time_step_mean: 3161.907909501019
    time_step_min: 2977
  date: 2020-10-15_21-54-35
  done: true
  episode_len_mean: 775.0345044632639
  episode_reward_max: 338.5353535353541
  episode_reward_mean: 310.5390885277588
  episode_reward_min: 107.32323232323165
  episodes_this_iter: 185
  episodes_total: 87380
  experiment_id: a4c1e5496be74ef0b37a0b7e65714b40
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.891698383147458
        cur_lr: 5.0e-05
        entropy: 0.05092853338768085
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005785435942622523
        total_loss: .inf
        vf_explained_var: 0.9993137717247009
        vf_loss: 0.2652246604363124
    num_steps_sampled: 67790848
    num_steps_trained: 67790848
  iterations_since_restore: 419
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.049999999999997
    gpu_util_percent0: 0.30874999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 38736
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628175865971188
    mean_env_wait_ms: 1.2020179893632075
    mean_inference_ms: 4.283129928106696
    mean_raw_obs_processing_ms: 0.37760933997301477
  time_since_restore: 10825.333883047104
  time_this_iter_s: 26.253105878829956
  time_total_s: 10825.333883047104
  timers:
    learn_throughput: 8647.406
    learn_time_ms: 18709.887
    sample_throughput: 23615.496
    sample_time_ms: 6851.095
    update_time_ms: 34.814
  timestamp: 1602798875
  timesteps_since_restore: 0
  timesteps_total: 67790848
  training_iteration: 419
  trial_id: f76cb_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 21:54:36,947	WARNING util.py:136 -- The `process_trial` operation took 1.3202812671661377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | TERMINATED |       |    419 |          10825.3 | 67790848 |  310.539 |              338.535 |              107.323 |            775.035 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.42 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_f76cb_00000 | TERMINATED |       |    419 |          10825.3 | 67790848 |  310.539 |              338.535 |              107.323 |            775.035 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


