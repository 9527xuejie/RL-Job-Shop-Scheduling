2020-10-11 15:54:12,150	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_0254c_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=10986)[0m 2020-10-11 15:54:14,974	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=10906)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10906)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10956)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10956)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11004)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11004)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10981)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10981)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10963)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10963)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11007)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11007)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10967)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10967)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10914)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10914)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10989)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10989)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11009)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11009)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10992)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10992)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10998)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10998)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10907)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10907)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10975)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10975)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10970)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10970)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10923)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10923)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11005)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11005)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10990)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10990)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10916)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10916)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11011)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11011)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10968)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10968)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10899)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10899)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10966)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10966)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10903)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10903)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10954)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10954)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10894)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10894)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10972)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10972)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10980)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10980)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10979)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10979)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11013)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11013)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10901)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10901)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10997)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10997)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10977)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10977)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10900)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10900)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11001)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11001)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10904)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10904)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10960)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10960)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10891)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10891)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10915)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10915)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10962)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10962)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10945)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10945)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10982)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10982)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10949)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10949)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10999)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10999)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10951)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10951)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10978)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10978)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10994)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10994)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10944)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10944)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10892)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10892)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10905)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10905)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11012)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11012)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11014)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11014)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10983)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10983)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10973)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10973)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10961)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10961)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10911)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10911)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10976)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10976)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10895)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10895)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10924)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10924)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10913)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10913)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10958)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10958)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10988)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10988)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10898)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10898)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10940)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10940)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=11003)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=11003)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10920)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10920)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10971)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10971)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10969)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10969)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10893)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10893)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10908)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10908)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10974)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10974)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10909)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10909)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10985)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10985)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10896)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10896)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10912)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10912)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10890)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10890)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10921)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10921)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10910)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10910)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10917)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10917)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_0254c_00000:
  custom_metrics: {}
  date: 2020-10-11_15-54-35
  done: false
  episode_len_mean: .nan
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.193138313293457
        entropy_coeff: 0.0001
        kl: 0.007721627131104469
        model: {}
        policy_loss: -0.014717266522347927
        total_loss: 550.752490234375
        vf_explained_var: -0.5167572498321533
        vf_loss: 550.7658081054688
    num_steps_sampled: 60672
    num_steps_trained: 60672
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 34.62777777777778
    gpu_util_percent0: 0.25
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3277777777777775
    vram_util_percent0: 0.07845012936846325
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  time_since_restore: 15.325592994689941
  time_this_iter_s: 15.325592994689941
  time_total_s: 15.325592994689941
  timers:
    learn_throughput: 6811.854
    learn_time_ms: 8906.826
    sample_throughput: 9538.184
    sample_time_ms: 6360.959
    update_time_ms: 35.621
  timestamp: 1602431675
  timesteps_since_restore: 0
  timesteps_total: 60672
  training_iteration: 1
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      1 |          15.3256 | 60672 |      nan |                  nan |                  nan |                nan |
+-------------------------+----------+------------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 3929
    time_step_mean: 3605.2025316455697
    time_step_min: 3274
  date: 2020-10-11_15-54-50
  done: false
  episode_len_mean: 886.6582278481013
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 219.77739419511545
  episode_reward_min: 170.71717171717202
  episodes_this_iter: 79
  episodes_total: 79
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1673843383789062
        entropy_coeff: 0.0001
        kl: 0.006144535727798939
        model: {}
        policy_loss: -0.013755168556235731
        total_loss: 535.4020385742188
        vf_explained_var: 0.31667008996009827
        vf_loss: 535.4146850585937
    num_steps_sampled: 121344
    num_steps_trained: 121344
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.725
    gpu_util_percent0: 0.27125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1335579653699614
    mean_env_wait_ms: 0.6396630418185466
    mean_inference_ms: 5.461916608139601
    mean_raw_obs_processing_ms: 0.29134479201721725
  time_since_restore: 29.482505321502686
  time_this_iter_s: 14.156912326812744
  time_total_s: 29.482505321502686
  timers:
    learn_throughput: 6897.036
    learn_time_ms: 8796.822
    sample_throughput: 10311.754
    sample_time_ms: 5883.771
    update_time_ms: 30.512
  timestamp: 1602431690
  timesteps_since_restore: 0
  timesteps_total: 121344
  training_iteration: 2
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      2 |          29.4825 | 121344 |  219.777 |               269.96 |              170.717 |            886.658 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 3967
    time_step_mean: 3626.0632911392404
    time_step_min: 3274
  date: 2020-10-11_15-55-03
  done: false
  episode_len_mean: 882.9303797468355
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 216.61667305971088
  episode_reward_min: 164.95959595959556
  episodes_this_iter: 79
  episodes_total: 158
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1481393575668335
        entropy_coeff: 0.0001
        kl: 0.006490523274987936
        model: {}
        policy_loss: -0.015036692284047604
        total_loss: 317.61456909179685
        vf_explained_var: 0.6613558530807495
        vf_loss: 317.6284118652344
    num_steps_sampled: 182016
    num_steps_trained: 182016
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 33.61333333333334
    gpu_util_percent0: 0.214
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13096906242211057
    mean_env_wait_ms: 0.6401871641256086
    mean_inference_ms: 5.325638128384396
    mean_raw_obs_processing_ms: 0.2857010919150765
  time_since_restore: 43.16357231140137
  time_this_iter_s: 13.681066989898682
  time_total_s: 43.16357231140137
  timers:
    learn_throughput: 6910.861
    learn_time_ms: 8779.224
    sample_throughput: 10936.073
    sample_time_ms: 5547.878
    update_time_ms: 26.906
  timestamp: 1602431703
  timesteps_since_restore: 0
  timesteps_total: 182016
  training_iteration: 3
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      3 |          43.1636 | 182016 |  216.617 |               269.96 |               164.96 |             882.93 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3631.721518987342
    time_step_min: 3274
  date: 2020-10-11_15-55-17
  done: false
  episode_len_mean: 877.1476793248945
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 215.75936580999857
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 237
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.133927011489868
        entropy_coeff: 0.0001
        kl: 0.006118734646588564
        model: {}
        policy_loss: -0.015129859372973443
        total_loss: 199.39960327148438
        vf_explained_var: 0.7791659235954285
        vf_loss: 199.41362609863282
    num_steps_sampled: 242688
    num_steps_trained: 242688
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.575
    gpu_util_percent0: 0.32375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12929822493889637
    mean_env_wait_ms: 0.6416855568072275
    mean_inference_ms: 5.215910171762147
    mean_raw_obs_processing_ms: 0.2820417254294907
  time_since_restore: 56.6672842502594
  time_this_iter_s: 13.503711938858032
  time_total_s: 56.6672842502594
  timers:
    learn_throughput: 6928.737
    learn_time_ms: 8756.574
    sample_throughput: 11348.28
    sample_time_ms: 5346.361
    update_time_ms: 29.016
  timestamp: 1602431717
  timesteps_since_restore: 0
  timesteps_total: 242688
  training_iteration: 4
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      4 |          56.6673 | 242688 |  215.759 |               269.96 |               116.02 |            877.148 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3621.4493670886077
    time_step_min: 3274
  date: 2020-10-11_15-55-30
  done: false
  episode_len_mean: 871.5411392405064
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 217.31575246132192
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 316
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1258406400680543
        entropy_coeff: 0.0001
        kl: 0.006548813544213772
        model: {}
        policy_loss: -0.01587002072483301
        total_loss: 101.75677337646485
        vf_explained_var: 0.8550761342048645
        vf_loss: 101.77144622802734
    num_steps_sampled: 303360
    num_steps_trained: 303360
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.74666666666667
    gpu_util_percent0: 0.3306666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1280293362903634
    mean_env_wait_ms: 0.6430775167613013
    mean_inference_ms: 5.129046763534478
    mean_raw_obs_processing_ms: 0.27937883325702817
  time_since_restore: 70.24723482131958
  time_this_iter_s: 13.57995057106018
  time_total_s: 70.24723482131958
  timers:
    learn_throughput: 6925.054
    learn_time_ms: 8761.231
    sample_throughput: 11611.382
    sample_time_ms: 5225.218
    update_time_ms: 28.584
  timestamp: 1602431730
  timesteps_since_restore: 0
  timesteps_total: 303360
  training_iteration: 5
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      5 |          70.2472 | 303360 |  217.316 |               269.96 |               116.02 |            871.541 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3609.5620253164557
    time_step_min: 3226
  date: 2020-10-11_15-55-44
  done: false
  episode_len_mean: 867.2329113924051
  episode_reward_max: 277.23232323232315
  episode_reward_mean: 219.1168648510419
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 395
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.112362313270569
        entropy_coeff: 0.0001
        kl: 0.00727074546739459
        model: {}
        policy_loss: -0.01704823523759842
        total_loss: 71.98140106201171
        vf_explained_var: 0.8918706774711609
        vf_loss: 71.99710693359376
    num_steps_sampled: 364032
    num_steps_trained: 364032
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.481250000000003
    gpu_util_percent0: 0.31375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12705942527607186
    mean_env_wait_ms: 0.6442925509912155
    mean_inference_ms: 5.058719438900431
    mean_raw_obs_processing_ms: 0.27730945071524615
  time_since_restore: 83.75674295425415
  time_this_iter_s: 13.50950813293457
  time_total_s: 83.75674295425415
  timers:
    learn_throughput: 6924.139
    learn_time_ms: 8762.389
    sample_throughput: 11814.723
    sample_time_ms: 5135.288
    update_time_ms: 27.799
  timestamp: 1602431744
  timesteps_since_restore: 0
  timesteps_total: 364032
  training_iteration: 6
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      6 |          83.7567 | 364032 |  219.117 |              277.232 |               116.02 |            867.233 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3593.96835443038
    time_step_min: 3187
  date: 2020-10-11_15-55-57
  done: false
  episode_len_mean: 862.3860759493671
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 221.47954225802314
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 474
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0965974807739258
        entropy_coeff: 0.0001
        kl: 0.006363364960998296
        model: {}
        policy_loss: -0.01626312769949436
        total_loss: 60.179150390625
        vf_explained_var: 0.905462920665741
        vf_loss: 60.19425048828125
    num_steps_sampled: 424704
    num_steps_trained: 424704
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.640000000000004
    gpu_util_percent0: 0.23333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12626278519887724
    mean_env_wait_ms: 0.6453433568672141
    mean_inference_ms: 5.000835188725645
    mean_raw_obs_processing_ms: 0.2756329424921561
  time_since_restore: 97.15614581108093
  time_this_iter_s: 13.399402856826782
  time_total_s: 97.15614581108093
  timers:
    learn_throughput: 6924.229
    learn_time_ms: 8762.275
    sample_throughput: 12004.493
    sample_time_ms: 5054.108
    update_time_ms: 28.865
  timestamp: 1602431757
  timesteps_since_restore: 0
  timesteps_total: 424704
  training_iteration: 7
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      7 |          97.1561 | 424704 |   221.48 |              283.141 |               116.02 |            862.386 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3587.43115942029
    time_step_min: 3187
  date: 2020-10-11_15-56-11
  done: false
  episode_len_mean: 857.5561594202899
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 222.47002635046098
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 552
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0854148864746094
        entropy_coeff: 0.0001
        kl: 0.006640845071524381
        model: {}
        policy_loss: -0.017912944313138723
        total_loss: 57.62085189819336
        vf_explained_var: 0.9167647361755371
        vf_loss: 57.637541961669925
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.30625
    gpu_util_percent0: 0.273125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12561918815334655
    mean_env_wait_ms: 0.6463205594611693
    mean_inference_ms: 4.95316471179105
    mean_raw_obs_processing_ms: 0.27431672518997435
  time_since_restore: 110.58362245559692
  time_this_iter_s: 13.427476644515991
  time_total_s: 110.58362245559692
  timers:
    learn_throughput: 6923.851
    learn_time_ms: 8762.753
    sample_throughput: 12139.372
    sample_time_ms: 4997.952
    update_time_ms: 27.628
  timestamp: 1602431771
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 8
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      8 |          110.584 | 485376 |   222.47 |              283.141 |               116.02 |            857.556 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3579.633914421553
    time_step_min: 3187
  date: 2020-10-11_15-56-24
  done: false
  episode_len_mean: 853.2773375594295
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 223.65142710784536
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 631
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0685611724853517
        entropy_coeff: 0.0001
        kl: 0.006435621529817581
        model: {}
        policy_loss: -0.017283295653760432
        total_loss: 42.61400680541992
        vf_explained_var: 0.9355558156967163
        vf_loss: 42.63011016845703
    num_steps_sampled: 546048
    num_steps_trained: 546048
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.56666666666667
    gpu_util_percent0: 0.282
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12506485263094025
    mean_env_wait_ms: 0.6472223465212444
    mean_inference_ms: 4.911849955044988
    mean_raw_obs_processing_ms: 0.27313970445811725
  time_since_restore: 123.83380699157715
  time_this_iter_s: 13.250184535980225
  time_total_s: 123.83380699157715
  timers:
    learn_throughput: 6928.914
    learn_time_ms: 8756.351
    sample_throughput: 12277.675
    sample_time_ms: 4941.652
    update_time_ms: 26.462
  timestamp: 1602431784
  timesteps_since_restore: 0
  timesteps_total: 546048
  training_iteration: 9
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |      9 |          123.834 | 546048 |  223.651 |              283.141 |               116.02 |            853.277 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3572.5885714285714
    time_step_min: 3187
  date: 2020-10-11_15-56-38
  done: false
  episode_len_mean: 848.9885714285714
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 224.71890331890322
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 69
  episodes_total: 700
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0564856290817262
        entropy_coeff: 0.0001
        kl: 0.0069581371732056144
        model: {}
        policy_loss: -0.017678908724337816
        total_loss: 35.82207412719727
        vf_explained_var: 0.9436900019645691
        vf_loss: 35.83846588134766
    num_steps_sampled: 606720
    num_steps_trained: 606720
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.625
    gpu_util_percent0: 0.29874999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.41875
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12465399711658876
    mean_env_wait_ms: 0.648056121056164
    mean_inference_ms: 4.879984642120459
    mean_raw_obs_processing_ms: 0.27227299804019345
  time_since_restore: 137.2602949142456
  time_this_iter_s: 13.426487922668457
  time_total_s: 137.2602949142456
  timers:
    learn_throughput: 6929.944
    learn_time_ms: 8755.05
    sample_throughput: 12378.447
    sample_time_ms: 4901.423
    update_time_ms: 34.448
  timestamp: 1602431798
  timesteps_since_restore: 0
  timesteps_total: 606720
  training_iteration: 10
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     10 |           137.26 | 606720 |  224.719 |              283.141 |               116.02 |            848.989 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3569.1739707835327
    time_step_min: 3187
  date: 2020-10-11_15-56-51
  done: false
  episode_len_mean: 846.2270916334661
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 225.236267053
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 53
  episodes_total: 753
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0427685260772706
        entropy_coeff: 0.0001
        kl: 0.006341448985040188
        model: {}
        policy_loss: -0.016971242520958184
        total_loss: 35.50624237060547
        vf_explained_var: 0.9447349309921265
        vf_loss: 35.52204895019531
    num_steps_sampled: 667392
    num_steps_trained: 667392
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.833333333333332
    gpu_util_percent0: 0.26266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12433623200377432
    mean_env_wait_ms: 0.6486852836553401
    mean_inference_ms: 4.8575339590357
    mean_raw_obs_processing_ms: 0.27152519320797486
  time_since_restore: 150.54876828193665
  time_this_iter_s: 13.28847336769104
  time_total_s: 150.54876828193665
  timers:
    learn_throughput: 6945.642
    learn_time_ms: 8735.262
    sample_throughput: 12860.23
    sample_time_ms: 4717.801
    update_time_ms: 32.912
  timestamp: 1602431811
  timesteps_since_restore: 0
  timesteps_total: 667392
  training_iteration: 11
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     11 |          150.549 | 667392 |  225.236 |              283.141 |               116.02 |            846.227 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3563.3507462686566
    time_step_min: 3187
  date: 2020-10-11_15-57-04
  done: false
  episode_len_mean: 844.0559701492538
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 226.11857379767818
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 51
  episodes_total: 804
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0320799827575684
        entropy_coeff: 0.0001
        kl: 0.006349154934287071
        model: {}
        policy_loss: -0.018102188082411885
        total_loss: 23.2988582611084
        vf_explained_var: 0.9564861059188843
        vf_loss: 23.315794372558592
    num_steps_sampled: 728064
    num_steps_trained: 728064
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.28666666666667
    gpu_util_percent0: 0.3973333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12406581800779055
    mean_env_wait_ms: 0.6492691546624914
    mean_inference_ms: 4.837351513112054
    mean_raw_obs_processing_ms: 0.27095958437703355
  time_since_restore: 163.72990822792053
  time_this_iter_s: 13.181139945983887
  time_total_s: 163.72990822792053
  timers:
    learn_throughput: 6951.133
    learn_time_ms: 8728.361
    sample_throughput: 13110.742
    sample_time_ms: 4627.656
    update_time_ms: 32.559
  timestamp: 1602431824
  timesteps_since_restore: 0
  timesteps_total: 728064
  training_iteration: 12
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     12 |           163.73 | 728064 |  226.119 |              283.141 |               116.02 |            844.056 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3557.2402745995423
    time_step_min: 3187
  date: 2020-10-11_15-57-18
  done: false
  episode_len_mean: 841.3981693363844
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 227.04440283845307
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 70
  episodes_total: 874
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.033131980895996
        entropy_coeff: 0.0001
        kl: 0.006343710143119097
        model: {}
        policy_loss: -0.017480041179805995
        total_loss: 19.233250427246094
        vf_explained_var: 0.9637705087661743
        vf_loss: 19.249564743041994
    num_steps_sampled: 788736
    num_steps_trained: 788736
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.746666666666673
    gpu_util_percent0: 0.28
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12373696501888903
    mean_env_wait_ms: 0.6498939859770279
    mean_inference_ms: 4.812738635870073
    mean_raw_obs_processing_ms: 0.2701312535684045
  time_since_restore: 177.0780427455902
  time_this_iter_s: 13.348134517669678
  time_total_s: 177.0780427455902
  timers:
    learn_throughput: 6946.551
    learn_time_ms: 8734.118
    sample_throughput: 13222.879
    sample_time_ms: 4588.411
    update_time_ms: 33.566
  timestamp: 1602431838
  timesteps_since_restore: 0
  timesteps_total: 788736
  training_iteration: 13
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     13 |          177.078 | 788736 |  227.044 |              283.141 |               116.02 |            841.398 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3549.9789029535864
    time_step_min: 3187
  date: 2020-10-11_15-57-31
  done: false
  episode_len_mean: 838.9367088607595
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 228.1446106635979
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 74
  episodes_total: 948
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0167401790618897
        entropy_coeff: 0.0001
        kl: 0.007002000138163567
        model: {}
        policy_loss: -0.018312944937497376
        total_loss: 17.715657043457032
        vf_explained_var: 0.9672698974609375
        vf_loss: 17.732671356201173
    num_steps_sampled: 849408
    num_steps_trained: 849408
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.625
    gpu_util_percent0: 0.27875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12342031345688508
    mean_env_wait_ms: 0.6505408949897603
    mean_inference_ms: 4.789156144582933
    mean_raw_obs_processing_ms: 0.2693535752713011
  time_since_restore: 190.60786366462708
  time_this_iter_s: 13.529820919036865
  time_total_s: 190.60786366462708
  timers:
    learn_throughput: 6937.619
    learn_time_ms: 8745.363
    sample_throughput: 13251.943
    sample_time_ms: 4578.347
    update_time_ms: 35.259
  timestamp: 1602431851
  timesteps_since_restore: 0
  timesteps_total: 849408
  training_iteration: 14
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     14 |          190.608 | 849408 |  228.145 |              283.141 |               116.02 |            838.937 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3543.60564751704
    time_step_min: 3187
  date: 2020-10-11_15-57-45
  done: false
  episode_len_mean: 836.4985394352483
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 229.11025542671103
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1027
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9943946719169616
        entropy_coeff: 0.0001
        kl: 0.007912519946694374
        model: {}
        policy_loss: -0.018420965038239957
        total_loss: 17.297443771362303
        vf_explained_var: 0.9709770083427429
        vf_loss: 17.314381790161132
    num_steps_sampled: 910080
    num_steps_trained: 910080
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.22
    gpu_util_percent0: 0.2553333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12311990239994437
    mean_env_wait_ms: 0.6512540754970045
    mean_inference_ms: 4.766607307829725
    mean_raw_obs_processing_ms: 0.26859772667314763
  time_since_restore: 203.8491177558899
  time_this_iter_s: 13.241254091262817
  time_total_s: 203.8491177558899
  timers:
    learn_throughput: 6948.253
    learn_time_ms: 8731.979
    sample_throughput: 13314.017
    sample_time_ms: 4557.002
    update_time_ms: 36.008
  timestamp: 1602431865
  timesteps_since_restore: 0
  timesteps_total: 910080
  training_iteration: 15
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     15 |          203.849 | 910080 |   229.11 |              283.141 |               116.02 |            836.499 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3535.533453887884
    time_step_min: 3187
  date: 2020-10-11_15-57-58
  done: false
  episode_len_mean: 834.1925858951175
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 230.33331506749218
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1106
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9890294790267944
        entropy_coeff: 0.0001
        kl: 0.0057542574591934684
        model: {}
        policy_loss: -0.016859600320458412
        total_loss: 22.078242111206055
        vf_explained_var: 0.9638479948043823
        vf_loss: 22.09404945373535
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.439999999999998
    gpu_util_percent0: 0.24866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12284977928720761
    mean_env_wait_ms: 0.6519194255420189
    mean_inference_ms: 4.746385724624431
    mean_raw_obs_processing_ms: 0.2679060922505163
  time_since_restore: 217.11126279830933
  time_this_iter_s: 13.262145042419434
  time_total_s: 217.11126279830933
  timers:
    learn_throughput: 6952.965
    learn_time_ms: 8726.062
    sample_throughput: 13370.649
    sample_time_ms: 4537.7
    update_time_ms: 35.577
  timestamp: 1602431878
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 16
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     16 |          217.111 | 970752 |  230.333 |              283.141 |               116.02 |            834.193 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3527.42194092827
    time_step_min: 3187
  date: 2020-10-11_15-58-11
  done: false
  episode_len_mean: 832.0430379746836
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 231.56233218258527
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1185
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9727253675460815
        entropy_coeff: 0.0001
        kl: 0.006370769906789064
        model: {}
        policy_loss: -0.01666537211276591
        total_loss: 16.41226272583008
        vf_explained_var: 0.971873939037323
        vf_loss: 16.42775077819824
    num_steps_sampled: 1031424
    num_steps_trained: 1031424
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.580000000000002
    gpu_util_percent0: 0.24333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12260554522131402
    mean_env_wait_ms: 0.6525447829778301
    mean_inference_ms: 4.728098652422307
    mean_raw_obs_processing_ms: 0.26727086732837113
  time_since_restore: 230.35052013397217
  time_this_iter_s: 13.239257335662842
  time_total_s: 230.35052013397217
  timers:
    learn_throughput: 6956.359
    learn_time_ms: 8721.804
    sample_throughput: 13401.743
    sample_time_ms: 4527.172
    update_time_ms: 33.824
  timestamp: 1602431891
  timesteps_since_restore: 0
  timesteps_total: 1031424
  training_iteration: 17
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     17 |          230.351 | 1031424 |  231.562 |              283.141 |               116.02 |            832.043 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3521.1756329113923
    time_step_min: 3187
  date: 2020-10-11_15-58-25
  done: false
  episode_len_mean: 830.375
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 232.50874248817277
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1264
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9600818872451782
        entropy_coeff: 0.0001
        kl: 0.006693596951663494
        model: {}
        policy_loss: -0.017358003184199333
        total_loss: 18.817015075683592
        vf_explained_var: 0.9695835113525391
        vf_loss: 18.833131790161133
    num_steps_sampled: 1092096
    num_steps_trained: 1092096
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.900000000000002
    gpu_util_percent0: 0.31125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12238541135239141
    mean_env_wait_ms: 0.6531369692901041
    mean_inference_ms: 4.711528553519437
    mean_raw_obs_processing_ms: 0.2666955382496055
  time_since_restore: 243.7254707813263
  time_this_iter_s: 13.374950647354126
  time_total_s: 243.7254707813263
  timers:
    learn_throughput: 6960.665
    learn_time_ms: 8716.408
    sample_throughput: 13406.891
    sample_time_ms: 4525.434
    update_time_ms: 35.896
  timestamp: 1602431905
  timesteps_since_restore: 0
  timesteps_total: 1092096
  training_iteration: 18
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     18 |          243.725 | 1092096 |  232.509 |              283.141 |               116.02 |            830.375 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3516.3827252419956
    time_step_min: 3187
  date: 2020-10-11_15-58-38
  done: false
  episode_len_mean: 829.0067014147431
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 233.23494061989956
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1343
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9440988779067994
        entropy_coeff: 0.0001
        kl: 0.007389881648123264
        model: {}
        policy_loss: -0.01705184131860733
        total_loss: 15.939278411865235
        vf_explained_var: 0.9752111434936523
        vf_loss: 15.954946899414063
    num_steps_sampled: 1152768
    num_steps_trained: 1152768
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.26
    gpu_util_percent0: 0.25133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1221860119313473
    mean_env_wait_ms: 0.6536970250442272
    mean_inference_ms: 4.696402876530246
    mean_raw_obs_processing_ms: 0.2661655553941498
  time_since_restore: 257.01133251190186
  time_this_iter_s: 13.285861730575562
  time_total_s: 257.01133251190186
  timers:
    learn_throughput: 6968.979
    learn_time_ms: 8706.01
    sample_throughput: 13371.633
    sample_time_ms: 4537.366
    update_time_ms: 37.907
  timestamp: 1602431918
  timesteps_since_restore: 0
  timesteps_total: 1152768
  training_iteration: 19
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     19 |          257.011 | 1152768 |  233.235 |              283.141 |               116.02 |            829.007 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3510.7897327707456
    time_step_min: 3187
  date: 2020-10-11_15-58-52
  done: false
  episode_len_mean: 828.0991561181435
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 234.08236372160414
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1422
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9336613178253174
        entropy_coeff: 0.0001
        kl: 0.006462276726961136
        model: {}
        policy_loss: -0.017750857584178447
        total_loss: 12.616872406005859
        vf_explained_var: 0.9794406890869141
        vf_loss: 12.63342456817627
    num_steps_sampled: 1213440
    num_steps_trained: 1213440
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.500000000000007
    gpu_util_percent0: 0.36666666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12200509085547602
    mean_env_wait_ms: 0.6542092230193569
    mean_inference_ms: 4.682519514645896
    mean_raw_obs_processing_ms: 0.26567865937886914
  time_since_restore: 270.40074610710144
  time_this_iter_s: 13.389413595199585
  time_total_s: 270.40074610710144
  timers:
    learn_throughput: 6973.737
    learn_time_ms: 8700.07
    sample_throughput: 13345.344
    sample_time_ms: 4546.305
    update_time_ms: 30.911
  timestamp: 1602431932
  timesteps_since_restore: 0
  timesteps_total: 1213440
  training_iteration: 20
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     20 |          270.401 | 1213440 |  234.082 |              283.141 |               116.02 |            828.099 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3505.7934710193204
    time_step_min: 3187
  date: 2020-10-11_15-59-05
  done: false
  episode_len_mean: 827.3597601598934
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 234.83937307788065
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1501
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9111692786216736
        entropy_coeff: 0.0001
        kl: 0.007237232103943825
        model: {}
        policy_loss: -0.016399275790899993
        total_loss: 14.045680236816406
        vf_explained_var: 0.9778404235839844
        vf_loss: 14.060722923278808
    num_steps_sampled: 1274112
    num_steps_trained: 1274112
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.075000000000003
    gpu_util_percent0: 0.305625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12183713496784085
    mean_env_wait_ms: 0.6546777091680613
    mean_inference_ms: 4.669743037173543
    mean_raw_obs_processing_ms: 0.2652312054653271
  time_since_restore: 283.9569752216339
  time_this_iter_s: 13.55622911453247
  time_total_s: 283.9569752216339
  timers:
    learn_throughput: 6962.288
    learn_time_ms: 8714.376
    sample_throughput: 13314.663
    sample_time_ms: 4556.781
    update_time_ms: 32.387
  timestamp: 1602431945
  timesteps_since_restore: 0
  timesteps_total: 1274112
  training_iteration: 21
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     21 |          283.957 | 1274112 |  234.839 |              283.141 |               116.02 |             827.36 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3500.7069620253164
    time_step_min: 3187
  date: 2020-10-11_15-59-19
  done: false
  episode_len_mean: 826.9227848101266
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 235.61005625879037
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1580
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8991423726081849
        entropy_coeff: 0.0001
        kl: 0.005821902584284544
        model: {}
        policy_loss: -0.01675959237618372
        total_loss: 11.816276931762696
        vf_explained_var: 0.9811431169509888
        vf_loss: 11.83196144104004
    num_steps_sampled: 1334784
    num_steps_trained: 1334784
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.193333333333335
    gpu_util_percent0: 0.24600000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.121681789485281
    mean_env_wait_ms: 0.6550983362033309
    mean_inference_ms: 4.657925233192698
    mean_raw_obs_processing_ms: 0.2648172095844593
  time_since_restore: 297.25005173683167
  time_this_iter_s: 13.293076515197754
  time_total_s: 297.25005173683167
  timers:
    learn_throughput: 6958.474
    learn_time_ms: 8719.153
    sample_throughput: 13299.556
    sample_time_ms: 4561.957
    update_time_ms: 33.32
  timestamp: 1602431959
  timesteps_since_restore: 0
  timesteps_total: 1334784
  training_iteration: 22
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     22 |           297.25 | 1334784 |   235.61 |              283.141 |               116.02 |            826.923 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3496.0596745027124
    time_step_min: 3187
  date: 2020-10-11_15-59-32
  done: false
  episode_len_mean: 826.6172393007836
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 236.3141907319122
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1659
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8894487977027893
        entropy_coeff: 0.0001
        kl: 0.00622055558487773
        model: {}
        policy_loss: -0.016685663908720016
        total_loss: 12.154040336608887
        vf_explained_var: 0.9802371859550476
        vf_loss: 12.169570541381836
    num_steps_sampled: 1395456
    num_steps_trained: 1395456
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.74000000000001
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4199999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12153697127540337
    mean_env_wait_ms: 0.655473859315079
    mean_inference_ms: 4.646941960611419
    mean_raw_obs_processing_ms: 0.2644309640735632
  time_since_restore: 310.55483388900757
  time_this_iter_s: 13.304782152175903
  time_total_s: 310.55483388900757
  timers:
    learn_throughput: 6960.758
    learn_time_ms: 8716.292
    sample_throughput: 13305.845
    sample_time_ms: 4559.801
    update_time_ms: 33.628
  timestamp: 1602431972
  timesteps_since_restore: 0
  timesteps_total: 1395456
  training_iteration: 23
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     23 |          310.555 | 1395456 |  236.314 |              283.141 |               116.02 |            826.617 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3493.070195627158
    time_step_min: 3187
  date: 2020-10-11_15-59-45
  done: false
  episode_len_mean: 826.337169159954
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 236.7671420766932
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1738
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8807594299316406
        entropy_coeff: 0.0001
        kl: 0.005982859991490841
        model: {}
        policy_loss: -0.016763895750045776
        total_loss: 12.645524978637695
        vf_explained_var: 0.9802120923995972
        vf_loss: 12.661180877685547
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.819999999999997
    gpu_util_percent0: 0.2433333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1214031377499436
    mean_env_wait_ms: 0.6558162618321063
    mean_inference_ms: 4.636744336810363
    mean_raw_obs_processing_ms: 0.2640767432812346
  time_since_restore: 323.7830502986908
  time_this_iter_s: 13.228216409683228
  time_total_s: 323.7830502986908
  timers:
    learn_throughput: 6970.567
    learn_time_ms: 8704.027
    sample_throughput: 13350.301
    sample_time_ms: 4544.617
    update_time_ms: 29.989
  timestamp: 1602431985
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 24
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     24 |          323.783 | 1456128 |  236.767 |              283.141 |               116.02 |            826.337 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3489.458998348927
    time_step_min: 3187
  date: 2020-10-11_15-59-59
  done: false
  episode_len_mean: 826.1761144744083
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 237.31429317945543
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1817
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8732481479644776
        entropy_coeff: 0.0001
        kl: 0.00523865120485425
        model: {}
        policy_loss: -0.015956384362652898
        total_loss: 13.58818016052246
        vf_explained_var: 0.9780164957046509
        vf_loss: 13.603175735473632
    num_steps_sampled: 1516800
    num_steps_trained: 1516800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.926666666666662
    gpu_util_percent0: 0.336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12127871828117874
    mean_env_wait_ms: 0.6561269763112696
    mean_inference_ms: 4.627235935246541
    mean_raw_obs_processing_ms: 0.26375211427761996
  time_since_restore: 337.0192449092865
  time_this_iter_s: 13.236194610595703
  time_total_s: 337.0192449092865
  timers:
    learn_throughput: 6967.733
    learn_time_ms: 8707.566
    sample_throughput: 13360.034
    sample_time_ms: 4541.306
    update_time_ms: 28.564
  timestamp: 1602431999
  timesteps_since_restore: 0
  timesteps_total: 1516800
  training_iteration: 25
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     25 |          337.019 | 1516800 |  237.314 |              283.141 |               116.02 |            826.176 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3486.208443271768
    time_step_min: 3187
  date: 2020-10-11_16-00-12
  done: false
  episode_len_mean: 825.932981530343
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 237.80680152447957
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 1895
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8687670111656189
        entropy_coeff: 0.0001
        kl: 0.005749080330133438
        model: {}
        policy_loss: -0.016765564773231746
        total_loss: 11.381985664367676
        vf_explained_var: 0.9814356565475464
        vf_loss: 11.397688293457032
    num_steps_sampled: 1577472
    num_steps_trained: 1577472
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.268749999999997
    gpu_util_percent0: 0.33375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12115824527452636
    mean_env_wait_ms: 0.6564064837434892
    mean_inference_ms: 4.618442471884918
    mean_raw_obs_processing_ms: 0.2634490220819825
  time_since_restore: 350.4165382385254
  time_this_iter_s: 13.397293329238892
  time_total_s: 350.4165382385254
  timers:
    learn_throughput: 6957.032
    learn_time_ms: 8720.961
    sample_throughput: 13364.095
    sample_time_ms: 4539.926
    update_time_ms: 30.258
  timestamp: 1602432012
  timesteps_since_restore: 0
  timesteps_total: 1577472
  training_iteration: 26
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     26 |          350.417 | 1577472 |  237.807 |              283.141 |               116.02 |            825.933 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3483.598268839104
    time_step_min: 3187
  date: 2020-10-11_16-00-25
  done: false
  episode_len_mean: 825.6252545824847
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 238.2022824991256
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 69
  episodes_total: 1964
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8563682079315186
        entropy_coeff: 0.0001
        kl: 0.005709396209567785
        model: {}
        policy_loss: -0.017698555067181588
        total_loss: 10.851219177246094
        vf_explained_var: 0.9818906784057617
        vf_loss: 10.867861557006837
    num_steps_sampled: 1638144
    num_steps_trained: 1638144
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.98666666666666
    gpu_util_percent0: 0.368
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12106144903411631
    mean_env_wait_ms: 0.65663010350919
    mean_inference_ms: 4.611047299197732
    mean_raw_obs_processing_ms: 0.2631941726080479
  time_since_restore: 363.66497015953064
  time_this_iter_s: 13.248431921005249
  time_total_s: 363.66497015953064
  timers:
    learn_throughput: 6962.683
    learn_time_ms: 8713.882
    sample_throughput: 13342.129
    sample_time_ms: 4547.4
    update_time_ms: 30.61
  timestamp: 1602432025
  timesteps_since_restore: 0
  timesteps_total: 1638144
  training_iteration: 27
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     27 |          363.665 | 1638144 |  238.202 |              283.141 |               116.02 |            825.625 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3481.9901429275506
    time_step_min: 3187
  date: 2020-10-11_16-00-39
  done: false
  episode_len_mean: 825.3652045342533
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 238.44593794027003
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 65
  episodes_total: 2029
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8387965321540832
        entropy_coeff: 0.0001
        kl: 0.006011899933218956
        model: {}
        policy_loss: -0.01685419175773859
        total_loss: 10.585357475280762
        vf_explained_var: 0.9818204045295715
        vf_loss: 10.601093292236328
    num_steps_sampled: 1698816
    num_steps_trained: 1698816
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.373333333333335
    gpu_util_percent0: 0.31666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12098182650706206
    mean_env_wait_ms: 0.656839713845296
    mean_inference_ms: 4.604512070314659
    mean_raw_obs_processing_ms: 0.2629729069126024
  time_since_restore: 377.0492670536041
  time_this_iter_s: 13.384296894073486
  time_total_s: 377.0492670536041
  timers:
    learn_throughput: 6958.282
    learn_time_ms: 8719.393
    sample_throughput: 13354.543
    sample_time_ms: 4543.173
    update_time_ms: 29.93
  timestamp: 1602432039
  timesteps_since_restore: 0
  timesteps_total: 1698816
  training_iteration: 28
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     28 |          377.049 | 1698816 |  238.446 |              283.141 |               116.02 |            825.365 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3479.421254188607
    time_step_min: 3187
  date: 2020-10-11_16-00-52
  done: false
  episode_len_mean: 825.2077549066539
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 238.83516350677664
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 60
  episodes_total: 2089
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8409842371940612
        entropy_coeff: 0.0001
        kl: 0.006058331951498986
        model: {}
        policy_loss: -0.017604468390345573
        total_loss: 8.032962226867676
        vf_explained_var: 0.9850233197212219
        vf_loss: 8.049439334869385
    num_steps_sampled: 1759488
    num_steps_trained: 1759488
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.226666666666667
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12090547620256412
    mean_env_wait_ms: 0.6570354909118539
    mean_inference_ms: 4.5988118399764675
    mean_raw_obs_processing_ms: 0.2627840875420408
  time_since_restore: 390.4477002620697
  time_this_iter_s: 13.398433208465576
  time_total_s: 390.4477002620697
  timers:
    learn_throughput: 6949.219
    learn_time_ms: 8730.765
    sample_throughput: 13351.476
    sample_time_ms: 4544.217
    update_time_ms: 28.118
  timestamp: 1602432052
  timesteps_since_restore: 0
  timesteps_total: 1759488
  training_iteration: 29
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     29 |          390.448 | 1759488 |  238.835 |              283.141 |               116.02 |            825.208 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3477.6100511865984
    time_step_min: 3169
  date: 2020-10-11_16-01-06
  done: false
  episode_len_mean: 825.1763610981852
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 239.10958820405065
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 60
  episodes_total: 2149
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8300541996955871
        entropy_coeff: 0.0001
        kl: 0.006526672095060348
        model: {}
        policy_loss: -0.017102842684835196
        total_loss: 9.383557319641113
        vf_explained_var: 0.9828168153762817
        vf_loss: 9.399437713623048
    num_steps_sampled: 1820160
    num_steps_trained: 1820160
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.106666666666666
    gpu_util_percent0: 0.26666666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12083493613574385
    mean_env_wait_ms: 0.6571775091137801
    mean_inference_ms: 4.593183145199037
    mean_raw_obs_processing_ms: 0.2626079323019862
  time_since_restore: 403.7302429676056
  time_this_iter_s: 13.282542705535889
  time_total_s: 403.7302429676056
  timers:
    learn_throughput: 6953.349
    learn_time_ms: 8725.58
    sample_throughput: 13362.1
    sample_time_ms: 4540.603
    update_time_ms: 26.462
  timestamp: 1602432066
  timesteps_since_restore: 0
  timesteps_total: 1820160
  training_iteration: 30
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     30 |           403.73 | 1820160 |   239.11 |              285.869 |               116.02 |            825.176 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3474.5842342342344
    time_step_min: 3169
  date: 2020-10-11_16-01-19
  done: false
  episode_len_mean: 825.2738738738739
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 239.56804531804525
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 71
  episodes_total: 2220
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8318358898162842
        entropy_coeff: 0.0001
        kl: 0.005705495923757553
        model: {}
        policy_loss: -0.016056472854688763
        total_loss: 9.408385086059571
        vf_explained_var: 0.9828255772590637
        vf_loss: 9.423383331298828
    num_steps_sampled: 1880832
    num_steps_trained: 1880832
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.400000000000002
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12075211819346843
    mean_env_wait_ms: 0.6573531301258803
    mean_inference_ms: 4.586823952399324
    mean_raw_obs_processing_ms: 0.2623892144532047
  time_since_restore: 416.916544675827
  time_this_iter_s: 13.186301708221436
  time_total_s: 416.916544675827
  timers:
    learn_throughput: 6971.674
    learn_time_ms: 8702.645
    sample_throughput: 13399.302
    sample_time_ms: 4527.997
    update_time_ms: 24.959
  timestamp: 1602432079
  timesteps_since_restore: 0
  timesteps_total: 1880832
  training_iteration: 31
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     31 |          416.917 | 1880832 |  239.568 |              285.869 |               116.02 |            825.274 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3471.0850043591977
    time_step_min: 3169
  date: 2020-10-11_16-01-33
  done: false
  episode_len_mean: 825.2794245858762
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 240.0982316627477
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 74
  episodes_total: 2294
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8104691743850708
        entropy_coeff: 0.0001
        kl: 0.005790698435157537
        model: {}
        policy_loss: -0.016167748929001392
        total_loss: 10.063735961914062
        vf_explained_var: 0.982425332069397
        vf_loss: 10.078826713562012
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.887500000000003
    gpu_util_percent0: 0.271875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12066404371282796
    mean_env_wait_ms: 0.657500318704452
    mean_inference_ms: 4.580569539725109
    mean_raw_obs_processing_ms: 0.2621707368723776
  time_since_restore: 430.2910635471344
  time_this_iter_s: 13.374518871307373
  time_total_s: 430.2910635471344
  timers:
    learn_throughput: 6951.438
    learn_time_ms: 8727.978
    sample_throughput: 13453.933
    sample_time_ms: 4509.611
    update_time_ms: 25.83
  timestamp: 1602432093
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 32
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     32 |          430.291 | 1941504 |  240.098 |              285.869 |               116.02 |            825.279 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3468.250105440742
    time_step_min: 3169
  date: 2020-10-11_16-01-46
  done: false
  episode_len_mean: 825.065795023197
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 240.52776180190762
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 77
  episodes_total: 2371
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8031792521476746
        entropy_coeff: 0.0001
        kl: 0.005553171876817942
        model: {}
        policy_loss: -0.01612788438796997
        total_loss: 8.300242805480957
        vf_explained_var: 0.9860474467277527
        vf_loss: 8.31534023284912
    num_steps_sampled: 2002176
    num_steps_trained: 2002176
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.61333333333334
    gpu_util_percent0: 0.264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12058245855496046
    mean_env_wait_ms: 0.6576537296479603
    mean_inference_ms: 4.574343663239295
    mean_raw_obs_processing_ms: 0.26195669747331446
  time_since_restore: 443.59071135520935
  time_this_iter_s: 13.299647808074951
  time_total_s: 443.59071135520935
  timers:
    learn_throughput: 6946.573
    learn_time_ms: 8734.091
    sample_throughput: 13475.346
    sample_time_ms: 4502.445
    update_time_ms: 25.53
  timestamp: 1602432106
  timesteps_since_restore: 0
  timesteps_total: 2002176
  training_iteration: 33
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     33 |          443.591 | 2002176 |  240.528 |              285.869 |               116.02 |            825.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3465.450204081633
    time_step_min: 3169
  date: 2020-10-11_16-01-59
  done: false
  episode_len_mean: 824.8004081632653
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 240.95198928056067
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2450
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7909312963485717
        entropy_coeff: 0.0001
        kl: 0.005124823935329914
        model: {}
        policy_loss: -0.01604766147211194
        total_loss: 9.810696792602538
        vf_explained_var: 0.9840415716171265
        vf_loss: 9.825798606872558
    num_steps_sampled: 2062848
    num_steps_trained: 2062848
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.77333333333333
    gpu_util_percent0: 0.26866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12050173079399744
    mean_env_wait_ms: 0.6578011058508888
    mean_inference_ms: 4.568264627887463
    mean_raw_obs_processing_ms: 0.26174526053956815
  time_since_restore: 456.79246163368225
  time_this_iter_s: 13.2017502784729
  time_total_s: 456.79246163368225
  timers:
    learn_throughput: 6941.001
    learn_time_ms: 8741.102
    sample_throughput: 13507.081
    sample_time_ms: 4491.866
    update_time_ms: 26.626
  timestamp: 1602432119
  timesteps_since_restore: 0
  timesteps_total: 2062848
  training_iteration: 34
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     34 |          456.792 | 2062848 |  240.952 |              285.869 |               116.02 |              824.8 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3462.2349683544303
    time_step_min: 3169
  date: 2020-10-11_16-02-13
  done: false
  episode_len_mean: 824.4497626582279
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 241.4391462089246
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 2528
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7874290704727173
        entropy_coeff: 0.0001
        kl: 0.00535367289558053
        model: {}
        policy_loss: -0.015816283226013184
        total_loss: 8.599153900146485
        vf_explained_var: 0.9853988885879517
        vf_loss: 8.613978385925293
    num_steps_sampled: 2123520
    num_steps_trained: 2123520
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.153333333333332
    gpu_util_percent0: 0.3366666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12042366165133124
    mean_env_wait_ms: 0.6579429388606653
    mean_inference_ms: 4.56254680786855
    mean_raw_obs_processing_ms: 0.2615465353064913
  time_since_restore: 470.0361771583557
  time_this_iter_s: 13.243715524673462
  time_total_s: 470.0361771583557
  timers:
    learn_throughput: 6939.014
    learn_time_ms: 8743.605
    sample_throughput: 13519.013
    sample_time_ms: 4487.902
    update_time_ms: 28.558
  timestamp: 1602432133
  timesteps_since_restore: 0
  timesteps_total: 2123520
  training_iteration: 35
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     35 |          470.036 | 2123520 |  241.439 |              285.869 |               116.02 |             824.45 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3459.7096279248176
    time_step_min: 3169
  date: 2020-10-11_16-02-26
  done: false
  episode_len_mean: 824.2305331799002
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 241.82177354674468
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2607
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7673699378967285
        entropy_coeff: 0.0001
        kl: 0.005303710792213678
        model: {}
        policy_loss: -0.015531449741683901
        total_loss: 9.45707950592041
        vf_explained_var: 0.9850137829780579
        vf_loss: 9.47162685394287
    num_steps_sampled: 2184192
    num_steps_trained: 2184192
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.14666666666667
    gpu_util_percent0: 0.34600000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12035006653693807
    mean_env_wait_ms: 0.6580753825207725
    mean_inference_ms: 4.556990252644127
    mean_raw_obs_processing_ms: 0.26135253126859137
  time_since_restore: 483.28974437713623
  time_this_iter_s: 13.253567218780518
  time_total_s: 483.28974437713623
  timers:
    learn_throughput: 6945.36
    learn_time_ms: 8735.617
    sample_throughput: 13539.679
    sample_time_ms: 4481.051
    update_time_ms: 28.793
  timestamp: 1602432146
  timesteps_since_restore: 0
  timesteps_total: 2184192
  training_iteration: 36
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     36 |           483.29 | 2184192 |  241.822 |              285.869 |               116.02 |            824.231 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3457.1046165301564
    time_step_min: 3169
  date: 2020-10-11_16-02-39
  done: false
  episode_len_mean: 823.8406552494415
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 242.21647224290552
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2686
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7685246229171753
        entropy_coeff: 0.0001
        kl: 0.005332937464118004
        model: {}
        policy_loss: -0.015074999816715718
        total_loss: 10.110424613952636
        vf_explained_var: 0.9831159710884094
        vf_loss: 10.124510192871094
    num_steps_sampled: 2244864
    num_steps_trained: 2244864
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.96875
    gpu_util_percent0: 0.30999999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12027973471638467
    mean_env_wait_ms: 0.658205448627044
    mean_inference_ms: 4.5516748794788455
    mean_raw_obs_processing_ms: 0.26116646832530793
  time_since_restore: 496.7275276184082
  time_this_iter_s: 13.437783241271973
  time_total_s: 496.7275276184082
  timers:
    learn_throughput: 6922.742
    learn_time_ms: 8764.158
    sample_throughput: 13575.987
    sample_time_ms: 4469.067
    update_time_ms: 30.271
  timestamp: 1602432159
  timesteps_since_restore: 0
  timesteps_total: 2244864
  training_iteration: 37
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     37 |          496.728 | 2244864 |  242.216 |              285.869 |               116.02 |            823.841 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3454.723327305606
    time_step_min: 3169
  date: 2020-10-11_16-02-53
  done: false
  episode_len_mean: 823.6517179023508
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 242.57727364056467
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2765
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7509182333946228
        entropy_coeff: 0.0001
        kl: 0.00532805984839797
        model: {}
        policy_loss: -0.01556666032411158
        total_loss: 8.812029457092285
        vf_explained_var: 0.9855332374572754
        vf_loss: 8.826605606079102
    num_steps_sampled: 2305536
    num_steps_trained: 2305536
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.94666666666667
    gpu_util_percent0: 0.2786666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12021245248126033
    mean_env_wait_ms: 0.6583288256972009
    mean_inference_ms: 4.546589884580029
    mean_raw_obs_processing_ms: 0.26098810296950836
  time_since_restore: 509.97461771965027
  time_this_iter_s: 13.247090101242065
  time_total_s: 509.97461771965027
  timers:
    learn_throughput: 6925.375
    learn_time_ms: 8760.825
    sample_throughput: 13610.856
    sample_time_ms: 4457.618
    update_time_ms: 30.876
  timestamp: 1602432173
  timesteps_since_restore: 0
  timesteps_total: 2305536
  training_iteration: 38
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     38 |          509.975 | 2305536 |  242.577 |              285.869 |               116.02 |            823.652 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3452.709563994374
    time_step_min: 3169
  date: 2020-10-11_16-03-06
  done: false
  episode_len_mean: 823.4556962025316
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 242.8823892937816
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2844
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7447753310203552
        entropy_coeff: 0.0001
        kl: 0.0048968297429382805
        model: {}
        policy_loss: -0.015580352582037448
        total_loss: 9.111823654174804
        vf_explained_var: 0.9850462079048157
        vf_loss: 9.126498985290528
    num_steps_sampled: 2366208
    num_steps_trained: 2366208
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.6
    gpu_util_percent0: 0.29933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12014789314603108
    mean_env_wait_ms: 0.6584441665611782
    mean_inference_ms: 4.541718618823155
    mean_raw_obs_processing_ms: 0.26081771478569926
  time_since_restore: 523.1847684383392
  time_this_iter_s: 13.210150718688965
  time_total_s: 523.1847684383392
  timers:
    learn_throughput: 6923.937
    learn_time_ms: 8762.645
    sample_throughput: 13672.937
    sample_time_ms: 4437.379
    update_time_ms: 30.636
  timestamp: 1602432186
  timesteps_since_restore: 0
  timesteps_total: 2366208
  training_iteration: 39
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     39 |          523.185 | 2366208 |  242.882 |              285.869 |               116.02 |            823.456 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3450.620595278823
    time_step_min: 3169
  date: 2020-10-11_16-03-20
  done: false
  episode_len_mean: 823.1758467328087
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 243.19889970522874
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2923
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7417478799819947
        entropy_coeff: 0.0001
        kl: 0.005580193921923637
        model: {}
        policy_loss: -0.014510815404355526
        total_loss: 12.874164962768555
        vf_explained_var: 0.9789665341377258
        vf_loss: 12.88819179534912
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.487499999999997
    gpu_util_percent0: 0.291875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12008566779531311
    mean_env_wait_ms: 0.658553620312004
    mean_inference_ms: 4.537043319707952
    mean_raw_obs_processing_ms: 0.2606554858331845
  time_since_restore: 536.526570558548
  time_this_iter_s: 13.34180212020874
  time_total_s: 536.526570558548
  timers:
    learn_throughput: 6909.967
    learn_time_ms: 8780.361
    sample_throughput: 13717.06
    sample_time_ms: 4423.105
    update_time_ms: 31.99
  timestamp: 1602432200
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 40
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     40 |          536.527 | 2426880 |  243.199 |              285.869 |               116.02 |            823.176 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3448.715856095936
    time_step_min: 3169
  date: 2020-10-11_16-03-33
  done: false
  episode_len_mean: 822.8854097268488
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 243.48749655112073
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 3002
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7339844465255737
        entropy_coeff: 0.0001
        kl: 0.006111154239624739
        model: {}
        policy_loss: -0.015608488768339156
        total_loss: 11.980018806457519
        vf_explained_var: 0.9802266359329224
        vf_loss: 11.995088958740235
    num_steps_sampled: 2487552
    num_steps_trained: 2487552
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.453333333333333
    gpu_util_percent0: 0.3173333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12002562127903713
    mean_env_wait_ms: 0.6586582582973108
    mean_inference_ms: 4.5325578877618105
    mean_raw_obs_processing_ms: 0.2605001259860025
  time_since_restore: 549.8374593257904
  time_this_iter_s: 13.310888767242432
  time_total_s: 549.8374593257904
  timers:
    learn_throughput: 6898.689
    learn_time_ms: 8794.714
    sample_throughput: 13723.492
    sample_time_ms: 4421.032
    update_time_ms: 31.755
  timestamp: 1602432213
  timesteps_since_restore: 0
  timesteps_total: 2487552
  training_iteration: 41
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     41 |          549.837 | 2487552 |  243.487 |              285.869 |               116.02 |            822.885 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3446.6886363636363
    time_step_min: 3169
  date: 2020-10-11_16-03-46
  done: false
  episode_len_mean: 822.4435064935064
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 243.7946510560146
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 3080
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7262521624565125
        entropy_coeff: 0.0001
        kl: 0.006154146790504455
        model: {}
        policy_loss: -0.015755283087491988
        total_loss: 9.44112319946289
        vf_explained_var: 0.9838287234306335
        vf_loss: 9.456335830688477
    num_steps_sampled: 2548224
    num_steps_trained: 2548224
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.040000000000003
    gpu_util_percent0: 0.2786666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11996818677997442
    mean_env_wait_ms: 0.658761393096635
    mean_inference_ms: 4.528281717882777
    mean_raw_obs_processing_ms: 0.26035232371452727
  time_since_restore: 562.987548828125
  time_this_iter_s: 13.150089502334595
  time_total_s: 562.987548828125
  timers:
    learn_throughput: 6917.615
    learn_time_ms: 8770.652
    sample_throughput: 13717.044
    sample_time_ms: 4423.11
    update_time_ms: 29.849
  timestamp: 1602432226
  timesteps_since_restore: 0
  timesteps_total: 2548224
  training_iteration: 42
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     42 |          562.988 | 2548224 |  243.795 |              285.869 |               116.02 |            822.444 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3444.746907706946
    time_step_min: 3165
  date: 2020-10-11_16-04-00
  done: false
  episode_len_mean: 822.0837297811609
  episode_reward_max: 286.47474747474723
  episode_reward_mean: 244.08885236763436
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 73
  episodes_total: 3153
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7186455488204956
        entropy_coeff: 0.0001
        kl: 0.006436989177018404
        model: {}
        policy_loss: -0.016508414037525655
        total_loss: 9.405962371826172
        vf_explained_var: 0.983354926109314
        vf_loss: 9.421898651123048
    num_steps_sampled: 2608896
    num_steps_trained: 2608896
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.81875
    gpu_util_percent0: 0.286875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11991691589410147
    mean_env_wait_ms: 0.6588630418483384
    mean_inference_ms: 4.524409380273655
    mean_raw_obs_processing_ms: 0.26021799573953674
  time_since_restore: 576.3550531864166
  time_this_iter_s: 13.367504358291626
  time_total_s: 576.3550531864166
  timers:
    learn_throughput: 6922.718
    learn_time_ms: 8764.188
    sample_throughput: 13678.815
    sample_time_ms: 4435.472
    update_time_ms: 30.861
  timestamp: 1602432240
  timesteps_since_restore: 0
  timesteps_total: 2608896
  training_iteration: 43
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     43 |          576.355 | 2608896 |  244.089 |              286.475 |               116.02 |            822.084 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3442.7816377171216
    time_step_min: 3165
  date: 2020-10-11_16-04-13
  done: false
  episode_len_mean: 821.6752481389578
  episode_reward_max: 286.47474747474723
  episode_reward_mean: 244.3866205479108
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 71
  episodes_total: 3224
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7102178335189819
        entropy_coeff: 0.0001
        kl: 0.006092663202434778
        model: {}
        policy_loss: -0.016193848475813864
        total_loss: 7.7973559379577635
        vf_explained_var: 0.9857897758483887
        vf_loss: 7.813011932373047
    num_steps_sampled: 2669568
    num_steps_trained: 2669568
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.013333333333335
    gpu_util_percent0: 0.2613333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1198664242616846
    mean_env_wait_ms: 0.6589631458590229
    mean_inference_ms: 4.520789945644346
    mean_raw_obs_processing_ms: 0.26009408393208483
  time_since_restore: 589.6252753734589
  time_this_iter_s: 13.270222187042236
  time_total_s: 589.6252753734589
  timers:
    learn_throughput: 6917.919
    learn_time_ms: 8770.267
    sample_throughput: 13676.552
    sample_time_ms: 4436.206
    update_time_ms: 30.756
  timestamp: 1602432253
  timesteps_since_restore: 0
  timesteps_total: 2669568
  training_iteration: 44
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | RUNNING  | 172.17.0.4:10986 |     44 |          589.625 | 2669568 |  244.387 |              286.475 |               116.02 |            821.675 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_0254c_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3441.1525629360026
    time_step_min: 3165
  date: 2020-10-11_16-04-27
  done: true
  episode_len_mean: 821.3342432514407
  episode_reward_max: 286.47474747474723
  episode_reward_mean: 244.6334500602016
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 73
  episodes_total: 3297
  experiment_id: 4e4d4af88c6a4e308baad1d66aa25375
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7077718257904053
        entropy_coeff: 0.0001
        kl: 0.006186609528958798
        model: {}
        policy_loss: -0.015196176012977958
        total_loss: 7.407955741882324
        vf_explained_var: 0.9867888689041138
        vf_loss: 7.422603893280029
    num_steps_sampled: 2730240
    num_steps_trained: 2730240
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.193333333333335
    gpu_util_percent0: 0.36866666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10986
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11981876939093743
    mean_env_wait_ms: 0.6590464095428158
    mean_inference_ms: 4.517161100822282
    mean_raw_obs_processing_ms: 0.2599740087722748
  time_since_restore: 602.8806293010712
  time_this_iter_s: 13.255353927612305
  time_total_s: 602.8806293010712
  timers:
    learn_throughput: 6912.729
    learn_time_ms: 8776.852
    sample_throughput: 13692.621
    sample_time_ms: 4431.0
    update_time_ms: 30.218
  timestamp: 1602432267
  timesteps_since_restore: 0
  timesteps_total: 2730240
  training_iteration: 45
  trial_id: 0254c_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | TERMINATED |       |     45 |          602.881 | 2730240 |  244.633 |              286.475 |               116.02 |            821.334 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.11 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_0254c_00000 | TERMINATED |       |     45 |          602.881 | 2730240 |  244.633 |              286.475 |               116.02 |            821.334 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Traceback (most recent call last):
  File "train.py", line 72, in <module>
    train_func()
  File "train.py", line 57, in train_func
    result = analysis.dataframe().to_dict('index')[0]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 89, in dataframe
    metric = self._validate_metric(metric)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 64, in _validate_metric
    raise ValueError(
ValueError: No `metric` has been passed and  `default_metric` has not been set. Please specify the `metric` parameter.
