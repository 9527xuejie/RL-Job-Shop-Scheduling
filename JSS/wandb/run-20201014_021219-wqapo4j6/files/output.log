2020-10-14 02:12:23,988	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_b391f_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=30672)[0m 2020-10-14 02:12:26,726	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=30622)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30622)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30557)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30557)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30659)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30659)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30661)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30661)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30626)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30626)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30617)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30617)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30675)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30675)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30648)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30648)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30668)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30668)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30646)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30646)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30662)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30662)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30613)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30613)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30641)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30641)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30620)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30620)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30625)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30625)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30644)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30644)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30614)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30614)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30629)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30629)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30550)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30550)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30658)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30658)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30643)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30643)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30615)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30615)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30671)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30671)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30633)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30633)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30606)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30606)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30640)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30640)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30549)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30549)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30618)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30618)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30542)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30542)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30610)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30610)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30571)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30571)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30621)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30621)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30566)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30566)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30570)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30570)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30616)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30616)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30632)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30632)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30654)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30654)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30666)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30666)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30561)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30561)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30623)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30623)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30559)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30559)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30656)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30656)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30573)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30573)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30653)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30653)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30546)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30546)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30635)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30635)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30579)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30579)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30569)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30569)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30619)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30619)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30607)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30607)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30601)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30601)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30624)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30624)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30551)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30551)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30650)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30650)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30547)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30547)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30572)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30572)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30605)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30605)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30554)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30554)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30541)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30541)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30552)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30552)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30555)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30555)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30558)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30558)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30543)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30543)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30652)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30652)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30609)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30609)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30611)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30611)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30574)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30574)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30553)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30553)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30540)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30540)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30649)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30649)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30600)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30600)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30603)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30603)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30578)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30578)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30556)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30556)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30545)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30545)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30563)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30563)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30631)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30631)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30577)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30577)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=30663)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=30663)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4040
    time_step_mean: 3708.8660714285716
    time_step_min: 3400
  date: 2020-10-14_02-13-00
  done: false
  episode_len_mean: 905.6582278481013
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 216.4606188466943
  episode_reward_min: 164.28282828282764
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1647747655709584
        entropy_coeff: 0.0005000000000000001
        kl: 0.00473203028862675
        model: {}
        policy_loss: -0.00807673400170946
        total_loss: 407.51378631591797
        vf_explained_var: 0.5518081784248352
        vf_loss: 407.52150472005206
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 27.366666666666667
    gpu_util_percent0: 0.32606060606060605
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5575757575757576
    vram_util_percent0: 0.08750757824224535
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16785528601669683
    mean_env_wait_ms: 1.170478996283922
    mean_inference_ms: 5.5052758882575
    mean_raw_obs_processing_ms: 0.44340177537777353
  time_since_restore: 28.628854990005493
  time_this_iter_s: 28.628854990005493
  time_total_s: 28.628854990005493
  timers:
    learn_throughput: 8171.362
    learn_time_ms: 19799.882
    sample_throughput: 18480.413
    sample_time_ms: 8754.783
    update_time_ms: 44.22
  timestamp: 1602641580
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      1 |          28.6289 | 161792 |  216.461 |               262.01 |              164.283 |            905.658 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3723.151851851852
    time_step_min: 3400
  date: 2020-10-14_02-13-28
  done: false
  episode_len_mean: 903.9620253164557
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 214.48708605037675
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1318883697191875
        entropy_coeff: 0.0005000000000000001
        kl: 0.009589768092458447
        model: {}
        policy_loss: -0.01046546475845389
        total_loss: 97.49119822184245
        vf_explained_var: 0.8148381114006042
        vf_loss: 97.50126775105794
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.090625
    gpu_util_percent0: 0.34375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.753125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1643964954356483
    mean_env_wait_ms: 1.170611543523788
    mean_inference_ms: 5.386932963825775
    mean_raw_obs_processing_ms: 0.437180185610682
  time_since_restore: 56.09865856170654
  time_this_iter_s: 27.46980357170105
  time_total_s: 56.09865856170654
  timers:
    learn_throughput: 8235.288
    learn_time_ms: 19646.187
    sample_throughput: 19440.093
    sample_time_ms: 8322.594
    update_time_ms: 40.692
  timestamp: 1602641608
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      2 |          56.0987 | 323584 |  214.487 |               262.01 |              131.859 |            903.962 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3720.32476635514
    time_step_min: 3400
  date: 2020-10-14_02-13-55
  done: false
  episode_len_mean: 899.9831223628692
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 214.78020713463707
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.12220632036527
        entropy_coeff: 0.0005000000000000001
        kl: 0.009560395032167435
        model: {}
        policy_loss: -0.011902896052864284
        total_loss: 49.42488066355387
        vf_explained_var: 0.8946843147277832
        vf_loss: 49.43638769785563
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.541935483870972
    gpu_util_percent0: 0.337741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16180355529143098
    mean_env_wait_ms: 1.1723319859809036
    mean_inference_ms: 5.25523569161911
    mean_raw_obs_processing_ms: 0.4311674780471061
  time_since_restore: 83.1092164516449
  time_this_iter_s: 27.010557889938354
  time_total_s: 83.1092164516449
  timers:
    learn_throughput: 8226.414
    learn_time_ms: 19667.378
    sample_throughput: 20344.179
    sample_time_ms: 7952.742
    update_time_ms: 39.581
  timestamp: 1602641635
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      3 |          83.1092 | 485376 |   214.78 |               262.01 |              131.859 |            899.983 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3716.0580204778157
    time_step_min: 3400
  date: 2020-10-14_02-14-22
  done: false
  episode_len_mean: 895.9810126582279
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 215.04304117120526
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1039417584737141
        entropy_coeff: 0.0005000000000000001
        kl: 0.00867797884469231
        model: {}
        policy_loss: -0.013174800493288785
        total_loss: 35.44446245829264
        vf_explained_var: 0.9264928698539734
        vf_loss: 35.45732275644938
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.715625
    gpu_util_percent0: 0.31156249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7687500000000003
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15984388157838175
    mean_env_wait_ms: 1.1738504629721933
    mean_inference_ms: 5.147390874951158
    mean_raw_obs_processing_ms: 0.42571354153951907
  time_since_restore: 109.99848651885986
  time_this_iter_s: 26.889270067214966
  time_total_s: 109.99848651885986
  timers:
    learn_throughput: 8230.803
    learn_time_ms: 19656.89
    sample_throughput: 20899.594
    sample_time_ms: 7741.395
    update_time_ms: 56.02
  timestamp: 1602641662
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      4 |          109.998 | 647168 |  215.043 |               262.01 |              131.859 |            895.981 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3714.5846774193546
    time_step_min: 3400
  date: 2020-10-14_02-14-48
  done: false
  episode_len_mean: 891.0253164556962
  episode_reward_max: 262.01010101010036
  episode_reward_mean: 215.17791842475344
  episode_reward_min: 131.85858585858549
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.082091599702835
        entropy_coeff: 0.0005000000000000001
        kl: 0.0076408466168989735
        model: {}
        policy_loss: -0.012829113693442196
        total_loss: 26.67149782180786
        vf_explained_var: 0.9462361335754395
        vf_loss: 26.68410348892212
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.836666666666666
    gpu_util_percent0: 0.3333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1583466060664108
    mean_env_wait_ms: 1.1755547544299036
    mean_inference_ms: 5.061270654898265
    mean_raw_obs_processing_ms: 0.4211292889071863
  time_since_restore: 136.55703043937683
  time_this_iter_s: 26.558543920516968
  time_total_s: 136.55703043937683
  timers:
    learn_throughput: 8245.374
    learn_time_ms: 19622.155
    sample_throughput: 21306.578
    sample_time_ms: 7593.524
    update_time_ms: 49.527
  timestamp: 1602641688
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      5 |          136.557 | 808960 |  215.178 |               262.01 |              131.859 |            891.025 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3709.934759358289
    time_step_min: 3400
  date: 2020-10-14_02-15-15
  done: false
  episode_len_mean: 885.1569826707441
  episode_reward_max: 266.2525252525247
  episode_reward_mean: 215.74500355234255
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 981
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0345047016938527
        entropy_coeff: 0.0005000000000000001
        kl: 0.008740813548987111
        model: {}
        policy_loss: -0.011801244584300244
        total_loss: 30.00030755996704
        vf_explained_var: 0.9570322036743164
        vf_loss: 30.011751174926758
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64193548387097
    gpu_util_percent0: 0.3274193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15694766165869248
    mean_env_wait_ms: 1.1781758344991005
    mean_inference_ms: 4.977451708047666
    mean_raw_obs_processing_ms: 0.41659008587698637
  time_since_restore: 163.0490210056305
  time_this_iter_s: 26.491990566253662
  time_total_s: 163.0490210056305
  timers:
    learn_throughput: 8259.736
    learn_time_ms: 19588.036
    sample_throughput: 21595.984
    sample_time_ms: 7491.763
    update_time_ms: 48.38
  timestamp: 1602641715
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 27.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      6 |          163.049 | 970752 |  215.745 |              266.253 |              126.556 |            885.157 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3703.7594417077175
    time_step_min: 3393
  date: 2020-10-14_02-15-42
  done: false
  episode_len_mean: 878.3599683544304
  episode_reward_max: 266.2525252525247
  episode_reward_mean: 216.81483346119379
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 283
  episodes_total: 1264
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0508220891157787
        entropy_coeff: 0.0005000000000000001
        kl: 0.007821178723437091
        model: {}
        policy_loss: -0.010197055215636889
        total_loss: 25.470292727152508
        vf_explained_var: 0.9604597687721252
        vf_loss: 25.480233669281006
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.919354838709673
    gpu_util_percent0: 0.2803225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15548321275781465
    mean_env_wait_ms: 1.181767552114967
    mean_inference_ms: 4.8897985470288035
    mean_raw_obs_processing_ms: 0.4120207302398123
  time_since_restore: 189.7619845867157
  time_this_iter_s: 26.712963581085205
  time_total_s: 189.7619845867157
  timers:
    learn_throughput: 8245.948
    learn_time_ms: 19620.788
    sample_throughput: 21879.55
    sample_time_ms: 7394.668
    update_time_ms: 46.433
  timestamp: 1602641742
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      7 |          189.762 | 1132544 |  216.815 |              266.253 |              126.556 |             878.36 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3693.2005813953488
    time_step_min: 3342
  date: 2020-10-14_02-16-08
  done: false
  episode_len_mean: 874.0970464135021
  episode_reward_max: 270.0404040404039
  episode_reward_mean: 218.1367685291732
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0273421903451283
        entropy_coeff: 0.0005000000000000001
        kl: 0.008351543258565167
        model: {}
        policy_loss: -0.014941895632849386
        total_loss: 16.31189688046773
        vf_explained_var: 0.966367244720459
        vf_loss: 16.326517740885418
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.036666666666658
    gpu_util_percent0: 0.36066666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15484914774894987
    mean_env_wait_ms: 1.1836846525337397
    mean_inference_ms: 4.85073004624818
    mean_raw_obs_processing_ms: 0.4099714911046557
  time_since_restore: 216.23638129234314
  time_this_iter_s: 26.47439670562744
  time_total_s: 216.23638129234314
  timers:
    learn_throughput: 8246.159
    learn_time_ms: 19620.286
    sample_throughput: 22112.865
    sample_time_ms: 7316.646
    update_time_ms: 45.1
  timestamp: 1602641768
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      8 |          216.236 | 1294336 |  218.137 |               270.04 |              126.556 |            874.097 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3685.148631029987
    time_step_min: 3342
  date: 2020-10-14_02-16-35
  done: false
  episode_len_mean: 869.632911392405
  episode_reward_max: 271.40404040403985
  episode_reward_mean: 219.33883135148915
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0084790935118992
        entropy_coeff: 0.0005000000000000001
        kl: 0.007280519581399858
        model: {}
        policy_loss: -0.0127268874590906
        total_loss: 15.402397950490316
        vf_explained_var: 0.9677316546440125
        vf_loss: 15.414900859196981
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14516129032258
    gpu_util_percent0: 0.3132258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15428850322912296
    mean_env_wait_ms: 1.1855874880000705
    mean_inference_ms: 4.816066390772739
    mean_raw_obs_processing_ms: 0.40811142569418823
  time_since_restore: 242.7293906211853
  time_this_iter_s: 26.493009328842163
  time_total_s: 242.7293906211853
  timers:
    learn_throughput: 8246.373
    learn_time_ms: 19619.778
    sample_throughput: 22287.733
    sample_time_ms: 7259.24
    update_time_ms: 42.48
  timestamp: 1602641795
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |      9 |          242.729 | 1456128 |  219.339 |              271.404 |              126.556 |            869.633 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3677.5730270906947
    time_step_min: 3342
  date: 2020-10-14_02-17-01
  done: false
  episode_len_mean: 865.266628440367
  episode_reward_max: 271.40404040403985
  episode_reward_mean: 220.30364424057046
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 164
  episodes_total: 1744
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9659438480933508
        entropy_coeff: 0.0005000000000000001
        kl: 0.00674354382014523
        model: {}
        policy_loss: -0.012388948060106486
        total_loss: 21.63013219833374
        vf_explained_var: 0.9617543816566467
        vf_loss: 21.642329851786297
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.360000000000007
    gpu_util_percent0: 0.3223333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15377661846030277
    mean_env_wait_ms: 1.187529238170763
    mean_inference_ms: 4.783943434983417
    mean_raw_obs_processing_ms: 0.40636194149648314
  time_since_restore: 269.20483016967773
  time_this_iter_s: 26.47543954849243
  time_total_s: 269.20483016967773
  timers:
    learn_throughput: 8246.141
    learn_time_ms: 19620.329
    sample_throughput: 22439.532
    sample_time_ms: 7210.133
    update_time_ms: 41.458
  timestamp: 1602641821
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     10 |          269.205 | 1617920 |  220.304 |              271.404 |              126.556 |            865.267 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3664.233416458853
    time_step_min: 3342
  date: 2020-10-14_02-17-28
  done: false
  episode_len_mean: 858.008288639688
  episode_reward_max: 272.61616161616115
  episode_reward_mean: 222.2659998325523
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 307
  episodes_total: 2051
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9442435453335444
        entropy_coeff: 0.0005000000000000001
        kl: 0.0063606204542641836
        model: {}
        policy_loss: -0.00961103243753314
        total_loss: 22.137783527374268
        vf_explained_var: 0.9693326950073242
        vf_loss: 22.147231101989746
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.861290322580647
    gpu_util_percent0: 0.30193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1529817266302679
    mean_env_wait_ms: 1.191080382600999
    mean_inference_ms: 4.734815712155124
    mean_raw_obs_processing_ms: 0.4037125660266316
  time_since_restore: 295.8744637966156
  time_this_iter_s: 26.669633626937866
  time_total_s: 295.8744637966156
  timers:
    learn_throughput: 8250.029
    learn_time_ms: 19611.083
    sample_throughput: 23040.231
    sample_time_ms: 7022.152
    update_time_ms: 40.457
  timestamp: 1602641848
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     11 |          295.874 | 1779712 |  222.266 |              272.616 |              126.556 |            858.008 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3657.9593721144965
    time_step_min: 3342
  date: 2020-10-14_02-17-55
  done: false
  episode_len_mean: 854.4254068716094
  episode_reward_max: 272.61616161616115
  episode_reward_mean: 223.03865051966278
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 161
  episodes_total: 2212
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9357274522384008
        entropy_coeff: 0.0005000000000000001
        kl: 0.006479084258899093
        model: {}
        policy_loss: -0.011161864347135028
        total_loss: 13.993338823318481
        vf_explained_var: 0.9730336666107178
        vf_loss: 14.004320621490479
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.34
    gpu_util_percent0: 0.3323333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15262827682461352
    mean_env_wait_ms: 1.1927916949972301
    mean_inference_ms: 4.712895567952311
    mean_raw_obs_processing_ms: 0.40251800615405237
  time_since_restore: 322.30917024612427
  time_this_iter_s: 26.434706449508667
  time_total_s: 322.30917024612427
  timers:
    learn_throughput: 8252.768
    learn_time_ms: 19604.575
    sample_throughput: 23365.578
    sample_time_ms: 6924.374
    update_time_ms: 41.073
  timestamp: 1602641875
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     12 |          322.309 | 1941504 |  223.039 |              272.616 |              126.556 |            854.425 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3652.6041308089502
    time_step_min: 3283
  date: 2020-10-14_02-18-21
  done: false
  episode_len_mean: 851.2143459915612
  episode_reward_max: 278.97979797979787
  episode_reward_mean: 223.793312875591
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 2370
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9198300937811533
        entropy_coeff: 0.0005000000000000001
        kl: 0.006205780586848657
        model: {}
        policy_loss: -0.011087854742072523
        total_loss: 15.218324422836304
        vf_explained_var: 0.9686550498008728
        vf_loss: 15.229251782099405
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.596774193548388
    gpu_util_percent0: 0.30387096774193556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15231639967065283
    mean_env_wait_ms: 1.194475861139225
    mean_inference_ms: 4.693354999059792
    mean_raw_obs_processing_ms: 0.40142111038388517
  time_since_restore: 349.1283586025238
  time_this_iter_s: 26.819188356399536
  time_total_s: 349.1283586025238
  timers:
    learn_throughput: 8253.601
    learn_time_ms: 19602.596
    sample_throughput: 23428.82
    sample_time_ms: 6905.683
    update_time_ms: 41.307
  timestamp: 1602641901
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     13 |          349.128 | 2103296 |  223.793 |               278.98 |              126.556 |            851.214 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3647.3826915442637
    time_step_min: 3283
  date: 2020-10-14_02-18-48
  done: false
  episode_len_mean: 847.929044834308
  episode_reward_max: 278.97979797979787
  episode_reward_mean: 224.61173134857304
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 2565
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8750834216674169
        entropy_coeff: 0.0005000000000000001
        kl: 0.006541201728396118
        model: {}
        policy_loss: -0.011567620793357491
        total_loss: 15.469610452651978
        vf_explained_var: 0.975344181060791
        vf_loss: 15.480961720148722
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.874193548387098
    gpu_util_percent0: 0.2725806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1519783171629409
    mean_env_wait_ms: 1.1965832289812657
    mean_inference_ms: 4.671467367651689
    mean_raw_obs_processing_ms: 0.4002121438445709
  time_since_restore: 375.6684112548828
  time_this_iter_s: 26.54005265235901
  time_total_s: 375.6684112548828
  timers:
    learn_throughput: 8254.882
    learn_time_ms: 19599.553
    sample_throughput: 23515.359
    sample_time_ms: 6880.269
    update_time_ms: 34.647
  timestamp: 1602641928
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     14 |          375.668 | 2265088 |  224.612 |               278.98 |              126.556 |            847.929 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3638.271459227468
    time_step_min: 3283
  date: 2020-10-14_02-19-15
  done: false
  episode_len_mean: 844.0601688951442
  episode_reward_max: 278.97979797979787
  episode_reward_mean: 225.9424576518169
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 277
  episodes_total: 2842
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.866677130262057
        entropy_coeff: 0.0005000000000000001
        kl: 0.006368907323728005
        model: {}
        policy_loss: -0.013498592539690435
        total_loss: 13.751517534255981
        vf_explained_var: 0.9780821800231934
        vf_loss: 13.764812390009562
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.283333333333342
    gpu_util_percent0: 0.36
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15153906632971484
    mean_env_wait_ms: 1.1991581901544945
    mean_inference_ms: 4.644391367102595
    mean_raw_obs_processing_ms: 0.39868364494549224
  time_since_restore: 402.0248100757599
  time_this_iter_s: 26.356398820877075
  time_total_s: 402.0248100757599
  timers:
    learn_throughput: 8254.427
    learn_time_ms: 19600.633
    sample_throughput: 23591.456
    sample_time_ms: 6858.076
    update_time_ms: 34.311
  timestamp: 1602641955
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     15 |          402.025 | 2426880 |  225.942 |               278.98 |              126.556 |             844.06 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3633.0091339648175
    time_step_min: 3278
  date: 2020-10-14_02-19-41
  done: false
  episode_len_mean: 842.4756828780813
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 226.62454996332374
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 160
  episodes_total: 3002
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8529908210039139
        entropy_coeff: 0.0005000000000000001
        kl: 0.006265266837241749
        model: {}
        policy_loss: -0.010987085717109343
        total_loss: 13.621409257253012
        vf_explained_var: 0.9727893471717834
        vf_loss: 13.632196267445883
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09666666666667
    gpu_util_percent0: 0.333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15131789903021517
    mean_env_wait_ms: 1.200547410042725
    mean_inference_ms: 4.630394675684626
    mean_raw_obs_processing_ms: 0.3979161497883508
  time_since_restore: 428.34447026252747
  time_this_iter_s: 26.319660186767578
  time_total_s: 428.34447026252747
  timers:
    learn_throughput: 8255.835
    learn_time_ms: 19597.291
    sample_throughput: 23642.038
    sample_time_ms: 6843.403
    update_time_ms: 34.317
  timestamp: 1602641981
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     16 |          428.344 | 2588672 |  226.625 |              279.737 |              126.556 |            842.476 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3627.756262042389
    time_step_min: 3278
  date: 2020-10-14_02-20-07
  done: false
  episode_len_mean: 840.5006329113924
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 227.3260772279756
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8390568047761917
        entropy_coeff: 0.0005000000000000001
        kl: 0.006382488917248945
        model: {}
        policy_loss: -0.010932213355166217
        total_loss: 14.478002707163492
        vf_explained_var: 0.9691622853279114
        vf_loss: 14.488715966542562
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.15806451612903
    gpu_util_percent0: 0.36129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15111685475213013
    mean_env_wait_ms: 1.2018815989888918
    mean_inference_ms: 4.617569762561681
    mean_raw_obs_processing_ms: 0.39719619131619555
  time_since_restore: 454.6406180858612
  time_this_iter_s: 26.29614782333374
  time_total_s: 454.6406180858612
  timers:
    learn_throughput: 8276.317
    learn_time_ms: 19548.791
    sample_throughput: 23629.035
    sample_time_ms: 6847.169
    update_time_ms: 35.743
  timestamp: 1602642007
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     17 |          454.641 | 2750464 |  227.326 |              279.737 |              126.556 |            840.501 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3621.189189189189
    time_step_min: 3278
  date: 2020-10-14_02-20-34
  done: false
  episode_len_mean: 838.0269557573981
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 228.16246851758098
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 253
  episodes_total: 3413
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8095408777395884
        entropy_coeff: 0.0005000000000000001
        kl: 0.006256838950018088
        model: {}
        policy_loss: -0.011800330537274325
        total_loss: 17.46291907628377
        vf_explained_var: 0.9745703339576721
        vf_loss: 17.474498589833576
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.713333333333342
    gpu_util_percent0: 0.289
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15083478774165596
    mean_env_wait_ms: 1.2039127871279753
    mean_inference_ms: 4.598907780664461
    mean_raw_obs_processing_ms: 0.39617926273041065
  time_since_restore: 480.90682315826416
  time_this_iter_s: 26.266205072402954
  time_total_s: 480.90682315826416
  timers:
    learn_throughput: 8285.906
    learn_time_ms: 19526.168
    sample_throughput: 23628.482
    sample_time_ms: 6847.33
    update_time_ms: 36.516
  timestamp: 1602642034
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     18 |          480.907 | 2912256 |  228.162 |              279.737 |              126.556 |            838.027 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3616.110119877335
    time_step_min: 3278
  date: 2020-10-14_02-21-00
  done: false
  episode_len_mean: 836.1951555188549
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 228.9022679311693
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 3633
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8085831900437673
        entropy_coeff: 0.0005000000000000001
        kl: 0.005845786615585287
        model: {}
        policy_loss: -0.011725900910581307
        total_loss: 13.433278640111288
        vf_explained_var: 0.9770469665527344
        vf_loss: 13.44482429822286
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.951612903225808
    gpu_util_percent0: 0.3293548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15059776579598969
    mean_env_wait_ms: 1.205477975884139
    mean_inference_ms: 4.584321689896124
    mean_raw_obs_processing_ms: 0.39535770486577304
  time_since_restore: 507.24067902565
  time_this_iter_s: 26.333855867385864
  time_total_s: 507.24067902565
  timers:
    learn_throughput: 8295.228
    learn_time_ms: 19504.227
    sample_throughput: 23614.232
    sample_time_ms: 6851.461
    update_time_ms: 38.132
  timestamp: 1602642060
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     19 |          507.241 | 3074048 |  228.902 |              279.737 |              126.556 |            836.195 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3612.632140950347
    time_step_min: 3278
  date: 2020-10-14_02-21-27
  done: false
  episode_len_mean: 835.0163502109705
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 229.41168808336496
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 159
  episodes_total: 3792
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8010916958252589
        entropy_coeff: 0.0005000000000000001
        kl: 0.005972905977008243
        model: {}
        policy_loss: -0.009757631652367612
        total_loss: 13.557638883590698
        vf_explained_var: 0.9725897908210754
        vf_loss: 13.56720002492269
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.163333333333338
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15044646807055465
    mean_env_wait_ms: 1.206544909509574
    mean_inference_ms: 4.5745442696268555
    mean_raw_obs_processing_ms: 0.39481516467148114
  time_since_restore: 533.5104994773865
  time_this_iter_s: 26.26982045173645
  time_total_s: 533.5104994773865
  timers:
    learn_throughput: 8303.247
    learn_time_ms: 19485.39
    sample_throughput: 23619.991
    sample_time_ms: 6849.791
    update_time_ms: 36.698
  timestamp: 1602642087
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     20 |           533.51 | 3235840 |  229.412 |              279.737 |              126.556 |            835.016 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3609.6734067059124
    time_step_min: 3278
  date: 2020-10-14_02-21-53
  done: false
  episode_len_mean: 834.0045535036681
  episode_reward_max: 279.73737373737356
  episode_reward_mean: 229.8466501595767
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 161
  episodes_total: 3953
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7751961300770441
        entropy_coeff: 0.0005000000000000001
        kl: 0.006770414882339537
        model: {}
        policy_loss: -0.010245077855264148
        total_loss: 13.407913049062094
        vf_explained_var: 0.9741201400756836
        vf_loss: 13.417869011561075
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.276666666666674
    gpu_util_percent0: 0.3020000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15030317421716202
    mean_env_wait_ms: 1.2075828680126945
    mean_inference_ms: 4.565216695590352
    mean_raw_obs_processing_ms: 0.39429096135450964
  time_since_restore: 559.7068326473236
  time_this_iter_s: 26.196333169937134
  time_total_s: 559.7068326473236
  timers:
    learn_throughput: 8318.298
    learn_time_ms: 19450.133
    sample_throughput: 23669.189
    sample_time_ms: 6835.553
    update_time_ms: 37.542
  timestamp: 1602642113
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     21 |          559.707 | 3397632 |  229.847 |              279.737 |              126.556 |            834.005 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3604.7112068965516
    time_step_min: 3212
  date: 2020-10-14_02-22-19
  done: false
  episode_len_mean: 832.7110374230223
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 230.63824651058155
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 269
  episodes_total: 4222
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7545836915572485
        entropy_coeff: 0.0005000000000000001
        kl: 0.005760976967091362
        model: {}
        policy_loss: -0.009425222723317953
        total_loss: 14.846301396687826
        vf_explained_var: 0.978369951248169
        vf_loss: 14.855527798334757
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.764516129032263
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15008621130171718
    mean_env_wait_ms: 1.2092374040094418
    mean_inference_ms: 4.551028731718384
    mean_raw_obs_processing_ms: 0.393510445831334
  time_since_restore: 585.9769661426544
  time_this_iter_s: 26.27013349533081
  time_total_s: 585.9769661426544
  timers:
    learn_throughput: 8322.3
    learn_time_ms: 19440.78
    sample_throughput: 23698.737
    sample_time_ms: 6827.031
    update_time_ms: 36.827
  timestamp: 1602642139
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     22 |          585.977 | 3559424 |  230.638 |              289.737 |              126.556 |            832.711 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3600.905643134567
    time_step_min: 3212
  date: 2020-10-14_02-22-46
  done: false
  episode_len_mean: 832.1042278996157
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 231.138554890985
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 201
  episodes_total: 4423
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.747990553577741
        entropy_coeff: 0.0005000000000000001
        kl: 0.005494295114961763
        model: {}
        policy_loss: -0.010693619498245729
        total_loss: 12.16190242767334
        vf_explained_var: 0.9790762066841125
        vf_loss: 12.172420819600424
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.14
    gpu_util_percent0: 0.3883333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14992645155898734
    mean_env_wait_ms: 1.210218928961977
    mean_inference_ms: 4.541056947989305
    mean_raw_obs_processing_ms: 0.3929427935236842
  time_since_restore: 612.2953429222107
  time_this_iter_s: 26.318376779556274
  time_total_s: 612.2953429222107
  timers:
    learn_throughput: 8335.535
    learn_time_ms: 19409.913
    sample_throughput: 23768.785
    sample_time_ms: 6806.911
    update_time_ms: 36.238
  timestamp: 1602642166
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     23 |          612.295 | 3721216 |  231.139 |              289.737 |              126.556 |            832.104 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3597.770061728395
    time_step_min: 3212
  date: 2020-10-14_02-23-12
  done: false
  episode_len_mean: 831.8441728502837
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 231.6387863797288
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 159
  episodes_total: 4582
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7424417585134506
        entropy_coeff: 0.0005000000000000001
        kl: 0.006447711571430166
        model: {}
        policy_loss: -0.01032647981082846
        total_loss: 9.735356251398722
        vf_explained_var: 0.9797441959381104
        vf_loss: 9.74540893236796
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.756666666666668
    gpu_util_percent0: 0.32399999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14981375135985692
    mean_env_wait_ms: 1.211001536255141
    mean_inference_ms: 4.533740773659267
    mean_raw_obs_processing_ms: 0.3925351770656534
  time_since_restore: 638.3203866481781
  time_this_iter_s: 26.025043725967407
  time_total_s: 638.3203866481781
  timers:
    learn_throughput: 8354.868
    learn_time_ms: 19364.998
    sample_throughput: 23792.131
    sample_time_ms: 6800.232
    update_time_ms: 34.941
  timestamp: 1602642192
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     24 |           638.32 | 3883008 |  231.639 |              289.737 |              126.556 |            831.844 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3594.882102575016
    time_step_min: 3212
  date: 2020-10-14_02-23-39
  done: false
  episode_len_mean: 831.540779768177
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 232.10848208108447
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 163
  episodes_total: 4745
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7228637884060541
        entropy_coeff: 0.0005000000000000001
        kl: 0.005764813938488563
        model: {}
        policy_loss: -0.010635872866259888
        total_loss: 10.174961964289347
        vf_explained_var: 0.9803434014320374
        vf_loss: 10.185382525126139
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.12666666666667
    gpu_util_percent0: 0.33566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14970508078670253
    mean_env_wait_ms: 1.2117602327206478
    mean_inference_ms: 4.526624677185767
    mean_raw_obs_processing_ms: 0.3921357587606369
  time_since_restore: 664.7608435153961
  time_this_iter_s: 26.440456867218018
  time_total_s: 664.7608435153961
  timers:
    learn_throughput: 8357.162
    learn_time_ms: 19359.683
    sample_throughput: 23750.156
    sample_time_ms: 6812.25
    update_time_ms: 36.463
  timestamp: 1602642219
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     25 |          664.761 | 4044800 |  232.108 |              289.737 |              126.556 |            831.541 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3590.406376109766
    time_step_min: 3212
  date: 2020-10-14_02-24-05
  done: false
  episode_len_mean: 830.890643742503
  episode_reward_max: 289.73737373737316
  episode_reward_mean: 232.71905985080681
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 257
  episodes_total: 5002
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7000831613938013
        entropy_coeff: 0.0005000000000000001
        kl: 0.005935194281240304
        model: {}
        policy_loss: -0.009080819242323438
        total_loss: 15.407442092895508
        vf_explained_var: 0.9771532416343689
        vf_loss: 15.416279872258505
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.500000000000004
    gpu_util_percent0: 0.30096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14954888305170821
    mean_env_wait_ms: 1.2129024591964084
    mean_inference_ms: 4.51620683406835
    mean_raw_obs_processing_ms: 0.39156002157114356
  time_since_restore: 691.1091873645782
  time_this_iter_s: 26.34834384918213
  time_total_s: 691.1091873645782
  timers:
    learn_throughput: 8354.353
    learn_time_ms: 19366.192
    sample_throughput: 23765.883
    sample_time_ms: 6807.742
    update_time_ms: 35.659
  timestamp: 1602642245
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     26 |          691.109 | 4206592 |  232.719 |              289.737 |              126.556 |            830.891 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3586.547706599574
    time_step_min: 3195
  date: 2020-10-14_02-24-32
  done: false
  episode_len_mean: 830.3422213696527
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 233.21004404296139
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 5213
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.69150643547376
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060880950186401606
        model: {}
        policy_loss: -0.01142608958374088
        total_loss: 10.433403253555298
        vf_explained_var: 0.981803834438324
        vf_loss: 10.444566488265991
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71
    gpu_util_percent0: 0.33266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14942013309963031
    mean_env_wait_ms: 1.2136638337679377
    mean_inference_ms: 4.508170903264685
    mean_raw_obs_processing_ms: 0.3911058368192237
  time_since_restore: 717.4844961166382
  time_this_iter_s: 26.375308752059937
  time_total_s: 717.4844961166382
  timers:
    learn_throughput: 8361.227
    learn_time_ms: 19350.271
    sample_throughput: 23681.737
    sample_time_ms: 6831.931
    update_time_ms: 35.019
  timestamp: 1602642272
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     27 |          717.484 | 4368384 |   233.21 |              292.313 |              126.556 |            830.342 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3584.3854675178372
    time_step_min: 3195
  date: 2020-10-14_02-24-58
  done: false
  episode_len_mean: 830.1734921816828
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 233.533121610746
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 159
  episodes_total: 5372
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6903532048066457
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056189208989962935
        model: {}
        policy_loss: -0.011535130285968384
        total_loss: 10.064338286717733
        vf_explained_var: 0.9800041317939758
        vf_loss: 10.075656414031982
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.916666666666668
    gpu_util_percent0: 0.2836666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14933346089706942
    mean_env_wait_ms: 1.2142464582958932
    mean_inference_ms: 4.502476439205163
    mean_raw_obs_processing_ms: 0.3907920461138063
  time_since_restore: 743.6581799983978
  time_this_iter_s: 26.173683881759644
  time_total_s: 743.6581799983978
  timers:
    learn_throughput: 8365.736
    learn_time_ms: 19339.84
    sample_throughput: 23680.839
    sample_time_ms: 6832.19
    update_time_ms: 34.779
  timestamp: 1602642298
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     28 |          743.658 | 4530176 |  233.533 |              292.313 |              126.556 |            830.173 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3582.0121995630006
    time_step_min: 3195
  date: 2020-10-14_02-25-25
  done: false
  episode_len_mean: 830.0234741784037
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 233.9449150223795
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 166
  episodes_total: 5538
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6738361716270447
        entropy_coeff: 0.0005000000000000001
        kl: 0.005155688966624439
        model: {}
        policy_loss: -0.010013353089258695
        total_loss: 9.565504391988119
        vf_explained_var: 0.9816081523895264
        vf_loss: 9.575339555740356
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.086666666666666
    gpu_util_percent0: 0.327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14924771630213732
    mean_env_wait_ms: 1.2148395022064067
    mean_inference_ms: 4.496786757897866
    mean_raw_obs_processing_ms: 0.39047735964398006
  time_since_restore: 769.9455268383026
  time_this_iter_s: 26.287346839904785
  time_total_s: 769.9455268383026
  timers:
    learn_throughput: 8363.406
    learn_time_ms: 19345.228
    sample_throughput: 23718.317
    sample_time_ms: 6821.395
    update_time_ms: 34.674
  timestamp: 1602642325
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     29 |          769.946 | 4691968 |  233.945 |              292.313 |              126.556 |            830.023 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3578.8517096999303
    time_step_min: 3195
  date: 2020-10-14_02-25-51
  done: false
  episode_len_mean: 829.8369678089305
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 234.43644125575554
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 240
  episodes_total: 5778
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6407706787188848
        entropy_coeff: 0.0005000000000000001
        kl: 0.005420890559131901
        model: {}
        policy_loss: -0.009607533769061169
        total_loss: 10.791298151016235
        vf_explained_var: 0.9838590025901794
        vf_loss: 10.800683975219727
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.896774193548385
    gpu_util_percent0: 0.32967741935483874
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14913324810438505
    mean_env_wait_ms: 1.2156074598192417
    mean_inference_ms: 4.489101454912886
    mean_raw_obs_processing_ms: 0.3900523268013142
  time_since_restore: 796.1621499061584
  time_this_iter_s: 26.216623067855835
  time_total_s: 796.1621499061584
  timers:
    learn_throughput: 8369.381
    learn_time_ms: 19331.419
    sample_throughput: 23700.462
    sample_time_ms: 6826.533
    update_time_ms: 37.23
  timestamp: 1602642351
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     30 |          796.162 | 4853760 |  234.436 |              292.313 |              126.556 |            829.837 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3575.6610112548296
    time_step_min: 3195
  date: 2020-10-14_02-26-17
  done: false
  episode_len_mean: 829.5845974329055
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 234.90505319910184
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 5999
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6333951304356257
        entropy_coeff: 0.0005000000000000001
        kl: 0.005369040261333187
        model: {}
        policy_loss: -0.011245574724550048
        total_loss: 8.461874643961588
        vf_explained_var: 0.9855525493621826
        vf_loss: 8.472900152206421
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.283333333333335
    gpu_util_percent0: 0.2873333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14902888726081498
    mean_env_wait_ms: 1.21620586619991
    mean_inference_ms: 4.482411223461508
    mean_raw_obs_processing_ms: 0.38967903672999027
  time_since_restore: 822.4339680671692
  time_this_iter_s: 26.271818161010742
  time_total_s: 822.4339680671692
  timers:
    learn_throughput: 8369.749
    learn_time_ms: 19330.569
    sample_throughput: 23673.429
    sample_time_ms: 6834.329
    update_time_ms: 37.454
  timestamp: 1602642377
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     31 |          822.434 | 5015552 |  234.905 |              292.313 |              126.556 |            829.585 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3572.9936232831915
    time_step_min: 3195
  date: 2020-10-14_02-26-44
  done: false
  episode_len_mean: 829.3567023693606
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 235.2598084053777
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 163
  episodes_total: 6162
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6266387303670248
        entropy_coeff: 0.0005000000000000001
        kl: 0.005494208540767431
        model: {}
        policy_loss: -0.011165471594722476
        total_loss: 7.235161264737447
        vf_explained_var: 0.9854392409324646
        vf_loss: 7.246090531349182
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49666666666667
    gpu_util_percent0: 0.316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14895594561821068
    mean_env_wait_ms: 1.2166328610131996
    mean_inference_ms: 4.477683986974228
    mean_raw_obs_processing_ms: 0.3894157555559595
  time_since_restore: 848.6910495758057
  time_this_iter_s: 26.257081508636475
  time_total_s: 848.6910495758057
  timers:
    learn_throughput: 8368.473
    learn_time_ms: 19333.516
    sample_throughput: 23690.941
    sample_time_ms: 6829.277
    update_time_ms: 37.726
  timestamp: 1602642404
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     32 |          848.691 | 5177344 |   235.26 |              292.313 |              126.556 |            829.357 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3570.6457106477797
    time_step_min: 3195
  date: 2020-10-14_02-27-11
  done: false
  episode_len_mean: 829.0303365460578
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 235.6446452197752
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 167
  episodes_total: 6329
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6156158596277237
        entropy_coeff: 0.0005000000000000001
        kl: 0.005225458842081328
        model: {}
        policy_loss: -0.01106348225827484
        total_loss: 9.687271992365519
        vf_explained_var: 0.9811480641365051
        vf_loss: 9.698120911916098
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.651612903225807
    gpu_util_percent0: 0.3519354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488859027924325
    mean_env_wait_ms: 1.2170724206145376
    mean_inference_ms: 4.473047131473126
    mean_raw_obs_processing_ms: 0.38915826674679016
  time_since_restore: 875.2617998123169
  time_this_iter_s: 26.57075023651123
  time_total_s: 875.2617998123169
  timers:
    learn_throughput: 8359.89
    learn_time_ms: 19353.365
    sample_throughput: 23673.129
    sample_time_ms: 6834.415
    update_time_ms: 38.151
  timestamp: 1602642431
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     33 |          875.262 | 5339136 |  235.645 |              292.313 |              126.556 |             829.03 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3567.117845634494
    time_step_min: 3195
  date: 2020-10-14_02-27-37
  done: false
  episode_len_mean: 828.7211641017827
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 236.1450586929785
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 234
  episodes_total: 6563
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5944310575723648
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047600786201655865
        model: {}
        policy_loss: -0.009058705977319429
        total_loss: 11.138667662938436
        vf_explained_var: 0.982389509677887
        vf_loss: 11.147547562917074
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.10666666666667
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14879568159514653
    mean_env_wait_ms: 1.2176369308093886
    mean_inference_ms: 4.466986742132495
    mean_raw_obs_processing_ms: 0.3888284666325231
  time_since_restore: 901.5183033943176
  time_this_iter_s: 26.256503582000732
  time_total_s: 901.5183033943176
  timers:
    learn_throughput: 8347.903
    learn_time_ms: 19381.154
    sample_throughput: 23697.461
    sample_time_ms: 6827.398
    update_time_ms: 39.951
  timestamp: 1602642457
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     34 |          901.518 | 5500928 |  236.145 |              292.313 |              126.556 |            828.721 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3563.8186267240103
    time_step_min: 3195
  date: 2020-10-14_02-28-03
  done: false
  episode_len_mean: 828.4669318014435
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 236.62070104491633
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 226
  episodes_total: 6789
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.58115021387736
        entropy_coeff: 0.0005000000000000001
        kl: 0.005653778595539431
        model: {}
        policy_loss: -0.01025580000714399
        total_loss: 9.081565141677856
        vf_explained_var: 0.9846951365470886
        vf_loss: 9.09182890256246
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.34666666666666
    gpu_util_percent0: 0.28800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14870592685066425
    mean_env_wait_ms: 1.2181049392652596
    mean_inference_ms: 4.4613297042141955
    mean_raw_obs_processing_ms: 0.3885124101156429
  time_since_restore: 927.6815769672394
  time_this_iter_s: 26.163273572921753
  time_total_s: 927.6815769672394
  timers:
    learn_throughput: 8356.918
    learn_time_ms: 19360.246
    sample_throughput: 23721.508
    sample_time_ms: 6820.477
    update_time_ms: 39.248
  timestamp: 1602642483
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     35 |          927.682 | 5662720 |  236.621 |              292.313 |              126.556 |            828.467 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3561.5114393281206
    time_step_min: 3195
  date: 2020-10-14_02-28-30
  done: false
  episode_len_mean: 828.2651035673188
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 236.97882594646086
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 163
  episodes_total: 6952
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5712917496760687
        entropy_coeff: 0.0005000000000000001
        kl: 0.006062560287925105
        model: {}
        policy_loss: -0.009961116263487687
        total_loss: 6.571681658426921
        vf_explained_var: 0.9867879748344421
        vf_loss: 6.581625501314799
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.0
    gpu_util_percent0: 0.3133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14864597007893993
    mean_env_wait_ms: 1.2184257655103143
    mean_inference_ms: 4.457425721582168
    mean_raw_obs_processing_ms: 0.3882966506666507
  time_since_restore: 953.847571849823
  time_this_iter_s: 26.165994882583618
  time_total_s: 953.847571849823
  timers:
    learn_throughput: 8365.446
    learn_time_ms: 19340.511
    sample_throughput: 23718.651
    sample_time_ms: 6821.299
    update_time_ms: 39.825
  timestamp: 1602642510
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     36 |          953.848 | 5824512 |  236.979 |              292.313 |              126.556 |            828.265 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3558.5696309910927
    time_step_min: 3195
  date: 2020-10-14_02-28-56
  done: false
  episode_len_mean: 827.9617923865711
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 237.40879932915294
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 167
  episodes_total: 7119
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5635714183251063
        entropy_coeff: 0.0005000000000000001
        kl: 0.005341902802077432
        model: {}
        policy_loss: -0.011165030511619989
        total_loss: 7.088760217030843
        vf_explained_var: 0.9857079982757568
        vf_loss: 7.099939902623494
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50645161290323
    gpu_util_percent0: 0.38387096774193546
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14858838326591858
    mean_env_wait_ms: 1.2187624681052416
    mean_inference_ms: 4.453618445308142
    mean_raw_obs_processing_ms: 0.3880883797537124
  time_since_restore: 980.3337180614471
  time_this_iter_s: 26.486146211624146
  time_total_s: 980.3337180614471
  timers:
    learn_throughput: 8354.536
    learn_time_ms: 19365.768
    sample_throughput: 23765.563
    sample_time_ms: 6807.834
    update_time_ms: 38.044
  timestamp: 1602642536
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     37 |          980.334 | 5986304 |  237.409 |              292.313 |              126.556 |            827.962 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3554.3725731473887
    time_step_min: 3195
  date: 2020-10-14_02-29-23
  done: false
  episode_len_mean: 827.4813858695652
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 238.01187143170804
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 241
  episodes_total: 7360
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5369780013958613
        entropy_coeff: 0.0005000000000000001
        kl: 0.005353876428368191
        model: {}
        policy_loss: -0.007315323096311961
        total_loss: 10.488156159718832
        vf_explained_var: 0.9829527735710144
        vf_loss: 10.495472351710001
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.930000000000003
    gpu_util_percent0: 0.315
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485106844910034
    mean_env_wait_ms: 1.2192088787431028
    mean_inference_ms: 4.4483403189058945
    mean_raw_obs_processing_ms: 0.3878030860963005
  time_since_restore: 1006.6847407817841
  time_this_iter_s: 26.351022720336914
  time_total_s: 1006.6847407817841
  timers:
    learn_throughput: 8347.357
    learn_time_ms: 19382.422
    sample_throughput: 23762.701
    sample_time_ms: 6808.654
    update_time_ms: 37.499
  timestamp: 1602642563
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     38 |          1006.68 | 6148096 |  238.012 |              292.313 |              126.556 |            827.481 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3550.4837425348373
    time_step_min: 3195
  date: 2020-10-14_02-29-49
  done: false
  episode_len_mean: 826.9894473024667
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 238.60389810251274
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 7581
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5339523702859879
        entropy_coeff: 0.0005000000000000001
        kl: 0.005730630713514984
        model: {}
        policy_loss: -0.009004498375967765
        total_loss: 8.086578289667765
        vf_explained_var: 0.9852219223976135
        vf_loss: 8.095563252766928
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.770967741935486
    gpu_util_percent0: 0.29838709677419356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14843835826096466
    mean_env_wait_ms: 1.2195764384548011
    mean_inference_ms: 4.4437580340316645
    mean_raw_obs_processing_ms: 0.3875506369505688
  time_since_restore: 1033.1511726379395
  time_this_iter_s: 26.466431856155396
  time_total_s: 1033.1511726379395
  timers:
    learn_throughput: 8346.609
    learn_time_ms: 19384.16
    sample_throughput: 23711.954
    sample_time_ms: 6823.225
    update_time_ms: 37.764
  timestamp: 1602642589
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     39 |          1033.15 | 6309888 |  238.604 |              292.313 |              126.556 |            826.989 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3547.675285862786
    time_step_min: 3195
  date: 2020-10-14_02-30-16
  done: false
  episode_len_mean: 826.5464996125032
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 239.03030955381737
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 161
  episodes_total: 7742
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5321356455485026
        entropy_coeff: 0.0005000000000000001
        kl: 0.00547590844022731
        model: {}
        policy_loss: -0.011284213736265277
        total_loss: 8.507766087849935
        vf_explained_var: 0.9816858768463135
        vf_loss: 8.519042531649271
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.470000000000002
    gpu_util_percent0: 0.31133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483889520801433
    mean_env_wait_ms: 1.2198399472997563
    mean_inference_ms: 4.440523670045289
    mean_raw_obs_processing_ms: 0.3873757538100296
  time_since_restore: 1059.5533356666565
  time_this_iter_s: 26.40216302871704
  time_total_s: 1059.5533356666565
  timers:
    learn_throughput: 8339.031
    learn_time_ms: 19401.774
    sample_throughput: 23709.207
    sample_time_ms: 6824.016
    update_time_ms: 37.201
  timestamp: 1602642616
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     40 |          1059.55 | 6471680 |   239.03 |              292.313 |              126.556 |            826.546 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3544.3294834369844
    time_step_min: 3195
  date: 2020-10-14_02-30-43
  done: false
  episode_len_mean: 826.0644794952682
  episode_reward_max: 292.31313131313107
  episode_reward_mean: 239.5284262180158
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 183
  episodes_total: 7925
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.52472884953022
        entropy_coeff: 0.0005000000000000001
        kl: 0.004711794084869325
        model: {}
        policy_loss: -0.010675423662178218
        total_loss: 8.3647092183431
        vf_explained_var: 0.9835845828056335
        vf_loss: 8.37541139125824
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56451612903226
    gpu_util_percent0: 0.28516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148336675760588
    mean_env_wait_ms: 1.2201486268889248
    mean_inference_ms: 4.436992221258814
    mean_raw_obs_processing_ms: 0.38718442375509765
  time_since_restore: 1086.0807995796204
  time_this_iter_s: 26.527463912963867
  time_total_s: 1086.0807995796204
  timers:
    learn_throughput: 8329.039
    learn_time_ms: 19425.05
    sample_throughput: 23697.415
    sample_time_ms: 6827.411
    update_time_ms: 35.151
  timestamp: 1602642643
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     41 |          1086.08 | 6633472 |  239.528 |              292.313 |              126.556 |            826.064 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3540.2243944423954
    time_step_min: 3192
  date: 2020-10-14_02-31-09
  done: false
  episode_len_mean: 825.4166768553613
  episode_reward_max: 292.76767676767633
  episode_reward_mean: 240.2004221207054
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 254
  episodes_total: 8179
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5037064626812935
        entropy_coeff: 0.0005000000000000001
        kl: 0.005376056535169482
        model: {}
        policy_loss: -0.009181031452802321
        total_loss: 8.43454376856486
        vf_explained_var: 0.9857959747314453
        vf_loss: 8.4438423315684
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.026666666666667
    gpu_util_percent0: 0.3433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826791069474193
    mean_env_wait_ms: 1.2205371431555472
    mean_inference_ms: 4.432331355836782
    mean_raw_obs_processing_ms: 0.38693366276775915
  time_since_restore: 1112.3881754875183
  time_this_iter_s: 26.30737590789795
  time_total_s: 1112.3881754875183
  timers:
    learn_throughput: 8327.885
    learn_time_ms: 19427.741
    sample_throughput: 23693.71
    sample_time_ms: 6828.479
    update_time_ms: 35.26
  timestamp: 1602642669
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     42 |          1112.39 | 6795264 |    240.2 |              292.768 |              126.556 |            825.417 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3536.559438040346
    time_step_min: 3192
  date: 2020-10-14_02-31-36
  done: false
  episode_len_mean: 825.0002388344877
  episode_reward_max: 292.76767676767633
  episode_reward_mean: 240.70895967074583
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 195
  episodes_total: 8374
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.49372132619222003
        entropy_coeff: 0.0005000000000000001
        kl: 0.00568037914733092
        model: {}
        policy_loss: -0.008380183891858906
        total_loss: 7.80752154191335
        vf_explained_var: 0.9841473698616028
        vf_loss: 7.816006541252136
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.81935483870968
    gpu_util_percent0: 0.28838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14821360530444724
    mean_env_wait_ms: 1.2208112616319526
    mean_inference_ms: 4.4288827548039365
    mean_raw_obs_processing_ms: 0.38674661162478946
  time_since_restore: 1138.8163573741913
  time_this_iter_s: 26.428181886672974
  time_total_s: 1138.8163573741913
  timers:
    learn_throughput: 8332.265
    learn_time_ms: 19417.53
    sample_throughput: 23706.745
    sample_time_ms: 6824.724
    update_time_ms: 34.532
  timestamp: 1602642696
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     43 |          1138.82 | 6957056 |  240.709 |              292.768 |              126.556 |                825 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3533.6711828463713
    time_step_min: 3192
  date: 2020-10-14_02-32-03
  done: false
  episode_len_mean: 824.5495664401219
  episode_reward_max: 292.76767676767633
  episode_reward_mean: 241.11308302144926
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 160
  episodes_total: 8534
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.49470652639865875
        entropy_coeff: 0.0005000000000000001
        kl: 0.005724602417709927
        model: {}
        policy_loss: -0.010496073344256729
        total_loss: 7.473387956619263
        vf_explained_var: 0.9833481311798096
        vf_loss: 7.483988404273987
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.977419354838716
    gpu_util_percent0: 0.37064516129032266
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14817253883283313
    mean_env_wait_ms: 1.2210472206826295
    mean_inference_ms: 4.426172217346893
    mean_raw_obs_processing_ms: 0.386600467838292
  time_since_restore: 1165.2997829914093
  time_this_iter_s: 26.483425617218018
  time_total_s: 1165.2997829914093
  timers:
    learn_throughput: 8330.928
    learn_time_ms: 19420.646
    sample_throughput: 23644.18
    sample_time_ms: 6842.783
    update_time_ms: 34.021
  timestamp: 1602642723
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     44 |           1165.3 | 7118848 |  241.113 |              292.768 |              126.556 |             824.55 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3530.374583093732
    time_step_min: 3186
  date: 2020-10-14_02-32-29
  done: false
  episode_len_mean: 824.0272280059489
  episode_reward_max: 293.6767676767673
  episode_reward_mean: 241.63109645823258
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 8741
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.47812945147355396
        entropy_coeff: 0.0005000000000000001
        kl: 0.005789765544856588
        model: {}
        policy_loss: -0.010132045989545682
        total_loss: 7.940152247746785
        vf_explained_var: 0.9854769110679626
        vf_loss: 7.950378696123759
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.019999999999996
    gpu_util_percent0: 0.35900000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481235262504829
    mean_env_wait_ms: 1.221335555503909
    mean_inference_ms: 4.4227501401885165
    mean_raw_obs_processing_ms: 0.3864163053920692
  time_since_restore: 1191.5634644031525
  time_this_iter_s: 26.263681411743164
  time_total_s: 1191.5634644031525
  timers:
    learn_throughput: 8326.386
    learn_time_ms: 19431.24
    sample_throughput: 23653.157
    sample_time_ms: 6840.186
    update_time_ms: 34.772
  timestamp: 1602642749
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     45 |          1191.56 | 7280640 |  241.631 |              293.677 |              126.556 |            824.027 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3525.8327931150106
    time_step_min: 3186
  date: 2020-10-14_02-32-56
  done: false
  episode_len_mean: 823.4236628488825
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 242.30384799849904
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 252
  episodes_total: 8993
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4603457028667132
        entropy_coeff: 0.0005000000000000001
        kl: 0.005457567555519442
        model: {}
        policy_loss: -0.010726391825301107
        total_loss: 7.70986278851827
        vf_explained_var: 0.9865714907646179
        vf_loss: 7.720682978630066
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92333333333334
    gpu_util_percent0: 0.3106666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806231654129137
    mean_env_wait_ms: 1.2216549793013587
    mean_inference_ms: 4.4188487422805585
    mean_raw_obs_processing_ms: 0.3862090617649821
  time_since_restore: 1217.914843082428
  time_this_iter_s: 26.351378679275513
  time_total_s: 1217.914843082428
  timers:
    learn_throughput: 8321.251
    learn_time_ms: 19443.229
    sample_throughput: 23633.569
    sample_time_ms: 6845.856
    update_time_ms: 34.354
  timestamp: 1602642776
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     46 |          1217.91 | 7442432 |  242.304 |              298.525 |              126.556 |            823.424 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3522.869159903488
    time_step_min: 3186
  date: 2020-10-14_02-33-22
  done: false
  episode_len_mean: 822.9783937145352
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 242.75221772504585
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 171
  episodes_total: 9164
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.46012789259354275
        entropy_coeff: 0.0005000000000000001
        kl: 0.005554196114341418
        model: {}
        policy_loss: -0.009548212585893149
        total_loss: 6.0206065972646075
        vf_explained_var: 0.9863595962524414
        vf_loss: 6.030245979626973
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.212903225806457
    gpu_util_percent0: 0.30419354838709683
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14802210302063992
    mean_env_wait_ms: 1.2218596897992104
    mean_inference_ms: 4.416230816865145
    mean_raw_obs_processing_ms: 0.3860677102608848
  time_since_restore: 1244.361169576645
  time_this_iter_s: 26.44632649421692
  time_total_s: 1244.361169576645
  timers:
    learn_throughput: 8321.721
    learn_time_ms: 19442.132
    sample_throughput: 23654.579
    sample_time_ms: 6839.775
    update_time_ms: 36.194
  timestamp: 1602642802
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     47 |          1244.36 | 7604224 |  242.752 |              298.525 |              126.556 |            822.978 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3520.020225927918
    time_step_min: 3186
  date: 2020-10-14_02-33-49
  done: false
  episode_len_mean: 822.4426720907826
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 243.20301721853986
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 177
  episodes_total: 9341
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.46006551136573154
        entropy_coeff: 0.0005000000000000001
        kl: 0.005484122200869024
        model: {}
        policy_loss: -0.010118320487284413
        total_loss: 6.799661954243978
        vf_explained_var: 0.9850449562072754
        vf_loss: 6.8098730246226
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.15666666666667
    gpu_util_percent0: 0.2896666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14798429047787234
    mean_env_wait_ms: 1.2220990443201512
    mean_inference_ms: 4.413668800958503
    mean_raw_obs_processing_ms: 0.38593002159583734
  time_since_restore: 1270.6480739116669
  time_this_iter_s: 26.286904335021973
  time_total_s: 1270.6480739116669
  timers:
    learn_throughput: 8326.18
    learn_time_ms: 19431.72
    sample_throughput: 23645.041
    sample_time_ms: 6842.534
    update_time_ms: 36.882
  timestamp: 1602642829
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     48 |          1270.65 | 7766016 |  243.203 |              298.525 |              126.556 |            822.443 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3515.955126860977
    time_step_min: 3186
  date: 2020-10-14_02-34-15
  done: false
  episode_len_mean: 821.7557387312187
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 243.8023557781484
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 243
  episodes_total: 9584
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.43939946343501407
        entropy_coeff: 0.0005000000000000001
        kl: 0.005502724554389715
        model: {}
        policy_loss: -0.010579032047341267
        total_loss: 8.257195512453714
        vf_explained_var: 0.9856317043304443
        vf_loss: 8.267856558163961
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.729032258064517
    gpu_util_percent0: 0.3403225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76774193548387
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14793423970654893
    mean_env_wait_ms: 1.2223846448368556
    mean_inference_ms: 4.410254760081338
    mean_raw_obs_processing_ms: 0.3857553651203751
  time_since_restore: 1297.0062172412872
  time_this_iter_s: 26.35814332962036
  time_total_s: 1297.0062172412872
  timers:
    learn_throughput: 8333.535
    learn_time_ms: 19414.57
    sample_throughput: 23630.601
    sample_time_ms: 6846.715
    update_time_ms: 38.314
  timestamp: 1602642855
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     49 |          1297.01 | 7927808 |  243.802 |              298.525 |              126.556 |            821.756 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3512.627205580632
    time_step_min: 3186
  date: 2020-10-14_02-34-42
  done: false
  episode_len_mean: 821.2432101286502
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 244.3643562436698
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 210
  episodes_total: 9794
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4198617140452067
        entropy_coeff: 0.0005000000000000001
        kl: 0.004976712826949854
        model: {}
        policy_loss: -0.010781078589692092
        total_loss: 6.021040320396423
        vf_explained_var: 0.9875360131263733
        vf_loss: 6.031906882921855
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.969999999999995
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788788285415747
    mean_env_wait_ms: 1.2226025437847985
    mean_inference_ms: 4.407368219560803
    mean_raw_obs_processing_ms: 0.3855982453666078
  time_since_restore: 1323.4360356330872
  time_this_iter_s: 26.429818391799927
  time_total_s: 1323.4360356330872
  timers:
    learn_throughput: 8334.879
    learn_time_ms: 19411.441
    sample_throughput: 23608.721
    sample_time_ms: 6853.061
    update_time_ms: 36.445
  timestamp: 1602642882
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     50 |          1323.44 | 8089600 |  244.364 |              298.525 |              126.556 |            821.243 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3509.6742684157416
    time_step_min: 3186
  date: 2020-10-14_02-35-09
  done: false
  episode_len_mean: 820.8204098031338
  episode_reward_max: 298.5252525252519
  episode_reward_mean: 244.8066634606406
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 162
  episodes_total: 9956
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.428066464761893
        entropy_coeff: 0.0005000000000000001
        kl: 0.005687459566009541
        model: {}
        policy_loss: -0.011090482973183194
        total_loss: 5.234896739323934
        vf_explained_var: 0.987224280834198
        vf_loss: 5.246130108833313
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.735483870967748
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478553797858275
    mean_env_wait_ms: 1.2227904433461716
    mean_inference_ms: 4.405243182051271
    mean_raw_obs_processing_ms: 0.3854872542400093
  time_since_restore: 1349.9193694591522
  time_this_iter_s: 26.483333826065063
  time_total_s: 1349.9193694591522
  timers:
    learn_throughput: 8339.557
    learn_time_ms: 19400.552
    sample_throughput: 23593.636
    sample_time_ms: 6857.442
    update_time_ms: 37.436
  timestamp: 1602642909
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     51 |          1349.92 | 8251392 |  244.807 |              298.525 |              126.556 |             820.82 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3506.0282692497776
    time_step_min: 3157
  date: 2020-10-14_02-35-35
  done: false
  episode_len_mean: 820.325396044475
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 245.36338192512514
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 207
  episodes_total: 10163
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4246461093425751
        entropy_coeff: 0.0005000000000000001
        kl: 0.005070816182220976
        model: {}
        policy_loss: -0.009410695323216109
        total_loss: 6.691620945930481
        vf_explained_var: 0.9863157272338867
        vf_loss: 6.701180577278137
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.776666666666667
    gpu_util_percent0: 0.37066666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14781724900371526
    mean_env_wait_ms: 1.2230208466800776
    mean_inference_ms: 4.402609938570226
    mean_raw_obs_processing_ms: 0.3853482656331852
  time_since_restore: 1376.223308801651
  time_this_iter_s: 26.30393934249878
  time_total_s: 1376.223308801651
  timers:
    learn_throughput: 8338.411
    learn_time_ms: 19403.218
    sample_throughput: 23598.134
    sample_time_ms: 6856.135
    update_time_ms: 35.471
  timestamp: 1602642935
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     52 |          1376.22 | 8413184 |  245.363 |              300.646 |              126.556 |            820.325 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3501.8771100607696
    time_step_min: 3157
  date: 2020-10-14_02-36-02
  done: false
  episode_len_mean: 819.7823874003649
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 245.9991997183005
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 250
  episodes_total: 10413
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4001444876194
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047837618427972
        model: {}
        policy_loss: -0.0097249773874258
        total_loss: 6.271246870358785
        vf_explained_var: 0.9883804321289062
        vf_loss: 6.28111207485199
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.316129032258065
    gpu_util_percent0: 0.3467741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777030018220144
    mean_env_wait_ms: 1.2232656434085556
    mean_inference_ms: 4.399542816367664
    mean_raw_obs_processing_ms: 0.38518873347726595
  time_since_restore: 1402.6257631778717
  time_this_iter_s: 26.402454376220703
  time_total_s: 1402.6257631778717
  timers:
    learn_throughput: 8342.163
    learn_time_ms: 19394.49
    sample_throughput: 23588.953
    sample_time_ms: 6858.804
    update_time_ms: 36.095
  timestamp: 1602642962
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     53 |          1402.63 | 8574976 |  245.999 |              300.646 |              126.556 |            819.782 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3499.0690702087286
    time_step_min: 3157
  date: 2020-10-14_02-36-28
  done: false
  episode_len_mean: 819.409125259777
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 246.4077617283735
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 173
  episodes_total: 10586
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.39603111147880554
        entropy_coeff: 0.0005000000000000001
        kl: 0.005139228499804934
        model: {}
        policy_loss: -0.009373984765261412
        total_loss: 6.07805069287618
        vf_explained_var: 0.986314594745636
        vf_loss: 6.087590495745341
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.800000000000004
    gpu_util_percent0: 0.3206666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14773709895494797
    mean_env_wait_ms: 1.2234269701295333
    mean_inference_ms: 4.397474735951126
    mean_raw_obs_processing_ms: 0.38507903960977014
  time_since_restore: 1428.9778237342834
  time_this_iter_s: 26.352060556411743
  time_total_s: 1428.9778237342834
  timers:
    learn_throughput: 8340.491
    learn_time_ms: 19398.378
    sample_throughput: 23647.311
    sample_time_ms: 6841.877
    update_time_ms: 35.92
  timestamp: 1602642988
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     54 |          1428.98 | 8736768 |  246.408 |              300.646 |              126.556 |            819.409 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3496.3686076894364
    time_step_min: 3157
  date: 2020-10-14_02-36-55
  done: false
  episode_len_mean: 819.0632782010779
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 246.8354826841164
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 176
  episodes_total: 10762
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.40235375116268796
        entropy_coeff: 0.0005000000000000001
        kl: 0.005396728714307149
        model: {}
        policy_loss: -0.009094168592127971
        total_loss: 7.081272840499878
        vf_explained_var: 0.9844600558280945
        vf_loss: 7.090534528096517
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.870967741935488
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477062139877609
    mean_env_wait_ms: 1.2236044788459033
    mean_inference_ms: 4.395450329614387
    mean_raw_obs_processing_ms: 0.3849717014724248
  time_since_restore: 1455.3343787193298
  time_this_iter_s: 26.356554985046387
  time_total_s: 1455.3343787193298
  timers:
    learn_throughput: 8340.264
    learn_time_ms: 19398.906
    sample_throughput: 23647.579
    sample_time_ms: 6841.8
    update_time_ms: 35.938
  timestamp: 1602643015
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     55 |          1455.33 | 8898560 |  246.835 |              300.646 |              126.556 |            819.063 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3492.510138838144
    time_step_min: 3142
  date: 2020-10-14_02-37-21
  done: false
  episode_len_mean: 818.6212479534291
  episode_reward_max: 300.64646464646415
  episode_reward_mean: 247.40125100376113
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 232
  episodes_total: 10994
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3824375569820404
        entropy_coeff: 0.0005000000000000001
        kl: 0.004891937094119688
        model: {}
        policy_loss: -0.008490346993009249
        total_loss: 7.047441482543945
        vf_explained_var: 0.9873548150062561
        vf_loss: 7.056092619895935
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.036666666666672
    gpu_util_percent0: 0.3436666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14767020709996379
    mean_env_wait_ms: 1.223817069295764
    mean_inference_ms: 4.3928786879568404
    mean_raw_obs_processing_ms: 0.38484619560609146
  time_since_restore: 1481.5002701282501
  time_this_iter_s: 26.165891408920288
  time_total_s: 1481.5002701282501
  timers:
    learn_throughput: 8344.173
    learn_time_ms: 19389.819
    sample_throughput: 23679.064
    sample_time_ms: 6832.702
    update_time_ms: 34.751
  timestamp: 1602643041
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     56 |           1481.5 | 9060352 |  247.401 |              300.646 |              126.556 |            818.621 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3488.9225604297226
    time_step_min: 3134
  date: 2020-10-14_02-37-48
  done: false
  episode_len_mean: 818.2746968616262
  episode_reward_max: 301.5555555555554
  episode_reward_mean: 247.95323419645783
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 222
  episodes_total: 11216
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3699767291545868
        entropy_coeff: 0.0005000000000000001
        kl: 0.005115050434445341
        model: {}
        policy_loss: -0.007562085253539408
        total_loss: 5.690538167953491
        vf_explained_var: 0.9884591102600098
        vf_loss: 5.698269287745158
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.279999999999998
    gpu_util_percent0: 0.279
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476305750985053
    mean_env_wait_ms: 1.2239924459689104
    mean_inference_ms: 4.390455339698082
    mean_raw_obs_processing_ms: 0.38471348650210085
  time_since_restore: 1507.8769068717957
  time_this_iter_s: 26.376636743545532
  time_total_s: 1507.8769068717957
  timers:
    learn_throughput: 8340.77
    learn_time_ms: 19397.729
    sample_throughput: 23733.577
    sample_time_ms: 6817.009
    update_time_ms: 34.768
  timestamp: 1602643068
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     57 |          1507.88 | 9222144 |  247.953 |              301.556 |              126.556 |            818.275 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3486.4044299329335
    time_step_min: 3134
  date: 2020-10-14_02-38-15
  done: false
  episode_len_mean: 817.9564071014238
  episode_reward_max: 301.5555555555554
  episode_reward_mean: 248.3344270619711
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 162
  episodes_total: 11378
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.37272237489620846
        entropy_coeff: 0.0005000000000000001
        kl: 0.005328354930194716
        model: {}
        policy_loss: -0.010362972466585537
        total_loss: 5.935941974322001
        vf_explained_var: 0.9858854413032532
        vf_loss: 5.94647475083669
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.848387096774196
    gpu_util_percent0: 0.31129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760415845362673
    mean_env_wait_ms: 1.2241248202113546
    mean_inference_ms: 4.3887364918573795
    mean_raw_obs_processing_ms: 0.384624195524297
  time_since_restore: 1534.0868742465973
  time_this_iter_s: 26.209967374801636
  time_total_s: 1534.0868742465973
  timers:
    learn_throughput: 8341.878
    learn_time_ms: 19395.154
    sample_throughput: 23755.647
    sample_time_ms: 6810.675
    update_time_ms: 34.594
  timestamp: 1602643095
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     58 |          1534.09 | 9383936 |  248.334 |              301.556 |              126.556 |            817.956 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3483.210366646442
    time_step_min: 3106
  date: 2020-10-14_02-38-41
  done: false
  episode_len_mean: 817.6424069757403
  episode_reward_max: 305.7979797979797
  episode_reward_mean: 248.84653057380294
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 205
  episodes_total: 11583
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.36743522187074024
        entropy_coeff: 0.0005000000000000001
        kl: 0.005346729381320377
        model: {}
        policy_loss: -0.007888776114365706
        total_loss: 5.618445793787639
        vf_explained_var: 0.9883816838264465
        vf_loss: 5.6265015204747515
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84333333333333
    gpu_util_percent0: 0.2889999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475728619334222
    mean_env_wait_ms: 1.22428304542816
    mean_inference_ms: 4.386636318986181
    mean_raw_obs_processing_ms: 0.38451509709266796
  time_since_restore: 1560.3888704776764
  time_this_iter_s: 26.3019962310791
  time_total_s: 1560.3888704776764
  timers:
    learn_throughput: 8338.625
    learn_time_ms: 19402.719
    sample_throughput: 23794.47
    sample_time_ms: 6799.563
    update_time_ms: 31.364
  timestamp: 1602643121
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     59 |          1560.39 | 9545728 |  248.847 |              305.798 |              126.556 |            817.642 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3479.3053221288515
    time_step_min: 3106
  date: 2020-10-14_02-39-08
  done: false
  episode_len_mean: 817.2589836814069
  episode_reward_max: 305.7979797979797
  episode_reward_mean: 249.43620102265544
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 244
  episodes_total: 11827
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.35027463734149933
        entropy_coeff: 0.0005000000000000001
        kl: 0.005626743387741347
        model: {}
        policy_loss: -0.01029486587503925
        total_loss: 5.235284050305684
        vf_explained_var: 0.9902641773223877
        vf_loss: 5.245736440022786
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.37
    gpu_util_percent0: 0.324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753629951366365
    mean_env_wait_ms: 1.2244645820643727
    mean_inference_ms: 4.384257072358234
    mean_raw_obs_processing_ms: 0.3843949879374074
  time_since_restore: 1586.7322261333466
  time_this_iter_s: 26.343355655670166
  time_total_s: 1586.7322261333466
  timers:
    learn_throughput: 8337.913
    learn_time_ms: 19404.376
    sample_throughput: 23835.301
    sample_time_ms: 6787.915
    update_time_ms: 32.388
  timestamp: 1602643148
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     60 |          1586.73 | 9707520 |  249.436 |              305.798 |              126.556 |            817.259 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3476.7416819929776
    time_step_min: 3106
  date: 2020-10-14_02-39-34
  done: false
  episode_len_mean: 816.9816788807461
  episode_reward_max: 305.7979797979797
  episode_reward_mean: 249.83815251112017
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 181
  episodes_total: 12008
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3473878701527913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004786956473253667
        model: {}
        policy_loss: -0.008876402610136816
        total_loss: 5.055386622746785
        vf_explained_var: 0.9886049628257751
        vf_loss: 5.064421653747559
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.11935483870968
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147508166510041
    mean_env_wait_ms: 1.224579233740158
    mean_inference_ms: 4.382491647784923
    mean_raw_obs_processing_ms: 0.38430181678007036
  time_since_restore: 1613.0745868682861
  time_this_iter_s: 26.342360734939575
  time_total_s: 1613.0745868682861
  timers:
    learn_throughput: 8337.389
    learn_time_ms: 19405.596
    sample_throughput: 23892.435
    sample_time_ms: 6771.683
    update_time_ms: 31.955
  timestamp: 1602643174
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     61 |          1613.07 | 9869312 |  249.838 |              305.798 |              126.556 |            816.982 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3474.062129202373
    time_step_min: 3106
  date: 2020-10-14_02-40-01
  done: false
  episode_len_mean: 816.7635856181251
  episode_reward_max: 306.555555555555
  episode_reward_mean: 250.23456780910365
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 174
  episodes_total: 12182
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3518571654955546
        entropy_coeff: 0.0005000000000000001
        kl: 0.004656291256348292
        model: {}
        policy_loss: -0.010902227833867073
        total_loss: 5.461006919542949
        vf_explained_var: 0.9874956607818604
        vf_loss: 5.47207772731781
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.680645161290325
    gpu_util_percent0: 0.3003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748353515456744
    mean_env_wait_ms: 1.2246974059028413
    mean_inference_ms: 4.380857199930698
    mean_raw_obs_processing_ms: 0.3842174712705384
  time_since_restore: 1639.5561623573303
  time_this_iter_s: 26.48157548904419
  time_total_s: 1639.5561623573303
  timers:
    learn_throughput: 8341.155
    learn_time_ms: 19396.834
    sample_throughput: 23832.005
    sample_time_ms: 6788.854
    update_time_ms: 32.073
  timestamp: 1602643201
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     62 |          1639.56 | 10031104 |  250.235 |              306.556 |              126.556 |            816.764 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3470.7738856079604
    time_step_min: 3106
  date: 2020-10-14_02-40-27
  done: false
  episode_len_mean: 816.4106552752478
  episode_reward_max: 306.555555555555
  episode_reward_mean: 250.72531798194692
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 225
  episodes_total: 12407
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3490934396783511
        entropy_coeff: 0.0005000000000000001
        kl: 0.004778989280263583
        model: {}
        policy_loss: -0.008070748135954394
        total_loss: 6.770879467328389
        vf_explained_var: 0.9873373508453369
        vf_loss: 6.779120882352193
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.330000000000002
    gpu_util_percent0: 0.32100000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745356697762124
    mean_env_wait_ms: 1.2248384043135925
    mean_inference_ms: 4.378825841128263
    mean_raw_obs_processing_ms: 0.3841164330966089
  time_since_restore: 1665.711258649826
  time_this_iter_s: 26.155096292495728
  time_total_s: 1665.711258649826
  timers:
    learn_throughput: 8346.669
    learn_time_ms: 19384.02
    sample_throughput: 23872.441
    sample_time_ms: 6777.355
    update_time_ms: 32.395
  timestamp: 1602643227
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     63 |          1665.71 | 10192896 |  250.725 |              306.556 |              126.556 |            816.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3467.6379091197964
    time_step_min: 3106
  date: 2020-10-14_02-40-54
  done: false
  episode_len_mean: 816.1366946335286
  episode_reward_max: 306.555555555555
  episode_reward_mean: 251.2144246006044
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 227
  episodes_total: 12634
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.33230379472176236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004719143112500508
        model: {}
        policy_loss: -0.008517101462833429
        total_loss: 5.602464318275452
        vf_explained_var: 0.9889903664588928
        vf_loss: 5.611145615577698
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.44000000000001
    gpu_util_percent0: 0.27466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14742247247488413
    mean_env_wait_ms: 1.2249665796193978
    mean_inference_ms: 4.37684629296867
    mean_raw_obs_processing_ms: 0.3840119522134951
  time_since_restore: 1691.941496372223
  time_this_iter_s: 26.23023772239685
  time_total_s: 1691.941496372223
  timers:
    learn_throughput: 8354.172
    learn_time_ms: 19366.611
    sample_throughput: 23851.665
    sample_time_ms: 6783.258
    update_time_ms: 31.111
  timestamp: 1602643254
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     64 |          1691.94 | 10354688 |  251.214 |              306.556 |              126.556 |            816.137 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3465.2452756214225
    time_step_min: 3106
  date: 2020-10-14_02-41-20
  done: false
  episode_len_mean: 815.959059301508
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 251.58549397403962
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 165
  episodes_total: 12799
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.33149294306834537
        entropy_coeff: 0.0005000000000000001
        kl: 0.005118549180527528
        model: {}
        policy_loss: -0.011392354687510911
        total_loss: 4.358288009961446
        vf_explained_var: 0.9895090460777283
        vf_loss: 4.369845112164815
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.651612903225807
    gpu_util_percent0: 0.31580645161290327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739984023951375
    mean_env_wait_ms: 1.2250494196768338
    mean_inference_ms: 4.375404634821487
    mean_raw_obs_processing_ms: 0.3839358839625324
  time_since_restore: 1718.1557202339172
  time_this_iter_s: 26.214223861694336
  time_total_s: 1718.1557202339172
  timers:
    learn_throughput: 8355.993
    learn_time_ms: 19362.391
    sample_throughput: 23857.372
    sample_time_ms: 6781.635
    update_time_ms: 29.545
  timestamp: 1602643280
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     65 |          1718.16 | 10516480 |  251.585 |              306.556 |              126.556 |            815.959 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3462.5430315203957
    time_step_min: 3106
  date: 2020-10-14_02-41-47
  done: false
  episode_len_mean: 815.7602001539645
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 251.98010124338037
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 12990
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.33109505474567413
        entropy_coeff: 0.0005000000000000001
        kl: 0.004953681103264292
        model: {}
        policy_loss: -0.009910731421162685
        total_loss: 5.735819061597188
        vf_explained_var: 0.9879274368286133
        vf_loss: 5.7458943128585815
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.370000000000005
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737542631444941
    mean_env_wait_ms: 1.2251526987661643
    mean_inference_ms: 4.3738132528810265
    mean_raw_obs_processing_ms: 0.3838528097247363
  time_since_restore: 1744.5178005695343
  time_this_iter_s: 26.362080335617065
  time_total_s: 1744.5178005695343
  timers:
    learn_throughput: 8351.361
    learn_time_ms: 19373.131
    sample_throughput: 23835.316
    sample_time_ms: 6787.911
    update_time_ms: 31.397
  timestamp: 1602643307
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     66 |          1744.52 | 10678272 |   251.98 |              306.556 |              126.556 |             815.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3459.4306408797875
    time_step_min: 3104
  date: 2020-10-14_02-42-14
  done: false
  episode_len_mean: 815.5501473811503
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 252.4616347130893
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 241
  episodes_total: 13231
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.31809532394011814
        entropy_coeff: 0.0005000000000000001
        kl: 0.005060969425054888
        model: {}
        policy_loss: -0.00855924459756352
        total_loss: 6.23681366443634
        vf_explained_var: 0.9886898994445801
        vf_loss: 6.245531439781189
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98709677419355
    gpu_util_percent0: 0.3058064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734749157432875
    mean_env_wait_ms: 1.225270354104294
    mean_inference_ms: 4.371886424055133
    mean_raw_obs_processing_ms: 0.3837562326223604
  time_since_restore: 1770.8292729854584
  time_this_iter_s: 26.311472415924072
  time_total_s: 1770.8292729854584
  timers:
    learn_throughput: 8354.846
    learn_time_ms: 19365.049
    sample_throughput: 23830.567
    sample_time_ms: 6789.264
    update_time_ms: 31.195
  timestamp: 1602643334
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     67 |          1770.83 | 10840064 |  252.462 |              306.556 |              126.556 |             815.55 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3456.7052010162906
    time_step_min: 3104
  date: 2020-10-14_02-42-40
  done: false
  episode_len_mean: 815.3170241286863
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 252.86491817188832
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 197
  episodes_total: 13428
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3082869350910187
        entropy_coeff: 0.0005000000000000001
        kl: 0.005168662561724584
        model: {}
        policy_loss: -0.010725375924569866
        total_loss: 4.686489701271057
        vf_explained_var: 0.989706814289093
        vf_loss: 4.697368621826172
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.470000000000006
    gpu_util_percent0: 0.25333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732117592408706
    mean_env_wait_ms: 1.2253498083951286
    mean_inference_ms: 4.37033402339079
    mean_raw_obs_processing_ms: 0.3836728380025664
  time_since_restore: 1797.0660903453827
  time_this_iter_s: 26.236817359924316
  time_total_s: 1797.0660903453827
  timers:
    learn_throughput: 8354.262
    learn_time_ms: 19366.402
    sample_throughput: 23826.242
    sample_time_ms: 6790.496
    update_time_ms: 30.78
  timestamp: 1602643360
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     68 |          1797.07 | 11001856 |  252.865 |              306.556 |              126.556 |            815.317 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3454.3622814137093
    time_step_min: 3104
  date: 2020-10-14_02-43-07
  done: false
  episode_len_mean: 815.1194205456284
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 253.2205398347023
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 171
  episodes_total: 13599
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3148614540696144
        entropy_coeff: 0.0005000000000000001
        kl: 0.005381360262011488
        model: {}
        policy_loss: -0.008486275027583664
        total_loss: 5.1140109697977705
        vf_explained_var: 0.9879872798919678
        vf_loss: 5.122654477755229
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.76129032258064
    gpu_util_percent0: 0.3193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730014589629917
    mean_env_wait_ms: 1.2254244212733232
    mean_inference_ms: 4.369010023909479
    mean_raw_obs_processing_ms: 0.3836016262468963
  time_since_restore: 1823.6118483543396
  time_this_iter_s: 26.54575800895691
  time_total_s: 1823.6118483543396
  timers:
    learn_throughput: 8348.797
    learn_time_ms: 19379.079
    sample_throughput: 23796.927
    sample_time_ms: 6798.861
    update_time_ms: 33.022
  timestamp: 1602643387
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     69 |          1823.61 | 11163648 |  253.221 |              306.556 |              126.556 |            815.119 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3451.5268622041526
    time_step_min: 3104
  date: 2020-10-14_02-43-34
  done: false
  episode_len_mean: 814.8107091172214
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 253.6615394173278
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 13820
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.30804723252852756
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044756393957262235
        model: {}
        policy_loss: -0.007979019816654423
        total_loss: 6.014165600140889
        vf_explained_var: 0.988081693649292
        vf_loss: 6.022298177083333
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.767741935483876
    gpu_util_percent0: 0.3667741935483872
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.803225806451612
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727544057877215
    mean_env_wait_ms: 1.2255122550843678
    mean_inference_ms: 4.367366955699428
    mean_raw_obs_processing_ms: 0.3835194809980846
  time_since_restore: 1849.9139518737793
  time_this_iter_s: 26.302103519439697
  time_total_s: 1849.9139518737793
  timers:
    learn_throughput: 8357.066
    learn_time_ms: 19359.905
    sample_throughput: 23750.308
    sample_time_ms: 6812.206
    update_time_ms: 33.816
  timestamp: 1602643414
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     70 |          1849.91 | 11325440 |  253.662 |              306.556 |              126.556 |            814.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3448.369822907741
    time_step_min: 3104
  date: 2020-10-14_02-44-00
  done: false
  episode_len_mean: 814.4635587188612
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 254.14217261583772
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 230
  episodes_total: 14050
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2893260220686595
        entropy_coeff: 0.0005000000000000001
        kl: 0.00460086219633619
        model: {}
        policy_loss: -0.011059359380548509
        total_loss: 4.970375736554463
        vf_explained_var: 0.9899899363517761
        vf_loss: 4.981579661369324
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97666666666667
    gpu_util_percent0: 0.33399999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724879679156644
    mean_env_wait_ms: 1.2255996072667628
    mean_inference_ms: 4.3657048691040865
    mean_raw_obs_processing_ms: 0.3834303225308423
  time_since_restore: 1876.1648433208466
  time_this_iter_s: 26.25089144706726
  time_total_s: 1876.1648433208466
  timers:
    learn_throughput: 8360.51
    learn_time_ms: 19351.93
    sample_throughput: 23759.628
    sample_time_ms: 6809.534
    update_time_ms: 34.47
  timestamp: 1602643440
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     71 |          1876.16 | 11487232 |  254.142 |              306.556 |              126.556 |            814.464 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3446.1246648793567
    time_step_min: 3104
  date: 2020-10-14_02-44-27
  done: false
  episode_len_mean: 814.1860759493671
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 254.48115117418882
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 170
  episodes_total: 14220
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.29010538508494693
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044984914517651005
        model: {}
        policy_loss: -0.010564355572569184
        total_loss: 4.877623597780864
        vf_explained_var: 0.988173246383667
        vf_loss: 4.888332962989807
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.248387096774195
    gpu_util_percent0: 0.3490322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722915713521806
    mean_env_wait_ms: 1.2256567465057286
    mean_inference_ms: 4.364489446514925
    mean_raw_obs_processing_ms: 0.38336559425288863
  time_since_restore: 1902.3662388324738
  time_this_iter_s: 26.201395511627197
  time_total_s: 1902.3662388324738
  timers:
    learn_throughput: 8366.259
    learn_time_ms: 19338.631
    sample_throughput: 23788.986
    sample_time_ms: 6801.13
    update_time_ms: 35.77
  timestamp: 1602643467
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     72 |          1902.37 | 11649024 |  254.481 |              306.556 |              126.556 |            814.186 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3443.412202588006
    time_step_min: 3104
  date: 2020-10-14_02-44-54
  done: false
  episode_len_mean: 813.8409847434119
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 254.8801888510623
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 200
  episodes_total: 14420
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.29735204577445984
        entropy_coeff: 0.0005000000000000001
        kl: 0.00498960407761236
        model: {}
        policy_loss: -0.0081715231644921
        total_loss: 5.799649635950725
        vf_explained_var: 0.9872967600822449
        vf_loss: 5.807969888051351
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.783333333333335
    gpu_util_percent0: 0.23999999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472076436966957
    mean_env_wait_ms: 1.2257300800717448
    mean_inference_ms: 4.363099545037447
    mean_raw_obs_processing_ms: 0.38329227904984686
  time_since_restore: 1928.7555158138275
  time_this_iter_s: 26.38927698135376
  time_total_s: 1928.7555158138275
  timers:
    learn_throughput: 8359.293
    learn_time_ms: 19354.747
    sample_throughput: 23766.645
    sample_time_ms: 6807.524
    update_time_ms: 35.705
  timestamp: 1602643494
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     73 |          1928.76 | 11810816 |   254.88 |              306.556 |              126.556 |            813.841 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3440.172949305603
    time_step_min: 3104
  date: 2020-10-14_02-45-21
  done: false
  episode_len_mean: 813.4098751960718
  episode_reward_max: 306.55555555555515
  episode_reward_mean: 255.36681828859383
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 243
  episodes_total: 14663
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.27952173352241516
        entropy_coeff: 0.0005000000000000001
        kl: 0.004244102844192336
        model: {}
        policy_loss: -0.007911291910810784
        total_loss: 5.4590301513671875
        vf_explained_var: 0.9894629120826721
        vf_loss: 5.467081030209859
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01935483870968
    gpu_util_percent0: 0.2787096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718306623278313
    mean_env_wait_ms: 1.2258172247005732
    mean_inference_ms: 4.361486771100333
    mean_raw_obs_processing_ms: 0.38320755736587486
  time_since_restore: 1955.3845677375793
  time_this_iter_s: 26.62905192375183
  time_total_s: 1955.3845677375793
  timers:
    learn_throughput: 8345.769
    learn_time_ms: 19386.111
    sample_throughput: 23747.194
    sample_time_ms: 6813.1
    update_time_ms: 37.403
  timestamp: 1602643521
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     74 |          1955.38 | 11972608 |  255.367 |              306.556 |              126.556 |             813.41 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3437.6310955018234
    time_step_min: 3099
  date: 2020-10-14_02-45-48
  done: false
  episode_len_mean: 813.1072582817129
  episode_reward_max: 306.8585858585855
  episode_reward_mean: 255.75442888350204
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 14852
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.27546490480502445
        entropy_coeff: 0.0005000000000000001
        kl: 0.004323257560220857
        model: {}
        policy_loss: -0.010564397797376538
        total_loss: 3.991108854611715
        vf_explained_var: 0.9904248118400574
        vf_loss: 4.001810888449351
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280645161290323
    gpu_util_percent0: 0.3367741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716183080233977
    mean_env_wait_ms: 1.225871562992225
    mean_inference_ms: 4.360223962165258
    mean_raw_obs_processing_ms: 0.3831385783715471
  time_since_restore: 1982.0029051303864
  time_this_iter_s: 26.618337392807007
  time_total_s: 1982.0029051303864
  timers:
    learn_throughput: 8340.883
    learn_time_ms: 19397.466
    sample_throughput: 23686.168
    sample_time_ms: 6830.653
    update_time_ms: 38.977
  timestamp: 1602643548
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     75 |             1982 | 12134400 |  255.754 |              306.859 |              126.556 |            813.107 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3435.2666355202564
    time_step_min: 3094
  date: 2020-10-14_02-46-14
  done: false
  episode_len_mean: 812.79326635172
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 256.1104464029472
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 177
  episodes_total: 15029
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2823597143093745
        entropy_coeff: 0.0005000000000000001
        kl: 0.00407797460987543
        model: {}
        policy_loss: -0.010367831244366243
        total_loss: 5.212655345598857
        vf_explained_var: 0.9874860644340515
        vf_loss: 5.223164399464925
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.080000000000002
    gpu_util_percent0: 0.3346666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714419941496878
    mean_env_wait_ms: 1.2259319444540602
    mean_inference_ms: 4.35906834058936
    mean_raw_obs_processing_ms: 0.3830761436719248
  time_since_restore: 2008.4281311035156
  time_this_iter_s: 26.425225973129272
  time_total_s: 2008.4281311035156
  timers:
    learn_throughput: 8338.764
    learn_time_ms: 19402.395
    sample_throughput: 23685.671
    sample_time_ms: 6830.797
    update_time_ms: 39.234
  timestamp: 1602643574
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     76 |          2008.43 | 12296192 |   256.11 |              310.192 |              126.556 |            812.793 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3432.109526938239
    time_step_min: 3094
  date: 2020-10-14_02-46-41
  done: false
  episode_len_mean: 812.3892309707848
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 256.5805070222727
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 237
  episodes_total: 15266
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.2738444482286771
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043979672870288295
        model: {}
        policy_loss: -0.010670721899562826
        total_loss: 5.201486905415853
        vf_explained_var: 0.9896658062934875
        vf_loss: 5.212294499079387
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73548387096774
    gpu_util_percent0: 0.30387096774193545
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712215827539005
    mean_env_wait_ms: 1.2260088698086122
    mean_inference_ms: 4.357600741351961
    mean_raw_obs_processing_ms: 0.3830007599071151
  time_since_restore: 2034.7559368610382
  time_this_iter_s: 26.327805757522583
  time_total_s: 2034.7559368610382
  timers:
    learn_throughput: 8338.349
    learn_time_ms: 19403.361
    sample_throughput: 23685.673
    sample_time_ms: 6830.796
    update_time_ms: 39.07
  timestamp: 1602643601
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     77 |          2034.76 | 12457984 |  256.581 |              310.192 |              126.556 |            812.389 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3429.3970454839964
    time_step_min: 3094
  date: 2020-10-14_02-47-07
  done: false
  episode_len_mean: 812.0084625322997
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 257.00416960300646
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 214
  episodes_total: 15480
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2587315614024798
        entropy_coeff: 0.0005000000000000001
        kl: 0.00425412164380153
        model: {}
        policy_loss: -0.009416911265968034
        total_loss: 5.0734580755233765
        vf_explained_var: 0.9885712265968323
        vf_loss: 5.083004355430603
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.053333333333338
    gpu_util_percent0: 0.2936666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709995885336238
    mean_env_wait_ms: 1.2260661494868395
    mean_inference_ms: 4.356294743984879
    mean_raw_obs_processing_ms: 0.3829278998554249
  time_since_restore: 2060.914869785309
  time_this_iter_s: 26.15893292427063
  time_total_s: 2060.914869785309
  timers:
    learn_throughput: 8339.633
    learn_time_ms: 19400.374
    sample_throughput: 23700.642
    sample_time_ms: 6826.482
    update_time_ms: 37.497
  timestamp: 1602643627
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     78 |          2060.91 | 12619776 |  257.004 |              310.192 |              126.556 |            812.008 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3427.051387198052
    time_step_min: 3094
  date: 2020-10-14_02-47-34
  done: false
  episode_len_mean: 811.671500670798
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 257.35035914630845
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 173
  episodes_total: 15653
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2661385287841161
        entropy_coeff: 0.0005000000000000001
        kl: 0.004082795834013571
        model: {}
        policy_loss: -0.00830440545299401
        total_loss: 4.488071044286092
        vf_explained_var: 0.9884998202323914
        vf_loss: 4.496508518854777
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.10000000000001
    gpu_util_percent0: 0.28096774193548385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708329860339478
    mean_env_wait_ms: 1.2261202581509494
    mean_inference_ms: 4.355239164368865
    mean_raw_obs_processing_ms: 0.3828692920488074
  time_since_restore: 2087.409206867218
  time_this_iter_s: 26.49433708190918
  time_total_s: 2087.409206867218
  timers:
    learn_throughput: 8337.616
    learn_time_ms: 19405.066
    sample_throughput: 23735.337
    sample_time_ms: 6816.503
    update_time_ms: 37.128
  timestamp: 1602643654
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     79 |          2087.41 | 12781568 |   257.35 |              310.192 |              126.556 |            811.672 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3424.3442633308064
    time_step_min: 3094
  date: 2020-10-14_02-48-01
  done: false
  episode_len_mean: 811.2717021544664
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 257.7709888350555
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 15874
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.262748584151268
        entropy_coeff: 0.0005000000000000001
        kl: 0.004657868994399905
        model: {}
        policy_loss: -0.010715616052038968
        total_loss: 4.881109197934468
        vf_explained_var: 0.9897063374519348
        vf_loss: 4.8919559717178345
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.400000000000006
    gpu_util_percent0: 0.337741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706396790055232
    mean_env_wait_ms: 1.2261803234062112
    mean_inference_ms: 4.35396906319703
    mean_raw_obs_processing_ms: 0.38280176404551897
  time_since_restore: 2113.8983886241913
  time_this_iter_s: 26.489181756973267
  time_total_s: 2113.8983886241913
  timers:
    learn_throughput: 8333.614
    learn_time_ms: 19414.386
    sample_throughput: 23710.362
    sample_time_ms: 6823.683
    update_time_ms: 37.207
  timestamp: 1602643681
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     80 |           2113.9 | 12943360 |  257.771 |              310.192 |              126.556 |            811.272 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3421.39697365963
    time_step_min: 3079
  date: 2020-10-14_02-48-28
  done: false
  episode_len_mean: 810.9262340887923
  episode_reward_max: 310.1919191919191
  episode_reward_mean: 258.19645382731346
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 231
  episodes_total: 16105
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.24882828195889792
        entropy_coeff: 0.0005000000000000001
        kl: 0.004502150307719906
        model: {}
        policy_loss: -0.009274985020359358
        total_loss: 5.348036726315816
        vf_explained_var: 0.9888719916343689
        vf_loss: 5.3574360609054565
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.032258064516128
    gpu_util_percent0: 0.3151612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704235059094586
    mean_env_wait_ms: 1.2262455611498853
    mean_inference_ms: 4.352645260766153
    mean_raw_obs_processing_ms: 0.3827275283737256
  time_since_restore: 2140.6311361789703
  time_this_iter_s: 26.732747554779053
  time_total_s: 2140.6311361789703
  timers:
    learn_throughput: 8329.109
    learn_time_ms: 19424.886
    sample_throughput: 23584.156
    sample_time_ms: 6860.199
    update_time_ms: 37.906
  timestamp: 1602643708
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     81 |          2140.63 | 13105152 |  258.196 |              310.192 |              126.556 |            810.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3419.207257716715
    time_step_min: 3059
  date: 2020-10-14_02-48-55
  done: false
  episode_len_mean: 810.6713767893347
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 258.52902248509514
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 172
  episodes_total: 16277
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.25001314654946327
        entropy_coeff: 0.0005000000000000001
        kl: 0.004377193244484563
        model: {}
        policy_loss: -0.011408900939083347
        total_loss: 3.865146736303965
        vf_explained_var: 0.9900562167167664
        vf_loss: 3.876680632432302
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.738709677419358
    gpu_util_percent0: 0.30838709677419357
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702632957190817
    mean_env_wait_ms: 1.2262867766510808
    mean_inference_ms: 4.351676246405522
    mean_raw_obs_processing_ms: 0.38267333378498575
  time_since_restore: 2167.295089006424
  time_this_iter_s: 26.663952827453613
  time_total_s: 2167.295089006424
  timers:
    learn_throughput: 8320.383
    learn_time_ms: 19445.258
    sample_throughput: 23533.224
    sample_time_ms: 6875.046
    update_time_ms: 45.266
  timestamp: 1602643735
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     82 |           2167.3 | 13266944 |  258.529 |              312.919 |              126.556 |            810.671 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3416.6888348037724
    time_step_min: 3059
  date: 2020-10-14_02-49-22
  done: false
  episode_len_mean: 810.4195740549724
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 258.90100201088575
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 16481
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.25243983417749405
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041825734078884125
        model: {}
        policy_loss: -0.007913017482981862
        total_loss: 4.611546874046326
        vf_explained_var: 0.9898908734321594
        vf_loss: 4.619586269060771
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71935483870968
    gpu_util_percent0: 0.31935483870967746
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700981856496312
    mean_env_wait_ms: 1.2263448561557653
    mean_inference_ms: 4.350576473706687
    mean_raw_obs_processing_ms: 0.3826143359999753
  time_since_restore: 2193.8457617759705
  time_this_iter_s: 26.55067276954651
  time_total_s: 2193.8457617759705
  timers:
    learn_throughput: 8317.477
    learn_time_ms: 19452.052
    sample_throughput: 23532.735
    sample_time_ms: 6875.189
    update_time_ms: 45.513
  timestamp: 1602643762
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     83 |          2193.85 | 13428736 |  258.901 |              312.919 |              126.556 |             810.42 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3413.9280830134358
    time_step_min: 3059
  date: 2020-10-14_02-49-49
  done: false
  episode_len_mean: 810.1138294054313
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 259.3166513804146
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 237
  episodes_total: 16718
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2376090536514918
        entropy_coeff: 0.0005000000000000001
        kl: 0.004108809516765177
        model: {}
        policy_loss: -0.007634020197050025
        total_loss: 5.304140488306682
        vf_explained_var: 0.9895066618919373
        vf_loss: 5.3118932247161865
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996666666666666
    gpu_util_percent0: 0.3390000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698976586840834
    mean_env_wait_ms: 1.2263993380285063
    mean_inference_ms: 4.349308090551618
    mean_raw_obs_processing_ms: 0.38254170214696775
  time_since_restore: 2220.244416952133
  time_this_iter_s: 26.39865517616272
  time_total_s: 2220.244416952133
  timers:
    learn_throughput: 8325.42
    learn_time_ms: 19433.493
    sample_throughput: 23548.645
    sample_time_ms: 6870.544
    update_time_ms: 44.995
  timestamp: 1602643789
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     84 |          2220.24 | 13590528 |  259.317 |              312.919 |              126.556 |            810.114 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3411.651286917329
    time_step_min: 3059
  date: 2020-10-14_02-50-15
  done: false
  episode_len_mean: 809.8594156612255
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 259.65288799994227
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 16908
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.22809491430719694
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043174341941873235
        model: {}
        policy_loss: -0.009084208014731606
        total_loss: 4.11304239432017
        vf_explained_var: 0.9900930523872375
        vf_loss: 4.122240662574768
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.812903225806462
    gpu_util_percent0: 0.31258064516129036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697278854594467
    mean_env_wait_ms: 1.2264378985058109
    mean_inference_ms: 4.3483097977757925
    mean_raw_obs_processing_ms: 0.3824862295949271
  time_since_restore: 2246.593471765518
  time_this_iter_s: 26.34905481338501
  time_total_s: 2246.593471765518
  timers:
    learn_throughput: 8327.786
    learn_time_ms: 19427.972
    sample_throughput: 23594.623
    sample_time_ms: 6857.155
    update_time_ms: 45.276
  timestamp: 1602643815
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     85 |          2246.59 | 13752320 |  259.653 |              312.919 |              126.556 |            809.859 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3409.515837634913
    time_step_min: 3059
  date: 2020-10-14_02-50-42
  done: false
  episode_len_mean: 809.5757575757576
  episode_reward_max: 312.91919191919175
  episode_reward_mean: 259.98211670938906
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 17094
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.23633441453178725
        entropy_coeff: 0.0005000000000000001
        kl: 0.00409562202791373
        model: {}
        policy_loss: -0.00990123131001989
        total_loss: 4.715577721595764
        vf_explained_var: 0.9888358116149902
        vf_loss: 4.725596904754639
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15483870967742
    gpu_util_percent0: 0.37870967741935485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695756179861155
    mean_env_wait_ms: 1.226483143251107
    mean_inference_ms: 4.3473636897009555
    mean_raw_obs_processing_ms: 0.38243305356628116
  time_since_restore: 2273.151126384735
  time_this_iter_s: 26.55765461921692
  time_total_s: 2273.151126384735
  timers:
    learn_throughput: 8322.998
    learn_time_ms: 19439.149
    sample_throughput: 23611.203
    sample_time_ms: 6852.34
    update_time_ms: 49.576
  timestamp: 1602643842
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     86 |          2273.15 | 13914112 |  259.982 |              312.919 |              126.556 |            809.576 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3406.5946680545917
    time_step_min: 3041
  date: 2020-10-14_02-51-09
  done: false
  episode_len_mean: 809.2171530741723
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 260.430578713656
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 244
  episodes_total: 17338
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.2280901608367761
        entropy_coeff: 0.0005000000000000001
        kl: 0.004126150025210033
        model: {}
        policy_loss: -0.010167474383100247
        total_loss: 4.037740608056386
        vf_explained_var: 0.9916778206825256
        vf_loss: 4.048022071520488
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.330000000000002
    gpu_util_percent0: 0.32033333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469390115311865
    mean_env_wait_ms: 1.226533176482154
    mean_inference_ms: 4.346122829081546
    mean_raw_obs_processing_ms: 0.38236538372775164
  time_since_restore: 2299.5291006565094
  time_this_iter_s: 26.377974271774292
  time_total_s: 2299.5291006565094
  timers:
    learn_throughput: 8322.29
    learn_time_ms: 19440.802
    sample_throughput: 23601.582
    sample_time_ms: 6855.134
    update_time_ms: 49.209
  timestamp: 1602643869
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     87 |          2299.53 | 14075904 |  260.431 |              315.646 |              126.556 |            809.217 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3404.26346483705
    time_step_min: 3041
  date: 2020-10-14_02-51-36
  done: false
  episode_len_mean: 808.9263800182482
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 260.78137326734463
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 17536
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.21779571970303854
        entropy_coeff: 0.0005000000000000001
        kl: 0.004063838452566415
        model: {}
        policy_loss: -0.00898709589819191
        total_loss: 3.872125267982483
        vf_explained_var: 0.9910173416137695
        vf_loss: 3.881221274534861
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.332258064516132
    gpu_util_percent0: 0.38096774193548394
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692286759413997
    mean_env_wait_ms: 1.2265719456401214
    mean_inference_ms: 4.345181151792245
    mean_raw_obs_processing_ms: 0.3823128392730813
  time_since_restore: 2326.0966413021088
  time_this_iter_s: 26.567540645599365
  time_total_s: 2326.0966413021088
  timers:
    learn_throughput: 8314.901
    learn_time_ms: 19458.079
    sample_throughput: 23532.975
    sample_time_ms: 6875.119
    update_time_ms: 51.259
  timestamp: 1602643896
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     88 |           2326.1 | 14237696 |  260.781 |              315.646 |              126.556 |            808.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3402.2683838097932
    time_step_min: 3041
  date: 2020-10-14_02-52-03
  done: false
  episode_len_mean: 808.6676641635142
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 261.09638192095383
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 175
  episodes_total: 17711
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.22601024061441422
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043747064579899115
        model: {}
        policy_loss: -0.009735454455949366
        total_loss: 3.2111634810765586
        vf_explained_var: 0.9917590618133545
        vf_loss: 3.2210118770599365
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82903225806452
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691026246159516
    mean_env_wait_ms: 1.2266070620055458
    mean_inference_ms: 4.344332678330306
    mean_raw_obs_processing_ms: 0.38226518490102157
  time_since_restore: 2352.5125687122345
  time_this_iter_s: 26.415927410125732
  time_total_s: 2352.5125687122345
  timers:
    learn_throughput: 8318.918
    learn_time_ms: 19448.683
    sample_throughput: 23526.708
    sample_time_ms: 6876.95
    update_time_ms: 49.438
  timestamp: 1602643923
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     89 |          2352.51 | 14399488 |  261.096 |              315.646 |              126.556 |            808.668 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3399.7814351179168
    time_step_min: 3041
  date: 2020-10-14_02-52-29
  done: false
  episode_len_mean: 808.33751393534
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 261.4747812573896
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 229
  episodes_total: 17940
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.2251779946188132
        entropy_coeff: 0.0005000000000000001
        kl: 0.004280702366183202
        model: {}
        policy_loss: -0.009217358068174994
        total_loss: 4.5346901416778564
        vf_explained_var: 0.9908356070518494
        vf_loss: 4.544020175933838
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.783333333333335
    gpu_util_percent0: 0.306
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689395144205858
    mean_env_wait_ms: 1.2266541973641671
    mean_inference_ms: 4.343277498941868
    mean_raw_obs_processing_ms: 0.38221057658653784
  time_since_restore: 2378.789090871811
  time_this_iter_s: 26.276522159576416
  time_total_s: 2378.789090871811
  timers:
    learn_throughput: 8318.396
    learn_time_ms: 19449.903
    sample_throughput: 23605.986
    sample_time_ms: 6853.855
    update_time_ms: 50.068
  timestamp: 1602643949
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     90 |          2378.79 | 14561280 |  261.475 |              315.646 |              126.556 |            808.338 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3397.2853751449234
    time_step_min: 3041
  date: 2020-10-14_02-52-56
  done: false
  episode_len_mean: 808.0633294784955
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 261.849290860029
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 219
  episodes_total: 18159
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.20786967501044273
        entropy_coeff: 0.0005000000000000001
        kl: 0.003977705802147587
        model: {}
        policy_loss: -0.007640881580300629
        total_loss: 3.522599736849467
        vf_explained_var: 0.9922268390655518
        vf_loss: 3.530344545841217
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77096774193548
    gpu_util_percent0: 0.2880645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687649634216934
    mean_env_wait_ms: 1.2266908233836278
    mean_inference_ms: 4.3422436645760625
    mean_raw_obs_processing_ms: 0.3821501014311619
  time_since_restore: 2405.222806453705
  time_this_iter_s: 26.43371558189392
  time_total_s: 2405.222806453705
  timers:
    learn_throughput: 8319.923
    learn_time_ms: 19446.334
    sample_throughput: 23698.132
    sample_time_ms: 6827.205
    update_time_ms: 49.483
  timestamp: 1602643976
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     91 |          2405.22 | 14723072 |  261.849 |              315.646 |              126.556 |            808.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3395.3088741866695
    time_step_min: 3041
  date: 2020-10-14_02-53-23
  done: false
  episode_len_mean: 807.8764112353423
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 262.13997350103125
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 176
  episodes_total: 18335
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.21612624699870744
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025729528317849
        model: {}
        policy_loss: -0.009867655540195605
        total_loss: 3.189829170703888
        vf_explained_var: 0.99192214012146
        vf_loss: 3.1998048027356467
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.24666666666667
    gpu_util_percent0: 0.30733333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686357446832954
    mean_env_wait_ms: 1.2267211386464636
    mean_inference_ms: 4.3414514576671985
    mean_raw_obs_processing_ms: 0.38210579020975016
  time_since_restore: 2431.3591804504395
  time_this_iter_s: 26.13637399673462
  time_total_s: 2431.3591804504395
  timers:
    learn_throughput: 8326.989
    learn_time_ms: 19429.833
    sample_throughput: 23792.736
    sample_time_ms: 6800.059
    update_time_ms: 41.687
  timestamp: 1602644003
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     92 |          2431.36 | 14884864 |   262.14 |              315.646 |              126.556 |            807.876 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3393.09152835595
    time_step_min: 3041
  date: 2020-10-14_02-53-49
  done: false
  episode_len_mean: 807.6551798522354
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 262.4695817583696
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 208
  episodes_total: 18543
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.2216253305474917
        entropy_coeff: 0.0005000000000000001
        kl: 0.003869050279414902
        model: {}
        policy_loss: -0.008594987156053927
        total_loss: 4.247597336769104
        vf_explained_var: 0.9907740950584412
        vf_loss: 4.256303230921428
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.077419354838714
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685057291350262
    mean_env_wait_ms: 1.22676107168388
    mean_inference_ms: 4.340550238815478
    mean_raw_obs_processing_ms: 0.38206002640339487
  time_since_restore: 2457.742814540863
  time_this_iter_s: 26.383634090423584
  time_total_s: 2457.742814540863
  timers:
    learn_throughput: 8330.157
    learn_time_ms: 19422.444
    sample_throughput: 23798.877
    sample_time_ms: 6798.304
    update_time_ms: 41.38
  timestamp: 1602644029
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     93 |          2457.74 | 15046656 |   262.47 |              315.646 |              126.556 |            807.655 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3390.6616123865456
    time_step_min: 3041
  date: 2020-10-14_02-54-16
  done: false
  episode_len_mean: 807.4059970174691
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 262.8312169414637
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 233
  episodes_total: 18776
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.20370794832706451
        entropy_coeff: 0.0005000000000000001
        kl: 0.004867842265715201
        model: {}
        policy_loss: -0.007855712035961915
        total_loss: 4.6312341292699175
        vf_explained_var: 0.9905586242675781
        vf_loss: 4.639191627502441
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.232258064516135
    gpu_util_percent0: 0.3354838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468334571301226
    mean_env_wait_ms: 1.226793184145426
    mean_inference_ms: 4.3395256868745955
    mean_raw_obs_processing_ms: 0.3820004469472215
  time_since_restore: 2483.8783390522003
  time_this_iter_s: 26.13552451133728
  time_total_s: 2483.8783390522003
  timers:
    learn_throughput: 8341.679
    learn_time_ms: 19395.615
    sample_throughput: 23800.179
    sample_time_ms: 6797.932
    update_time_ms: 41.154
  timestamp: 1602644056
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     94 |          2483.88 | 15208448 |  262.831 |              315.646 |              126.556 |            807.406 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3388.7727320786635
    time_step_min: 3041
  date: 2020-10-14_02-54-43
  done: false
  episode_len_mean: 807.2020883873009
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 263.1109710116667
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 18962
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.20504934092362723
        entropy_coeff: 0.0005000000000000001
        kl: 0.004358735672819118
        model: {}
        policy_loss: -0.007940617130467823
        total_loss: 4.1041334470113116
        vf_explained_var: 0.9900287985801697
        vf_loss: 4.1121765573819475
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.964516129032255
    gpu_util_percent0: 0.3387096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681981708589442
    mean_env_wait_ms: 1.2268190129978245
    mean_inference_ms: 4.338729927007096
    mean_raw_obs_processing_ms: 0.3819570105885526
  time_since_restore: 2510.404488801956
  time_this_iter_s: 26.52614974975586
  time_total_s: 2510.404488801956
  timers:
    learn_throughput: 8332.092
    learn_time_ms: 19417.933
    sample_throughput: 23820.277
    sample_time_ms: 6792.196
    update_time_ms: 41.807
  timestamp: 1602644083
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     95 |           2510.4 | 15370240 |  263.111 |              315.646 |              126.556 |            807.202 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3386.904799288219
    time_step_min: 3041
  date: 2020-10-14_02-55-10
  done: false
  episode_len_mean: 806.9766616195897
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 263.4010216507472
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 191
  episodes_total: 19153
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.21467343469460806
        entropy_coeff: 0.0005000000000000001
        kl: 0.003984760240806888
        model: {}
        policy_loss: -0.009691227329312824
        total_loss: 4.16459047794342
        vf_explained_var: 0.9902827143669128
        vf_loss: 4.174388984839122
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.264516129032256
    gpu_util_percent0: 0.28096774193548385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680736535475056
    mean_env_wait_ms: 1.2268499881452422
    mean_inference_ms: 4.337940307211846
    mean_raw_obs_processing_ms: 0.38191329227671095
  time_since_restore: 2536.892306804657
  time_this_iter_s: 26.487818002700806
  time_total_s: 2536.892306804657
  timers:
    learn_throughput: 8333.85
    learn_time_ms: 19413.837
    sample_throughput: 23812.238
    sample_time_ms: 6794.489
    update_time_ms: 37.297
  timestamp: 1602644110
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     96 |          2536.89 | 15532032 |  263.401 |              315.646 |              126.556 |            806.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3384.5454498475533
    time_step_min: 3041
  date: 2020-10-14_02-55-37
  done: false
  episode_len_mean: 806.6928906531938
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 263.7569425241742
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 244
  episodes_total: 19397
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.2060975357890129
        entropy_coeff: 0.0005000000000000001
        kl: 0.004208495491184294
        model: {}
        policy_loss: -0.009226624349442622
        total_loss: 5.099639892578125
        vf_explained_var: 0.9899895191192627
        vf_loss: 5.108969648679097
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19354838709678
    gpu_util_percent0: 0.32774193548387104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679166999378737
    mean_env_wait_ms: 1.2268812698756424
    mean_inference_ms: 4.3369360507261
    mean_raw_obs_processing_ms: 0.38185923529580296
  time_since_restore: 2563.498750925064
  time_this_iter_s: 26.606444120407104
  time_total_s: 2563.498750925064
  timers:
    learn_throughput: 8329.651
    learn_time_ms: 19423.623
    sample_throughput: 23775.591
    sample_time_ms: 6804.962
    update_time_ms: 37.168
  timestamp: 1602644137
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     97 |           2563.5 | 15693824 |  263.757 |              315.646 |              126.556 |            806.693 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3382.5015093374263
    time_step_min: 3041
  date: 2020-10-14_02-56-04
  done: false
  episode_len_mean: 806.4680210300648
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.04889278678223
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 19591
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.19606210912267366
        entropy_coeff: 0.0005000000000000001
        kl: 0.004457082715816796
        model: {}
        policy_loss: -0.009334245774274072
        total_loss: 3.3258522351582847
        vf_explained_var: 0.9921269416809082
        vf_loss: 3.3352845112482705
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.06666666666667
    gpu_util_percent0: 0.29733333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677871686846736
    mean_env_wait_ms: 1.2269043622524887
    mean_inference_ms: 4.336168503270015
    mean_raw_obs_processing_ms: 0.3818161211296245
  time_since_restore: 2589.77530002594
  time_this_iter_s: 26.276549100875854
  time_total_s: 2589.77530002594
  timers:
    learn_throughput: 8332.348
    learn_time_ms: 19417.335
    sample_throughput: 23852.271
    sample_time_ms: 6783.086
    update_time_ms: 36.109
  timestamp: 1602644164
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     98 |          2589.78 | 15855616 |  264.049 |              315.646 |              126.556 |            806.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3380.7767413565853
    time_step_min: 3041
  date: 2020-10-14_02-56-30
  done: false
  episode_len_mean: 806.2371029739024
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.3132968364605
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 181
  episodes_total: 19772
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.20596252754330635
        entropy_coeff: 0.0005000000000000001
        kl: 0.004483936936594546
        model: {}
        policy_loss: -0.010501946392954173
        total_loss: 3.6415778398513794
        vf_explained_var: 0.9911496043205261
        vf_loss: 3.6521827578544617
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.590322580645164
    gpu_util_percent0: 0.35548387096774203
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676696453420973
    mean_env_wait_ms: 1.226932066352738
    mean_inference_ms: 4.335451688969078
    mean_raw_obs_processing_ms: 0.38177633784588777
  time_since_restore: 2616.196223974228
  time_this_iter_s: 26.420923948287964
  time_total_s: 2616.196223974228
  timers:
    learn_throughput: 8333.894
    learn_time_ms: 19413.733
    sample_throughput: 23846.51
    sample_time_ms: 6784.725
    update_time_ms: 38.123
  timestamp: 1602644190
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |     99 |           2616.2 | 16017408 |  264.313 |              315.646 |              126.556 |            806.237 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3378.5574591724276
    time_step_min: 3041
  date: 2020-10-14_02-56-57
  done: false
  episode_len_mean: 805.9456217512994
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.6598441431505
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 236
  episodes_total: 20008
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.19885142520070076
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046679679459581775
        model: {}
        policy_loss: -0.008300370262683524
        total_loss: 4.468468944231669
        vf_explained_var: 0.9907791614532471
        vf_loss: 4.476868947347005
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.564516129032263
    gpu_util_percent0: 0.2848387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675326565681626
    mean_env_wait_ms: 1.2269553348155866
    mean_inference_ms: 4.334525922792921
    mean_raw_obs_processing_ms: 0.38172784241364166
  time_since_restore: 2642.6692259311676
  time_this_iter_s: 26.473001956939697
  time_total_s: 2642.6692259311676
  timers:
    learn_throughput: 8334.129
    learn_time_ms: 19413.187
    sample_throughput: 23805.227
    sample_time_ms: 6796.491
    update_time_ms: 45.123
  timestamp: 1602644217
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    100 |          2642.67 | 16179200 |   264.66 |              315.646 |              126.556 |            805.946 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3376.479304020225
    time_step_min: 3041
  date: 2020-10-14_02-57-24
  done: false
  episode_len_mean: 805.6904891438746
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 264.96498742806637
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 211
  episodes_total: 20219
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.1829805995027224
        entropy_coeff: 0.0005000000000000001
        kl: 0.004511198436375707
        model: {}
        policy_loss: -0.010293429057734707
        total_loss: 3.4685230056444802
        vf_explained_var: 0.9919838309288025
        vf_loss: 3.478907903035482
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.53870967741935
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467393481323269
    mean_env_wait_ms: 1.2269806756513475
    mean_inference_ms: 4.333747444810453
    mean_raw_obs_processing_ms: 0.381682367294211
  time_since_restore: 2669.223000049591
  time_this_iter_s: 26.553774118423462
  time_total_s: 2669.223000049591
  timers:
    learn_throughput: 8332.288
    learn_time_ms: 19417.475
    sample_throughput: 23782.605
    sample_time_ms: 6802.955
    update_time_ms: 45.647
  timestamp: 1602644244
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    101 |          2669.22 | 16340992 |  264.965 |              315.646 |              126.556 |             805.69 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3374.8642060254583
    time_step_min: 3041
  date: 2020-10-14_02-57-51
  done: false
  episode_len_mean: 805.4894816848919
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 265.2111325583592
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 174
  episodes_total: 20393
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.19006827473640442
        entropy_coeff: 0.0005000000000000001
        kl: 0.004485640830049912
        model: {}
        policy_loss: -0.008435653697233647
        total_loss: 3.467149575551351
        vf_explained_var: 0.9913205504417419
        vf_loss: 3.475680331389109
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.190000000000005
    gpu_util_percent0: 0.28733333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467290348229396
    mean_env_wait_ms: 1.2270026749629968
    mean_inference_ms: 4.333097803704815
    mean_raw_obs_processing_ms: 0.38164670412987706
  time_since_restore: 2695.5332090854645
  time_this_iter_s: 26.310209035873413
  time_total_s: 2695.5332090854645
  timers:
    learn_throughput: 8325.735
    learn_time_ms: 19432.758
    sample_throughput: 23774.219
    sample_time_ms: 6805.355
    update_time_ms: 44.555
  timestamp: 1602644271
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    102 |          2695.53 | 16502784 |  265.211 |              315.646 |              126.556 |            805.489 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3372.810063198833
    time_step_min: 3041
  date: 2020-10-14_02-58-18
  done: false
  episode_len_mean: 805.2298214978657
  episode_reward_max: 315.64646464646415
  episode_reward_mean: 265.5199962371088
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 20616
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.19109124317765236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004055134175966184
        model: {}
        policy_loss: -0.00795338962537547
        total_loss: 4.456182678540547
        vf_explained_var: 0.9906907677650452
        vf_loss: 4.464231689771016
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.258064516129036
    gpu_util_percent0: 0.3538709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671702948559204
    mean_env_wait_ms: 1.2270247438070518
    mean_inference_ms: 4.332278423788499
    mean_raw_obs_processing_ms: 0.3816048472634811
  time_since_restore: 2722.01207113266
  time_this_iter_s: 26.478862047195435
  time_total_s: 2722.01207113266
  timers:
    learn_throughput: 8323.519
    learn_time_ms: 19437.933
    sample_throughput: 23758.564
    sample_time_ms: 6809.839
    update_time_ms: 43.593
  timestamp: 1602644298
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    103 |          2722.01 | 16664576 |   265.52 |              315.646 |              126.556 |             805.23 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3370.64876430426
    time_step_min: 3033
  date: 2020-10-14_02-58-45
  done: false
  episode_len_mean: 804.9702552293226
  episode_reward_max: 316.8585858585855
  episode_reward_mean: 265.83654187237914
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 228
  episodes_total: 20844
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.17321047807733217
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038343543031563363
        model: {}
        policy_loss: -0.008931881204868356
        total_loss: 4.151850720246633
        vf_explained_var: 0.991041362285614
        vf_loss: 4.1608690818150835
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.774193548387103
    gpu_util_percent0: 0.29870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670284757447918
    mean_env_wait_ms: 1.2270494012262882
    mean_inference_ms: 4.33146137323967
    mean_raw_obs_processing_ms: 0.38155712285917676
  time_since_restore: 2748.578600168228
  time_this_iter_s: 26.566529035568237
  time_total_s: 2748.578600168228
  timers:
    learn_throughput: 8310.669
    learn_time_ms: 19467.987
    sample_throughput: 23721.426
    sample_time_ms: 6820.5
    update_time_ms: 45.326
  timestamp: 1602644325
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    104 |          2748.58 | 16826368 |  265.837 |              316.859 |              126.556 |             804.97 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3368.9494588280168
    time_step_min: 3033
  date: 2020-10-14_02-59-12
  done: false
  episode_len_mean: 804.738141681336
  episode_reward_max: 316.8585858585855
  episode_reward_mean: 266.08907284943217
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 175
  episodes_total: 21019
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.17105705539385477
        entropy_coeff: 0.0005000000000000001
        kl: 0.004058195103425533
        model: {}
        policy_loss: -0.009294179457356222
        total_loss: 3.1212044954299927
        vf_explained_var: 0.9918803572654724
        vf_loss: 3.130584160486857
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.245161290322578
    gpu_util_percent0: 0.2829032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669296305047969
    mean_env_wait_ms: 1.2270666630327138
    mean_inference_ms: 4.330834872784729
    mean_raw_obs_processing_ms: 0.3815226570076092
  time_since_restore: 2775.0871121883392
  time_this_iter_s: 26.508512020111084
  time_total_s: 2775.0871121883392
  timers:
    learn_throughput: 8313.274
    learn_time_ms: 19461.886
    sample_throughput: 23710.446
    sample_time_ms: 6823.659
    update_time_ms: 45.136
  timestamp: 1602644352
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    105 |          2775.09 | 16988160 |  266.089 |              316.859 |              126.556 |            804.738 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3366.8982818842633
    time_step_min: 3029
  date: 2020-10-14_02-59-39
  done: false
  episode_len_mean: 804.4678786737
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 266.3871919077736
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 213
  episodes_total: 21232
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17971546202898026
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041747875123595195
        model: {}
        policy_loss: -0.008395435870625079
        total_loss: 4.011339048544566
        vf_explained_var: 0.9910714626312256
        vf_loss: 4.019824266433716
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.013333333333335
    gpu_util_percent0: 0.3436666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466826621029811
    mean_env_wait_ms: 1.2270914877792387
    mean_inference_ms: 4.330114367316774
    mean_raw_obs_processing_ms: 0.3814843327356257
  time_since_restore: 2801.3108468055725
  time_this_iter_s: 26.223734617233276
  time_total_s: 2801.3108468055725
  timers:
    learn_throughput: 8320.299
    learn_time_ms: 19445.455
    sample_throughput: 23742.375
    sample_time_ms: 6814.482
    update_time_ms: 43.353
  timestamp: 1602644379
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    106 |          2801.31 | 17149952 |  266.387 |              317.465 |              126.556 |            804.468 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3364.65216376453
    time_step_min: 3029
  date: 2020-10-14_03-00-05
  done: false
  episode_len_mean: 804.152792658499
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 266.72214670109076
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 235
  episodes_total: 21467
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.16006832818190256
        entropy_coeff: 0.0005000000000000001
        kl: 0.003974445261216412
        model: {}
        policy_loss: -0.007680329455373188
        total_loss: 3.5257490475972495
        vf_explained_var: 0.9923901557922363
        vf_loss: 3.5335094928741455
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.967741935483875
    gpu_util_percent0: 0.30387096774193556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466682441802373
    mean_env_wait_ms: 1.2271124183463795
    mean_inference_ms: 4.32930845131869
    mean_raw_obs_processing_ms: 0.38143839285708087
  time_since_restore: 2827.5538251399994
  time_this_iter_s: 26.24297833442688
  time_total_s: 2827.5538251399994
  timers:
    learn_throughput: 8330.035
    learn_time_ms: 19422.727
    sample_throughput: 23789.861
    sample_time_ms: 6800.88
    update_time_ms: 41.878
  timestamp: 1602644405
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    107 |          2827.55 | 17311744 |  266.722 |              317.465 |              126.556 |            804.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3362.985605183985
    time_step_min: 3029
  date: 2020-10-14_03-00-32
  done: false
  episode_len_mean: 803.8867488799593
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 266.9693232729116
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 184
  episodes_total: 21651
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.15804871171712875
        entropy_coeff: 0.0005000000000000001
        kl: 0.003186442811662952
        model: {}
        policy_loss: -0.010185310539479056
        total_loss: 2.83365664879481
        vf_explained_var: 0.9927825927734375
        vf_loss: 2.843920946121216
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.23548387096774
    gpu_util_percent0: 0.2493548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665823521783003
    mean_env_wait_ms: 1.2271281421514662
    mean_inference_ms: 4.328683271465621
    mean_raw_obs_processing_ms: 0.38140483116208623
  time_since_restore: 2854.109138250351
  time_this_iter_s: 26.555313110351562
  time_total_s: 2854.109138250351
  timers:
    learn_throughput: 8323.357
    learn_time_ms: 19438.311
    sample_throughput: 23753.814
    sample_time_ms: 6811.201
    update_time_ms: 42.562
  timestamp: 1602644432
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    108 |          2854.11 | 17473536 |  266.969 |              317.465 |              126.556 |            803.887 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3361.1240772158285
    time_step_min: 3029
  date: 2020-10-14_03-00-59
  done: false
  episode_len_mean: 803.5962479981697
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 267.25517356128165
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 204
  episodes_total: 21855
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16283127665519714
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035293440256888666
        model: {}
        policy_loss: -0.009930140882109603
        total_loss: 3.3155870834986367
        vf_explained_var: 0.992332398891449
        vf_loss: 3.32559867699941
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.13225806451613
    gpu_util_percent0: 0.31129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466488680648231
    mean_env_wait_ms: 1.2271485577210752
    mean_inference_ms: 4.328020868079851
    mean_raw_obs_processing_ms: 0.3813688789683956
  time_since_restore: 2880.4460122585297
  time_this_iter_s: 26.33687400817871
  time_total_s: 2880.4460122585297
  timers:
    learn_throughput: 8324.576
    learn_time_ms: 19435.465
    sample_throughput: 23767.613
    sample_time_ms: 6807.246
    update_time_ms: 40.119
  timestamp: 1602644459
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    109 |          2880.45 | 17635328 |  267.255 |              317.465 |              126.556 |            803.596 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3358.919040275762
    time_step_min: 3029
  date: 2020-10-14_03-01-26
  done: false
  episode_len_mean: 803.2683986602698
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 267.59086337256844
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 239
  episodes_total: 22094
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1533804809053739
        entropy_coeff: 0.0005000000000000001
        kl: 0.003702786828701695
        model: {}
        policy_loss: -0.00756536432163557
        total_loss: 3.3041084011395774
        vf_explained_var: 0.9929428696632385
        vf_loss: 3.3117505510648093
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39032258064516
    gpu_util_percent0: 0.28967741935483865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663521424904258
    mean_env_wait_ms: 1.2271693641480015
    mean_inference_ms: 4.32724771547934
    mean_raw_obs_processing_ms: 0.3813259502431445
  time_since_restore: 2907.044682264328
  time_this_iter_s: 26.59867000579834
  time_total_s: 2907.044682264328
  timers:
    learn_throughput: 8320.061
    learn_time_ms: 19446.011
    sample_throughput: 23730.905
    sample_time_ms: 6817.776
    update_time_ms: 30.74
  timestamp: 1602644486
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    110 |          2907.04 | 17797120 |  267.591 |              317.465 |              126.556 |            803.268 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3357.2335297027475
    time_step_min: 3029
  date: 2020-10-14_03-01-53
  done: false
  episode_len_mean: 803.0206435399183
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 267.8546842567393
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 189
  episodes_total: 22283
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1497357413172722
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038113862392492592
        model: {}
        policy_loss: -0.008905402617529035
        total_loss: 2.79697593053182
        vf_explained_var: 0.992814302444458
        vf_loss: 2.805956264336904
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.029032258064515
    gpu_util_percent0: 0.29161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662527519682267
    mean_env_wait_ms: 1.2271851120770738
    mean_inference_ms: 4.326643719082094
    mean_raw_obs_processing_ms: 0.38129275894477654
  time_since_restore: 2933.6794736385345
  time_this_iter_s: 26.634791374206543
  time_total_s: 2933.6794736385345
  timers:
    learn_throughput: 8314.532
    learn_time_ms: 19458.942
    sample_throughput: 23753.699
    sample_time_ms: 6811.234
    update_time_ms: 31.479
  timestamp: 1602644513
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    111 |          2933.68 | 17958912 |  267.855 |              317.465 |              126.556 |            803.021 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3355.3479675521485
    time_step_min: 3029
  date: 2020-10-14_03-02-20
  done: false
  episode_len_mean: 802.7871185837558
  episode_reward_max: 317.46464646464614
  episode_reward_mean: 268.1298048539838
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 199
  episodes_total: 22482
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.15533562252918878
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034227332022661963
        model: {}
        policy_loss: -0.008599819440860301
        total_loss: 2.7747987310091653
        vf_explained_var: 0.9934248924255371
        vf_loss: 2.783476233482361
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4
    gpu_util_percent0: 0.2977419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661639642993998
    mean_env_wait_ms: 1.2272025435721658
    mean_inference_ms: 4.326030262858492
    mean_raw_obs_processing_ms: 0.38125924121399046
  time_since_restore: 2960.221446275711
  time_this_iter_s: 26.541972637176514
  time_total_s: 2960.221446275711
  timers:
    learn_throughput: 8312.763
    learn_time_ms: 19463.083
    sample_throughput: 23724.001
    sample_time_ms: 6819.76
    update_time_ms: 40.332
  timestamp: 1602644540
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    112 |          2960.22 | 18120704 |   268.13 |              317.465 |              126.556 |            802.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3353.2744354269585
    time_step_min: 3029
  date: 2020-10-14_03-02-47
  done: false
  episode_len_mean: 802.5259265780438
  episode_reward_max: 317.7676767676764
  episode_reward_mean: 268.44471299845856
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 236
  episodes_total: 22718
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.14903269583980241
        entropy_coeff: 0.0005000000000000001
        kl: 0.003485022229142487
        model: {}
        policy_loss: -0.009994654499071961
        total_loss: 3.5449649492899575
        vf_explained_var: 0.9928091168403625
        vf_loss: 3.555034101009369
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09666666666667
    gpu_util_percent0: 0.25233333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660436972870178
    mean_env_wait_ms: 1.227219728151151
    mean_inference_ms: 4.3253125345204175
    mean_raw_obs_processing_ms: 0.3812199391923653
  time_since_restore: 2986.5579402446747
  time_this_iter_s: 26.336493968963623
  time_total_s: 2986.5579402446747
  timers:
    learn_throughput: 8316.653
    learn_time_ms: 19453.98
    sample_throughput: 23748.288
    sample_time_ms: 6812.786
    update_time_ms: 41.47
  timestamp: 1602644567
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    113 |          2986.56 | 18282496 |  268.445 |              317.768 |              126.556 |            802.526 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3351.645337648705
    time_step_min: 3029
  date: 2020-10-14_03-03-14
  done: false
  episode_len_mean: 802.3264076822348
  episode_reward_max: 317.7676767676764
  episode_reward_mean: 268.6938040377583
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 192
  episodes_total: 22910
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.1389683187007904
        entropy_coeff: 0.0005000000000000001
        kl: 0.003841286369909843
        model: {}
        policy_loss: -0.008985845527301231
        total_loss: 2.876243789990743
        vf_explained_var: 0.9931276440620422
        vf_loss: 2.8852990667025247
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.6
    gpu_util_percent0: 0.2767741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659426050263288
    mean_env_wait_ms: 1.2272307466964945
    mean_inference_ms: 4.324727359923908
    mean_raw_obs_processing_ms: 0.38118672175854584
  time_since_restore: 3012.942694425583
  time_this_iter_s: 26.384754180908203
  time_total_s: 3012.942694425583
  timers:
    learn_throughput: 8323.147
    learn_time_ms: 19438.801
    sample_throughput: 23759.035
    sample_time_ms: 6809.704
    update_time_ms: 40.552
  timestamp: 1602644594
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    114 |          3012.94 | 18444288 |  268.694 |              317.768 |              126.556 |            802.326 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3349.990501799887
    time_step_min: 3029
  date: 2020-10-14_03-03-41
  done: false
  episode_len_mean: 802.1040124659136
  episode_reward_max: 317.9191919191917
  episode_reward_mean: 268.9509876062268
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 193
  episodes_total: 23103
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.1504321781297525
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044296266278252006
        model: {}
        policy_loss: -0.008044386690016836
        total_loss: 2.621249715487162
        vf_explained_var: 0.9937052726745605
        vf_loss: 2.6293693582216897
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.30322580645161
    gpu_util_percent0: 0.2793548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465855822170264
    mean_env_wait_ms: 1.227246979745648
    mean_inference_ms: 4.3241570994165235
    mean_raw_obs_processing_ms: 0.38115538004103705
  time_since_restore: 3039.693533182144
  time_this_iter_s: 26.75083875656128
  time_total_s: 3039.693533182144
  timers:
    learn_throughput: 8315.564
    learn_time_ms: 19456.528
    sample_throughput: 23739.002
    sample_time_ms: 6815.451
    update_time_ms: 40.772
  timestamp: 1602644621
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    115 |          3039.69 | 18606080 |  268.951 |              317.919 |              126.556 |            802.104 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3347.8674507447313
    time_step_min: 3029
  date: 2020-10-14_03-04-08
  done: false
  episode_len_mean: 801.8417941138671
  episode_reward_max: 317.9191919191917
  episode_reward_mean: 269.2704438031513
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 240
  episodes_total: 23343
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.14614903181791306
        entropy_coeff: 0.0005000000000000001
        kl: 0.003426988007655988
        model: {}
        policy_loss: -0.008290602791627558
        total_loss: 3.1870890259742737
        vf_explained_var: 0.9931678175926208
        vf_loss: 3.1954526702562966
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690322580645166
    gpu_util_percent0: 0.3461290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465743195363783
    mean_env_wait_ms: 1.2272602317943047
    mean_inference_ms: 4.323452346366854
    mean_raw_obs_processing_ms: 0.38111660168340755
  time_since_restore: 3065.9695925712585
  time_this_iter_s: 26.27605938911438
  time_total_s: 3065.9695925712585
  timers:
    learn_throughput: 8318.117
    learn_time_ms: 19450.555
    sample_throughput: 23712.122
    sample_time_ms: 6823.177
    update_time_ms: 41.721
  timestamp: 1602644648
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    116 |          3065.97 | 18767872 |   269.27 |              317.919 |              126.556 |            801.842 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3346.197914449883
    time_step_min: 3019
  date: 2020-10-14_03-04-35
  done: false
  episode_len_mean: 801.6325559661866
  episode_reward_max: 318.97979797979735
  episode_reward_mean: 269.5193015924502
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 198
  episodes_total: 23541
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.13649629428982735
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034725334068449834
        model: {}
        policy_loss: -0.00788314324139113
        total_loss: 2.9185951153437295
        vf_explained_var: 0.9930727481842041
        vf_loss: 2.9265465140342712
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.254838709677426
    gpu_util_percent0: 0.27096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465642505980802
    mean_env_wait_ms: 1.2272697513736932
    mean_inference_ms: 4.3228884982595766
    mean_raw_obs_processing_ms: 0.38108444576368244
  time_since_restore: 3092.551374912262
  time_this_iter_s: 26.581782341003418
  time_total_s: 3092.551374912262
  timers:
    learn_throughput: 8310.886
    learn_time_ms: 19467.48
    sample_throughput: 23683.868
    sample_time_ms: 6831.317
    update_time_ms: 51.055
  timestamp: 1602644675
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    117 |          3092.55 | 18929664 |  269.519 |               318.98 |              126.556 |            801.633 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3344.594326480645
    time_step_min: 3019
  date: 2020-10-14_03-05-01
  done: false
  episode_len_mean: 801.403539077312
  episode_reward_max: 318.97979797979735
  episode_reward_mean: 269.763074179758
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 194
  episodes_total: 23735
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.14318112283945084
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033448990046357117
        model: {}
        policy_loss: -0.007795179767223696
        total_loss: 3.4445244471232095
        vf_explained_var: 0.9916593432426453
        vf_loss: 3.45239120721817
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94666666666667
    gpu_util_percent0: 0.2996666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655602963825504
    mean_env_wait_ms: 1.2272840042457018
    mean_inference_ms: 4.322344818270063
    mean_raw_obs_processing_ms: 0.38105452228023867
  time_since_restore: 3118.869304895401
  time_this_iter_s: 26.317929983139038
  time_total_s: 3118.869304895401
  timers:
    learn_throughput: 8319.442
    learn_time_ms: 19447.459
    sample_throughput: 23700.882
    sample_time_ms: 6826.413
    update_time_ms: 51.245
  timestamp: 1602644701
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    118 |          3118.87 | 19091456 |  269.763 |               318.98 |              126.556 |            801.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3342.6826923076924
    time_step_min: 3019
  date: 2020-10-14_03-05-28
  done: false
  episode_len_mean: 801.1335224901944
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.05587418877036
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 231
  episodes_total: 23966
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.14044669270515442
        entropy_coeff: 0.0005000000000000001
        kl: 0.004190241316488634
        model: {}
        policy_loss: -0.0066900534826951725
        total_loss: 3.2881863514582315
        vf_explained_var: 0.9929413199424744
        vf_loss: 3.2949467102686563
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73548387096774
    gpu_util_percent0: 0.2832258064516128
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654597236022682
    mean_env_wait_ms: 1.2272949957333026
    mean_inference_ms: 4.32169683085145
    mean_raw_obs_processing_ms: 0.381019094329169
  time_since_restore: 3145.3988251686096
  time_this_iter_s: 26.529520273208618
  time_total_s: 3145.3988251686096
  timers:
    learn_throughput: 8313.963
    learn_time_ms: 19460.274
    sample_throughput: 23695.007
    sample_time_ms: 6828.105
    update_time_ms: 52.827
  timestamp: 1602644728
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    119 |           3145.4 | 19253248 |  270.056 |               318.98 |              126.556 |            801.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3340.914697836359
    time_step_min: 3019
  date: 2020-10-14_03-05-55
  done: false
  episode_len_mean: 800.9029041866622
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.3153026207797
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 206
  episodes_total: 24172
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.13026844213406244
        entropy_coeff: 0.0005000000000000001
        kl: 0.004086127446498722
        model: {}
        policy_loss: -0.00882166401182379
        total_loss: 2.5615656773249307
        vf_explained_var: 0.9938321113586426
        vf_loss: 2.5704525113105774
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.83225806451613
    gpu_util_percent0: 0.35612903225806436
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653589077744136
    mean_env_wait_ms: 1.2273016853056546
    mean_inference_ms: 4.321137801304064
    mean_raw_obs_processing_ms: 0.3809865792400376
  time_since_restore: 3171.735474586487
  time_this_iter_s: 26.336649417877197
  time_total_s: 3171.735474586487
  timers:
    learn_throughput: 8316.637
    learn_time_ms: 19454.018
    sample_throughput: 23774.097
    sample_time_ms: 6805.39
    update_time_ms: 54.768
  timestamp: 1602644755
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    120 |          3171.74 | 19415040 |  270.315 |               318.98 |              126.556 |            800.903 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3339.3204474420136
    time_step_min: 3019
  date: 2020-10-14_03-06-22
  done: false
  episode_len_mean: 800.6956325424842
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.5538091696039
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 190
  episodes_total: 24362
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.1346232071518898
        entropy_coeff: 0.0005000000000000001
        kl: 0.003765755915082991
        model: {}
        policy_loss: -0.006242665423390766
        total_loss: 2.6872461438179016
        vf_explained_var: 0.9934821128845215
        vf_loss: 2.6935561696688333
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.035483870967745
    gpu_util_percent0: 0.29483870967741926
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652797508308127
    mean_env_wait_ms: 1.2273139750126427
    mean_inference_ms: 4.320625073723211
    mean_raw_obs_processing_ms: 0.3809584279166957
  time_since_restore: 3198.203498840332
  time_this_iter_s: 26.468024253845215
  time_total_s: 3198.203498840332
  timers:
    learn_throughput: 8322.775
    learn_time_ms: 19439.67
    sample_throughput: 23776.459
    sample_time_ms: 6804.714
    update_time_ms: 52.161
  timestamp: 1602644782
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    121 |           3198.2 | 19576832 |  270.554 |               318.98 |              126.556 |            800.696 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3337.4372835784416
    time_step_min: 3019
  date: 2020-10-14_03-06-49
  done: false
  episode_len_mean: 800.4400845769121
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 270.83929483095875
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 231
  episodes_total: 24593
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.1360724059243997
        entropy_coeff: 0.0005000000000000001
        kl: 0.004647130689894159
        model: {}
        policy_loss: -0.007822828213344716
        total_loss: 2.812016487121582
        vf_explained_var: 0.9938087463378906
        vf_loss: 2.819907327493032
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.851612903225814
    gpu_util_percent0: 0.3612903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465185349966775
    mean_env_wait_ms: 1.2273218800326517
    mean_inference_ms: 4.320000690995401
    mean_raw_obs_processing_ms: 0.3809244646166497
  time_since_restore: 3224.856968641281
  time_this_iter_s: 26.653469800949097
  time_total_s: 3224.856968641281
  timers:
    learn_throughput: 8313.722
    learn_time_ms: 19460.838
    sample_throughput: 23781.148
    sample_time_ms: 6803.372
    update_time_ms: 43.633
  timestamp: 1602644809
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    122 |          3224.86 | 19738624 |  270.839 |               318.98 |              126.556 |             800.44 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3335.7258038455325
    time_step_min: 3019
  date: 2020-10-14_03-07-16
  done: false
  episode_len_mean: 800.2232481251511
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 271.0951210353674
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 209
  episodes_total: 24802
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.12468166587253411
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032854301195281246
        model: {}
        policy_loss: -0.009902211371809244
        total_loss: 2.6866191029548645
        vf_explained_var: 0.9937338829040527
        vf_loss: 2.69658362865448
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79677419354839
    gpu_util_percent0: 0.29032258064516137
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650887177896024
    mean_env_wait_ms: 1.2273267361860545
    mean_inference_ms: 4.319461533021429
    mean_raw_obs_processing_ms: 0.38089254831536784
  time_since_restore: 3251.2888128757477
  time_this_iter_s: 26.431844234466553
  time_total_s: 3251.2888128757477
  timers:
    learn_throughput: 8310.762
    learn_time_ms: 19467.77
    sample_throughput: 23775.37
    sample_time_ms: 6805.026
    update_time_ms: 43.14
  timestamp: 1602644836
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    123 |          3251.29 | 19900416 |  271.095 |               318.98 |              126.556 |            800.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3334.2120920535644
    time_step_min: 3019
  date: 2020-10-14_03-07-43
  done: false
  episode_len_mean: 800.0065231311029
  episode_reward_max: 318.97979797979747
  episode_reward_mean: 271.32087927457667
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 186
  episodes_total: 24988
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.1282355785369873
        entropy_coeff: 0.0005000000000000001
        kl: 0.003834822273347527
        model: {}
        policy_loss: -0.0064158480187567575
        total_loss: 2.2745142579078674
        vf_explained_var: 0.9942635893821716
        vf_loss: 2.280994196732839
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.506451612903227
    gpu_util_percent0: 0.26483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650164384905506
    mean_env_wait_ms: 1.22733661407733
    mean_inference_ms: 4.318982252909617
    mean_raw_obs_processing_ms: 0.38086666908201844
  time_since_restore: 3277.7974033355713
  time_this_iter_s: 26.50859045982361
  time_total_s: 3277.7974033355713
  timers:
    learn_throughput: 8301.816
    learn_time_ms: 19488.748
    sample_throughput: 23806.629
    sample_time_ms: 6796.09
    update_time_ms: 41.018
  timestamp: 1602644863
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    124 |           3277.8 | 20062208 |  271.321 |               318.98 |              126.556 |            800.007 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3332.377910210568
    time_step_min: 3006
  date: 2020-10-14_03-08-10
  done: false
  episode_len_mean: 799.7603505710659
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 271.5929776829202
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 228
  episodes_total: 25216
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.1308365762233734
        entropy_coeff: 0.0005000000000000001
        kl: 0.003845173962569485
        model: {}
        policy_loss: -0.00844521870991836
        total_loss: 2.3771212697029114
        vf_explained_var: 0.994819700717926
        vf_loss: 2.3856319387753806
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.767741935483873
    gpu_util_percent0: 0.3270967741935485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649283860171455
    mean_env_wait_ms: 1.2273437490191568
    mean_inference_ms: 4.318404927833164
    mean_raw_obs_processing_ms: 0.38083454570940267
  time_since_restore: 3304.1395428180695
  time_this_iter_s: 26.34213948249817
  time_total_s: 3304.1395428180695
  timers:
    learn_throughput: 8316.469
    learn_time_ms: 19454.41
    sample_throughput: 23825.252
    sample_time_ms: 6790.778
    update_time_ms: 38.234
  timestamp: 1602644890
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    125 |          3304.14 | 20224000 |  271.593 |              320.949 |              126.556 |             799.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3330.7236271960924
    time_step_min: 3006
  date: 2020-10-14_03-08-37
  done: false
  episode_len_mean: 799.5307879836427
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 271.8491310557601
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 216
  episodes_total: 25432
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.12018661511441071
        entropy_coeff: 0.0005000000000000001
        kl: 0.003371649499361714
        model: {}
        policy_loss: -0.008376645351139208
        total_loss: 2.056699365377426
        vf_explained_var: 0.9952095150947571
        vf_loss: 2.065136104822159
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23870967741936
    gpu_util_percent0: 0.31677419354838704
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648284781621826
    mean_env_wait_ms: 1.2273459439953565
    mean_inference_ms: 4.317859320505656
    mean_raw_obs_processing_ms: 0.3808022960430509
  time_since_restore: 3330.6712000370026
  time_this_iter_s: 26.531657218933105
  time_total_s: 3330.6712000370026
  timers:
    learn_throughput: 8311.465
    learn_time_ms: 19466.122
    sample_throughput: 23776.686
    sample_time_ms: 6804.649
    update_time_ms: 38.429
  timestamp: 1602644917
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    126 |          3330.67 | 20385792 |  271.849 |              320.949 |              126.556 |            799.531 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3329.2597270558795
    time_step_min: 3006
  date: 2020-10-14_03-09-04
  done: false
  episode_len_mean: 799.3294820250595
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 272.0700707058875
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 187
  episodes_total: 25619
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.12033804319798946
        entropy_coeff: 0.0005000000000000001
        kl: 0.003876735553300629
        model: {}
        policy_loss: -0.007069124665576965
        total_loss: 1.8338869710763295
        vf_explained_var: 0.9953691363334656
        vf_loss: 1.8410162925720215
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.883870967741938
    gpu_util_percent0: 0.39387096774193553
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146476196414543
    mean_env_wait_ms: 1.227356036844016
    mean_inference_ms: 4.317407542004712
    mean_raw_obs_processing_ms: 0.3807770199141136
  time_since_restore: 3357.240322113037
  time_this_iter_s: 26.569122076034546
  time_total_s: 3357.240322113037
  timers:
    learn_throughput: 8316.613
    learn_time_ms: 19454.074
    sample_throughput: 23708.972
    sample_time_ms: 6824.083
    update_time_ms: 28.815
  timestamp: 1602644944
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    127 |          3357.24 | 20547584 |   272.07 |              320.949 |              126.556 |            799.329 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3327.5211071054773
    time_step_min: 3006
  date: 2020-10-14_03-09-31
  done: false
  episode_len_mean: 799.0939132453663
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 272.33219749247263
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 224
  episodes_total: 25843
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.388131789017202e-22
        cur_lr: 5.0e-05
        entropy: 0.12678717821836472
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034322276478633285
        model: {}
        policy_loss: -0.008822339742134014
        total_loss: 2.5701075394948325
        vf_explained_var: 0.9944183826446533
        vf_loss: 2.578993320465088
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45806451612903
    gpu_util_percent0: 0.3270967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646759759836792
    mean_env_wait_ms: 1.2273604517009016
    mean_inference_ms: 4.316860694316076
    mean_raw_obs_processing_ms: 0.3807457625883122
  time_since_restore: 3383.7400510311127
  time_this_iter_s: 26.49972891807556
  time_total_s: 3383.7400510311127
  timers:
    learn_throughput: 8319.076
    learn_time_ms: 19448.315
    sample_throughput: 23628.162
    sample_time_ms: 6847.422
    update_time_ms: 28.612
  timestamp: 1602644971
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    128 |          3383.74 | 20709376 |  272.332 |              320.949 |              126.556 |            799.094 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3325.8222384503038
    time_step_min: 3006
  date: 2020-10-14_03-09-58
  done: false
  episode_len_mean: 798.8403928790669
  episode_reward_max: 320.94949494949475
  episode_reward_mean: 272.6001869523965
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 221
  episodes_total: 26064
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.694065894508601e-22
        cur_lr: 5.0e-05
        entropy: 0.11459821338454883
        entropy_coeff: 0.0005000000000000001
        kl: 0.004690838201592366
        model: {}
        policy_loss: -0.008129160579604408
        total_loss: 2.066160092751185
        vf_explained_var: 0.9952142834663391
        vf_loss: 2.0743466218312583
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43870967741936
    gpu_util_percent0: 0.3187096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464579124350292
    mean_env_wait_ms: 1.227359777905187
    mean_inference_ms: 4.316327784487333
    mean_raw_obs_processing_ms: 0.38071451573061255
  time_since_restore: 3410.3634181022644
  time_this_iter_s: 26.623367071151733
  time_total_s: 3410.3634181022644
  timers:
    learn_throughput: 8321.092
    learn_time_ms: 19443.601
    sample_throughput: 23579.486
    sample_time_ms: 6861.558
    update_time_ms: 29.593
  timestamp: 1602644998
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    129 |          3410.36 | 20871168 |    272.6 |              320.949 |              126.556 |             798.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3324.338205548983
    time_step_min: 3003
  date: 2020-10-14_03-10-25
  done: false
  episode_len_mean: 798.6358718427368
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 272.8229458284314
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 185
  episodes_total: 26249
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.470329472543005e-23
        cur_lr: 5.0e-05
        entropy: 0.11800270341336727
        entropy_coeff: 0.0005000000000000001
        kl: 0.004974550295931597
        model: {}
        policy_loss: -0.0069315843187117325
        total_loss: 1.7854573428630829
        vf_explained_var: 0.9954321384429932
        vf_loss: 1.7924479246139526
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.8741935483871
    gpu_util_percent0: 0.3516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645139203895038
    mean_env_wait_ms: 1.227369358405791
    mean_inference_ms: 4.315904591725409
    mean_raw_obs_processing_ms: 0.38069043251713425
  time_since_restore: 3436.9545471668243
  time_this_iter_s: 26.591129064559937
  time_total_s: 3436.9545471668243
  timers:
    learn_throughput: 8316.734
    learn_time_ms: 19453.791
    sample_throughput: 23526.685
    sample_time_ms: 6876.957
    update_time_ms: 29.093
  timestamp: 1602645025
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    130 |          3436.95 | 21032960 |  272.823 |              321.404 |              126.556 |            798.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3322.637591765685
    time_step_min: 3003
  date: 2020-10-14_03-10-52
  done: false
  episode_len_mean: 798.3990253853128
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.07911503979005
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 223
  episodes_total: 26472
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2351647362715025e-23
        cur_lr: 5.0e-05
        entropy: 0.12755438623329005
        entropy_coeff: 0.0005000000000000001
        kl: 0.003397385085312029
        model: {}
        policy_loss: -0.007831684575648978
        total_loss: 3.084522763888041
        vf_explained_var: 0.9931008219718933
        vf_loss: 3.0924181739489236
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.567741935483873
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644384738294625
    mean_env_wait_ms: 1.2273723349278725
    mean_inference_ms: 4.315389122565819
    mean_raw_obs_processing_ms: 0.38066121009408715
  time_since_restore: 3463.404358625412
  time_this_iter_s: 26.449811458587646
  time_total_s: 3463.404358625412
  timers:
    learn_throughput: 8321.097
    learn_time_ms: 19443.591
    sample_throughput: 23501.812
    sample_time_ms: 6884.235
    update_time_ms: 28.656
  timestamp: 1602645052
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    131 |           3463.4 | 21194752 |  273.079 |              321.404 |              126.556 |            798.399 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3320.915634616828
    time_step_min: 3003
  date: 2020-10-14_03-11-19
  done: false
  episode_len_mean: 798.1718867076278
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.3368633131854
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 26692
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1175823681357513e-23
        cur_lr: 5.0e-05
        entropy: 0.11687162518501282
        entropy_coeff: 0.0005000000000000001
        kl: 0.004349517015119393
        model: {}
        policy_loss: -0.007446840754710138
        total_loss: 1.96782386302948
        vf_explained_var: 0.9955055713653564
        vf_loss: 1.975329081217448
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.18064516129032
    gpu_util_percent0: 0.2880645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464341516833649
    mean_env_wait_ms: 1.2273702912330984
    mean_inference_ms: 4.31488044235848
    mean_raw_obs_processing_ms: 0.38063100273106054
  time_since_restore: 3489.738259792328
  time_this_iter_s: 26.333901166915894
  time_total_s: 3489.738259792328
  timers:
    learn_throughput: 8333.325
    learn_time_ms: 19415.06
    sample_throughput: 23521.896
    sample_time_ms: 6878.357
    update_time_ms: 30.204
  timestamp: 1602645079
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    132 |          3489.74 | 21356544 |  273.337 |              321.404 |              126.556 |            798.172 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3319.461504062011
    time_step_min: 3003
  date: 2020-10-14_03-11-46
  done: false
  episode_len_mean: 797.9724330357143
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.551457656926
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 188
  episodes_total: 26880
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587911840678756e-23
        cur_lr: 5.0e-05
        entropy: 0.11650159396231174
        entropy_coeff: 0.0005000000000000001
        kl: 0.003250186098739505
        model: {}
        policy_loss: -0.008330285796546377
        total_loss: 2.13726536432902
        vf_explained_var: 0.994652271270752
        vf_loss: 2.145653943220774
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.974193548387106
    gpu_util_percent0: 0.32290322580645153
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464278538057596
    mean_env_wait_ms: 1.2273772160260337
    mean_inference_ms: 4.314469284790494
    mean_raw_obs_processing_ms: 0.38060741131327536
  time_since_restore: 3516.376214027405
  time_this_iter_s: 26.637954235076904
  time_total_s: 3516.376214027405
  timers:
    learn_throughput: 8327.85
    learn_time_ms: 19427.824
    sample_throughput: 23499.556
    sample_time_ms: 6884.896
    update_time_ms: 30.877
  timestamp: 1602645106
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    133 |          3516.38 | 21518336 |  273.551 |              321.404 |              126.556 |            797.972 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3317.772246858832
    time_step_min: 3003
  date: 2020-10-14_03-12-13
  done: false
  episode_len_mean: 797.7334907400575
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 273.81045718753194
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 226
  episodes_total: 27106
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.293955920339378e-24
        cur_lr: 5.0e-05
        entropy: 0.12073460221290588
        entropy_coeff: 0.0005000000000000001
        kl: 0.003983590131004651
        model: {}
        policy_loss: -0.006819716848743458
        total_loss: 2.212758799393972
        vf_explained_var: 0.9949829578399658
        vf_loss: 2.2196388443311057
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.861290322580647
    gpu_util_percent0: 0.33483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642046605840717
    mean_env_wait_ms: 1.227378438786725
    mean_inference_ms: 4.313969050374689
    mean_raw_obs_processing_ms: 0.38057876927386325
  time_since_restore: 3542.7785983085632
  time_this_iter_s: 26.402384281158447
  time_total_s: 3542.7785983085632
  timers:
    learn_throughput: 8333.402
    learn_time_ms: 19414.88
    sample_throughput: 23494.994
    sample_time_ms: 6886.233
    update_time_ms: 32.268
  timestamp: 1602645133
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    134 |          3542.78 | 21680128 |   273.81 |              321.404 |              126.556 |            797.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3316.07107771261
    time_step_min: 3003
  date: 2020-10-14_03-12-40
  done: false
  episode_len_mean: 797.4768718436653
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 274.06779830804527
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 220
  episodes_total: 27326
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.646977960169689e-24
        cur_lr: 5.0e-05
        entropy: 0.10746520323057969
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035201270366087556
        model: {}
        policy_loss: -0.008008376093736539
        total_loss: 2.132255037625631
        vf_explained_var: 0.9950239062309265
        vf_loss: 2.140317132075628
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080645161290324
    gpu_util_percent0: 0.35096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464109218827866
    mean_env_wait_ms: 1.227374519855304
    mean_inference_ms: 4.313481802801959
    mean_raw_obs_processing_ms: 0.38054894367127196
  time_since_restore: 3569.3365516662598
  time_this_iter_s: 26.557953357696533
  time_total_s: 3569.3365516662598
  timers:
    learn_throughput: 8323.224
    learn_time_ms: 19438.621
    sample_throughput: 23511.003
    sample_time_ms: 6881.544
    update_time_ms: 34.386
  timestamp: 1602645160
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    135 |          3569.34 | 21841920 |  274.068 |              321.404 |              126.556 |            797.477 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3314.5969051520115
    time_step_min: 3003
  date: 2020-10-14_03-13-07
  done: false
  episode_len_mean: 797.2722547344698
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 274.28967402937775
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 185
  episodes_total: 27511
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3234889800848445e-24
        cur_lr: 5.0e-05
        entropy: 0.10850300639867783
        entropy_coeff: 0.0005000000000000001
        kl: 0.005260383671460052
        model: {}
        policy_loss: -0.00861841247145397
        total_loss: 1.5884666939576466
        vf_explained_var: 0.9958114624023438
        vf_loss: 1.5971393684546153
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.322580645161295
    gpu_util_percent0: 0.35387096774193544
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640504762737283
    mean_env_wait_ms: 1.2273796379050352
    mean_inference_ms: 4.313092840436538
    mean_raw_obs_processing_ms: 0.38052698133119917
  time_since_restore: 3595.6732318401337
  time_this_iter_s: 26.3366801738739
  time_total_s: 3595.6732318401337
  timers:
    learn_throughput: 8326.631
    learn_time_ms: 19430.667
    sample_throughput: 23554.171
    sample_time_ms: 6868.932
    update_time_ms: 34.389
  timestamp: 1602645187
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | RUNNING  | 172.17.0.4:30672 |    136 |          3595.67 | 22003712 |   274.29 |              321.404 |              126.556 |            797.272 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_b391f_00000:
  custom_metrics:
    time_step_max: 4254
    time_step_mean: 3312.8313353072867
    time_step_min: 3003
  date: 2020-10-14_03-13-34
  done: true
  episode_len_mean: 797.0237923576063
  episode_reward_max: 321.40404040404013
  episode_reward_mean: 274.5610503011364
  episode_reward_min: 126.55555555555502
  episodes_this_iter: 229
  episodes_total: 27740
  experiment_id: 2b1aa22ecf224db495132c80d416ceb6
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3234889800848445e-24
        cur_lr: 5.0e-05
        entropy: 0.1124781674395005
        entropy_coeff: 0.0005000000000000001
        kl: 0.003990503881747524
        model: {}
        policy_loss: -0.009042980663555985
        total_loss: 1.8409209648768108
        vf_explained_var: 0.9957380890846252
        vf_loss: 1.8500201602776845
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4
    gpu_util_percent0: 0.3280645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 30672
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639772551548977
    mean_env_wait_ms: 1.2273802816718886
    mean_inference_ms: 4.3126066337074205
    mean_raw_obs_processing_ms: 0.38049927057914124
  time_since_restore: 3622.2023515701294
  time_this_iter_s: 26.529119729995728
  time_total_s: 3622.2023515701294
  timers:
    learn_throughput: 8320.599
    learn_time_ms: 19444.754
    sample_throughput: 23624.686
    sample_time_ms: 6848.43
    update_time_ms: 36.638
  timestamp: 1602645214
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: b391f_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | TERMINATED |       |    137 |           3622.2 | 22165504 |  274.561 |              321.404 |              126.556 |            797.024 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.71 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_b391f_00000 | TERMINATED |       |    137 |           3622.2 | 22165504 |  274.561 |              321.404 |              126.556 |            797.024 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


