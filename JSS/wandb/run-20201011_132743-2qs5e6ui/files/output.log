2020-10-11 13:27:47,424	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_8e329_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=41170)[0m 2020-10-11 13:27:50,196	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=41145)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41145)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41173)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41173)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41115)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41115)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41142)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41142)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41154)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41154)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41138)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41138)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41163)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41163)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41164)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41164)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41165)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41165)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41160)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41160)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41152)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41152)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41127)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41127)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41102)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41102)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41167)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41167)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41132)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41132)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41111)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41111)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41135)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41135)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41108)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41108)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41034)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41034)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41052)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41052)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41088)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41088)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41047)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41047)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41043)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41043)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41037)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41037)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41035)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41035)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41066)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41066)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41161)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41161)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41048)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41048)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41104)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41104)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41125)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41125)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41087)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41087)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41064)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41064)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41159)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41159)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41038)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41038)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41044)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41044)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41049)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41049)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41062)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41062)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41095)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41095)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41036)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41036)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41060)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41060)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41063)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41063)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41140)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41140)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41053)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41053)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41106)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41106)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41120)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41120)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41116)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41116)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41051)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41051)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41109)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41109)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41086)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41086)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41030)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41030)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41092)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41092)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41148)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41148)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41042)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41042)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41089)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41089)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41050)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41050)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41110)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41110)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41097)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41097)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41113)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41113)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41085)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41085)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41144)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41144)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41057)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41057)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41040)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41040)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41168)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41168)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41098)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41098)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41118)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41118)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41129)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41129)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41119)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41119)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41031)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41031)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41033)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41033)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41046)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41046)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41096)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41096)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41056)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41056)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41032)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41032)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41157)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41157)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41112)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41112)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41091)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41091)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41114)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41114)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41059)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41059)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=41045)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=41045)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_13-28-41
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000007
        cur_lr: 5.000000000000001e-05
        entropy: 1.1836259921391805
        entropy_coeff: 0.00010000000000000002
        kl: 0.00453860421354572
        model: {}
        policy_loss: -0.011187315204491217
        total_loss: 545.98173828125
        vf_explained_var: 0.6184774041175842
        vf_loss: 545.9921427408855
    num_steps_sampled: 242688
    num_steps_trained: 242688
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.81454545454545
    gpu_util_percent0: 0.3563636363636363
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.683636363636365
    vram_util_percent0: 0.0925951876919407
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15961125313999572
    mean_env_wait_ms: 1.1384890069935862
    mean_inference_ms: 5.243066169661045
    mean_raw_obs_processing_ms: 0.4197070199670438
  time_since_restore: 45.76901698112488
  time_this_iter_s: 45.76901698112488
  time_total_s: 45.76901698112488
  timers:
    learn_throughput: 7277.425
    learn_time_ms: 33348.058
    sample_throughput: 19759.205
    sample_time_ms: 12282.276
    update_time_ms: 104.977
  timestamp: 1602422921
  timesteps_since_restore: 0
  timesteps_total: 242688
  training_iteration: 1
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      1 |           45.769 | 242688 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3624.2197309417043
    time_step_min: 3320
  date: 2020-10-11_13-29-25
  done: false
  episode_len_mean: 891.4029535864979
  episode_reward_max: 267.9898989898985
  episode_reward_mean: 217.05171972893467
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 316
  episodes_total: 474
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 1.1429621219635009
        entropy_coeff: 0.00010000000000000002
        kl: 0.00895286314189434
        model: {}
        policy_loss: -0.013155245742139717
        total_loss: 109.84361216227214
        vf_explained_var: 0.85866779088974
        vf_loss: 109.85598907470703
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.59230769230769
    gpu_util_percent0: 0.3613461538461538
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9461538461538463
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15598876569730846
    mean_env_wait_ms: 1.1477638196372097
    mean_inference_ms: 4.95675498529613
    mean_raw_obs_processing_ms: 0.41084055776896705
  time_since_restore: 90.13572072982788
  time_this_iter_s: 44.366703748703
  time_total_s: 90.13572072982788
  timers:
    learn_throughput: 7263.039
    learn_time_ms: 33414.113
    sample_throughput: 21039.069
    sample_time_ms: 11535.111
    update_time_ms: 71.781
  timestamp: 1602422965
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 2
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      2 |          90.1357 | 485376 |  217.052 |               267.99 |              104.505 |            891.403 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3608.4186351706035
    time_step_min: 3271
  date: 2020-10-11_13-30-10
  done: false
  episode_len_mean: 889.6746835443038
  episode_reward_max: 270.41414141414134
  episode_reward_mean: 219.2352001022885
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 316
  episodes_total: 790
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 1.1224578857421874
        entropy_coeff: 0.00010000000000000002
        kl: 0.009366466663777828
        model: {}
        policy_loss: -0.014123790695642431
        total_loss: 47.39630355834961
        vf_explained_var: 0.9297736883163452
        vf_loss: 47.409603373209634
    num_steps_sampled: 728064
    num_steps_trained: 728064
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.130769230769225
    gpu_util_percent0: 0.3692307692307692
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9826923076923078
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15436230326047637
    mean_env_wait_ms: 1.1516819852021623
    mean_inference_ms: 4.830949178514057
    mean_raw_obs_processing_ms: 0.4063440985134774
  time_since_restore: 134.29185461997986
  time_this_iter_s: 44.15613389015198
  time_total_s: 134.29185461997986
  timers:
    learn_throughput: 7253.626
    learn_time_ms: 33457.473
    sample_throughput: 21701.532
    sample_time_ms: 11182.99
    update_time_ms: 62.47
  timestamp: 1602423010
  timesteps_since_restore: 0
  timesteps_total: 728064
  training_iteration: 3
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      3 |          134.292 | 728064 |  219.235 |              270.414 |              104.505 |            889.675 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3599.247608926674
    time_step_min: 3271
  date: 2020-10-11_13-30-53
  done: false
  episode_len_mean: 886.1052631578947
  episode_reward_max: 270.41414141414134
  episode_reward_mean: 220.53672952434542
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 179
  episodes_total: 969
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 1.1104890664418539
        entropy_coeff: 0.00010000000000000002
        kl: 0.009613243117928504
        model: {}
        policy_loss: -0.01490510346678396
        total_loss: 27.5052609761556
        vf_explained_var: 0.9527851939201355
        vf_loss: 27.519315592447917
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.001923076923074
    gpu_util_percent0: 0.34153846153846157
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.984615384615385
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1536822602217629
    mean_env_wait_ms: 1.1533722566620883
    mean_inference_ms: 4.7802595909823
    mean_raw_obs_processing_ms: 0.40396013539837605
  time_since_restore: 178.1227469444275
  time_this_iter_s: 43.83089232444763
  time_total_s: 178.1227469444275
  timers:
    learn_throughput: 7258.23
    learn_time_ms: 33436.249
    sample_throughput: 22151.942
    sample_time_ms: 10955.608
    update_time_ms: 57.319
  timestamp: 1602423053
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 4
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      4 |          178.123 | 970752 |  220.537 |              270.414 |              104.505 |            886.105 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3590.378640776699
    time_step_min: 3271
  date: 2020-10-11_13-31-38
  done: false
  episode_len_mean: 879.8900316455696
  episode_reward_max: 270.41414141414134
  episode_reward_mean: 222.21062683800002
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 295
  episodes_total: 1264
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 1.1098390181859334
        entropy_coeff: 0.00010000000000000002
        kl: 0.009571694272259871
        model: {}
        policy_loss: -0.01428980501368642
        total_loss: 20.329190953572592
        vf_explained_var: 0.9621326327323914
        vf_loss: 20.34263470967611
    num_steps_sampled: 1213440
    num_steps_trained: 1213440
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.059615384615384
    gpu_util_percent0: 0.34673076923076923
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9711538461538463
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15279636839388935
    mean_env_wait_ms: 1.1557184647829477
    mean_inference_ms: 4.713568984655809
    mean_raw_obs_processing_ms: 0.40073992087963817
  time_since_restore: 222.32757019996643
  time_this_iter_s: 44.20482325553894
  time_total_s: 222.32757019996643
  timers:
    learn_throughput: 7246.966
    learn_time_ms: 33488.221
    sample_throughput: 22408.244
    sample_time_ms: 10830.3
    update_time_ms: 53.154
  timestamp: 1602423098
  timesteps_since_restore: 0
  timesteps_total: 1213440
  training_iteration: 5
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      5 |          222.328 | 1213440 |  222.211 |              270.414 |              104.505 |             879.89 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3573.6494845360826
    time_step_min: 3207
  date: 2020-10-11_13-32-22
  done: false
  episode_len_mean: 872.7037974683544
  episode_reward_max: 280.1111111111112
  episode_reward_mean: 224.8686868686867
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 316
  episodes_total: 1580
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 1.0716416517893472
        entropy_coeff: 0.00010000000000000002
        kl: 0.008727749871710936
        model: {}
        policy_loss: -0.014466517977416516
        total_loss: 18.322268295288087
        vf_explained_var: 0.9726875424385071
        vf_loss: 18.335969161987304
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.42549019607843
    gpu_util_percent0: 0.3347058823529412
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.966666666666667
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15208833930397078
    mean_env_wait_ms: 1.1585191020645622
    mean_inference_ms: 4.662321333959507
    mean_raw_obs_processing_ms: 0.39826054266093713
  time_since_restore: 266.23140835762024
  time_this_iter_s: 43.90383815765381
  time_total_s: 266.23140835762024
  timers:
    learn_throughput: 7251.11
    learn_time_ms: 33469.083
    sample_throughput: 22551.928
    sample_time_ms: 10761.297
    update_time_ms: 50.913
  timestamp: 1602423142
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 6
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      6 |          266.231 | 1456128 |  224.869 |              280.111 |              104.505 |            872.704 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3555.532119914347
    time_step_min: 3187
  date: 2020-10-11_13-33-05
  done: false
  episode_len_mean: 866.131329113924
  episode_reward_max: 283.14141414141426
  episode_reward_mean: 227.77788965605401
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 316
  episodes_total: 1896
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 1.042626929283142
        entropy_coeff: 0.00010000000000000002
        kl: 0.008922017676134904
        model: {}
        policy_loss: -0.014446820039302111
        total_loss: 16.306547292073567
        vf_explained_var: 0.9743930101394653
        vf_loss: 16.320206451416016
    num_steps_sampled: 1698816
    num_steps_trained: 1698816
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.75882352941176
    gpu_util_percent0: 0.351764705882353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.980392156862745
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15154191100167583
    mean_env_wait_ms: 1.1611934882493709
    mean_inference_ms: 4.623265885645822
    mean_raw_obs_processing_ms: 0.3963553221171304
  time_since_restore: 309.63760709762573
  time_this_iter_s: 43.40619874000549
  time_total_s: 309.63760709762573
  timers:
    learn_throughput: 7260.882
    learn_time_ms: 33424.04
    sample_throughput: 22738.548
    sample_time_ms: 10672.977
    update_time_ms: 49.876
  timestamp: 1602423185
  timesteps_since_restore: 0
  timesteps_total: 1698816
  training_iteration: 7
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      7 |          309.638 | 1698816 |  227.778 |              283.141 |              104.505 |            866.131 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3540.456043956044
    time_step_min: 3187
  date: 2020-10-11_13-33-49
  done: false
  episode_len_mean: 861.506329113924
  episode_reward_max: 283.14141414141426
  episode_reward_mean: 230.1770051327012
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 316
  episodes_total: 2212
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 1.0142097155253091
        entropy_coeff: 0.00010000000000000002
        kl: 0.008461852930486203
        model: {}
        policy_loss: -0.014874251186847686
        total_loss: 14.363299496968587
        vf_explained_var: 0.9771299362182617
        vf_loss: 14.377429135640462
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.915686274509806
    gpu_util_percent0: 0.40588235294117647
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9862745098039216
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15110490827395756
    mean_env_wait_ms: 1.1634933932972082
    mean_inference_ms: 4.5920220527201465
    mean_raw_obs_processing_ms: 0.39484062125846653
  time_since_restore: 353.26364755630493
  time_this_iter_s: 43.6260404586792
  time_total_s: 353.26364755630493
  timers:
    learn_throughput: 7262.853
    learn_time_ms: 33414.969
    sample_throughput: 22872.779
    sample_time_ms: 10610.342
    update_time_ms: 48.353
  timestamp: 1602423229
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 8
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      8 |          353.264 | 1941504 |  230.177 |              283.141 |              104.505 |            861.506 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3523.8592057761734
    time_step_min: 3187
  date: 2020-10-11_13-34-33
  done: false
  episode_len_mean: 857.3439111463705
  episode_reward_max: 283.14141414141426
  episode_reward_mean: 232.5115534560198
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 309
  episodes_total: 2521
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 0.9919515490531922
        entropy_coeff: 0.00010000000000000002
        kl: 0.007678900131334861
        model: {}
        policy_loss: -0.013160765778350954
        total_loss: 15.556368891398112
        vf_explained_var: 0.9745832681655884
        vf_loss: 15.568861198425292
    num_steps_sampled: 2184192
    num_steps_trained: 2184192
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.878431372549024
    gpu_util_percent0: 0.3752941176470589
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.990196078431373
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15075653339286577
    mean_env_wait_ms: 1.1654211749024792
    mean_inference_ms: 4.567040935798658
    mean_raw_obs_processing_ms: 0.39360459695069644
  time_since_restore: 396.81020069122314
  time_this_iter_s: 43.54655313491821
  time_total_s: 396.81020069122314
  timers:
    learn_throughput: 7264.697
    learn_time_ms: 33406.488
    sample_throughput: 22994.557
    sample_time_ms: 10554.15
    update_time_ms: 47.207
  timestamp: 1602423273
  timesteps_since_restore: 0
  timesteps_total: 2184192
  training_iteration: 9
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |      9 |           396.81 | 2184192 |  232.512 |              283.141 |              104.505 |            857.344 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3514.401245877611
    time_step_min: 3187
  date: 2020-10-11_13-35-16
  done: false
  episode_len_mean: 854.53173739572
  episode_reward_max: 283.14141414141426
  episode_reward_mean: 233.94246051373352
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 236
  episodes_total: 2757
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 0.9651933272679647
        entropy_coeff: 0.00010000000000000002
        kl: 0.007387231849133968
        model: {}
        policy_loss: -0.01327707921154797
        total_loss: 12.323877080281575
        vf_explained_var: 0.97861248254776
        vf_loss: 12.336511739095052
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.89411764705882
    gpu_util_percent0: 0.3737254901960784
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9843137254901957
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15052204929428417
    mean_env_wait_ms: 1.1667117806854257
    mean_inference_ms: 4.55057401997421
    mean_raw_obs_processing_ms: 0.39273002919461775
  time_since_restore: 440.2906367778778
  time_this_iter_s: 43.48043608665466
  time_total_s: 440.2906367778778
  timers:
    learn_throughput: 7269.263
    learn_time_ms: 33385.503
    sample_throughput: 23079.869
    sample_time_ms: 10515.138
    update_time_ms: 46.454
  timestamp: 1602423316
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 10
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |     10 |          440.291 | 2426880 |  233.942 |              283.141 |              104.505 |            854.532 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3504.11025210084
    time_step_min: 3187
  date: 2020-10-11_13-36-00
  done: false
  episode_len_mean: 852.7742257742258
  episode_reward_max: 283.14141414141426
  episode_reward_mean: 235.4020323111231
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 246
  episodes_total: 3003
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 0.9411561250686645
        entropy_coeff: 0.00010000000000000002
        kl: 0.007863671022156874
        model: {}
        policy_loss: -0.013980729039758444
        total_loss: 9.806815083821615
        vf_explained_var: 0.9810381531715393
        vf_loss: 9.820103200276693
    num_steps_sampled: 2669568
    num_steps_trained: 2669568
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.992156862745095
    gpu_util_percent0: 0.325686274509804
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9784313725490197
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15031538158407967
    mean_env_wait_ms: 1.1678945950094397
    mean_inference_ms: 4.535028366229757
    mean_raw_obs_processing_ms: 0.39190689267932477
  time_since_restore: 483.74223589897156
  time_this_iter_s: 43.45159912109375
  time_total_s: 483.74223589897156
  timers:
    learn_throughput: 7275.013
    learn_time_ms: 33359.115
    sample_throughput: 23532.256
    sample_time_ms: 10312.993
    update_time_ms: 38.442
  timestamp: 1602423360
  timesteps_since_restore: 0
  timesteps_total: 2669568
  training_iteration: 11
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |     11 |          483.742 | 2669568 |  235.402 |              283.141 |              104.505 |            852.774 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3493.0048632218845
    time_step_min: 3185
  date: 2020-10-11_13-36-43
  done: false
  episode_len_mean: 850.4011452682339
  episode_reward_max: 288.2929292929293
  episode_reward_mean: 237.2107238752807
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 315
  episodes_total: 3318
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 0.9120607892672221
        entropy_coeff: 0.00010000000000000002
        kl: 0.007893045525997877
        model: {}
        policy_loss: -0.013711243526389201
        total_loss: 11.874747212727865
        vf_explained_var: 0.9800742864608765
        vf_loss: 11.887759908040364
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.176
    gpu_util_percent0: 0.36840000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.98
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1500790582593129
    mean_env_wait_ms: 1.169242339896712
    mean_inference_ms: 4.517749073692294
    mean_raw_obs_processing_ms: 0.3909656550186306
  time_since_restore: 527.114818572998
  time_this_iter_s: 43.37258267402649
  time_total_s: 527.114818572998
  timers:
    learn_throughput: 7283.02
    learn_time_ms: 33322.442
    sample_throughput: 23676.983
    sample_time_ms: 10249.955
    update_time_ms: 37.335
  timestamp: 1602423403
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 12
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |     12 |          527.115 | 2912256 |  237.211 |              288.293 |              104.505 |            850.401 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3481.382418191902
    time_step_min: 3180
  date: 2020-10-11_13-37-27
  done: false
  episode_len_mean: 848.0055035773253
  episode_reward_max: 288.59595959595924
  episode_reward_mean: 238.89060945169905
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 316
  episodes_total: 3634
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 0.8850514888763428
        entropy_coeff: 0.00010000000000000002
        kl: 0.0076703119402130445
        model: {}
        policy_loss: -0.01318574029331406
        total_loss: 11.05382506052653
        vf_explained_var: 0.9821346402168274
        vf_loss: 11.066332054138183
    num_steps_sampled: 3154944
    num_steps_trained: 3154944
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.840384615384615
    gpu_util_percent0: 0.37538461538461537
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9826923076923078
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14987212254513332
    mean_env_wait_ms: 1.1704602550680259
    mean_inference_ms: 4.502684691317172
    mean_raw_obs_processing_ms: 0.390138569122368
  time_since_restore: 570.9465341567993
  time_this_iter_s: 43.83171558380127
  time_total_s: 570.9465341567993
  timers:
    learn_throughput: 7282.897
    learn_time_ms: 33323.005
    sample_throughput: 23746.399
    sample_time_ms: 10219.992
    update_time_ms: 36.48
  timestamp: 1602423447
  timesteps_since_restore: 0
  timesteps_total: 3154944
  training_iteration: 13
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | RUNNING  | 172.17.0.4:41170 |     13 |          570.947 | 3154944 |  238.891 |              288.596 |              104.505 |            848.006 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_8e329_00000:
  custom_metrics:
    time_step_max: 4068
    time_step_mean: 3471.784548699643
    time_step_min: 3148
  date: 2020-10-11_13-38-12
  done: true
  episode_len_mean: 845.487088607595
  episode_reward_max: 293.74747474747494
  episode_reward_mean: 240.34739803094223
  episode_reward_min: 104.50505050505022
  episodes_this_iter: 316
  episodes_total: 3950
  experiment_id: 8f13aa0e9ada45dd9f3ae7c16675a031
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000003
        cur_lr: 5.000000000000001e-05
        entropy: 0.8645058353741963
        entropy_coeff: 0.00010000000000000002
        kl: 0.006960580994685491
        model: {}
        policy_loss: -0.013238898893663038
        total_loss: 10.485941632588704
        vf_explained_var: 0.9829925298690796
        vf_loss: 10.498570760091146
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.45384615384615
    gpu_util_percent0: 0.3515384615384616
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.9826923076923078
    vram_util_percent0: 0.10897015414890127
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 41170
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496914364711925
    mean_env_wait_ms: 1.171620078953128
    mean_inference_ms: 4.489457066703543
    mean_raw_obs_processing_ms: 0.38942676483620237
  time_since_restore: 615.1968057155609
  time_this_iter_s: 44.2502715587616
  time_total_s: 615.1968057155609
  timers:
    learn_throughput: 7270.319
    learn_time_ms: 33380.653
    sample_throughput: 23767.557
    sample_time_ms: 10210.894
    update_time_ms: 36.096
  timestamp: 1602423492
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 14
  trial_id: 8e329_00000
  
== Status ==
Memory usage on this node: 29.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | TERMINATED |       |     14 |          615.197 | 3397632 |  240.347 |              293.747 |              104.505 |            845.487 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 29.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.2 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_8e329_00000 | TERMINATED |       |     14 |          615.197 | 3397632 |  240.347 |              293.747 |              104.505 |            845.487 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


