2020-11-30 14:59:57,676	INFO services.py:1090 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
2020-11-30 14:59:59,985	INFO trainer.py:592 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution
[2m[36m(pid=6212)[0m WARNING:tensorflow:From /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
[2m[36m(pid=6212)[0m Instructions for updating:
[2m[36m(pid=6212)[0m non-resource variables are not supported in the long term
[2m[36m(pid=6212)[0m 2020-11-30 15:00:02,285	INFO rollout_worker.py:437 -- Could not seed torch
[2m[36m(pid=6212)[0m 2020-11-30 15:00:02,348	INFO catalog.py:306 -- Wrapping <class 'JSS.models.FCMaskedActionsModelTF'> as None
[2m[36m(pid=6212)[0m WARNING:tensorflow:From /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
[2m[36m(pid=6212)[0m Instructions for updating:
[2m[36m(pid=6212)[0m If using Keras pass *_constraint arguments to layers.
2020-11-30 15:00:03,998	INFO rollout_worker.py:429 -- Env doesn't support env.seed(): None
2020-11-30 15:00:03,998	INFO rollout_worker.py:437 -- Could not seed torch
2020-11-30 15:00:04,222	INFO catalog.py:306 -- Wrapping <class 'JSS.models.FCMaskedActionsModelTF'> as None
WARNING:tensorflow:From /root/miniconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2020-11-30 15:00:06,867	INFO rollout_worker.py:1063 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7f6f480f2910>}
2020-11-30 15:00:06,867	INFO rollout_worker.py:1064 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f6f480f2670>}
2020-11-30 15:00:06,867	INFO rollout_worker.py:495 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f6f480f2520>}
2020-11-30 15:00:09,508	WARNING util.py:40 -- Install gputil for GPU system monitoring.
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,514	INFO rollout_worker.py:618 -- Generating sample batch of size 512
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,516	INFO sampler.py:554 -- Raw obs from env: { 0: { 'agent0': { 'action_mask': np.ndarray((51,), dtype=bool, min=0.0, max=1.0, mean=0.98),
[2m[36m(pid=6212)[0m                    'real_obs': np.ndarray((50, 7), dtype=float64, min=0.0, max=1.0, mean=0.143)}}}
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,516	INFO sampler.py:556 -- Info return from env: {0: {'agent0': None}}
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,517	INFO sampler.py:795 -- Preprocessed obs: np.ndarray((401,), dtype=float64, min=0.0, max=1.0, mean=0.249)
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,518	INFO sampler.py:800 -- Filtered obs: np.ndarray((401,), dtype=float64, min=0.0, max=1.0, mean=0.249)
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,521	INFO sampler.py:1198 -- Inputs to compute_actions():
[2m[36m(pid=6212)[0m 
[2m[36m(pid=6212)[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',
[2m[36m(pid=6212)[0m                                   'env_id': 0,
[2m[36m(pid=6212)[0m                                   'info': None,
[2m[36m(pid=6212)[0m                                   'obs': np.ndarray((401,), dtype=float64, min=0.0, max=1.0, mean=0.249),
[2m[36m(pid=6212)[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=6212)[0m                                   'prev_reward': 0.0,
[2m[36m(pid=6212)[0m                                   'rnn_state': []},
[2m[36m(pid=6212)[0m                         'type': 'PolicyEvalData'}]}
[2m[36m(pid=6212)[0m 
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,522	INFO tf_run_builder.py:86 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.
[2m[36m(pid=6212)[0m 2020-11-30 15:00:09,606	INFO sampler.py:1244 -- Outputs of compute_actions():
[2m[36m(pid=6212)[0m 
[2m[36m(pid=6212)[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=12.0, max=12.0, mean=12.0),
[2m[36m(pid=6212)[0m                       [],
[2m[36m(pid=6212)[0m                       { 'action_dist_inputs': np.ndarray((1, 51), dtype=float32, min=-0.005, max=0.004, mean=-0.0),
[2m[36m(pid=6212)[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.932, max=-3.932, mean=-3.932),
[2m[36m(pid=6212)[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.02, max=0.02, mean=0.02),
[2m[36m(pid=6212)[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.0, max=-0.0, mean=-0.0)})}
[2m[36m(pid=6212)[0m 
2020-11-30 15:00:10,992	INFO rollout_ops.py:165 -- Collected more training samples than expected (actual=512, expected=100). This may be because you have many workers or long episodes in 'complete_episodes' batch mode.
2020-11-30 15:00:10,993	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(350, 1024) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(1024,) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/kernel:0' shape=(350, 1024) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_value_1/bias:0' shape=(1024,) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(1024, 1024) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(1024,) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/kernel:0' shape=(1024, 1024) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_value_2/bias:0' shape=(1024,) dtype=float32>
2020-11-30 15:00:10,994	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_out/kernel:0' shape=(1024, 51) dtype=float32>
2020-11-30 15:00:10,995	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/fc_out/bias:0' shape=(51,) dtype=float32>
2020-11-30 15:00:10,995	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/value_out/kernel:0' shape=(1024, 1) dtype=float32>
2020-11-30 15:00:10,995	INFO tf_policy.py:625 -- Optimizing variable <tf.Variable 'default_policy/value_out/bias:0' shape=(1,) dtype=float32>
2020-11-30 15:00:11,002	INFO multi_gpu_impl.py:138 -- Training on concatenated sample batches:

{ 'inputs': [ np.ndarray((512,), dtype=int64, min=0.0, max=50.0, mean=24.213),
              np.ndarray((512,), dtype=float32, min=-0.212, max=1.0, mean=0.654),
              np.ndarray((512, 401), dtype=float32, min=0.0, max=1.0, mean=0.113),
              np.ndarray((512, 51), dtype=float32, min=-0.005, max=0.005, mean=0.0),
              np.ndarray((512,), dtype=float32, min=-3.936, max=-3.928, mean=-3.932),
              np.ndarray((512,), dtype=int64, min=0.0, max=50.0, mean=24.285),
              np.ndarray((512,), dtype=float32, min=-1.768, max=1.641, mean=0.0),
              np.ndarray((512, 401), dtype=float32, min=0.0, max=1.0, mean=0.113),
              np.ndarray((512,), dtype=int64, min=0.0, max=50.0, mean=24.213),
              np.ndarray((512,), dtype=float32, min=-0.212, max=1.0, mean=0.654),
              np.ndarray((512,), dtype=float32, min=0.552, max=335.198, mean=174.127),
              np.ndarray((512,), dtype=float32, min=-0.004, max=0.001, mean=-0.002)],
  'placeholders': [ <tf.Tensor 'default_policy/prev_action:0' shape=(?,) dtype=int64>,
                    <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
                    <tf.Tensor 'default_policy/observation:0' shape=(?, 401) dtype=float32>,
                    <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 51) dtype=float32>,
                    <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,
                    <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,
                    <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,
                    <tf.Tensor 'default_policy/observation:0' shape=(?, 401) dtype=float32>,
                    <tf.Tensor 'default_policy/prev_action:0' shape=(?,) dtype=int64>,
                    <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,
                    <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,
                    <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],
  'state_inputs': []}

2020-11-30 15:00:11,002	INFO multi_gpu_impl.py:184 -- Divided 512 rollout sequences, each of length 1, among 1 devices.
[2m[36m(pid=6212)[0m 2020-11-30 15:00:10,978	INFO sample_batch_builder.py:209 -- Trajectory fragment after postprocess_trajectory():
[2m[36m(pid=6212)[0m 
[2m[36m(pid=6212)[0m { 'agent0': { 'data': { 'action_dist_inputs': np.ndarray((512, 51), dtype=float32, min=-0.005, max=0.005, mean=0.0),
[2m[36m(pid=6212)[0m                         'action_logp': np.ndarray((512,), dtype=float32, min=-3.936, max=-3.928, mean=-3.932),
[2m[36m(pid=6212)[0m                         'action_prob': np.ndarray((512,), dtype=float32, min=0.02, max=0.02, mean=0.02),
[2m[36m(pid=6212)[0m                         'actions': np.ndarray((512,), dtype=int64, min=0.0, max=50.0, mean=24.285),
[2m[36m(pid=6212)[0m                         'advantages': np.ndarray((512,), dtype=float32, min=0.556, max=335.199, mean=174.129),
[2m[36m(pid=6212)[0m                         'agent_index': np.ndarray((512,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=6212)[0m                         'dones': np.ndarray((512,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=6212)[0m                         'eps_id': np.ndarray((512,), dtype=int64, min=288545018.0, max=288545018.0, mean=288545018.0),
[2m[36m(pid=6212)[0m                         'infos': np.ndarray((512,), dtype=object, head={}),
[2m[36m(pid=6212)[0m                         'new_obs': np.ndarray((512, 401), dtype=float32, min=0.0, max=1.0, mean=0.113),
[2m[36m(pid=6212)[0m                         'obs': np.ndarray((512, 401), dtype=float32, min=0.0, max=1.0, mean=0.113),
[2m[36m(pid=6212)[0m                         'prev_actions': np.ndarray((512,), dtype=int64, min=0.0, max=50.0, mean=24.213),
[2m[36m(pid=6212)[0m                         'prev_rewards': np.ndarray((512,), dtype=float32, min=-0.212, max=1.0, mean=0.654),
[2m[36m(pid=6212)[0m                         'rewards': np.ndarray((512,), dtype=float32, min=-0.212, max=1.0, mean=0.655),
[2m[36m(pid=6212)[0m                         't': np.ndarray((512,), dtype=int64, min=0.0, max=511.0, mean=255.5),
[2m[36m(pid=6212)[0m                         'unroll_id': np.ndarray((512,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=6212)[0m                         'value_targets': np.ndarray((512,), dtype=float32, min=0.552, max=335.198, mean=174.127),
[2m[36m(pid=6212)[0m                         'vf_preds': np.ndarray((512,), dtype=float32, min=-0.004, max=0.001, mean=-0.002)},
[2m[36m(pid=6212)[0m               'type': 'SampleBatch'}}
[2m[36m(pid=6212)[0m 
[2m[36m(pid=6212)[0m 2020-11-30 15:00:10,987	INFO rollout_worker.py:652 -- Completed sample batch:
[2m[36m(pid=6212)[0m 
[2m[36m(pid=6212)[0m { 'data': { 'action_dist_inputs': np.ndarray((512, 51), dtype=float32, min=-0.005, max=0.005, mean=0.0),
[2m[36m(pid=6212)[0m             'action_logp': np.ndarray((512,), dtype=float32, min=-3.936, max=-3.928, mean=-3.932),
[2m[36m(pid=6212)[0m             'action_prob': np.ndarray((512,), dtype=float32, min=0.02, max=0.02, mean=0.02),
[2m[36m(pid=6212)[0m             'actions': np.ndarray((512,), dtype=int64, min=0.0, max=50.0, mean=24.285),
[2m[36m(pid=6212)[0m             'advantages': np.ndarray((512,), dtype=float32, min=0.556, max=335.199, mean=174.129),
[2m[36m(pid=6212)[0m             'agent_index': np.ndarray((512,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=6212)[0m             'dones': np.ndarray((512,), dtype=bool, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=6212)[0m             'eps_id': np.ndarray((512,), dtype=int64, min=288545018.0, max=288545018.0, mean=288545018.0),
[2m[36m(pid=6212)[0m             'infos': np.ndarray((512,), dtype=object, head={}),
[2m[36m(pid=6212)[0m             'new_obs': np.ndarray((512, 401), dtype=float32, min=0.0, max=1.0, mean=0.113),
[2m[36m(pid=6212)[0m             'obs': np.ndarray((512, 401), dtype=float32, min=0.0, max=1.0, mean=0.113),
[2m[36m(pid=6212)[0m             'prev_actions': np.ndarray((512,), dtype=int64, min=0.0, max=50.0, mean=24.213),
[2m[36m(pid=6212)[0m             'prev_rewards': np.ndarray((512,), dtype=float32, min=-0.212, max=1.0, mean=0.654),
[2m[36m(pid=6212)[0m             'rewards': np.ndarray((512,), dtype=float32, min=-0.212, max=1.0, mean=0.655),
[2m[36m(pid=6212)[0m             't': np.ndarray((512,), dtype=int64, min=0.0, max=511.0, mean=255.5),
[2m[36m(pid=6212)[0m             'unroll_id': np.ndarray((512,), dtype=int64, min=0.0, max=0.0, mean=0.0),
[2m[36m(pid=6212)[0m             'value_targets': np.ndarray((512,), dtype=float32, min=0.552, max=335.198, mean=174.127),
[2m[36m(pid=6212)[0m             'vf_preds': np.ndarray((512,), dtype=float32, min=-0.004, max=0.001, mean=-0.002)},
[2m[36m(pid=6212)[0m   'type': 'SampleBatch'}
[2m[36m(pid=6212)[0m 
WARNING:tensorflow:From /root/miniconda3/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Prefer Variable.assign which has equivalent behavior in 2.X.
[2m[36m(pid=6212)[0m WARNING:tensorflow:From /root/miniconda3/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:875: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.
[2m[36m(pid=6212)[0m Instructions for updating:
[2m[36m(pid=6212)[0m Prefer Variable.assign which has equivalent behavior in 2.X.
2020-11-30 15:00:14,650	INFO rollout_ops.py:165 -- Collected more training samples than expected (actual=512, expected=100). This may be because you have many workers or long episodes in 'complete_episodes' batch mode.
2020-11-30 15:00:17,834	INFO rollout_ops.py:165 -- Collected more training samples than expected (actual=512, expected=100). This may be because you have many workers or long episodes in 'complete_episodes' batch mode.
2020-11-30 15:00:21,050	INFO rollout_ops.py:165 -- Collected more training samples than expected (actual=512, expected=100). This may be because you have many workers or long episodes in 'complete_episodes' batch mode.
2020-11-30 15:00:23,216	INFO trainer.py:513 -- Worker crashed during call to train(). To attempt to continue training without the failed worker, set `'ignore_worker_failures': True`.
Traceback (most recent call last):
  File "train_wandb.py", line 142, in <module>
    train_func()
  File "train_wandb.py", line 132, in train_func
    result = trainer.train()
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 517, in train
    raise e
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer.py", line 506, in train
    result = Trainable.train(self)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/trainable.py", line 336, in train
    result = self.step()
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py", line 147, in step
    res = next(self.train_exec_impl)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 756, in __next__
    return next(self.built_iterator)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 843, in apply_filter
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  [Previous line repeated 1 more time]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 876, in apply_flatten
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 828, in add_wait_hooks
    item = next(it)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 783, in apply_foreach
    for item in it:
  [Previous line repeated 1 more time]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 471, in base_iterator
    yield ray.get(futures, timeout=timeout)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/worker.py", line 1452, in get
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ValueError): [36mray::RolloutWorker.par_iter_next()[39m (pid=6212, ip=172.17.0.16)
  File "python/ray/_raylet.pyx", line 482, in ray._raylet.execute_task
  File "python/ray/_raylet.pyx", line 436, in ray._raylet.execute_task.function_executor
  File "/root/miniconda3/lib/python3.8/site-packages/ray/util/iter.py", line 1152, in par_iter_next
    return next(self.local_it)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 317, in gen_rollouts
    yield self.sample()
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py", line 621, in sample
    batches = [self.input_reader.next()]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 94, in next
    batches = [self.get_data()]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 211, in get_data
    item = next(self.rollout_provider)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 584, in _env_runner
    active_envs, to_eval, outputs = _process_observations(
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/evaluation/sampler.py", line 792, in _process_observations
    prep_obs: EnvObsType = _get_or_raise(preprocessors,
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py", line 234, in transform
    self.check_shape(observation)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/rllib/models/preprocessors.py", line 62, in check_shape
    raise ValueError(
ValueError: ('Observation ({}) outside given space ({})!', {'real_obs': array([[0.00000000e+00, 2.92929293e-01, 7.33333333e-01, 7.98974359e-01,
        4.94949495e-01, 1.00512821e-03, 3.63076923e-03],
       [0.00000000e+00, 4.04040404e-02, 5.33333333e-01, 8.06153846e-01,
        3.03030303e-01, 1.23076923e-04, 4.36923077e-03],
       [0.00000000e+00, 4.14141414e-01, 2.00000000e-01, 8.02051282e-01,
        6.16161616e-01, 1.04615385e-03, 3.52820513e-03],
       [0.00000000e+00, 8.98989899e-01, 4.00000000e-01, 8.32820513e-01,
        8.28282828e-01, 2.66666667e-04, 3.69230769e-03],
       [0.00000000e+00, 0.00000000e+00, 7.33333333e-01, 8.26666667e-01,
        6.36363636e-01, 7.17948718e-04, 3.36410256e-03],
       [0.00000000e+00, 0.00000000e+00, 5.33333333e-01, 5.24102564e-01,
        9.09090909e-01, 2.13333333e-03, 8.00000000e-03],
       [1.00000000e+00, 0.00000000e+00, 5.33333333e-01, 7.28205128e-01,
        1.61616162e-01, 3.48717949e-04, 5.70256410e-03],
       [0.00000000e+00, 0.00000000e+00, 4.00000000e-01, 6.60512821e-01,
        7.27272727e-01, 2.09230769e-03, 5.31282051e-03],
       [0.00000000e+00, 0.00000000e+00, 2.66666667e-01, 8.93333333e-01,
        7.87878788e-01, 1.31282051e-03, 1.43589744e-03],
       [0.00000000e+00, 3.33333333e-01, 4.00000000e-01, 7.13846154e-01,
        7.17171717e-01, 1.39487179e-03, 4.94358974e-03],
       [0.00000000e+00, 3.43434343e-01, 6.00000000e-01, 7.38461538e-01,
        4.14141414e-01, 4.10256410e-05, 5.80512821e-03],
       [0.00000000e+00, 4.94949495e-01, 1.33333333e-01, 9.08717949e-01,
        8.68686869e-01, 1.02564103e-03, 1.41538462e-03],
       [0.00000000e+00, 2.32323232e-01, 4.00000000e-01, 6.08205128e-01,
        9.09090909e-01, 1.02564103e-03, 7.42564103e-03],
       [0.00000000e+00, 0.00000000e+00, 5.33333333e-01, 7.00512821e-01,
        5.75757576e-01, 2.25641026e-04, 6.37948718e-03],
       [0.00000000e+00, 4.04040404e-02, 4.66666667e-01, 7.77435897e-01,
        5.75757576e-01, 1.37435897e-03, 3.69230769e-03],
       [0.00000000e+00, 1.61616162e-01, 4.00000000e-01, 7.10769231e-01,
        7.27272727e-01, 1.23076923e-04, 6.27692308e-03],
       [0.00000000e+00, 0.00000000e+00, 4.00000000e-01, 7.28205128e-01,
        5.65656566e-01, 1.86666667e-03, 4.18461538e-03],
       [0.00000000e+00, 0.00000000e+00, 3.33333333e-01, 9.14871795e-01,
        8.68686869e-01, 5.33333333e-04, 1.78461538e-03],
       [0.00000000e+00, 0.00000000e+00, 6.00000000e-01, 6.72820513e-01,
        5.45454545e-01, 1.02564103e-03, 6.13333333e-03],
       [0.00000000e+00, 0.00000000e+00, 3.33333333e-01, 8.10256410e-01,
        5.85858586e-01, 3.28205128e-04, 4.08205128e-03],
       [0.00000000e+00, 6.16161616e-01, 2.00000000e-01, 9.94871795e-01,
        6.06060606e-01, 1.64102564e-04, 5.53846154e-04],
       [0.00000000e+00, 9.59595960e-01, 2.66666667e-01, 7.85641026e-01,
        5.55555556e-01, 3.17948718e-03, 1.72307692e-03],
       [0.00000000e+00, 2.82828283e-01, 5.33333333e-01, 8.89230769e-01,
        1.01010101e-01, 8.20512821e-05, 2.74871795e-03],
       [0.00000000e+00, 1.61616162e-01, 2.66666667e-01, 8.96410256e-01,
        5.75757576e-01, 3.07692308e-04, 2.37948718e-03],
       [0.00000000e+00, 0.00000000e+00, 3.33333333e-01, 7.73333333e-01,
        4.44444444e-01, 4.10256410e-05, 5.10769231e-03],
       [0.00000000e+00, 5.15151515e-01, 6.66666667e-02, 9.64102564e-01,
        2.02020202e-01, 1.21025641e-03, 1.23076923e-04],
       [0.00000000e+00, 5.05050505e-01, 1.33333333e-01, 1.02256410e+00,
        4.94949495e-01, 1.64102564e-04, 0.00000000e+00],
       [0.00000000e+00, 5.75757576e-01, 2.66666667e-01, 9.63076923e-01,
        3.43434343e-01, 1.64102564e-04, 1.18974359e-03],
       [0.00000000e+00, 2.92929293e-01, 1.33333333e-01, 9.50769231e-01,
        5.85858586e-01, 1.14871795e-03, 4.51282051e-04],
       [0.00000000e+00, 4.24242424e-01, 2.66666667e-01, 9.26153846e-01,
        7.07070707e-02, 1.43589744e-04, 1.94871795e-03],
       [0.00000000e+00, 0.00000000e+00, 2.66666667e-01, 8.08205128e-