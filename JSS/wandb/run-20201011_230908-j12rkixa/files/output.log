2020-10-11 23:09:12,426	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_c73fa_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=7427)[0m 2020-10-11 23:09:15,085	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=7367)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7367)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7351)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7351)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7303)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7303)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7369)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7369)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7371)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7371)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7363)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7363)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7365)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7365)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7384)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7384)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7403)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7403)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7319)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7319)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7360)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7360)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7299)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7299)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7419)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7419)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7389)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7389)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7305)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7305)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7425)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7425)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7395)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7395)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7362)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7362)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7291)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7291)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7290)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7290)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7412)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7412)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7373)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7373)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7418)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7418)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7407)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7407)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7327)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7327)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7358)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7358)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7289)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7289)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7392)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7392)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7304)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7304)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7394)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7394)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7408)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7408)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7383)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7383)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7405)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7405)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7295)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7295)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7316)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7316)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7297)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7297)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7415)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7415)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7298)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7298)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7310)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7310)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7307)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7307)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7288)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7288)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7379)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7379)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7306)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7306)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7410)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7410)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7301)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7301)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7404)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7404)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7308)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7308)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7293)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7293)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7361)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7361)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7390)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7390)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7323)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7323)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7318)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7318)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7366)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7366)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7364)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7364)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7423)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7423)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7380)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7380)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7294)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7294)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7353)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7353)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7349)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7349)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7376)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7376)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7292)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7292)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7357)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7357)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7368)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7368)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7302)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7302)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7387)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7387)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7320)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7320)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7359)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7359)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7397)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7397)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7325)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7325)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7375)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7375)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7331)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7331)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7313)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7313)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7385)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7385)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7374)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7374)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7396)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7396)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7372)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7372)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7355)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7355)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7356)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7356)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=7329)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=7329)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_23-09-53
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.29999999999999993
        cur_lr: 5.0e-05
        entropy: 1.1857036153475444
        entropy_coeff: 0.0001
        kl: 0.003818406257778406
        model: {}
        policy_loss: -0.01046836975729093
        total_loss: 502.23729197184247
        vf_explained_var: 0.5664147734642029
        vf_loss: 502.24672444661456
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.757894736842108
    gpu_util_percent0: 0.2897368421052631
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5842105263157893
    vram_util_percent0: 0.08960228547755092
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17058384863820186
    mean_env_wait_ms: 1.1782302544344427
    mean_inference_ms: 5.6949464875115545
    mean_raw_obs_processing_ms: 0.452946992375373
  time_since_restore: 32.82228660583496
  time_this_iter_s: 32.82228660583496
  time_total_s: 32.82228660583496
  timers:
    learn_throughput: 6791.922
    learn_time_ms: 23821.239
    sample_throughput: 18128.718
    sample_time_ms: 8924.625
    update_time_ms: 29.501
  timestamp: 1602457793
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      1 |          32.8223 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4186
    time_step_mean: 3620.6111111111113
    time_step_min: 3334
  date: 2020-10-11_23-10-24
  done: false
  episode_len_mean: 890.2246835443038
  episode_reward_max: 260.86868686868667
  episode_reward_mean: 216.58838383838358
  episode_reward_min: 131.77777777777794
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.1552298963069916
        entropy_coeff: 0.0001
        kl: 0.007592004529821376
        model: {}
        policy_loss: -0.0106424290764456
        total_loss: 122.92242622375488
        vf_explained_var: 0.8179605007171631
        vf_loss: 122.93204689025879
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.675675675675677
    gpu_util_percent0: 0.3156756756756758
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.75945945945946
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1655228162368614
    mean_env_wait_ms: 1.171351383766481
    mean_inference_ms: 5.466615229543468
    mean_raw_obs_processing_ms: 0.4399084878326994
  time_since_restore: 63.83453631401062
  time_this_iter_s: 31.01224970817566
  time_total_s: 63.83453631401062
  timers:
    learn_throughput: 6851.412
    learn_time_ms: 23614.401
    sample_throughput: 19655.975
    sample_time_ms: 8231.187
    update_time_ms: 24.092
  timestamp: 1602457824
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      2 |          63.8345 | 323584 |  216.588 |              260.869 |              131.778 |            890.225 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4186
    time_step_mean: 3609.3946188340806
    time_step_min: 3334
  date: 2020-10-11_23-10-55
  done: false
  episode_len_mean: 888.6286919831224
  episode_reward_max: 267.9898989898989
  episode_reward_mean: 217.7939521800279
  episode_reward_min: 131.77777777777794
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.1472870608170826
        entropy_coeff: 0.0001
        kl: 0.007466870786932607
        model: {}
        policy_loss: -0.014080654674520096
        total_loss: 58.20101261138916
        vf_explained_var: 0.9001145362854004
        vf_loss: 58.2140858968099
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.299999999999997
    gpu_util_percent0: 0.3016666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7777777777777786
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16235574892956264
    mean_env_wait_ms: 1.1680532268158041
    mean_inference_ms: 5.284768151011748
    mean_raw_obs_processing_ms: 0.430307961840447
  time_since_restore: 94.49788045883179
  time_this_iter_s: 30.663344144821167
  time_total_s: 94.49788045883179
  timers:
    learn_throughput: 6863.308
    learn_time_ms: 23573.474
    sample_throughput: 20597.85
    sample_time_ms: 7854.8
    update_time_ms: 22.524
  timestamp: 1602457855
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      3 |          94.4979 | 485376 |  217.794 |               267.99 |              131.778 |            888.629 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4186
    time_step_mean: 3603.317880794702
    time_step_min: 3334
  date: 2020-10-11_23-11-25
  done: false
  episode_len_mean: 886.123417721519
  episode_reward_max: 267.9898989898989
  episode_reward_mean: 219.07423922772008
  episode_reward_min: 131.77777777777794
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.1310810446739197
        entropy_coeff: 0.0001
        kl: 0.008038774443169435
        model: {}
        policy_loss: -0.016193074416757252
        total_loss: 38.56773980458578
        vf_explained_var: 0.9316202998161316
        vf_loss: 38.58284091949463
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.38611111111111
    gpu_util_percent0: 0.43472222222222223
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7777777777777786
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16016654890791351
    mean_env_wait_ms: 1.1662297712743352
    mean_inference_ms: 5.153413922205754
    mean_raw_obs_processing_ms: 0.4232774737963219
  time_since_restore: 124.9723608493805
  time_this_iter_s: 30.474480390548706
  time_total_s: 124.9723608493805
  timers:
    learn_throughput: 6883.923
    learn_time_ms: 23502.876
    sample_throughput: 21149.921
    sample_time_ms: 7649.769
    update_time_ms: 40.463
  timestamp: 1602457885
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      4 |          124.972 | 647168 |  219.074 |               267.99 |              131.778 |            886.123 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4186
    time_step_mean: 3596.740157480315
    time_step_min: 3334
  date: 2020-10-11_23-11-56
  done: false
  episode_len_mean: 883.4721518987342
  episode_reward_max: 267.9898989898989
  episode_reward_mean: 220.81747858330115
  episode_reward_min: 131.77777777777794
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.1041202346483867
        entropy_coeff: 0.0001
        kl: 0.008082523049476245
        model: {}
        policy_loss: -0.015520271030254662
        total_loss: 27.83992910385132
        vf_explained_var: 0.9494257569313049
        vf_loss: 27.854347864786785
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.425714285714285
    gpu_util_percent0: 0.3311428571428572
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780000000000001
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15857822776093836
    mean_env_wait_ms: 1.1653872197011566
    mean_inference_ms: 5.055492689014016
    mean_raw_obs_processing_ms: 0.4179458524520778
  time_since_restore: 155.34798979759216
  time_this_iter_s: 30.37562894821167
  time_total_s: 155.34798979759216
  timers:
    learn_throughput: 6885.956
    learn_time_ms: 23495.94
    sample_throughput: 21610.751
    sample_time_ms: 7486.644
    update_time_ms: 37.189
  timestamp: 1602457916
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      5 |          155.348 | 808960 |  220.817 |               267.99 |              131.778 |            883.472 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3575.325787401575
    time_step_min: 3276
  date: 2020-10-11_23-12-26
  done: false
  episode_len_mean: 877.4204980842912
  episode_reward_max: 273.8989898989896
  episode_reward_mean: 223.98780912573997
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 254
  episodes_total: 1044
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.0778295993804932
        entropy_coeff: 0.0001
        kl: 0.007533402453797559
        model: {}
        policy_loss: -0.013732989638810977
        total_loss: 31.966050624847412
        vf_explained_var: 0.96038419008255
        vf_loss: 31.978761514027912
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.594285714285718
    gpu_util_percent0: 0.2631428571428572
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7714285714285714
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15680131174094752
    mean_env_wait_ms: 1.1663591271603018
    mean_inference_ms: 4.944914891453437
    mean_raw_obs_processing_ms: 0.4120684634709995
  time_since_restore: 185.63322067260742
  time_this_iter_s: 30.28523087501526
  time_total_s: 185.63322067260742
  timers:
    learn_throughput: 6898.543
    learn_time_ms: 23453.07
    sample_throughput: 21862.238
    sample_time_ms: 7400.523
    update_time_ms: 34.446
  timestamp: 1602457946
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      6 |          185.633 | 970752 |  223.988 |              273.899 |              124.202 |             877.42 |
+-------------------------+----------+-----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3557.621359223301
    time_step_min: 3241
  date: 2020-10-11_23-12-56
  done: false
  episode_len_mean: 872.7365506329114
  episode_reward_max: 279.5050505050503
  episode_reward_mean: 226.69542737501578
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 220
  episodes_total: 1264
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.0843857328097026
        entropy_coeff: 0.0001
        kl: 0.007813459689108035
        model: {}
        policy_loss: -0.015326471572431425
        total_loss: 18.214545885721844
        vf_explained_var: 0.9676322340965271
        vf_loss: 18.228808561960857
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.905555555555555
    gpu_util_percent0: 0.42250000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7722222222222235
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15570036322144254
    mean_env_wait_ms: 1.166911322507692
    mean_inference_ms: 4.875530925724019
    mean_raw_obs_processing_ms: 0.40839211834649747
  time_since_restore: 216.00572109222412
  time_this_iter_s: 30.3725004196167
  time_total_s: 216.00572109222412
  timers:
    learn_throughput: 6901.465
    learn_time_ms: 23443.138
    sample_throughput: 22068.743
    sample_time_ms: 7331.274
    update_time_ms: 32.696
  timestamp: 1602457976
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      7 |          216.006 | 1132544 |  226.695 |              279.505 |              124.202 |            872.737 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3545.718077474892
    time_step_min: 3241
  date: 2020-10-11_23-13-27
  done: false
  episode_len_mean: 869.6462728551336
  episode_reward_max: 279.5050505050503
  episode_reward_mean: 228.2915441333161
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.0608759621779125
        entropy_coeff: 0.0001
        kl: 0.007835348097917935
        model: {}
        policy_loss: -0.015282795880921185
        total_loss: 17.042362372080486
        vf_explained_var: 0.9688735604286194
        vf_loss: 17.056575934092205
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.288571428571434
    gpu_util_percent0: 0.34828571428571425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7857142857142856
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15506140566716056
    mean_env_wait_ms: 1.1674291983210354
    mean_inference_ms: 4.835553141102174
    mean_raw_obs_processing_ms: 0.4062721332266458
  time_since_restore: 246.25150156021118
  time_this_iter_s: 30.24578046798706
  time_total_s: 246.25150156021118
  timers:
    learn_throughput: 6907.904
    learn_time_ms: 23421.287
    sample_throughput: 22229.72
    sample_time_ms: 7278.184
    update_time_ms: 30.825
  timestamp: 1602458007
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      8 |          246.252 | 1294336 |  228.292 |              279.505 |              124.202 |            869.646 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3535.3112113402062
    time_step_min: 3241
  date: 2020-10-11_23-13-57
  done: false
  episode_len_mean: 866.1056962025316
  episode_reward_max: 282.9898989898988
  episode_reward_mean: 229.90570259557586
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.0390401383241017
        entropy_coeff: 0.0001
        kl: 0.007570663661075135
        model: {}
        policy_loss: -0.014903288645048937
        total_loss: 13.663523038228353
        vf_explained_var: 0.9726396203041077
        vf_loss: 13.67739470799764
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.375000000000004
    gpu_util_percent0: 0.4419444444444445
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7805555555555563
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1545107772812273
    mean_env_wait_ms: 1.1680027658468553
    mean_inference_ms: 4.800725107088817
    mean_raw_obs_processing_ms: 0.40439583183081124
  time_since_restore: 276.65601229667664
  time_this_iter_s: 30.404510736465454
  time_total_s: 276.65601229667664
  timers:
    learn_throughput: 6909.168
    learn_time_ms: 23417.004
    sample_throughput: 22368.563
    sample_time_ms: 7233.008
    update_time_ms: 37.654
  timestamp: 1602458037
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |      9 |          276.656 | 1456128 |  229.906 |               282.99 |              124.202 |            866.106 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3523.4366685945633
    time_step_min: 3198
  date: 2020-10-11_23-14-28
  done: false
  episode_len_mean: 862.0933409220262
  episode_reward_max: 285.5656565656562
  episode_reward_mean: 231.97163438597687
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 177
  episodes_total: 1757
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.0027002692222595
        entropy_coeff: 0.0001
        kl: 0.006908581087676187
        model: {}
        policy_loss: -0.014077388395283682
        total_loss: 16.621090173721313
        vf_explained_var: 0.9722811579704285
        vf_loss: 16.634231487909954
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.64
    gpu_util_percent0: 0.2859999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7771428571428576
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1539869614611109
    mean_env_wait_ms: 1.168854530800689
    mean_inference_ms: 4.766879047388636
    mean_raw_obs_processing_ms: 0.4025437573179962
  time_since_restore: 306.96516156196594
  time_this_iter_s: 30.309149265289307
  time_total_s: 306.96516156196594
  timers:
    learn_throughput: 6910.219
    learn_time_ms: 23413.441
    sample_throughput: 22486.646
    sample_time_ms: 7195.026
    update_time_ms: 35.708
  timestamp: 1602458068
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     10 |          306.965 | 1617920 |  231.972 |              285.566 |              124.202 |            862.093 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3504.308489634748
    time_step_min: 3188
  date: 2020-10-11_23-14-58
  done: false
  episode_len_mean: 856.3495618305745
  episode_reward_max: 285.5656565656562
  episode_reward_mean: 235.0371976827672
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 297
  episodes_total: 2054
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 1.0050208469231923
        entropy_coeff: 0.0001
        kl: 0.007630100978227953
        model: {}
        policy_loss: -0.01387204317143187
        total_loss: 14.662333885828653
        vf_explained_var: 0.9782240390777588
        vf_loss: 14.675162076950073
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.461111111111116
    gpu_util_percent0: 0.3511111111111111
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7777777777777786
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1532497017752235
    mean_env_wait_ms: 1.1702651176909624
    mean_inference_ms: 4.720570075070834
    mean_raw_obs_processing_ms: 0.40011026613084005
  time_since_restore: 337.5141267776489
  time_this_iter_s: 30.548965215682983
  time_total_s: 337.5141267776489
  timers:
    learn_throughput: 6918.568
    learn_time_ms: 23385.185
    sample_throughput: 23126.553
    sample_time_ms: 6995.941
    update_time_ms: 34.581
  timestamp: 1602458098
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     11 |          337.514 | 1779712 |  235.037 |              285.566 |              124.202 |             856.35 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3496.1858974358975
    time_step_min: 3188
  date: 2020-10-11_23-15-29
  done: false
  episode_len_mean: 854.1934900542495
  episode_reward_max: 285.5656565656562
  episode_reward_mean: 236.26090014064684
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.9834624628225962
        entropy_coeff: 0.0001
        kl: 0.006846520079610248
        model: {}
        policy_loss: -0.014968924170413326
        total_loss: 12.761933167775473
        vf_explained_var: 0.9763520359992981
        vf_loss: 12.775973399480185
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.162857142857145
    gpu_util_percent0: 0.2811428571428572
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7885714285714283
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1529243888304218
    mean_env_wait_ms: 1.170961841791257
    mean_inference_ms: 4.699836407666486
    mean_raw_obs_processing_ms: 0.39901721082687186
  time_since_restore: 367.94033694267273
  time_this_iter_s: 30.426210165023804
  time_total_s: 367.94033694267273
  timers:
    learn_throughput: 6923.255
    learn_time_ms: 23369.354
    sample_throughput: 23271.875
    sample_time_ms: 6952.255
    update_time_ms: 34.766
  timestamp: 1602458129
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     12 |           367.94 | 1941504 |  236.261 |              285.566 |              124.202 |            854.193 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3489.131511528608
    time_step_min: 3152
  date: 2020-10-11_23-15-59
  done: false
  episode_len_mean: 852.0649789029536
  episode_reward_max: 288.4444444444444
  episode_reward_mean: 237.42238844137566
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 158
  episodes_total: 2370
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.967986598610878
        entropy_coeff: 0.0001
        kl: 0.007901284339216849
        model: {}
        policy_loss: -0.014957051142118871
        total_loss: 9.788663705190023
        vf_explained_var: 0.9801943898200989
        vf_loss: 9.802532275517782
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.234285714285715
    gpu_util_percent0: 0.43257142857142855
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15262930088467327
    mean_env_wait_ms: 1.1716061468001007
    mean_inference_ms: 4.680970926337851
    mean_raw_obs_processing_ms: 0.39801762191891343
  time_since_restore: 398.1551396846771
  time_this_iter_s: 30.214802742004395
  time_total_s: 398.1551396846771
  timers:
    learn_throughput: 6932.805
    learn_time_ms: 23337.162
    sample_throughput: 23319.418
    sample_time_ms: 6938.081
    update_time_ms: 35.45
  timestamp: 1602458159
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     13 |          398.155 | 2103296 |  237.422 |              288.444 |              124.202 |            852.065 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3479.8289682539685
    time_step_min: 3152
  date: 2020-10-11_23-16-30
  done: false
  episode_len_mean: 849.6479591836735
  episode_reward_max: 288.4444444444444
  episode_reward_mean: 238.7850562136275
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 178
  episodes_total: 2548
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.9314543853203455
        entropy_coeff: 0.0001
        kl: 0.007329633226618171
        model: {}
        policy_loss: -0.014218925120076165
        total_loss: 11.209327141443888
        vf_explained_var: 0.9804813265800476
        vf_loss: 11.22253966331482
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.08888888888889
    gpu_util_percent0: 0.37694444444444447
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7805555555555563
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15233241745184872
    mean_env_wait_ms: 1.1723793824940492
    mean_inference_ms: 4.661730975192304
    mean_raw_obs_processing_ms: 0.3970046607377272
  time_since_restore: 428.7753555774689
  time_this_iter_s: 30.620215892791748
  time_total_s: 428.7753555774689
  timers:
    learn_throughput: 6921.618
    learn_time_ms: 23374.88
    sample_throughput: 23380.918
    sample_time_ms: 6919.831
    update_time_ms: 30.206
  timestamp: 1602458190
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     14 |          428.775 | 2265088 |  238.785 |              288.444 |              124.202 |            849.648 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3467.659793814433
    time_step_min: 3138
  date: 2020-10-11_23-17-00
  done: false
  episode_len_mean: 846.2143611404435
  episode_reward_max: 296.62626262626253
  episode_reward_mean: 240.59850884771674
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 293
  episodes_total: 2841
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.9173099845647812
        entropy_coeff: 0.0001
        kl: 0.006677730901477237
        model: {}
        policy_loss: -0.012793799707045158
        total_loss: 14.65487257639567
        vf_explained_var: 0.9793903231620789
        vf_loss: 14.66675591468811
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.372222222222224
    gpu_util_percent0: 0.37388888888888894
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769444444444445
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15189603915614291
    mean_env_wait_ms: 1.173458274333655
    mean_inference_ms: 4.634071859341604
    mean_raw_obs_processing_ms: 0.39553535583407506
  time_since_restore: 459.04961466789246
  time_this_iter_s: 30.274259090423584
  time_total_s: 459.04961466789246
  timers:
    learn_throughput: 6926.362
    learn_time_ms: 23358.872
    sample_throughput: 23374.826
    sample_time_ms: 6921.634
    update_time_ms: 33.113
  timestamp: 1602458220
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     15 |           459.05 | 2426880 |  240.599 |              296.626 |              124.202 |            846.214 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3461.684599865501
    time_step_min: 3138
  date: 2020-10-11_23-17-31
  done: false
  episode_len_mean: 844.8364423717521
  episode_reward_max: 296.62626262626253
  episode_reward_mean: 241.43245243911457
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 161
  episodes_total: 3002
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.9061426271994909
        entropy_coeff: 0.0001
        kl: 0.006545906692432861
        model: {}
        policy_loss: -0.014556891086006848
        total_loss: 11.493727127710978
        vf_explained_var: 0.9787874221801758
        vf_loss: 11.507392962773642
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.862857142857138
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.788571428571429
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1516868548469299
    mean_env_wait_ms: 1.1740219492522674
    mean_inference_ms: 4.62072339977864
    mean_raw_obs_processing_ms: 0.3948402304766239
  time_since_restore: 489.34677600860596
  time_this_iter_s: 30.2971613407135
  time_total_s: 489.34677600860596
  timers:
    learn_throughput: 6923.165
    learn_time_ms: 23369.659
    sample_throughput: 23409.544
    sample_time_ms: 6911.369
    update_time_ms: 33.0
  timestamp: 1602458251
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     16 |          489.347 | 2588672 |  241.432 |              296.626 |              124.202 |            844.836 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3456.992017879949
    time_step_min: 3138
  date: 2020-10-11_23-18-01
  done: false
  episode_len_mean: 843.4417721518987
  episode_reward_max: 296.62626262626253
  episode_reward_mean: 242.111111111111
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.8975472648938497
        entropy_coeff: 0.0001
        kl: 0.006394312290164332
        model: {}
        policy_loss: -0.013568656945911547
        total_loss: 10.640838861465454
        vf_explained_var: 0.9792828559875488
        vf_loss: 10.653538386027018
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.522222222222222
    gpu_util_percent0: 0.4005555555555556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.794444444444445
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15149744029890383
    mean_env_wait_ms: 1.1745422220539175
    mean_inference_ms: 4.608557160813835
    mean_raw_obs_processing_ms: 0.39420111753036535
  time_since_restore: 519.9294676780701
  time_this_iter_s: 30.58269166946411
  time_total_s: 519.9294676780701
  timers:
    learn_throughput: 6918.171
    learn_time_ms: 23386.526
    sample_throughput: 23403.682
    sample_time_ms: 6913.1
    update_time_ms: 34.456
  timestamp: 1602458281
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     17 |          519.929 | 2750464 |  242.111 |              296.626 |              124.202 |            843.442 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3452.818648159324
    time_step_min: 3138
  date: 2020-10-11_23-18-32
  done: false
  episode_len_mean: 842.0598444045481
  episode_reward_max: 296.62626262626253
  episode_reward_mean: 242.76747728632816
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 182
  episodes_total: 3342
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.8629405746857325
        entropy_coeff: 0.0001
        kl: 0.007172064040787518
        model: {}
        policy_loss: -0.015292388154193759
        total_loss: 12.782607475916544
        vf_explained_var: 0.9797497391700745
        vf_loss: 12.796910603841146
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.965714285714284
    gpu_util_percent0: 0.3782857142857143
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782857142857143
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15130297698208933
    mean_env_wait_ms: 1.1751570945214873
    mean_inference_ms: 4.595855038770651
    mean_raw_obs_processing_ms: 0.3935338003022596
  time_since_restore: 550.2310283184052
  time_this_iter_s: 30.301560640335083
  time_total_s: 550.2310283184052
  timers:
    learn_throughput: 6916.19
    learn_time_ms: 23393.228
    sample_throughput: 23414.337
    sample_time_ms: 6909.954
    update_time_ms: 35.145
  timestamp: 1602458312
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     18 |          550.231 | 2912256 |  242.767 |              296.626 |              124.202 |             842.06 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3447.2602168473727
    time_step_min: 3138
  date: 2020-10-11_23-19-02
  done: false
  episode_len_mean: 840.624
  episode_reward_max: 296.62626262626253
  episode_reward_mean: 243.74245907349345
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 283
  episodes_total: 3625
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.847020094593366
        entropy_coeff: 0.0001
        kl: 0.005969295588632424
        model: {}
        policy_loss: -0.013674164680802884
        total_loss: 14.10198704401652
        vf_explained_var: 0.9804542064666748
        vf_loss: 14.114850203196207
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 20.936111111111114
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769444444444445
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15101137324064728
    mean_env_wait_ms: 1.1759395035539377
    mean_inference_ms: 4.577443233072338
    mean_raw_obs_processing_ms: 0.3925663746510852
  time_since_restore: 580.5396199226379
  time_this_iter_s: 30.308591604232788
  time_total_s: 580.5396199226379
  timers:
    learn_throughput: 6916.162
    learn_time_ms: 23393.32
    sample_throughput: 23424.767
    sample_time_ms: 6906.878
    update_time_ms: 27.737
  timestamp: 1602458342
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc             |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | RUNNING  | 172.17.0.4:7427 |     19 |           580.54 | 3074048 |  243.742 |              296.626 |              124.202 |            840.624 |
+-------------------------+----------+-----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_c73fa_00000:
  custom_metrics:
    time_step_max: 4236
    time_step_mean: 3443.5650903294368
    time_step_min: 3138
  date: 2020-10-11_23-19-33
  done: true
  episode_len_mean: 839.7315400843881
  episode_reward_max: 296.62626262626253
  episode_reward_mean: 244.31393843498262
  episode_reward_min: 124.20202020201997
  episodes_this_iter: 167
  episodes_total: 3792
  experiment_id: 9a549507a54d48918cb1a36865cf286b
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.14999999999999997
        cur_lr: 5.0e-05
        entropy: 0.8336994300285975
        entropy_coeff: 0.0001
        kl: 0.006006783611762027
        model: {}
        policy_loss: -0.013896335430520898
        total_loss: 10.192124525705973
        vf_explained_var: 0.9814110398292542
        vf_loss: 10.205202976862589
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.208571428571428
    gpu_util_percent0: 0.4011428571428572
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.791428571428572
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 7427
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1508624857710479
    mean_env_wait_ms: 1.176377138240167
    mean_inference_ms: 4.567748213333957
    mean_raw_obs_processing_ms: 0.39205571513201626
  time_since_restore: 610.90700507164
  time_this_iter_s: 30.367385149002075
  time_total_s: 610.90700507164
  timers:
    learn_throughput: 6913.151
    learn_time_ms: 23403.511
    sample_throughput: 23450.659
    sample_time_ms: 6899.252
    update_time_ms: 30.074
  timestamp: 1602458373
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: c73fa_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | TERMINATED |       |     20 |          610.907 | 3235840 |  244.314 |              296.626 |              124.202 |            839.732 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


[2m[33m(pid=raylet)[0m E1011 23:19:33.328572  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.328984  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.329039  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.329097  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.329144  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.06 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_c73fa_00000 | TERMINATED |       |     20 |          610.907 | 3235840 |  244.314 |              296.626 |              124.202 |            839.732 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


[2m[33m(pid=raylet)[0m E1011 23:19:33.384879  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.385267  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.385596  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.385643  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.385936  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.385967  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.386602  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
[2m[33m(pid=raylet)[0m E1011 23:19:33.387100  7247  7247 node_manager.cc:3307] Failed to send get core worker stats request: IOError: 2: HandleServiceClosed
