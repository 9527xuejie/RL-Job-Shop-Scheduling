2020-10-15 03:15:31,149	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_af50e_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=20935)[0m 2020-10-15 03:15:33,874	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=20842)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20842)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20909)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20909)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20965)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20965)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20854)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20854)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20896)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20896)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20857)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20857)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20924)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20924)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20903)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20903)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20829)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20829)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20843)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20843)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20831)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20831)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20833)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20833)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20893)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20893)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20888)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20888)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20945)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20945)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20832)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20832)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20836)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20836)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20897)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20897)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20889)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20889)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20949)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20949)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20834)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20834)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20955)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20955)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20844)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20844)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20920)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20920)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20950)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20950)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20923)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20923)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20927)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20927)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20901)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20901)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20915)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20915)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20941)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20941)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20942)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20942)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20848)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20848)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20928)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20928)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20913)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20913)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20869)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20869)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20863)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20863)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20925)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20925)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20839)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20839)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20947)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20947)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20837)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20837)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20908)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20908)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20898)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20898)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20902)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20902)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20905)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20905)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20835)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20835)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20959)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20959)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20894)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20894)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20860)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20860)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20937)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20937)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20830)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20830)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20892)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20892)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20944)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20944)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20932)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20932)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20847)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20847)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20849)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20849)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20916)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20916)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20899)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20899)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20846)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20846)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20921)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20921)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20841)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20841)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20886)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20886)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20861)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20861)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20934)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20934)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20910)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20910)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20911)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20911)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20866)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20866)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20953)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20953)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20856)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20856)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20855)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20855)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20933)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20933)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20851)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20851)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20881)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20881)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20931)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20931)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20884)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20884)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20939)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20939)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20828)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20828)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20845)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20845)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20966)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20966)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=20912)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=20912)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3669
    time_step_mean: 3363.603448275862
    time_step_min: 3125
  date: 2020-10-15_03-16-07
  done: false
  episode_len_mean: 881.5316455696203
  episode_reward_max: 283.8080808080806
  episode_reward_mean: 244.05069684183616
  episode_reward_min: 164.4141414141411
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1715679466724396
        entropy_coeff: 0.0005000000000000001
        kl: 0.003930501930881292
        model: {}
        policy_loss: -0.00778718293683293
        total_loss: 516.1756820678711
        vf_explained_var: 0.4896630346775055
        vf_loss: 516.1832809448242
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.478787878787877
    gpu_util_percent0: 0.3281818181818182
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5181818181818185
    vram_util_percent0: 0.08582297226114873
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17362642891799349
    mean_env_wait_ms: 1.1588369441937796
    mean_inference_ms: 5.95052654340555
    mean_raw_obs_processing_ms: 0.4671919482031807
  time_since_restore: 28.402875900268555
  time_this_iter_s: 28.402875900268555
  time_total_s: 28.402875900268555
  timers:
    learn_throughput: 8452.572
    learn_time_ms: 19141.155
    sample_throughput: 17634.052
    sample_time_ms: 9174.976
    update_time_ms: 48.895
  timestamp: 1602731767
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      1 |          28.4029 | 161792 |  244.051 |              283.808 |              164.414 |            881.532 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3869
    time_step_mean: 3377.0
    time_step_min: 3125
  date: 2020-10-15_03-16-34
  done: false
  episode_len_mean: 881.126582278481
  episode_reward_max: 283.8080808080806
  episode_reward_mean: 242.05942334739814
  episode_reward_min: 164.4141414141411
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1408922374248505
        entropy_coeff: 0.0005000000000000001
        kl: 0.006987970671616495
        model: {}
        policy_loss: -0.009066562740675485
        total_loss: 136.38060887654623
        vf_explained_var: 0.7857485413551331
        vf_loss: 136.38954798380533
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.57741935483871
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.74516129032258
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16919253142315674
    mean_env_wait_ms: 1.1508504355682332
    mean_inference_ms: 5.7240416095244555
    mean_raw_obs_processing_ms: 0.45569153598737167
  time_since_restore: 55.16504216194153
  time_this_iter_s: 26.762166261672974
  time_total_s: 55.16504216194153
  timers:
    learn_throughput: 8526.975
    learn_time_ms: 18974.137
    sample_throughput: 18972.23
    sample_time_ms: 8527.833
    update_time_ms: 37.63
  timestamp: 1602731794
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      2 |           55.165 | 323584 |  242.059 |              283.808 |              164.414 |            881.127 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3871
    time_step_mean: 3384.8194444444443
    time_step_min: 3104
  date: 2020-10-15_03-17-00
  done: false
  episode_len_mean: 875.0569620253165
  episode_reward_max: 284.86868686868706
  episode_reward_mean: 241.46439074287184
  episode_reward_min: 164.4141414141411
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1264353493849437
        entropy_coeff: 0.0005000000000000001
        kl: 0.009802788573627671
        model: {}
        policy_loss: -0.012675396748818457
        total_loss: 60.41511758168539
        vf_explained_var: 0.8837491869926453
        vf_loss: 60.427374521891274
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.79666666666667
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16602771419403764
    mean_env_wait_ms: 1.1480905790304274
    mean_inference_ms: 5.537019858915664
    mean_raw_obs_processing_ms: 0.4460422644313794
  time_since_restore: 81.22479057312012
  time_this_iter_s: 26.05974841117859
  time_total_s: 81.22479057312012
  timers:
    learn_throughput: 8573.545
    learn_time_ms: 18871.075
    sample_throughput: 19910.882
    sample_time_ms: 8125.808
    update_time_ms: 32.207
  timestamp: 1602731820
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 27.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      3 |          81.2248 | 485376 |  241.464 |              284.869 |              164.414 |            875.057 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3387.8050847457625
    time_step_min: 3104
  date: 2020-10-15_03-17-26
  done: false
  episode_len_mean: 871.5268987341772
  episode_reward_max: 284.86868686868706
  episode_reward_mean: 241.93859480884805
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1075291832288106
        entropy_coeff: 0.0005000000000000001
        kl: 0.00853245899391671
        model: {}
        policy_loss: -0.011526559355843347
        total_loss: 44.10186163584391
        vf_explained_var: 0.9141210913658142
        vf_loss: 44.1130895614624
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37
    gpu_util_percent0: 0.3496666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1636581327984584
    mean_env_wait_ms: 1.1471413812633613
    mean_inference_ms: 5.391033317013351
    mean_raw_obs_processing_ms: 0.4382663406928765
  time_since_restore: 107.15199947357178
  time_this_iter_s: 25.92720890045166
  time_total_s: 107.15199947357178
  timers:
    learn_throughput: 8574.066
    learn_time_ms: 18869.926
    sample_throughput: 20631.188
    sample_time_ms: 7842.108
    update_time_ms: 29.048
  timestamp: 1602731846
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      4 |          107.152 | 647168 |  241.939 |              284.869 |              160.172 |            871.527 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3381.8594164456235
    time_step_min: 3104
  date: 2020-10-15_03-17-52
  done: false
  episode_len_mean: 866.8391959798995
  episode_reward_max: 284.86868686868706
  episode_reward_mean: 242.8055301761333
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 164
  episodes_total: 796
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0599133570988972
        entropy_coeff: 0.0005000000000000001
        kl: 0.00805330699464927
        model: {}
        policy_loss: -0.009734194648141662
        total_loss: 32.486494382222496
        vf_explained_var: 0.9489476084709167
        vf_loss: 32.49595355987549
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.143333333333327
    gpu_util_percent0: 0.4116666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16178022728655306
    mean_env_wait_ms: 1.1478540434434417
    mean_inference_ms: 5.272899086844515
    mean_raw_obs_processing_ms: 0.43166420307572845
  time_since_restore: 133.2098786830902
  time_this_iter_s: 26.057879209518433
  time_total_s: 133.2098786830902
  timers:
    learn_throughput: 8553.768
    learn_time_ms: 18914.705
    sample_throughput: 21160.481
    sample_time_ms: 7645.951
    update_time_ms: 32.822
  timestamp: 1602731872
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      5 |           133.21 | 808960 |  242.806 |              284.869 |              160.172 |            866.839 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3367.4323308270677
    time_step_min: 3078
  date: 2020-10-15_03-18-18
  done: false
  episode_len_mean: 856.883363471971
  episode_reward_max: 288.8080808080812
  episode_reward_mean: 245.21240433265757
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 310
  episodes_total: 1106
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.071306178967158
        entropy_coeff: 0.0005000000000000001
        kl: 0.008238797464097539
        model: {}
        policy_loss: -0.011969462172904363
        total_loss: 28.96843910217285
        vf_explained_var: 0.9578704833984375
        vf_loss: 28.980120023091633
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.389655172413796
    gpu_util_percent0: 0.2972413793103448
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15933525919114602
    mean_env_wait_ms: 1.1500738596356168
    mean_inference_ms: 5.1181877300513525
    mean_raw_obs_processing_ms: 0.4234132170982465
  time_since_restore: 158.7162425518036
  time_this_iter_s: 25.50636386871338
  time_total_s: 158.7162425518036
  timers:
    learn_throughput: 8571.951
    learn_time_ms: 18874.583
    sample_throughput: 21577.299
    sample_time_ms: 7498.251
    update_time_ms: 31.18
  timestamp: 1602731898
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      6 |          158.716 | 970752 |  245.212 |              288.808 |              160.172 |            856.883 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3362.1800327332244
    time_step_min: 3078
  date: 2020-10-15_03-18-44
  done: false
  episode_len_mean: 852.179588607595
  episode_reward_max: 293.65656565656565
  episode_reward_mean: 246.19576300984542
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0482013523578644
        entropy_coeff: 0.0005000000000000001
        kl: 0.007311464326145749
        model: {}
        policy_loss: -0.011051583365770057
        total_loss: 19.482070922851562
        vf_explained_var: 0.9613587260246277
        vf_loss: 19.492915948232014
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85666666666667
    gpu_util_percent0: 0.3356666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15841040164883916
    mean_env_wait_ms: 1.1510096751041847
    mean_inference_ms: 5.060176059782607
    mean_raw_obs_processing_ms: 0.42019945047384594
  time_since_restore: 184.69712114334106
  time_this_iter_s: 25.980878591537476
  time_total_s: 184.69712114334106
  timers:
    learn_throughput: 8556.394
    learn_time_ms: 18908.9
    sample_throughput: 21879.431
    sample_time_ms: 7394.708
    update_time_ms: 32.035
  timestamp: 1602731924
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      7 |          184.697 | 1132544 |  246.196 |              293.657 |              160.172 |             852.18 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3353.78115942029
    time_step_min: 3064
  date: 2020-10-15_03-19-10
  done: false
  episode_len_mean: 847.381153305204
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 247.39319353876326
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0248040159543355
        entropy_coeff: 0.0005000000000000001
        kl: 0.007192811734663944
        model: {}
        policy_loss: -0.012482399101524303
        total_loss: 16.573991378148396
        vf_explained_var: 0.9641326069831848
        vf_loss: 16.586267232894897
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.76333333333334
    gpu_util_percent0: 0.38900000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15760268918026724
    mean_env_wait_ms: 1.1520917835422009
    mean_inference_ms: 5.009391426926265
    mean_raw_obs_processing_ms: 0.41731013344421297
  time_since_restore: 210.4323263168335
  time_this_iter_s: 25.73520517349243
  time_total_s: 210.4323263168335
  timers:
    learn_throughput: 8562.864
    learn_time_ms: 18894.613
    sample_throughput: 22109.805
    sample_time_ms: 7317.658
    update_time_ms: 41.419
  timestamp: 1602731950
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      8 |          210.432 | 1294336 |  247.393 |              296.687 |              160.172 |            847.381 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3344.8988764044943
    time_step_min: 3064
  date: 2020-10-15_03-19-35
  done: false
  episode_len_mean: 840.897201946472
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 248.74430435744316
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 1644
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9738969107468923
        entropy_coeff: 0.0005000000000000001
        kl: 0.006379361497238278
        model: {}
        policy_loss: -0.012259047235905504
        total_loss: 22.226102352142334
        vf_explained_var: 0.9665055871009827
        vf_loss: 22.23821036020915
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.827586206896555
    gpu_util_percent0: 0.36344827586206896
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7620689655172406
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15663416716015283
    mean_env_wait_ms: 1.1541503535407043
    mean_inference_ms: 4.949019804291408
    mean_raw_obs_processing_ms: 0.41380625905076646
  time_since_restore: 235.96166110038757
  time_this_iter_s: 25.529334783554077
  time_total_s: 235.96166110038757
  timers:
    learn_throughput: 8575.814
    learn_time_ms: 18866.081
    sample_throughput: 22278.739
    sample_time_ms: 7262.171
    update_time_ms: 39.078
  timestamp: 1602731975
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |      9 |          235.962 | 1456128 |  248.744 |              296.687 |              160.172 |            840.897 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3333.3327939590076
    time_step_min: 3036
  date: 2020-10-15_03-20-01
  done: false
  episode_len_mean: 834.3728902953586
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 250.30573136427577
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 252
  episodes_total: 1896
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.968529686331749
        entropy_coeff: 0.0005000000000000001
        kl: 0.006252618157304823
        model: {}
        policy_loss: -0.010011641182548678
        total_loss: 15.227359533309937
        vf_explained_var: 0.97138911485672
        vf_loss: 15.2372305393219
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.313793103448276
    gpu_util_percent0: 0.4155172413793103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7586206896551717
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15575772396034598
    mean_env_wait_ms: 1.1563641395203306
    mean_inference_ms: 4.892995896748657
    mean_raw_obs_processing_ms: 0.41068453707723857
  time_since_restore: 261.40170407295227
  time_this_iter_s: 25.440042972564697
  time_total_s: 261.40170407295227
  timers:
    learn_throughput: 8587.88
    learn_time_ms: 18839.573
    sample_throughput: 22431.87
    sample_time_ms: 7212.595
    update_time_ms: 36.961
  timestamp: 1602732001
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     10 |          261.402 | 1617920 |  250.306 |              296.687 |              160.172 |            834.373 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3328.4209741550694
    time_step_min: 3036
  date: 2020-10-15_03-20-27
  done: false
  episode_len_mean: 830.8437195715677
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 251.11142092787668
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9550741414229075
        entropy_coeff: 0.0005000000000000001
        kl: 0.006006665799456338
        model: {}
        policy_loss: -0.012216550302885784
        total_loss: 14.387996912002563
        vf_explained_var: 0.9684069752693176
        vf_loss: 14.400090297063192
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58
    gpu_util_percent0: 0.29533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15528454432090336
    mean_env_wait_ms: 1.1576375529664207
    mean_inference_ms: 4.86297481863929
    mean_raw_obs_processing_ms: 0.4089432193161837
  time_since_restore: 287.25953912734985
  time_this_iter_s: 25.857835054397583
  time_total_s: 287.25953912734985
  timers:
    learn_throughput: 8595.308
    learn_time_ms: 18823.293
    sample_throughput: 23197.249
    sample_time_ms: 6974.62
    update_time_ms: 34.635
  timestamp: 1602732027
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     11 |           287.26 | 1779712 |  251.111 |              296.687 |              160.172 |            830.844 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3323.5875862068965
    time_step_min: 3036
  date: 2020-10-15_03-20-53
  done: false
  episode_len_mean: 827.5119530897609
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 251.97233498722005
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 163
  episodes_total: 2217
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9188966850439707
        entropy_coeff: 0.0005000000000000001
        kl: 0.00623135210480541
        model: {}
        policy_loss: -0.012058262596838176
        total_loss: 13.930777152379354
        vf_explained_var: 0.9721924662590027
        vf_loss: 13.942671616872152
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57241379310345
    gpu_util_percent0: 0.34896551724137936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15483699583723187
    mean_env_wait_ms: 1.1590041880211064
    mean_inference_ms: 4.834881807691345
    mean_raw_obs_processing_ms: 0.4072779658085695
  time_since_restore: 312.9317307472229
  time_this_iter_s: 25.672191619873047
  time_total_s: 312.9317307472229
  timers:
    learn_throughput: 8602.841
    learn_time_ms: 18806.81
    sample_throughput: 23510.314
    sample_time_ms: 6881.746
    update_time_ms: 34.385
  timestamp: 1602732053
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     12 |          312.932 | 1941504 |  251.972 |              296.687 |              160.172 |            827.512 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3314.2779565567175
    time_step_min: 3036
  date: 2020-10-15_03-21-18
  done: false
  episode_len_mean: 821.9600474683544
  episode_reward_max: 296.6868686868687
  episode_reward_mean: 253.37343370412998
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 311
  episodes_total: 2528
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8911013106505076
        entropy_coeff: 0.0005000000000000001
        kl: 0.00614805705845356
        model: {}
        policy_loss: -0.01046816335777597
        total_loss: 14.937302589416504
        vf_explained_var: 0.9774847030639648
        vf_loss: 14.947601795196533
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.843333333333337
    gpu_util_percent0: 0.39266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15411334273149505
    mean_env_wait_ms: 1.161642193440134
    mean_inference_ms: 4.789059768401855
    mean_raw_obs_processing_ms: 0.404633039219865
  time_since_restore: 338.6854877471924
  time_this_iter_s: 25.753756999969482
  time_total_s: 338.6854877471924
  timers:
    learn_throughput: 8594.484
    learn_time_ms: 18825.099
    sample_throughput: 23681.465
    sample_time_ms: 6832.01
    update_time_ms: 34.626
  timestamp: 1602732078
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     13 |          338.685 | 2103296 |  253.373 |              296.687 |              160.172 |             821.96 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3309.3282904689863
    time_step_min: 3034
  date: 2020-10-15_03-21-44
  done: false
  episode_len_mean: 819.2516753536858
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 254.2009709906211
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 2686
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8883989155292511
        entropy_coeff: 0.0005000000000000001
        kl: 0.005655435457204779
        model: {}
        policy_loss: -0.011879076599143445
        total_loss: 9.973519086837769
        vf_explained_var: 0.9757456183433533
        vf_loss: 9.98527709643046
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.34137931034483
    gpu_util_percent0: 0.3241379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15379373090717724
    mean_env_wait_ms: 1.1628572412629317
    mean_inference_ms: 4.768908997860768
    mean_raw_obs_processing_ms: 0.40346646880397696
  time_since_restore: 364.14061212539673
  time_this_iter_s: 25.455124378204346
  time_total_s: 364.14061212539673
  timers:
    learn_throughput: 8608.704
    learn_time_ms: 18794.003
    sample_throughput: 23741.431
    sample_time_ms: 6814.753
    update_time_ms: 34.701
  timestamp: 1602732104
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     14 |          364.141 | 2265088 |  254.201 |              299.566 |              160.172 |            819.252 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3304.058110516934
    time_step_min: 3034
  date: 2020-10-15_03-22-10
  done: false
  episode_len_mean: 816.7463997190025
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 255.00758906238366
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 2847
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8732159187396368
        entropy_coeff: 0.0005000000000000001
        kl: 0.006058271353443463
        model: {}
        policy_loss: -0.012418695783708245
        total_loss: 9.868411302566528
        vf_explained_var: 0.9765015244483948
        vf_loss: 9.88066061337789
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.900000000000006
    gpu_util_percent0: 0.3696551724137932
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15349075642388346
    mean_env_wait_ms: 1.1641275492054641
    mean_inference_ms: 4.749899239884046
    mean_raw_obs_processing_ms: 0.4023525643884929
  time_since_restore: 389.7063322067261
  time_this_iter_s: 25.565720081329346
  time_total_s: 389.7063322067261
  timers:
    learn_throughput: 8624.212
    learn_time_ms: 18760.207
    sample_throughput: 23788.297
    sample_time_ms: 6801.328
    update_time_ms: 32.433
  timestamp: 1602732130
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     15 |          389.706 | 2426880 |  255.008 |              299.566 |              160.172 |            816.746 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3294.7485567671583
    time_step_min: 3016
  date: 2020-10-15_03-22-35
  done: false
  episode_len_mean: 812.3917721518987
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 256.1996228103824
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 313
  episodes_total: 3160
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8344785422086716
        entropy_coeff: 0.0005000000000000001
        kl: 0.00587861891835928
        model: {}
        policy_loss: -0.010444189373326177
        total_loss: 13.899945179621378
        vf_explained_var: 0.9787079691886902
        vf_loss: 13.910218795140585
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.023333333333333
    gpu_util_percent0: 0.31200000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1529684160768752
    mean_env_wait_ms: 1.166573801828463
    mean_inference_ms: 4.717178044303415
    mean_raw_obs_processing_ms: 0.400472508091541
  time_since_restore: 415.36418056488037
  time_this_iter_s: 25.657848358154297
  time_total_s: 415.36418056488037
  timers:
    learn_throughput: 8622.649
    learn_time_ms: 18763.607
    sample_throughput: 23749.878
    sample_time_ms: 6812.33
    update_time_ms: 32.127
  timestamp: 1602732155
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     16 |          415.364 | 2588672 |    256.2 |              299.566 |              160.172 |            812.392 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3290.7713675213677
    time_step_min: 3010
  date: 2020-10-15_03-23-01
  done: false
  episode_len_mean: 810.3351416515974
  episode_reward_max: 299.5656565656566
  episode_reward_mean: 256.7730377920252
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 3318
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8384240468343099
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053978989987323684
        model: {}
        policy_loss: -0.011896416496407861
        total_loss: 9.386300961176554
        vf_explained_var: 0.9776299595832825
        vf_loss: 9.398076931635538
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.773333333333337
    gpu_util_percent0: 0.39233333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15273496613531942
    mean_env_wait_ms: 1.1676969786187548
    mean_inference_ms: 4.7025494622006585
    mean_raw_obs_processing_ms: 0.39962597563110364
  time_since_restore: 441.29763984680176
  time_this_iter_s: 25.933459281921387
  time_total_s: 441.29763984680176
  timers:
    learn_throughput: 8630.973
    learn_time_ms: 18745.511
    sample_throughput: 23700.915
    sample_time_ms: 6826.403
    update_time_ms: 30.565
  timestamp: 1602732181
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     17 |          441.298 | 2750464 |  256.773 |              299.566 |              160.172 |            810.335 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3287.6860634274076
    time_step_min: 2984
  date: 2020-10-15_03-23-27
  done: false
  episode_len_mean: 808.4596148318483
  episode_reward_max: 303.05050505050514
  episode_reward_mean: 257.21201088203105
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 3479
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8263530830542246
        entropy_coeff: 0.0005000000000000001
        kl: 0.00601558949953566
        model: {}
        policy_loss: -0.013046822976320982
        total_loss: 10.768564303716024
        vf_explained_var: 0.9754762649536133
        vf_loss: 10.78142261505127
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.60344827586207
    gpu_util_percent0: 0.3589655172413794
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15251229346640346
    mean_env_wait_ms: 1.1688377907347944
    mean_inference_ms: 4.688593781406719
    mean_raw_obs_processing_ms: 0.39880660702645676
  time_since_restore: 466.80091667175293
  time_this_iter_s: 25.503276824951172
  time_total_s: 466.80091667175293
  timers:
    learn_throughput: 8639.279
    learn_time_ms: 18727.488
    sample_throughput: 23691.853
    sample_time_ms: 6829.014
    update_time_ms: 21.935
  timestamp: 1602732207
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     18 |          466.801 | 2912256 |  257.212 |              303.051 |              160.172 |             808.46 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3281.0016
    time_step_min: 2954
  date: 2020-10-15_03-23-53
  done: false
  episode_len_mean: 805.3974156118144
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 258.3012322593019
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 313
  episodes_total: 3792
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7947459469238917
        entropy_coeff: 0.0005000000000000001
        kl: 0.005477127657892804
        model: {}
        policy_loss: -0.012167421659493508
        total_loss: 14.687130689620972
        vf_explained_var: 0.977111279964447
        vf_loss: 14.699147860209147
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.517241379310345
    gpu_util_percent0: 0.34793103448275864
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7586206896551717
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15212105559957453
    mean_env_wait_ms: 1.1709800103896437
    mean_inference_ms: 4.664086337337076
    mean_raw_obs_processing_ms: 0.3973904795308054
  time_since_restore: 492.1092360019684
  time_this_iter_s: 25.308319330215454
  time_total_s: 492.1092360019684
  timers:
    learn_throughput: 8650.922
    learn_time_ms: 18702.285
    sample_throughput: 23685.227
    sample_time_ms: 6830.925
    update_time_ms: 22.643
  timestamp: 1602732233
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     19 |          492.109 | 3074048 |  258.301 |              307.596 |              160.172 |            805.397 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3277.4357727737975
    time_step_min: 2954
  date: 2020-10-15_03-24-18
  done: false
  episode_len_mean: 804.0949367088608
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 258.89404168264934
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 3950
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8061949710051218
        entropy_coeff: 0.0005000000000000001
        kl: 0.006130845169536769
        model: {}
        policy_loss: -0.010873976105358452
        total_loss: 8.992526610692343
        vf_explained_var: 0.9773077964782715
        vf_loss: 9.003190676371256
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09333333333333
    gpu_util_percent0: 0.3533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15194276427661754
    mean_env_wait_ms: 1.1719508903481868
    mean_inference_ms: 4.652903286686612
    mean_raw_obs_processing_ms: 0.3967450659606946
  time_since_restore: 517.7941360473633
  time_this_iter_s: 25.684900045394897
  time_total_s: 517.7941360473633
  timers:
    learn_throughput: 8645.336
    learn_time_ms: 18714.368
    sample_throughput: 23650.189
    sample_time_ms: 6841.045
    update_time_ms: 23.342
  timestamp: 1602732258
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     20 |          517.794 | 3235840 |  258.894 |              307.596 |              160.172 |            804.095 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3274.6622418879056
    time_step_min: 2954
  date: 2020-10-15_03-24-44
  done: false
  episode_len_mean: 803.0717761557178
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 259.3341812283419
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 4110
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.804583822687467
        entropy_coeff: 0.0005000000000000001
        kl: 0.006675932789221406
        model: {}
        policy_loss: -0.011738290796832493
        total_loss: 10.735313495000204
        vf_explained_var: 0.9747953414916992
        vf_loss: 10.746786197026571
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.048275862068966
    gpu_util_percent0: 0.3693103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15177233638991344
    mean_env_wait_ms: 1.1728912363670017
    mean_inference_ms: 4.642201131931685
    mean_raw_obs_processing_ms: 0.3961220824772998
  time_since_restore: 543.2923927307129
  time_this_iter_s: 25.49825668334961
  time_total_s: 543.2923927307129
  timers:
    learn_throughput: 8663.524
    learn_time_ms: 18675.08
    sample_throughput: 23639.018
    sample_time_ms: 6844.278
    update_time_ms: 23.032
  timestamp: 1602732284
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     21 |          543.292 | 3397632 |  259.334 |              307.596 |              160.172 |            803.072 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3269.2985825331502
    time_step_min: 2954
  date: 2020-10-15_03-25-10
  done: false
  episode_len_mean: 801.3052536231884
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 260.13030440272297
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 306
  episodes_total: 4416
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7650169382492701
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053621438176681595
        model: {}
        policy_loss: -0.01004139548361612
        total_loss: 15.226029952367147
        vf_explained_var: 0.9767293930053711
        vf_loss: 15.235918045043945
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.69000000000001
    gpu_util_percent0: 0.43000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1514731607193916
    mean_env_wait_ms: 1.1746211302870468
    mean_inference_ms: 4.623442079187225
    mean_raw_obs_processing_ms: 0.39504683244592037
  time_since_restore: 568.8355846405029
  time_this_iter_s: 25.54319190979004
  time_total_s: 568.8355846405029
  timers:
    learn_throughput: 8661.494
    learn_time_ms: 18679.456
    sample_throughput: 23703.429
    sample_time_ms: 6825.679
    update_time_ms: 23.084
  timestamp: 1602732310
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     22 |          568.836 | 3559424 |   260.13 |              307.596 |              160.172 |            801.305 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3266.2654185022025
    time_step_min: 2954
  date: 2020-10-15_03-25-35
  done: false
  episode_len_mean: 800.4751200349192
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 260.6454417593659
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 166
  episodes_total: 4582
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.758562500278155
        entropy_coeff: 0.0005000000000000001
        kl: 0.005646925768814981
        model: {}
        policy_loss: -0.011008052000155052
        total_loss: 7.616831421852112
        vf_explained_var: 0.9816978573799133
        vf_loss: 7.6276540358861284
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.67586206896552
    gpu_util_percent0: 0.346551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15132655720537733
    mean_env_wait_ms: 1.1754749656328005
    mean_inference_ms: 4.614078772227084
    mean_raw_obs_processing_ms: 0.3945174262577576
  time_since_restore: 594.1261310577393
  time_this_iter_s: 25.290546417236328
  time_total_s: 594.1261310577393
  timers:
    learn_throughput: 8680.615
    learn_time_ms: 18638.311
    sample_throughput: 23724.794
    sample_time_ms: 6819.532
    update_time_ms: 23.045
  timestamp: 1602732335
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     23 |          594.126 | 3721216 |  260.645 |              307.596 |              160.172 |            800.475 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3264.125744680851
    time_step_min: 2954
  date: 2020-10-15_03-26-01
  done: false
  episode_len_mean: 799.7191058625052
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 261.01878975329004
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 4742
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7632499734560648
        entropy_coeff: 0.0005000000000000001
        kl: 0.006014090691072245
        model: {}
        policy_loss: -0.012040382775012404
        total_loss: 8.783557653427124
        vf_explained_var: 0.9784457087516785
        vf_loss: 8.79537852605184
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.200000000000003
    gpu_util_percent0: 0.3479310344827587
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15119185544733216
    mean_env_wait_ms: 1.176255150480693
    mean_inference_ms: 4.605510913609876
    mean_raw_obs_processing_ms: 0.3940268829128436
  time_since_restore: 619.4277377128601
  time_this_iter_s: 25.30160665512085
  time_total_s: 619.4277377128601
  timers:
    learn_throughput: 8684.101
    learn_time_ms: 18630.83
    sample_throughput: 23754.568
    sample_time_ms: 6810.985
    update_time_ms: 23.333
  timestamp: 1602732361
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     24 |          619.428 | 3883008 |  261.019 |              307.596 |              160.172 |            799.719 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3260.5959879638917
    time_step_min: 2954
  date: 2020-10-15_03-26-26
  done: false
  episode_len_mean: 798.5134274915456
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 261.55970687580003
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 285
  episodes_total: 5027
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7313970873753229
        entropy_coeff: 0.0005000000000000001
        kl: 0.00564456715558966
        model: {}
        policy_loss: -0.011325549858156592
        total_loss: 12.305595318476358
        vf_explained_var: 0.9806763529777527
        vf_loss: 12.31672191619873
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.296551724137924
    gpu_util_percent0: 0.356551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7586206896551717
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15096606240891813
    mean_env_wait_ms: 1.177591753163911
    mean_inference_ms: 4.591316072518126
    mean_raw_obs_processing_ms: 0.39320562845393264
  time_since_restore: 645.172534942627
  time_this_iter_s: 25.744797229766846
  time_total_s: 645.172534942627
  timers:
    learn_throughput: 8683.316
    learn_time_ms: 18632.513
    sample_throughput: 23701.612
    sample_time_ms: 6826.202
    update_time_ms: 23.006
  timestamp: 1602732386
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     25 |          645.173 | 4044800 |   261.56 |              307.596 |              160.172 |            798.513 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3258.44682907966
    time_step_min: 2954
  date: 2020-10-15_03-26-52
  done: false
  episode_len_mean: 797.7278481012659
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 261.8983622182702
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 5214
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7117179979880651
        entropy_coeff: 0.0005000000000000001
        kl: 0.00563451717607677
        model: {}
        policy_loss: -0.010599843536814054
        total_loss: 8.839935143788656
        vf_explained_var: 0.9808418154716492
        vf_loss: 8.850327412287394
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.196551724137937
    gpu_util_percent0: 0.30103448275862066
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15083220794339533
    mean_env_wait_ms: 1.178413546842481
    mean_inference_ms: 4.582621847279518
    mean_raw_obs_processing_ms: 0.39272471824780886
  time_since_restore: 670.379932641983
  time_this_iter_s: 25.20739769935608
  time_total_s: 670.379932641983
  timers:
    learn_throughput: 8698.935
    learn_time_ms: 18599.059
    sample_throughput: 23745.313
    sample_time_ms: 6813.639
    update_time_ms: 22.978
  timestamp: 1602732412
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     26 |           670.38 | 4206592 |  261.898 |              307.596 |              160.172 |            797.728 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3256.272693173293
    time_step_min: 2954
  date: 2020-10-15_03-27-17
  done: false
  episode_len_mean: 797.1222553033123
  episode_reward_max: 307.59595959595924
  episode_reward_mean: 262.24409709299925
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 5374
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7274518807729086
        entropy_coeff: 0.0005000000000000001
        kl: 0.005952913508129616
        model: {}
        policy_loss: -0.013128781732424008
        total_loss: 8.445004940032959
        vf_explained_var: 0.9791472554206848
        vf_loss: 8.457902193069458
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.926666666666666
    gpu_util_percent0: 0.31933333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507222409007269
    mean_env_wait_ms: 1.179072463660362
    mean_inference_ms: 4.575572007861084
    mean_raw_obs_processing_ms: 0.39232237099721096
  time_since_restore: 695.7458291053772
  time_this_iter_s: 25.365896463394165
  time_total_s: 695.7458291053772
  timers:
    learn_throughput: 8720.405
    learn_time_ms: 18553.267
    sample_throughput: 23785.919
    sample_time_ms: 6802.008
    update_time_ms: 23.677
  timestamp: 1602732437
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     27 |          695.746 | 4368384 |  262.244 |              307.596 |              160.172 |            797.122 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3253.5230658768623
    time_step_min: 2954
  date: 2020-10-15_03-27-43
  done: false
  episode_len_mean: 796.3996080527347
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 262.647706352677
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 239
  episodes_total: 5613
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6916379481554031
        entropy_coeff: 0.0005000000000000001
        kl: 0.006040201522409916
        model: {}
        policy_loss: -0.011756399088577988
        total_loss: 11.367794275283813
        vf_explained_var: 0.980556309223175
        vf_loss: 11.379292249679565
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.175862068965518
    gpu_util_percent0: 0.34068965517241384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15056635352340852
    mean_env_wait_ms: 1.1800381116837546
    mean_inference_ms: 4.565546697709911
    mean_raw_obs_processing_ms: 0.39173801358381355
  time_since_restore: 721.2351441383362
  time_this_iter_s: 25.489315032958984
  time_total_s: 721.2351441383362
  timers:
    learn_throughput: 8727.461
    learn_time_ms: 18538.266
    sample_throughput: 23739.92
    sample_time_ms: 6815.187
    update_time_ms: 23.92
  timestamp: 1602732463
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     28 |          721.235 | 4530176 |  262.648 |              312.444 |              160.172 |              796.4 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3249.9229841488627
    time_step_min: 2954
  date: 2020-10-15_03-28-09
  done: false
  episode_len_mean: 795.7624016421485
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 263.1489665730172
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 233
  episodes_total: 5846
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6752575387557348
        entropy_coeff: 0.0005000000000000001
        kl: 0.00550704119571795
        model: {}
        policy_loss: -0.012052623749089738
        total_loss: 7.815642714500427
        vf_explained_var: 0.9837474822998047
        vf_loss: 7.827482144037883
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33333333333334
    gpu_util_percent0: 0.32500000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15042533138997324
    mean_env_wait_ms: 1.180890842608842
    mean_inference_ms: 4.5565665767454195
    mean_raw_obs_processing_ms: 0.3912380236374787
  time_since_restore: 747.2122163772583
  time_this_iter_s: 25.97707223892212
  time_total_s: 747.2122163772583
  timers:
    learn_throughput: 8704.645
    learn_time_ms: 18586.859
    sample_throughput: 23712.811
    sample_time_ms: 6822.979
    update_time_ms: 24.692
  timestamp: 1602732489
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     29 |          747.212 | 4691968 |  263.149 |              312.444 |              160.172 |            795.762 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3247.832606507883
    time_step_min: 2954
  date: 2020-10-15_03-28-35
  done: false
  episode_len_mean: 795.2989673550966
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 263.47953721088305
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 158
  episodes_total: 6004
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6948543091615041
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058716343094905215
        model: {}
        policy_loss: -0.00948784469316403
        total_loss: 7.720956802368164
        vf_explained_var: 0.9807594418525696
        vf_loss: 7.730204939842224
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.637931034482758
    gpu_util_percent0: 0.38275862068965516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15033460644555108
    mean_env_wait_ms: 1.1814372373879547
    mean_inference_ms: 4.550760598908886
    mean_raw_obs_processing_ms: 0.39090543393304117
  time_since_restore: 772.5920448303223
  time_this_iter_s: 25.379828453063965
  time_total_s: 772.5920448303223
  timers:
    learn_throughput: 8722.489
    learn_time_ms: 18548.833
    sample_throughput: 23692.167
    sample_time_ms: 6828.924
    update_time_ms: 26.111
  timestamp: 1602732515
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     30 |          772.592 | 4853760 |   263.48 |              312.444 |              160.172 |            795.299 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3245.0175182481753
    time_step_min: 2954
  date: 2020-10-15_03-29-01
  done: false
  episode_len_mean: 794.780248106976
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 263.85682831212074
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 203
  episodes_total: 6207
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6673331558704376
        entropy_coeff: 0.0005000000000000001
        kl: 0.005985880736261606
        model: {}
        policy_loss: -0.009181171145883127
        total_loss: 9.948039134343466
        vf_explained_var: 0.9806312918663025
        vf_loss: 9.956955432891846
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.063333333333333
    gpu_util_percent0: 0.37233333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15022581080523517
    mean_env_wait_ms: 1.1821333642838685
    mean_inference_ms: 4.543661608544004
    mean_raw_obs_processing_ms: 0.3904989111972905
  time_since_restore: 798.2964677810669
  time_this_iter_s: 25.70442295074463
  time_total_s: 798.2964677810669
  timers:
    learn_throughput: 8719.198
    learn_time_ms: 18555.836
    sample_throughput: 23677.302
    sample_time_ms: 6833.211
    update_time_ms: 27.306
  timestamp: 1602732541
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     31 |          798.296 | 5015552 |  263.857 |              312.444 |              160.172 |             794.78 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3241.925097125097
    time_step_min: 2954
  date: 2020-10-15_03-29-27
  done: false
  episode_len_mean: 794.0824455766559
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 264.24290613405947
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 270
  episodes_total: 6477
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6367034415404002
        entropy_coeff: 0.0005000000000000001
        kl: 0.004866847263959547
        model: {}
        policy_loss: -0.009333334203499058
        total_loss: 9.950478156407675
        vf_explained_var: 0.9824070930480957
        vf_loss: 9.959642966588339
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.620689655172413
    gpu_util_percent0: 0.31413793103448273
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15008391266578353
    mean_env_wait_ms: 1.1829826020087038
    mean_inference_ms: 4.534766618087938
    mean_raw_obs_processing_ms: 0.3899954279929681
  time_since_restore: 823.854710817337
  time_this_iter_s: 25.55824303627014
  time_total_s: 823.854710817337
  timers:
    learn_throughput: 8722.666
    learn_time_ms: 18548.457
    sample_throughput: 23647.785
    sample_time_ms: 6841.74
    update_time_ms: 26.904
  timestamp: 1602732567
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     32 |          823.855 | 5177344 |  264.243 |              312.444 |              160.172 |            794.082 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3240.3413709432816
    time_step_min: 2954
  date: 2020-10-15_03-29-52
  done: false
  episode_len_mean: 793.6538577456299
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 264.50458015964347
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 159
  episodes_total: 6636
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6493706206480662
        entropy_coeff: 0.0005000000000000001
        kl: 0.006435106275603175
        model: {}
        policy_loss: -0.011277024847610543
        total_loss: 7.387680570284526
        vf_explained_var: 0.9817662239074707
        vf_loss: 7.39896035194397
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.317241379310346
    gpu_util_percent0: 0.3344827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15000711375742692
    mean_env_wait_ms: 1.1834618397684398
    mean_inference_ms: 4.529813423573091
    mean_raw_obs_processing_ms: 0.38971446297879914
  time_since_restore: 849.3101749420166
  time_this_iter_s: 25.455464124679565
  time_total_s: 849.3101749420166
  timers:
    learn_throughput: 8714.087
    learn_time_ms: 18566.718
    sample_throughput: 23655.149
    sample_time_ms: 6839.61
    update_time_ms: 26.671
  timestamp: 1602732592
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     33 |           849.31 | 5339136 |  264.505 |              312.444 |              160.172 |            793.654 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3238.330138602182
    time_step_min: 2954
  date: 2020-10-15_03-30-18
  done: false
  episode_len_mean: 793.1553341148887
  episode_reward_max: 312.4444444444444
  episode_reward_mean: 264.828099281206
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 6824
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6318176537752151
        entropy_coeff: 0.0005000000000000001
        kl: 0.006097383797168732
        model: {}
        policy_loss: -0.012870894696485871
        total_loss: 8.550239086151123
        vf_explained_var: 0.9823779463768005
        vf_loss: 8.563121239344278
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.04333333333334
    gpu_util_percent0: 0.35766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1499228978069329
    mean_env_wait_ms: 1.184027424356708
    mean_inference_ms: 4.524216975483228
    mean_raw_obs_processing_ms: 0.3893982836556221
  time_since_restore: 875.107860326767
  time_this_iter_s: 25.797685384750366
  time_total_s: 875.107860326767
  timers:
    learn_throughput: 8702.833
    learn_time_ms: 18590.727
    sample_throughput: 23576.067
    sample_time_ms: 6862.553
    update_time_ms: 28.129
  timestamp: 1602732618
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     34 |          875.108 | 5500928 |  264.828 |              312.444 |              160.172 |            793.155 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3235.360600056609
    time_step_min: 2954
  date: 2020-10-15_03-30-44
  done: false
  episode_len_mean: 792.5229319077097
  episode_reward_max: 313.3535353535354
  episode_reward_mean: 265.3049643878288
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 284
  episodes_total: 7108
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6010112563769022
        entropy_coeff: 0.0005000000000000001
        kl: 0.005382241448387504
        model: {}
        policy_loss: -0.008189600625579866
        total_loss: 10.26532506942749
        vf_explained_var: 0.9825909733772278
        vf_loss: 10.273545980453491
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.779310344827582
    gpu_util_percent0: 0.383448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1497938374355732
    mean_env_wait_ms: 1.1848123733742386
    mean_inference_ms: 4.51612209408004
    mean_raw_obs_processing_ms: 0.3889384559232916
  time_since_restore: 900.4525640010834
  time_this_iter_s: 25.344703674316406
  time_total_s: 900.4525640010834
  timers:
    learn_throughput: 8717.134
    learn_time_ms: 18560.229
    sample_throughput: 23611.948
    sample_time_ms: 6852.124
    update_time_ms: 28.491
  timestamp: 1602732644
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     35 |          900.453 | 5662720 |  265.305 |              313.354 |              160.172 |            792.523 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3233.2241904234706
    time_step_min: 2901
  date: 2020-10-15_03-31-09
  done: false
  episode_len_mean: 792.2623830489819
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 265.6072294213462
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 7268
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6170352200667063
        entropy_coeff: 0.0005000000000000001
        kl: 0.0059466949896886945
        model: {}
        policy_loss: -0.01173960545565933
        total_loss: 6.957229097684224
        vf_explained_var: 0.9826402068138123
        vf_loss: 6.9689797560373945
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.693103448275863
    gpu_util_percent0: 0.3517241379310345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14972760718123163
    mean_env_wait_ms: 1.1852325080475885
    mean_inference_ms: 4.511822610568541
    mean_raw_obs_processing_ms: 0.38869904962255497
  time_since_restore: 925.7845115661621
  time_this_iter_s: 25.331947565078735
  time_total_s: 925.7845115661621
  timers:
    learn_throughput: 8711.51
    learn_time_ms: 18572.211
    sample_throughput: 23614.13
    sample_time_ms: 6851.491
    update_time_ms: 28.584
  timestamp: 1602732669
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     36 |          925.785 | 5824512 |  265.607 |              315.626 |              160.172 |            792.262 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3231.484586262845
    time_step_min: 2901
  date: 2020-10-15_03-31-35
  done: false
  episode_len_mean: 791.9973111051357
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 265.87070761391817
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 170
  episodes_total: 7438
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6063422014315923
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055696020523707075
        model: {}
        policy_loss: -0.01051254443397435
        total_loss: 8.678027391433716
        vf_explained_var: 0.9808769226074219
        vf_loss: 8.68856438000997
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.826666666666668
    gpu_util_percent0: 0.3206666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496610733465096
    mean_env_wait_ms: 1.1856617510503977
    mean_inference_ms: 4.507446090723431
    mean_raw_obs_processing_ms: 0.388455855284118
  time_since_restore: 951.7507071495056
  time_this_iter_s: 25.966195583343506
  time_total_s: 951.7507071495056
  timers:
    learn_throughput: 8698.433
    learn_time_ms: 18600.132
    sample_throughput: 23514.371
    sample_time_ms: 6880.558
    update_time_ms: 30.132
  timestamp: 1602732695
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     37 |          951.751 | 5986304 |  265.871 |              315.626 |              160.172 |            791.997 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3228.3207179087008
    time_step_min: 2901
  date: 2020-10-15_03-32-01
  done: false
  episode_len_mean: 791.5625404216789
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 266.34716979652956
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 293
  episodes_total: 7731
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5738042294979095
        entropy_coeff: 0.0005000000000000001
        kl: 0.00562783897233506
        model: {}
        policy_loss: -0.010044375330229135
        total_loss: 9.108115196228027
        vf_explained_var: 0.9847708344459534
        vf_loss: 9.118165254592896
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.38275862068966
    gpu_util_percent0: 0.32482758620689656
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1495457990932075
    mean_env_wait_ms: 1.1863652160928428
    mean_inference_ms: 4.50017869256843
    mean_raw_obs_processing_ms: 0.38804572712182966
  time_since_restore: 976.8239562511444
  time_this_iter_s: 25.073249101638794
  time_total_s: 976.8239562511444
  timers:
    learn_throughput: 8721.353
    learn_time_ms: 18551.251
    sample_throughput: 23494.356
    sample_time_ms: 6886.42
    update_time_ms: 30.021
  timestamp: 1602732721
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     38 |          976.824 | 6148096 |  266.347 |              315.626 |              160.172 |            791.563 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3226.70692288114
    time_step_min: 2901
  date: 2020-10-15_03-32-27
  done: false
  episode_len_mean: 791.2292405063291
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 266.6031517708733
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 169
  episodes_total: 7900
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5792370488246282
        entropy_coeff: 0.0005000000000000001
        kl: 0.00606264709495008
        model: {}
        policy_loss: -0.01067247850005515
        total_loss: 6.964995781580607
        vf_explained_var: 0.9832236170768738
        vf_loss: 6.975654721260071
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95
    gpu_util_percent0: 0.35833333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14948442975590978
    mean_env_wait_ms: 1.1867505098459175
    mean_inference_ms: 4.496202456263151
    mean_raw_obs_processing_ms: 0.38782942478307136
  time_since_restore: 1002.7801263332367
  time_this_iter_s: 25.956170082092285
  time_total_s: 1002.7801263332367
  timers:
    learn_throughput: 8718.795
    learn_time_ms: 18556.694
    sample_throughput: 23495.636
    sample_time_ms: 6886.045
    update_time_ms: 30.384
  timestamp: 1602732747
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     39 |          1002.78 | 6309888 |  266.603 |              315.626 |              160.172 |            791.229 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3225.08203465902
    time_step_min: 2901
  date: 2020-10-15_03-32-53
  done: false
  episode_len_mean: 790.9283145231303
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 266.82990891176433
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 163
  episodes_total: 8063
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5844850142796835
        entropy_coeff: 0.0005000000000000001
        kl: 0.005840145400725305
        model: {}
        policy_loss: -0.011256719739321852
        total_loss: 7.129114309946696
        vf_explained_var: 0.9831411838531494
        vf_loss: 7.140371203422546
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.61666666666667
    gpu_util_percent0: 0.328
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1494291669370158
    mean_env_wait_ms: 1.1871132795391799
    mean_inference_ms: 4.492525075340646
    mean_raw_obs_processing_ms: 0.38762746838795653
  time_since_restore: 1028.469747543335
  time_this_iter_s: 25.689621210098267
  time_total_s: 1028.469747543335
  timers:
    learn_throughput: 8698.525
    learn_time_ms: 18599.935
    sample_throughput: 23543.132
    sample_time_ms: 6872.153
    update_time_ms: 31.038
  timestamp: 1602732773
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     40 |          1028.47 | 6471680 |   266.83 |              315.626 |              160.172 |            790.928 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3222.5124503789248
    time_step_min: 2901
  date: 2020-10-15_03-33-19
  done: false
  episode_len_mean: 790.3646918013166
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 267.24296828246565
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 292
  episodes_total: 8355
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.547782431046168
        entropy_coeff: 0.0005000000000000001
        kl: 0.005563245193722348
        model: {}
        policy_loss: -0.00940501414394627
        total_loss: 8.741782426834106
        vf_explained_var: 0.9853256344795227
        vf_loss: 8.751183350880941
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.386206896551727
    gpu_util_percent0: 0.40310344827586203
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14932729847757928
    mean_env_wait_ms: 1.187734482815038
    mean_inference_ms: 4.48616146383872
    mean_raw_obs_processing_ms: 0.38727499870169224
  time_since_restore: 1054.1412484645844
  time_this_iter_s: 25.67150092124939
  time_total_s: 1054.1412484645844
  timers:
    learn_throughput: 8697.066
    learn_time_ms: 18603.055
    sample_throughput: 23539.376
    sample_time_ms: 6873.249
    update_time_ms: 29.815
  timestamp: 1602732799
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     41 |          1054.14 | 6633472 |  267.243 |              315.626 |              160.172 |            790.365 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3220.718492343934
    time_step_min: 2901
  date: 2020-10-15_03-33-44
  done: false
  episode_len_mean: 790.01031411158
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 267.49992186279104
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 177
  episodes_total: 8532
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5557845085859299
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057312653710444765
        model: {}
        policy_loss: -0.011130651265072325
        total_loss: 6.433091362317403
        vf_explained_var: 0.9845104217529297
        vf_loss: 6.444213231404622
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.77333333333333
    gpu_util_percent0: 0.3593333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492720504601222
    mean_env_wait_ms: 1.188100225371462
    mean_inference_ms: 4.482528689998873
    mean_raw_obs_processing_ms: 0.3870789682623309
  time_since_restore: 1079.8505864143372
  time_this_iter_s: 25.709337949752808
  time_total_s: 1079.8505864143372
  timers:
    learn_throughput: 8689.747
    learn_time_ms: 18618.724
    sample_throughput: 23549.567
    sample_time_ms: 6870.275
    update_time_ms: 31.405
  timestamp: 1602732824
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     42 |          1079.85 | 6795264 |    267.5 |              315.626 |              160.172 |             790.01 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3218.8590084363805
    time_step_min: 2901
  date: 2020-10-15_03-34-10
  done: false
  episode_len_mean: 789.5771132834963
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 267.7375073332521
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 163
  episodes_total: 8695
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.557673399647077
        entropy_coeff: 0.0005000000000000001
        kl: 0.005991844770809014
        model: {}
        policy_loss: -0.01311191441588259
        total_loss: 7.294785380363464
        vf_explained_var: 0.9819095134735107
        vf_loss: 7.307876666386922
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.162068965517246
    gpu_util_percent0: 0.3568965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492235639292956
    mean_env_wait_ms: 1.1884292801405512
    mean_inference_ms: 4.479280535648235
    mean_raw_obs_processing_ms: 0.38690330875939444
  time_since_restore: 1105.2959280014038
  time_this_iter_s: 25.44534158706665
  time_total_s: 1105.2959280014038
  timers:
    learn_throughput: 8692.485
    learn_time_ms: 18612.859
    sample_throughput: 23538.102
    sample_time_ms: 6873.621
    update_time_ms: 31.38
  timestamp: 1602732850
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     43 |           1105.3 | 6957056 |  267.738 |              315.626 |              160.172 |            789.577 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3216.1969120608637
    time_step_min: 2901
  date: 2020-10-15_03-34-36
  done: false
  episode_len_mean: 788.793986636971
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.1476907156195
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 285
  episodes_total: 8980
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5189951856931051
        entropy_coeff: 0.0005000000000000001
        kl: 0.005256618373095989
        model: {}
        policy_loss: -0.009501781566844633
        total_loss: 8.741390466690063
        vf_explained_var: 0.9847350120544434
        vf_loss: 8.750888903935751
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.323333333333334
    gpu_util_percent0: 0.32599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14913674796178875
    mean_env_wait_ms: 1.1889942547970123
    mean_inference_ms: 4.473768793316546
    mean_raw_obs_processing_ms: 0.38660149833296653
  time_since_restore: 1131.0033876895905
  time_this_iter_s: 25.707459688186646
  time_total_s: 1131.0033876895905
  timers:
    learn_throughput: 8689.543
    learn_time_ms: 18619.16
    sample_throughput: 23590.583
    sample_time_ms: 6858.33
    update_time_ms: 30.163
  timestamp: 1602732876
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     44 |             1131 | 7118848 |  268.148 |              315.626 |              160.172 |            788.794 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3214.5473580355183
    time_step_min: 2901
  date: 2020-10-15_03-35-02
  done: false
  episode_len_mean: 788.2790266259275
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.37831942295065
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 9164
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5165195763111115
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055745857922981186
        model: {}
        policy_loss: -0.011673807865008712
        total_loss: 6.475711345672607
        vf_explained_var: 0.9840334057807922
        vf_loss: 6.487364610036214
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.551724137931036
    gpu_util_percent0: 0.33862068965517234
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14908415548540419
    mean_env_wait_ms: 1.1893386008777616
    mean_inference_ms: 4.47039491317284
    mean_raw_obs_processing_ms: 0.3864212254776293
  time_since_restore: 1156.572803735733
  time_this_iter_s: 25.569416046142578
  time_total_s: 1156.572803735733
  timers:
    learn_throughput: 8680.255
    learn_time_ms: 18639.083
    sample_throughput: 23584.827
    sample_time_ms: 6860.004
    update_time_ms: 29.768
  timestamp: 1602732902
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     45 |          1156.57 | 7280640 |  268.378 |              315.626 |              160.172 |            788.279 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3212.951883745963
    time_step_min: 2901
  date: 2020-10-15_03-35-27
  done: false
  episode_len_mean: 787.7806472353193
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.5902087744137
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 168
  episodes_total: 9332
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5223224957784017
        entropy_coeff: 0.0005000000000000001
        kl: 0.0062071040738373995
        model: {}
        policy_loss: -0.011719808033376466
        total_loss: 7.1881502866744995
        vf_explained_var: 0.982536792755127
        vf_loss: 7.199820796648662
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.376666666666672
    gpu_util_percent0: 0.3543333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1490396513966964
    mean_env_wait_ms: 1.1896587347986778
    mean_inference_ms: 4.467425185711493
    mean_raw_obs_processing_ms: 0.38626285153063145
  time_since_restore: 1181.6744050979614
  time_this_iter_s: 25.101601362228394
  time_total_s: 1181.6744050979614
  timers:
    learn_throughput: 8697.471
    learn_time_ms: 18602.19
    sample_throughput: 23540.708
    sample_time_ms: 6872.861
    update_time_ms: 30.138
  timestamp: 1602732927
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     46 |          1181.67 | 7442432 |   268.59 |              315.626 |              160.172 |            787.781 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3210.696378248617
    time_step_min: 2901
  date: 2020-10-15_03-35-52
  done: false
  episode_len_mean: 786.9504312584434
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 268.94608665896214
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 291
  episodes_total: 9623
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.48436538875102997
        entropy_coeff: 0.0005000000000000001
        kl: 0.005025261780247092
        model: {}
        policy_loss: -0.009326218181134513
        total_loss: 10.158210357030233
        vf_explained_var: 0.9820401072502136
        vf_loss: 10.167527596155802
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.603571428571428
    gpu_util_percent0: 0.39249999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7642857142857133
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14896152241500404
    mean_env_wait_ms: 1.1902025784023724
    mean_inference_ms: 4.462399160326646
    mean_raw_obs_processing_ms: 0.3859917221160899
  time_since_restore: 1206.4377653598785
  time_this_iter_s: 24.763360261917114
  time_total_s: 1206.4377653598785
  timers:
    learn_throughput: 8737.97
    learn_time_ms: 18515.971
    sample_throughput: 23653.501
    sample_time_ms: 6840.087
    update_time_ms: 27.644
  timestamp: 1602732952
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     47 |          1206.44 | 7604224 |  268.946 |              315.626 |              160.172 |             786.95 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3209.3080787369286
    time_step_min: 2901
  date: 2020-10-15_03-36-18
  done: false
  episode_len_mean: 786.5277664352797
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.15540872176234
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 9796
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4940425877769788
        entropy_coeff: 0.0005000000000000001
        kl: 0.005495002182821433
        model: {}
        policy_loss: -0.011561811668798327
        total_loss: 6.041600624720256
        vf_explained_var: 0.984626293182373
        vf_loss: 6.053134838740031
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79333333333334
    gpu_util_percent0: 0.3436666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14891644556633035
    mean_env_wait_ms: 1.1905097013290211
    mean_inference_ms: 4.459538925632833
    mean_raw_obs_processing_ms: 0.38583978908931416
  time_since_restore: 1232.07382273674
  time_this_iter_s: 25.636057376861572
  time_total_s: 1232.07382273674
  timers:
    learn_throughput: 8705.349
    learn_time_ms: 18585.355
    sample_throughput: 23709.747
    sample_time_ms: 6823.86
    update_time_ms: 29.463
  timestamp: 1602732978
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     48 |          1232.07 | 7766016 |  269.155 |              315.626 |              160.172 |            786.528 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3208.2380904421393
    time_step_min: 2901
  date: 2020-10-15_03-36-44
  done: false
  episode_len_mean: 786.0723096981245
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.3472048739324
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 9971
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49546317259470624
        entropy_coeff: 0.0005000000000000001
        kl: 0.005329665184641878
        model: {}
        policy_loss: -0.009244550912020108
        total_loss: 7.911429484685262
        vf_explained_var: 0.9816546440124512
        vf_loss: 7.920655131340027
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.30344827586207
    gpu_util_percent0: 0.36275862068965514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14887532526098707
    mean_env_wait_ms: 1.1908221074057177
    mean_inference_ms: 4.456764966844876
    mean_raw_obs_processing_ms: 0.3856934072827619
  time_since_restore: 1257.6615715026855
  time_this_iter_s: 25.587748765945435
  time_total_s: 1257.6615715026855
  timers:
    learn_throughput: 8724.852
    learn_time_ms: 18543.81
    sample_throughput: 23697.668
    sample_time_ms: 6827.338
    update_time_ms: 29.708
  timestamp: 1602733004
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     49 |          1257.66 | 7927808 |  269.347 |              315.626 |              160.172 |            786.072 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3205.9704472061844
    time_step_min: 2901
  date: 2020-10-15_03-37-10
  done: false
  episode_len_mean: 785.3676054965402
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.68567558441845
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 290
  episodes_total: 10261
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.46066005776325863
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050635455797115965
        model: {}
        policy_loss: -0.008463503210805357
        total_loss: 9.269286394119263
        vf_explained_var: 0.9833102822303772
        vf_loss: 9.277727127075195
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996666666666666
    gpu_util_percent0: 0.30933333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14880472776053055
    mean_env_wait_ms: 1.1913352967395685
    mean_inference_ms: 4.452251607540964
    mean_raw_obs_processing_ms: 0.3854561921395165
  time_since_restore: 1283.242517709732
  time_this_iter_s: 25.58094620704651
  time_total_s: 1283.242517709732
  timers:
    learn_throughput: 8734.601
    learn_time_ms: 18523.113
    sample_throughput: 23659.258
    sample_time_ms: 6838.423
    update_time_ms: 27.046
  timestamp: 1602733030
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     50 |          1283.24 | 8089600 |  269.686 |              315.626 |              160.172 |            785.368 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3204.7633352590024
    time_step_min: 2901
  date: 2020-10-15_03-37-35
  done: false
  episode_len_mean: 784.9677790563867
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 269.8372301844684
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 167
  episodes_total: 10428
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4764942228794098
        entropy_coeff: 0.0005000000000000001
        kl: 0.005219100043177605
        model: {}
        policy_loss: -0.010618934946251102
        total_loss: 6.132206002871196
        vf_explained_var: 0.9842545390129089
        vf_loss: 6.142802357673645
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.38275862068966
    gpu_util_percent0: 0.37206896551724133
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14876634408994724
    mean_env_wait_ms: 1.191620502664859
    mean_inference_ms: 4.449749925515801
    mean_raw_obs_processing_ms: 0.38532561562206696
  time_since_restore: 1308.7400169372559
  time_this_iter_s: 25.497499227523804
  time_total_s: 1308.7400169372559
  timers:
    learn_throughput: 8739.965
    learn_time_ms: 18511.744
    sample_throughput: 23690.849
    sample_time_ms: 6829.304
    update_time_ms: 28.015
  timestamp: 1602733055
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     51 |          1308.74 | 8251392 |  269.837 |              315.626 |              160.172 |            784.968 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3203.441251536062
    time_step_min: 2901
  date: 2020-10-15_03-38-01
  done: false
  episode_len_mean: 784.4849825816779
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 270.05279896222373
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 193
  episodes_total: 10621
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47358305503924686
        entropy_coeff: 0.0005000000000000001
        kl: 0.005458969506435096
        model: {}
        policy_loss: -0.010881804259649167
        total_loss: 8.382658878962198
        vf_explained_var: 0.9813825488090515
        vf_loss: 8.39350430170695
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.416666666666675
    gpu_util_percent0: 0.2933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14872464342283853
    mean_env_wait_ms: 1.1919526719744022
    mean_inference_ms: 4.446953284375632
    mean_raw_obs_processing_ms: 0.3851813044859622
  time_since_restore: 1334.0923554897308
  time_this_iter_s: 25.352338552474976
  time_total_s: 1334.0923554897308
  timers:
    learn_throughput: 8757.378
    learn_time_ms: 18474.936
    sample_throughput: 23685.756
    sample_time_ms: 6830.772
    update_time_ms: 26.575
  timestamp: 1602733081
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     52 |          1334.09 | 8413184 |  270.053 |              315.626 |              160.172 |            784.485 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3201.2385295743507
    time_step_min: 2901
  date: 2020-10-15_03-38-27
  done: false
  episode_len_mean: 783.8873898678414
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 270.3710257864994
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 275
  episodes_total: 10896
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4343193396925926
        entropy_coeff: 0.0005000000000000001
        kl: 0.005061795896229644
        model: {}
        policy_loss: -0.009902053971018177
        total_loss: 7.9938474496205645
        vf_explained_var: 0.9849822521209717
        vf_loss: 8.003713607788086
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.36896551724138
    gpu_util_percent0: 0.35206896551724143
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148664123786829
    mean_env_wait_ms: 1.1924047847385688
    mean_inference_ms: 4.443109304585186
    mean_raw_obs_processing_ms: 0.38498005902088916
  time_since_restore: 1359.4326832294464
  time_this_iter_s: 25.340327739715576
  time_total_s: 1359.4326832294464
  timers:
    learn_throughput: 8762.666
    learn_time_ms: 18463.787
    sample_throughput: 23687.011
    sample_time_ms: 6830.41
    update_time_ms: 26.624
  timestamp: 1602733107
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     53 |          1359.43 | 8574976 |  270.371 |              315.626 |              160.172 |            783.887 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3199.685333091305
    time_step_min: 2901
  date: 2020-10-15_03-38-52
  done: false
  episode_len_mean: 783.5109403254972
  episode_reward_max: 315.62626262626287
  episode_reward_mean: 270.5860549436499
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 164
  episodes_total: 11060
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4581292246778806
        entropy_coeff: 0.0005000000000000001
        kl: 0.005291610917386909
        model: {}
        policy_loss: -0.010529855353524908
        total_loss: 5.566484133402507
        vf_explained_var: 0.9845022559165955
        vf_loss: 5.57697860399882
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.275862068965516
    gpu_util_percent0: 0.35827586206896556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486303311322275
    mean_env_wait_ms: 1.1926707921891795
    mean_inference_ms: 4.440876356698755
    mean_raw_obs_processing_ms: 0.3848634078368406
  time_since_restore: 1384.657611131668
  time_this_iter_s: 25.22492790222168
  time_total_s: 1384.657611131668
  timers:
    learn_throughput: 8780.609
    learn_time_ms: 18426.056
    sample_throughput: 23723.514
    sample_time_ms: 6819.9
    update_time_ms: 26.533
  timestamp: 1602733132
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     54 |          1384.66 | 8736768 |  270.586 |              315.626 |              160.172 |            783.511 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3197.8887800534285
    time_step_min: 2901
  date: 2020-10-15_03-39-18
  done: false
  episode_len_mean: 783.0655606813343
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 270.8659017427648
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 11272
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4526110664010048
        entropy_coeff: 0.0005000000000000001
        kl: 0.005053112360959251
        model: {}
        policy_loss: -0.010565120882044235
        total_loss: 7.863551656405131
        vf_explained_var: 0.9828733801841736
        vf_loss: 7.874090313911438
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.93666666666667
    gpu_util_percent0: 0.33666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1485884704050972
    mean_env_wait_ms: 1.1930102399394076
    mean_inference_ms: 4.438052064494798
    mean_raw_obs_processing_ms: 0.38472124845688227
  time_since_restore: 1410.0426182746887
  time_this_iter_s: 25.38500714302063
  time_total_s: 1410.0426182746887
  timers:
    learn_throughput: 8796.879
    learn_time_ms: 18391.977
    sample_throughput: 23698.756
    sample_time_ms: 6827.025
    update_time_ms: 34.196
  timestamp: 1602733158
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     55 |          1410.04 | 8898560 |  270.866 |              319.566 |              160.172 |            783.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3195.6956483899044
    time_step_min: 2901
  date: 2020-10-15_03-39-44
  done: false
  episode_len_mean: 782.5561047519944
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 271.1800050452496
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 260
  episodes_total: 11532
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4222170164187749
        entropy_coeff: 0.0005000000000000001
        kl: 0.004911028741238018
        model: {}
        policy_loss: -0.01075010719553878
        total_loss: 6.202483693758647
        vf_explained_var: 0.9874430298805237
        vf_loss: 6.213199337323506
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.724137931034484
    gpu_util_percent0: 0.3686206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14853641290529992
    mean_env_wait_ms: 1.1934112455299646
    mean_inference_ms: 4.434764202371664
    mean_raw_obs_processing_ms: 0.3845466156719964
  time_since_restore: 1435.673068523407
  time_this_iter_s: 25.63045024871826
  time_total_s: 1435.673068523407
  timers:
    learn_throughput: 8767.623
    learn_time_ms: 18453.349
    sample_throughput: 23732.673
    sample_time_ms: 6817.268
    update_time_ms: 33.871
  timestamp: 1602733184
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     56 |          1435.67 | 9060352 |   271.18 |              319.566 |              160.172 |            782.556 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3194.1242060085838
    time_step_min: 2901
  date: 2020-10-15_03-40-10
  done: false
  episode_len_mean: 782.2710400273692
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 271.4071643565314
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 11692
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4464958707491557
        entropy_coeff: 0.0005000000000000001
        kl: 0.005758141516707838
        model: {}
        policy_loss: -0.010978214733768255
        total_loss: 5.500918626785278
        vf_explained_var: 0.9844598770141602
        vf_loss: 5.511976043383281
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82666666666667
    gpu_util_percent0: 0.33699999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14850661934159018
    mean_env_wait_ms: 1.1936508454812655
    mean_inference_ms: 4.432782078391004
    mean_raw_obs_processing_ms: 0.38444459536863235
  time_since_restore: 1461.3734431266785
  time_this_iter_s: 25.700374603271484
  time_total_s: 1461.3734431266785
  timers:
    learn_throughput: 8727.008
    learn_time_ms: 18539.228
    sample_throughput: 23714.668
    sample_time_ms: 6822.444
    update_time_ms: 36.088
  timestamp: 1602733210
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     57 |          1461.37 | 9222144 |  271.407 |              319.566 |              160.172 |            782.271 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3192.0986615034935
    time_step_min: 2901
  date: 2020-10-15_03-40-36
  done: false
  episode_len_mean: 781.8736683164165
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 271.7133689042086
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 11921
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.43150653193394345
        entropy_coeff: 0.0005000000000000001
        kl: 0.005714221857488155
        model: {}
        policy_loss: -0.01010762486839667
        total_loss: 6.938943028450012
        vf_explained_var: 0.9855101704597473
        vf_loss: 6.949123382568359
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.810000000000006
    gpu_util_percent0: 0.33233333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14846524657528432
    mean_env_wait_ms: 1.1939860891958232
    mean_inference_ms: 4.430004264927324
    mean_raw_obs_processing_ms: 0.38430421697291534
  time_since_restore: 1487.109254360199
  time_this_iter_s: 25.735811233520508
  time_total_s: 1487.109254360199
  timers:
    learn_throughput: 8718.899
    learn_time_ms: 18556.472
    sample_throughput: 23736.414
    sample_time_ms: 6816.194
    update_time_ms: 34.439
  timestamp: 1602733236
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     58 |          1487.11 | 9383936 |  271.713 |              319.566 |              160.172 |            781.874 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3189.911153275037
    time_step_min: 2901
  date: 2020-10-15_03-41-02
  done: false
  episode_len_mean: 781.5081387701414
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.04074450522984
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 243
  episodes_total: 12164
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.404120405515035
        entropy_coeff: 0.0005000000000000001
        kl: 0.004897760188517471
        model: {}
        policy_loss: -0.010030022653154447
        total_loss: 6.890608310699463
        vf_explained_var: 0.9857112765312195
        vf_loss: 6.900717814763387
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92413793103448
    gpu_util_percent0: 0.3244827586206897
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14842090734533717
    mean_env_wait_ms: 1.1943245192540672
    mean_inference_ms: 4.42719579318257
    mean_raw_obs_processing_ms: 0.3841573397994461
  time_since_restore: 1512.8398098945618
  time_this_iter_s: 25.730555534362793
  time_total_s: 1512.8398098945618
  timers:
    learn_throughput: 8704.156
    learn_time_ms: 18587.903
    sample_throughput: 23800.509
    sample_time_ms: 6797.838
    update_time_ms: 32.303
  timestamp: 1602733262
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     59 |          1512.84 | 9545728 |  272.041 |              319.566 |              160.172 |            781.508 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3188.435072864935
    time_step_min: 2901
  date: 2020-10-15_03-41-28
  done: false
  episode_len_mean: 781.2807302231238
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.24465343086035
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 12325
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.42455152918895084
        entropy_coeff: 0.0005000000000000001
        kl: 0.005471882022296389
        model: {}
        policy_loss: -0.012106145654494563
        total_loss: 5.095531066258748
        vf_explained_var: 0.9861391186714172
        vf_loss: 5.107781012852986
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.887096774193544
    gpu_util_percent0: 0.4019354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14839346658313682
    mean_env_wait_ms: 1.1945431842834626
    mean_inference_ms: 4.425372773247266
    mean_raw_obs_processing_ms: 0.3840644467443304
  time_since_restore: 1538.6726100444794
  time_this_iter_s: 25.832800149917603
  time_total_s: 1538.6726100444794
  timers:
    learn_throughput: 8693.826
    learn_time_ms: 18609.988
    sample_throughput: 23799.838
    sample_time_ms: 6798.03
    update_time_ms: 33.035
  timestamp: 1602733288
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     60 |          1538.67 | 9707520 |  272.245 |              319.566 |              160.172 |            781.281 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3186.366214057508
    time_step_min: 2901
  date: 2020-10-15_03-41-54
  done: false
  episode_len_mean: 780.9582072918325
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.5457721619958
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 237
  episodes_total: 12562
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4055274575948715
        entropy_coeff: 0.0005000000000000001
        kl: 0.005624759243801236
        model: {}
        policy_loss: -0.0117449215540546
        total_loss: 7.4614401658376055
        vf_explained_var: 0.9850519299507141
        vf_loss: 7.473317543665568
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.39310344827587
    gpu_util_percent0: 0.3631034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483535508345925
    mean_env_wait_ms: 1.1948541795401106
    mean_inference_ms: 4.422721029244358
    mean_raw_obs_processing_ms: 0.38393059877447105
  time_since_restore: 1564.3942878246307
  time_this_iter_s: 25.721677780151367
  time_total_s: 1564.3942878246307
  timers:
    learn_throughput: 8690.067
    learn_time_ms: 18618.039
    sample_throughput: 23746.639
    sample_time_ms: 6813.259
    update_time_ms: 31.93
  timestamp: 1602733314
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     61 |          1564.39 | 9869312 |  272.546 |              319.566 |              160.172 |            780.958 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3184.6000784006274
    time_step_min: 2901
  date: 2020-10-15_03-42-20
  done: false
  episode_len_mean: 780.6356958662186
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 272.80721886363835
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 235
  episodes_total: 12797
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.381344939271609
        entropy_coeff: 0.0005000000000000001
        kl: 0.005135919394282003
        model: {}
        policy_loss: -0.008930078712486042
        total_loss: 6.595044096310933
        vf_explained_var: 0.9862293601036072
        vf_loss: 6.604100624720256
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.900000000000002
    gpu_util_percent0: 0.3113333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14831449008083558
    mean_env_wait_ms: 1.195151576479111
    mean_inference_ms: 4.42025854343422
    mean_raw_obs_processing_ms: 0.3838008964649393
  time_since_restore: 1590.0697085857391
  time_this_iter_s: 25.6754207611084
  time_total_s: 1590.0697085857391
  timers:
    learn_throughput: 8685.079
    learn_time_ms: 18628.731
    sample_throughput: 23675.016
    sample_time_ms: 6833.871
    update_time_ms: 31.899
  timestamp: 1602733340
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     62 |          1590.07 | 10031104 |  272.807 |              319.566 |              160.172 |            780.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3183.2258614014713
    time_step_min: 2897
  date: 2020-10-15_03-42-46
  done: false
  episode_len_mean: 780.4298062823184
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.02102525603334
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 160
  episodes_total: 12957
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4042324995001157
        entropy_coeff: 0.0005000000000000001
        kl: 0.005389541115922232
        model: {}
        policy_loss: -0.010069002693247361
        total_loss: 5.4410390456517534
        vf_explained_var: 0.9849755764007568
        vf_loss: 5.451243003209432
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.886206896551734
    gpu_util_percent0: 0.36999999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1482891864868483
    mean_env_wait_ms: 1.195348724386441
    mean_inference_ms: 4.4186024449372665
    mean_raw_obs_processing_ms: 0.3837163705422368
  time_since_restore: 1615.6917593479156
  time_this_iter_s: 25.622050762176514
  time_total_s: 1615.6917593479156
  timers:
    learn_throughput: 8675.926
    learn_time_ms: 18648.384
    sample_throughput: 23656.655
    sample_time_ms: 6839.175
    update_time_ms: 33.973
  timestamp: 1602733366
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     63 |          1615.69 | 10192896 |  273.021 |              319.566 |              160.172 |             780.43 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3181.1480777997267
    time_step_min: 2896
  date: 2020-10-15_03-43-12
  done: false
  episode_len_mean: 780.1008027870342
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.3253184679268
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 247
  episodes_total: 13204
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.38297328104575473
        entropy_coeff: 0.0005000000000000001
        kl: 0.005209332370820145
        model: {}
        policy_loss: -0.010129818848023811
        total_loss: 8.08438503742218
        vf_explained_var: 0.9840841889381409
        vf_loss: 8.094641208648682
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79
    gpu_util_percent0: 0.3746666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14825028470124074
    mean_env_wait_ms: 1.1956417956631162
    mean_inference_ms: 4.41606433205811
    mean_raw_obs_processing_ms: 0.38358807851929033
  time_since_restore: 1641.298626422882
  time_this_iter_s: 25.60686707496643
  time_total_s: 1641.298626422882
  timers:
    learn_throughput: 8664.011
    learn_time_ms: 18674.03
    sample_throughput: 23617.443
    sample_time_ms: 6850.53
    update_time_ms: 33.739
  timestamp: 1602733392
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     64 |           1641.3 | 10354688 |  273.325 |              319.566 |              160.172 |            780.101 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3179.368464296385
    time_step_min: 2896
  date: 2020-10-15_03-43-37
  done: false
  episode_len_mean: 779.8517498138496
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.5901156012846
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 13430
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.36295940975348157
        entropy_coeff: 0.0005000000000000001
        kl: 0.004922469689821203
        model: {}
        policy_loss: -0.008573521762931099
        total_loss: 5.949928323427836
        vf_explained_var: 0.9866483211517334
        vf_loss: 5.958621978759766
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.34827586206897
    gpu_util_percent0: 0.3462068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14821599392030227
    mean_env_wait_ms: 1.1959029214437988
    mean_inference_ms: 4.413881513808556
    mean_raw_obs_processing_ms: 0.38347553138443496
  time_since_restore: 1666.9100449085236
  time_this_iter_s: 25.61141848564148
  time_total_s: 1666.9100449085236
  timers:
    learn_throughput: 8649.406
    learn_time_ms: 18705.561
    sample_throughput: 23624.703
    sample_time_ms: 6848.425
    update_time_ms: 26.29
  timestamp: 1602733417
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     65 |          1666.91 | 10516480 |   273.59 |              319.566 |              160.172 |            779.852 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3178.149184321252
    time_step_min: 2884
  date: 2020-10-15_03-44-03
  done: false
  episode_len_mean: 779.7122672750019
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 273.7857811316491
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 159
  episodes_total: 13589
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.38605013489723206
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642061548617979
        model: {}
        policy_loss: -0.010416361369910495
        total_loss: 5.3233656485875445
        vf_explained_var: 0.9852609038352966
        vf_loss: 5.333939750989278
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.923333333333336
    gpu_util_percent0: 0.3143333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7866666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14819252209795336
    mean_env_wait_ms: 1.1960792186393157
    mean_inference_ms: 4.412362325683837
    mean_raw_obs_processing_ms: 0.3833980760159749
  time_since_restore: 1692.5184988975525
  time_this_iter_s: 25.60845398902893
  time_total_s: 1692.5184988975525
  timers:
    learn_throughput: 8656.03
    learn_time_ms: 18691.247
    sample_throughput: 23586.603
    sample_time_ms: 6859.487
    update_time_ms: 26.281
  timestamp: 1602733443
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     66 |          1692.52 | 10678272 |  273.786 |              319.566 |              160.172 |            779.712 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3176.05530187722
    time_step_min: 2884
  date: 2020-10-15_03-44-29
  done: false
  episode_len_mean: 779.4473589132162
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 274.07706153229674
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 250
  episodes_total: 13839
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.36056224753459293
        entropy_coeff: 0.0005000000000000001
        kl: 0.005020897253416479
        model: {}
        policy_loss: -0.010234104430613419
        total_loss: 7.156545162200928
        vf_explained_var: 0.9859125018119812
        vf_loss: 7.166928052902222
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.013793103448275
    gpu_util_percent0: 0.30068965517241375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481561142585321
    mean_env_wait_ms: 1.1963446936723845
    mean_inference_ms: 4.4099766319692915
    mean_raw_obs_processing_ms: 0.38327883439690874
  time_since_restore: 1717.98681640625
  time_this_iter_s: 25.46831750869751
  time_total_s: 1717.98681640625
  timers:
    learn_throughput: 8667.782
    learn_time_ms: 18665.905
    sample_throughput: 23578.893
    sample_time_ms: 6861.73
    update_time_ms: 24.121
  timestamp: 1602733469
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     67 |          1717.99 | 10840064 |  274.077 |              319.566 |              160.172 |            779.447 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3174.5090947999142
    time_step_min: 2884
  date: 2020-10-15_03-44-55
  done: false
  episode_len_mean: 779.2545338169405
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 274.31829280645155
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 14061
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.341914323469003
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052836076356470585
        model: {}
        policy_loss: -0.010361551064609861
        total_loss: 5.40642003218333
        vf_explained_var: 0.9880139827728271
        vf_loss: 5.416919390360515
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.020000000000003
    gpu_util_percent0: 0.32266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481246874845564
    mean_env_wait_ms: 1.1965747231428094
    mean_inference_ms: 4.4080155947574085
    mean_raw_obs_processing_ms: 0.3831742702304886
  time_since_restore: 1743.5204710960388
  time_this_iter_s: 25.53365468978882
  time_total_s: 1743.5204710960388
  timers:
    learn_throughput: 8679.941
    learn_time_ms: 18639.758
    sample_throughput: 23560.753
    sample_time_ms: 6867.013
    update_time_ms: 23.817
  timestamp: 1602733495
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     68 |          1743.52 | 11001856 |  274.318 |              319.566 |              160.172 |            779.255 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3173.3434414668545
    time_step_min: 2884
  date: 2020-10-15_03-45-21
  done: false
  episode_len_mean: 779.1016031500492
  episode_reward_max: 319.5656565656563
  episode_reward_mean: 274.4913904904764
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 14222
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.36352455367644626
        entropy_coeff: 0.0005000000000000001
        kl: 0.005572410494399567
        model: {}
        policy_loss: -0.011343164544086903
        total_loss: 5.250432054201762
        vf_explained_var: 0.9859452247619629
        vf_loss: 5.2619220813115435
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95
    gpu_util_percent0: 0.4193333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14810324645087666
    mean_env_wait_ms: 1.196736319483272
    mean_inference_ms: 4.406603121631371
    mean_raw_obs_processing_ms: 0.38310221146348433
  time_since_restore: 1769.1633801460266
  time_this_iter_s: 25.642909049987793
  time_total_s: 1769.1633801460266
  timers:
    learn_throughput: 8682.491
    learn_time_ms: 18634.284
    sample_throughput: 23597.555
    sample_time_ms: 6856.304
    update_time_ms: 24.096
  timestamp: 1602733521
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     69 |          1769.16 | 11163648 |  274.491 |              319.566 |              160.172 |            779.102 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3171.347124047124
    time_step_min: 2868
  date: 2020-10-15_03-45-47
  done: false
  episode_len_mean: 778.9200525152017
  episode_reward_max: 320.6262626262624
  episode_reward_mean: 274.8061104410607
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 250
  episodes_total: 14472
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3488816171884537
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051874959996591015
        model: {}
        policy_loss: -0.0095658558808888
        total_loss: 6.619034767150879
        vf_explained_var: 0.9869111180305481
        vf_loss: 6.628742774327596
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.50689655172414
    gpu_util_percent0: 0.34586206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8551724137931043
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806933592848273
    mean_env_wait_ms: 1.1969724577202363
    mean_inference_ms: 4.404425238241572
    mean_raw_obs_processing_ms: 0.382989488840724
  time_since_restore: 1794.5315909385681
  time_this_iter_s: 25.368210792541504
  time_total_s: 1794.5315909385681
  timers:
    learn_throughput: 8698.342
    learn_time_ms: 18600.327
    sample_throughput: 23636.44
    sample_time_ms: 6845.024
    update_time_ms: 23.63
  timestamp: 1602733547
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     70 |          1794.53 | 11325440 |  274.806 |              320.626 |              160.172 |             778.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3169.531704320524
    time_step_min: 2868
  date: 2020-10-15_03-46-13
  done: false
  episode_len_mean: 778.7327298713673
  episode_reward_max: 320.6262626262624
  episode_reward_mean: 275.083167480976
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 221
  episodes_total: 14693
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.32677702854077023
        entropy_coeff: 0.0005000000000000001
        kl: 0.004798869020305574
        model: {}
        policy_loss: -0.011433972329541575
        total_loss: 5.6798553466796875
        vf_explained_var: 0.9868795275688171
        vf_loss: 5.691422780354817
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203333333333333
    gpu_util_percent0: 0.36866666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.860000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14803995050274213
    mean_env_wait_ms: 1.197177346705275
    mean_inference_ms: 4.402603764859647
    mean_raw_obs_processing_ms: 0.38289181509129233
  time_since_restore: 1820.2522444725037
  time_this_iter_s: 25.720653533935547
  time_total_s: 1820.2522444725037
  timers:
    learn_throughput: 8695.565
    learn_time_ms: 18606.266
    sample_throughput: 23690.752
    sample_time_ms: 6829.332
    update_time_ms: 31.723
  timestamp: 1602733573
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     71 |          1820.25 | 11487232 |  275.083 |              320.626 |              160.172 |            778.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3168.1851201728327
    time_step_min: 2868
  date: 2020-10-15_03-46-39
  done: false
  episode_len_mean: 778.602800592433
  episode_reward_max: 322.7474747474748
  episode_reward_mean: 275.28919394565014
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 161
  episodes_total: 14854
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3494248017668724
        entropy_coeff: 0.0005000000000000001
        kl: 0.005758242992063363
        model: {}
        policy_loss: -0.007309712387116936
        total_loss: 5.1422200202941895
        vf_explained_var: 0.9858047366142273
        vf_loss: 5.149686296780904
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87586206896552
    gpu_util_percent0: 0.3896551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14802019928856222
    mean_env_wait_ms: 1.1973214960176486
    mean_inference_ms: 4.401300741550204
    mean_raw_obs_processing_ms: 0.382824096160546
  time_since_restore: 1845.8897905349731
  time_this_iter_s: 25.637546062469482
  time_total_s: 1845.8897905349731
  timers:
    learn_throughput: 8688.19
    learn_time_ms: 18622.061
    sample_throughput: 23762.525
    sample_time_ms: 6808.704
    update_time_ms: 31.705
  timestamp: 1602733599
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     72 |          1845.89 | 11649024 |  275.289 |              322.747 |              160.172 |            778.603 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3166.175416473087
    time_step_min: 2849
  date: 2020-10-15_03-47-04
  done: false
  episode_len_mean: 778.3403269574426
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 275.5920265598603
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 255
  episodes_total: 15109
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3287614757815997
        entropy_coeff: 0.0005000000000000001
        kl: 0.005143803466732304
        model: {}
        policy_loss: -0.00944469571307612
        total_loss: 7.402278820673625
        vf_explained_var: 0.9853112101554871
        vf_loss: 7.411871870358785
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85333333333334
    gpu_util_percent0: 0.36800000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14798816648005417
    mean_env_wait_ms: 1.1975419016616362
    mean_inference_ms: 4.399244953428487
    mean_raw_obs_processing_ms: 0.3827163303823725
  time_since_restore: 1871.3615827560425
  time_this_iter_s: 25.471792221069336
  time_total_s: 1871.3615827560425
  timers:
    learn_throughput: 8697.772
    learn_time_ms: 18601.545
    sample_throughput: 23764.982
    sample_time_ms: 6808.0
    update_time_ms: 36.713
  timestamp: 1602733624
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     73 |          1871.36 | 11810816 |  275.592 |              323.505 |              160.172 |             778.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3164.4845569951576
    time_step_min: 2849
  date: 2020-10-15_03-47-30
  done: false
  episode_len_mean: 778.1622291829809
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 275.854682296734
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 215
  episodes_total: 15324
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3131432856122653
        entropy_coeff: 0.0005000000000000001
        kl: 0.004759159792835514
        model: {}
        policy_loss: -0.008013654531775197
        total_loss: 4.949917833010356
        vf_explained_var: 0.9882403016090393
        vf_loss: 4.9580732981363935
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.486206896551728
    gpu_util_percent0: 0.36
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14796158277683658
    mean_env_wait_ms: 1.1977190218588016
    mean_inference_ms: 4.397603000998521
    mean_raw_obs_processing_ms: 0.38262853857572793
  time_since_restore: 1896.8732178211212
  time_this_iter_s: 25.511635065078735
  time_total_s: 1896.8732178211212
  timers:
    learn_throughput: 8701.55
    learn_time_ms: 18593.468
    sample_throughput: 23776.604
    sample_time_ms: 6804.672
    update_time_ms: 37.942
  timestamp: 1602733650
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     74 |          1896.87 | 11972608 |  275.855 |              323.505 |              160.172 |            778.162 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3163.1884789644014
    time_step_min: 2849
  date: 2020-10-15_03-47-56
  done: false
  episode_len_mean: 778.0116834495224
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.05051874281156
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 168
  episodes_total: 15492
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.33238303661346436
        entropy_coeff: 0.0005000000000000001
        kl: 0.005614192263844113
        model: {}
        policy_loss: -0.00885884469607845
        total_loss: 5.7727369864781695
        vf_explained_var: 0.9844608902931213
        vf_loss: 5.7817533413569135
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.16666666666667
    gpu_util_percent0: 0.3816666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14794301416787933
    mean_env_wait_ms: 1.1978551665942598
    mean_inference_ms: 4.396338475050271
    mean_raw_obs_processing_ms: 0.38256235543902334
  time_since_restore: 1922.296757221222
  time_this_iter_s: 25.423539400100708
  time_total_s: 1922.296757221222
  timers:
    learn_throughput: 8712.036
    learn_time_ms: 18571.09
    sample_throughput: 23766.431
    sample_time_ms: 6807.585
    update_time_ms: 37.603
  timestamp: 1602733676
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     75 |           1922.3 | 12134400 |  276.051 |              323.505 |              160.172 |            778.012 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3161.177204711875
    time_step_min: 2849
  date: 2020-10-15_03-48-22
  done: false
  episode_len_mean: 777.753857877691
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.3437730322851
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 255
  episodes_total: 15747
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3125377769271533
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045004094718024135
        model: {}
        policy_loss: -0.012710183019711016
        total_loss: 7.016487161318461
        vf_explained_var: 0.9863986372947693
        vf_loss: 7.029346426328023
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01724137931034
    gpu_util_percent0: 0.3693103448275863
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791301989543743
    mean_env_wait_ms: 1.1980571464447967
    mean_inference_ms: 4.394429125840351
    mean_raw_obs_processing_ms: 0.38246189308102424
  time_since_restore: 1947.6018371582031
  time_this_iter_s: 25.3050799369812
  time_total_s: 1947.6018371582031
  timers:
    learn_throughput: 8721.35
    learn_time_ms: 18551.256
    sample_throughput: 23804.091
    sample_time_ms: 6796.815
    update_time_ms: 37.188
  timestamp: 1602733702
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     76 |           1947.6 | 12296192 |  276.344 |              323.505 |              160.172 |            777.754 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3159.457458841272
    time_step_min: 2849
  date: 2020-10-15_03-48-47
  done: false
  episode_len_mean: 777.5397342692404
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.59342991205614
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 209
  episodes_total: 15956
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.30025260895490646
        entropy_coeff: 0.0005000000000000001
        kl: 0.004504078921551506
        model: {}
        policy_loss: -0.008543379177960256
        total_loss: 4.541686534881592
        vf_explained_var: 0.9885548949241638
        vf_loss: 4.5503761768341064
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.080000000000002
    gpu_util_percent0: 0.31566666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14788859950323513
    mean_env_wait_ms: 1.1982121583416618
    mean_inference_ms: 4.392927886921404
    mean_raw_obs_processing_ms: 0.3823802455798658
  time_since_restore: 1973.0399436950684
  time_this_iter_s: 25.438106536865234
  time_total_s: 1973.0399436950684
  timers:
    learn_throughput: 8719.237
    learn_time_ms: 18555.752
    sample_throughput: 23834.168
    sample_time_ms: 6788.238
    update_time_ms: 37.597
  timestamp: 1602733727
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     77 |          1973.04 | 12457984 |  276.593 |              323.505 |              160.172 |             777.54 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3158.1062480571964
    time_step_min: 2849
  date: 2020-10-15_03-49-14
  done: false
  episode_len_mean: 777.3722949091585
  episode_reward_max: 323.50505050505046
  episode_reward_mean: 276.8043734924742
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 16127
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.31814805666605633
        entropy_coeff: 0.0005000000000000001
        kl: 0.004912271319578092
        model: {}
        policy_loss: -0.011369439637443671
        total_loss: 4.840783596038818
        vf_explained_var: 0.9866332411766052
        vf_loss: 4.852310061454773
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.53103448275862
    gpu_util_percent0: 0.36862068965517236
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787122322170287
    mean_env_wait_ms: 1.1983416737603196
    mean_inference_ms: 4.391734437643483
    mean_raw_obs_processing_ms: 0.38231772177368
  time_since_restore: 1998.7327978610992
  time_this_iter_s: 25.692854166030884
  time_total_s: 1998.7327978610992
  timers:
    learn_throughput: 8714.32
    learn_time_ms: 18566.222
    sample_throughput: 23819.796
    sample_time_ms: 6792.333
    update_time_ms: 37.39
  timestamp: 1602733754
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     78 |          1998.73 | 12619776 |  276.804 |              323.505 |              160.172 |            777.372 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3156.116139685646
    time_step_min: 2849
  date: 2020-10-15_03-49-40
  done: false
  episode_len_mean: 777.1172451656195
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.0987037458092
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 266
  episodes_total: 16393
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.29372237126032513
        entropy_coeff: 0.0005000000000000001
        kl: 0.004794335303207238
        model: {}
        policy_loss: -0.010221628273332803
        total_loss: 7.2412718534469604
        vf_explained_var: 0.9855754375457764
        vf_loss: 7.251639485359192
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4
    gpu_util_percent0: 0.3396666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478411286363919
    mean_env_wait_ms: 1.1985292618509313
    mean_inference_ms: 4.389869090853413
    mean_raw_obs_processing_ms: 0.38221683451249244
  time_since_restore: 2024.5823857784271
  time_this_iter_s: 25.84958791732788
  time_total_s: 2024.5823857784271
  timers:
    learn_throughput: 8714.373
    learn_time_ms: 18566.109
    sample_throughput: 23727.871
    sample_time_ms: 6818.648
    update_time_ms: 39.064
  timestamp: 1602733780
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     79 |          2024.58 | 12781568 |  277.099 |              325.475 |              160.172 |            777.117 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3154.7213392155677
    time_step_min: 2849
  date: 2020-10-15_03-50-06
  done: false
  episode_len_mean: 776.934715775514
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.30936040737714
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 16589
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2889004598061244
        entropy_coeff: 0.0005000000000000001
        kl: 0.005035359490041931
        model: {}
        policy_loss: -0.0105992041232336
        total_loss: 4.523562908172607
        vf_explained_var: 0.9883689880371094
        vf_loss: 4.534306089083354
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.073333333333338
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478206919084441
    mean_env_wait_ms: 1.1986653172527337
    mean_inference_ms: 4.388561428445639
    mean_raw_obs_processing_ms: 0.3821470225398502
  time_since_restore: 2050.3031125068665
  time_this_iter_s: 25.72072672843933
  time_total_s: 2050.3031125068665
  timers:
    learn_throughput: 8698.933
    learn_time_ms: 18599.063
    sample_throughput: 23729.346
    sample_time_ms: 6818.224
    update_time_ms: 40.735
  timestamp: 1602733806
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     80 |           2050.3 | 12943360 |  277.309 |              325.475 |              160.172 |            776.935 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3153.5677033492825
    time_step_min: 2849
  date: 2020-10-15_03-50-32
  done: false
  episode_len_mean: 776.7821858966711
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.4740839971123
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 16762
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3076997523506482
        entropy_coeff: 0.0005000000000000001
        kl: 0.005693516267153124
        model: {}
        policy_loss: -0.00956670832723224
        total_loss: 5.133278687795003
        vf_explained_var: 0.9868350028991699
        vf_loss: 5.142998774846395
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.67666666666667
    gpu_util_percent0: 0.3516666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14780413783725732
    mean_env_wait_ms: 1.1987865214853013
    mean_inference_ms: 4.387430569461674
    mean_raw_obs_processing_ms: 0.38208659025285663
  time_since_restore: 2075.865389585495
  time_this_iter_s: 25.56227707862854
  time_total_s: 2075.865389585495
  timers:
    learn_throughput: 8707.649
    learn_time_ms: 18580.446
    sample_throughput: 23720.21
    sample_time_ms: 6820.85
    update_time_ms: 39.888
  timestamp: 1602733832
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     81 |          2075.87 | 13105152 |  277.474 |              325.475 |              160.172 |            776.782 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3151.7855125338356
    time_step_min: 2849
  date: 2020-10-15_03-50-58
  done: false
  episode_len_mean: 776.522070908664
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.74066207982617
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 274
  episodes_total: 17036
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2834418366352717
        entropy_coeff: 0.0005000000000000001
        kl: 0.005275121851203342
        model: {}
        policy_loss: -0.009265318689964866
        total_loss: 6.91647748152415
        vf_explained_var: 0.986660897731781
        vf_loss: 6.925883889198303
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.520689655172415
    gpu_util_percent0: 0.38482758620689655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147775380137946
    mean_env_wait_ms: 1.1989636109949018
    mean_inference_ms: 4.38562280881604
    mean_raw_obs_processing_ms: 0.38198868048847673
  time_since_restore: 2101.3718314170837
  time_this_iter_s: 25.506441831588745
  time_total_s: 2101.3718314170837
  timers:
    learn_throughput: 8710.938
    learn_time_ms: 18573.43
    sample_throughput: 23743.639
    sample_time_ms: 6814.12
    update_time_ms: 39.988
  timestamp: 1602733858
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     82 |          2101.37 | 13266944 |  277.741 |              325.475 |              160.172 |            776.522 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3150.613154831199
    time_step_min: 2849
  date: 2020-10-15_03-51-24
  done: false
  episode_len_mean: 776.3493787016606
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 277.91905760660836
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 17222
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2752121885617574
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554063236961762
        model: {}
        policy_loss: -0.010158129036426544
        total_loss: 4.648556550343831
        vf_explained_var: 0.9879704117774963
        vf_loss: 4.658851822217305
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.16666666666667
    gpu_util_percent0: 0.31133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477569776762327
    mean_env_wait_ms: 1.1990815877593484
    mean_inference_ms: 4.3844603227106616
    mean_raw_obs_processing_ms: 0.3819257301802225
  time_since_restore: 2127.0200068950653
  time_this_iter_s: 25.648175477981567
  time_total_s: 2127.0200068950653
  timers:
    learn_throughput: 8697.886
    learn_time_ms: 18601.302
    sample_throughput: 23755.392
    sample_time_ms: 6810.748
    update_time_ms: 32.669
  timestamp: 1602733884
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     83 |          2127.02 | 13428736 |  277.919 |              325.475 |              160.172 |            776.349 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3149.4643659618596
    time_step_min: 2849
  date: 2020-10-15_03-51-49
  done: false
  episode_len_mean: 776.1916776826255
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.0987662706727
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 177
  episodes_total: 17399
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.29051440705855686
        entropy_coeff: 0.0005000000000000001
        kl: 0.005475612551284333
        model: {}
        policy_loss: -0.00846125721000135
        total_loss: 5.494869629542033
        vf_explained_var: 0.9860510230064392
        vf_loss: 5.5034758647282915
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.76896551724138
    gpu_util_percent0: 0.3031034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14774102021242871
    mean_env_wait_ms: 1.1991945297202111
    mean_inference_ms: 4.383376734351931
    mean_raw_obs_processing_ms: 0.38186781395227426
  time_since_restore: 2152.301024198532
  time_this_iter_s: 25.281017303466797
  time_total_s: 2152.301024198532
  timers:
    learn_throughput: 8706.219
    learn_time_ms: 18583.498
    sample_throughput: 23775.672
    sample_time_ms: 6804.939
    update_time_ms: 31.821
  timestamp: 1602733909
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     84 |           2152.3 | 13590528 |  278.099 |              325.475 |              160.172 |            776.192 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3147.7114664851993
    time_step_min: 2849
  date: 2020-10-15_03-52-15
  done: false
  episode_len_mean: 775.9970015840688
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.3649358486426
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 277
  episodes_total: 17676
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.26342381288607913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004692555676835279
        model: {}
        policy_loss: -0.007998384205469241
        total_loss: 6.075648824373881
        vf_explained_var: 0.9882777333259583
        vf_loss: 6.083778540293376
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.776666666666674
    gpu_util_percent0: 0.26766666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.860000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771450193055888
    mean_env_wait_ms: 1.1993613037033655
    mean_inference_ms: 4.381671869157273
    mean_raw_obs_processing_ms: 0.38177538674579
  time_since_restore: 2178.03094124794
  time_this_iter_s: 25.72991704940796
  time_total_s: 2178.03094124794
  timers:
    learn_throughput: 8689.18
    learn_time_ms: 18619.938
    sample_throughput: 23802.583
    sample_time_ms: 6797.245
    update_time_ms: 32.603
  timestamp: 1602733935
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     85 |          2178.03 | 13752320 |  278.365 |              325.475 |              160.172 |            775.997 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3146.632382663373
    time_step_min: 2849
  date: 2020-10-15_03-52-42
  done: false
  episode_len_mean: 775.8654643217207
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.53880917384896
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 178
  episodes_total: 17854
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.2600918958584468
        entropy_coeff: 0.0005000000000000001
        kl: 0.004823713796213269
        model: {}
        policy_loss: -0.010017351460798333
        total_loss: 4.0900519490242
        vf_explained_var: 0.9890416264533997
        vf_loss: 4.100199202696483
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.856666666666673
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769738727709136
    mean_env_wait_ms: 1.1994628829496374
    mean_inference_ms: 4.3806168235244805
    mean_raw_obs_processing_ms: 0.3817181579945032
  time_since_restore: 2203.9854192733765
  time_this_iter_s: 25.9544780254364
  time_total_s: 2203.9854192733765
  timers:
    learn_throughput: 8671.456
    learn_time_ms: 18657.997
    sample_throughput: 23719.429
    sample_time_ms: 6821.075
    update_time_ms: 34.761
  timestamp: 1602733962
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     86 |          2203.99 | 13914112 |  278.539 |              325.475 |              160.172 |            775.865 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3145.5827406090243
    time_step_min: 2849
  date: 2020-10-15_03-53-08
  done: false
  episode_len_mean: 775.7283512584544
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.70278122168577
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 18038
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.27556434522072476
        entropy_coeff: 0.0005000000000000001
        kl: 0.005423782975412905
        model: {}
        policy_loss: -0.010063629000796936
        total_loss: 4.869331796964009
        vf_explained_var: 0.9880831837654114
        vf_loss: 4.879533012708028
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.826666666666668
    gpu_util_percent0: 0.36333333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14768174104962717
    mean_env_wait_ms: 1.1995689254836903
    mean_inference_ms: 4.379560258031098
    mean_raw_obs_processing_ms: 0.3816616748543843
  time_since_restore: 2229.8752658367157
  time_this_iter_s: 25.889846563339233
  time_total_s: 2229.8752658367157
  timers:
    learn_throughput: 8663.898
    learn_time_ms: 18674.273
    sample_throughput: 23625.268
    sample_time_ms: 6848.261
    update_time_ms: 36.39
  timestamp: 1602733988
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     87 |          2229.88 | 14075904 |  278.703 |              325.475 |              160.172 |            775.728 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3143.9228200777275
    time_step_min: 2849
  date: 2020-10-15_03-53-34
  done: false
  episode_len_mean: 775.5347059144776
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 278.94616637678183
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 273
  episodes_total: 18311
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2466991270581881
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045042647592102485
        model: {}
        policy_loss: -0.008488403292479537
        total_loss: 5.143421570460002
        vf_explained_var: 0.9899156093597412
        vf_loss: 5.152033408482869
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.409677419354836
    gpu_util_percent0: 0.3180645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.854838709677421
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14765691693681932
    mean_env_wait_ms: 1.1997190489843848
    mean_inference_ms: 4.3779683363474495
    mean_raw_obs_processing_ms: 0.3815758087613738
  time_since_restore: 2255.8066775798798
  time_this_iter_s: 25.931411743164062
  time_total_s: 2255.8066775798798
  timers:
    learn_throughput: 8664.117
    learn_time_ms: 18673.802
    sample_throughput: 23549.297
    sample_time_ms: 6870.354
    update_time_ms: 38.16
  timestamp: 1602734014
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     88 |          2255.81 | 14237696 |  278.946 |              325.475 |              160.172 |            775.535 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3142.8778464541315
    time_step_min: 2849
  date: 2020-10-15_03-54-00
  done: false
  episode_len_mean: 775.4229146381045
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.1155944383793
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 18486
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.25028316924969357
        entropy_coeff: 0.0005000000000000001
        kl: 0.004874534555710852
        model: {}
        policy_loss: -0.008682492460745076
        total_loss: 3.7969141801198325
        vf_explained_var: 0.9895575642585754
        vf_loss: 3.8057217399279275
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.334482758620688
    gpu_util_percent0: 0.37310344827586206
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476414464570092
    mean_env_wait_ms: 1.1998109798268894
    mean_inference_ms: 4.377014596999441
    mean_raw_obs_processing_ms: 0.38152239009909633
  time_since_restore: 2281.5304152965546
  time_this_iter_s: 25.723737716674805
  time_total_s: 2281.5304152965546
  timers:
    learn_throughput: 8672.23
    learn_time_ms: 18656.333
    sample_throughput: 23532.913
    sample_time_ms: 6875.137
    update_time_ms: 38.126
  timestamp: 1602734040
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     89 |          2281.53 | 14399488 |  279.116 |              325.475 |              160.172 |            775.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3141.683357484034
    time_step_min: 2849
  date: 2020-10-15_03-54-26
  done: false
  episode_len_mean: 775.3229451137885
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.29906291834004
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 189
  episodes_total: 18675
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.26849807302157086
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054943005088716745
        model: {}
        policy_loss: -0.009653241674338156
        total_loss: 4.333018501599629
        vf_explained_var: 0.9894297122955322
        vf_loss: 4.342805941899617
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89
    gpu_util_percent0: 0.3583333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762637425353695
    mean_env_wait_ms: 1.1999114823589432
    mean_inference_ms: 4.375993512180568
    mean_raw_obs_processing_ms: 0.3814679974777541
  time_since_restore: 2307.12881398201
  time_this_iter_s: 25.598398685455322
  time_total_s: 2307.12881398201
  timers:
    learn_throughput: 8685.864
    learn_time_ms: 18627.048
    sample_throughput: 23474.202
    sample_time_ms: 6892.332
    update_time_ms: 36.486
  timestamp: 1602734066
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     90 |          2307.13 | 14561280 |  279.299 |              325.475 |              160.172 |            775.323 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3139.9357278882776
    time_step_min: 2849
  date: 2020-10-15_03-54-52
  done: false
  episode_len_mean: 775.1970336746543
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.54248864662674
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 271
  episodes_total: 18946
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.23660230884949365
        entropy_coeff: 0.0005000000000000001
        kl: 0.004229308241823067
        model: {}
        policy_loss: -0.010361460653560547
        total_loss: 6.4848151206970215
        vf_explained_var: 0.9871156811714172
        vf_loss: 6.49529488881429
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.189655172413794
    gpu_util_percent0: 0.35517241379310344
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8586206896551736
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476031200087179
    mean_env_wait_ms: 1.2000431601691768
    mean_inference_ms: 4.374520711672533
    mean_raw_obs_processing_ms: 0.38138766965432674
  time_since_restore: 2332.212541818619
  time_this_iter_s: 25.083727836608887
  time_total_s: 2332.212541818619
  timers:
    learn_throughput: 8713.923
    learn_time_ms: 18567.068
    sample_throughput: 23409.555
    sample_time_ms: 6911.366
    update_time_ms: 29.203
  timestamp: 1602734092
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     91 |          2332.21 | 14723072 |  279.542 |              325.475 |              160.172 |            775.197 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3138.8942705876184
    time_step_min: 2849
  date: 2020-10-15_03-55-18
  done: false
  episode_len_mean: 775.1213452586433
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.7028039693974
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 19119
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.24192707613110542
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053346438411002355
        model: {}
        policy_loss: -0.007106361968908459
        total_loss: 4.053497612476349
        vf_explained_var: 0.9891877770423889
        vf_loss: 4.060725073019664
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.02
    gpu_util_percent0: 0.32699999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14758899449118362
    mean_env_wait_ms: 1.2001277119270997
    mean_inference_ms: 4.373633654109772
    mean_raw_obs_processing_ms: 0.3813382556485881
  time_since_restore: 2358.1291375160217
  time_this_iter_s: 25.916595697402954
  time_total_s: 2358.1291375160217
  timers:
    learn_throughput: 8703.442
    learn_time_ms: 18589.427
    sample_throughput: 23350.045
    sample_time_ms: 6928.98
    update_time_ms: 29.367
  timestamp: 1602734118
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     92 |          2358.13 | 14884864 |  279.703 |              325.475 |              160.172 |            775.121 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3137.7503503399594
    time_step_min: 2849
  date: 2020-10-15_03-55-44
  done: false
  episode_len_mean: 775.0277590760785
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 279.8723278148935
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 19309
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.25334326302011806
        entropy_coeff: 0.0005000000000000001
        kl: 0.005550891626626253
        model: {}
        policy_loss: -0.012007400044240057
        total_loss: 4.474414626757304
        vf_explained_var: 0.9896135926246643
        vf_loss: 4.486548860867818
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333336
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475742992456641
    mean_env_wait_ms: 1.2002177707169202
    mean_inference_ms: 4.372662273896408
    mean_raw_obs_processing_ms: 0.38128588108244404
  time_since_restore: 2383.7776041030884
  time_this_iter_s: 25.64846658706665
  time_total_s: 2383.7776041030884
  timers:
    learn_throughput: 8710.689
    learn_time_ms: 18573.961
    sample_throughput: 23301.096
    sample_time_ms: 6943.536
    update_time_ms: 29.528
  timestamp: 1602734144
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     93 |          2383.78 | 15046656 |  279.872 |              325.475 |              160.172 |            775.028 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3136.0240057327123
    time_step_min: 2839
  date: 2020-10-15_03-56-10
  done: false
  episode_len_mean: 774.8940191020992
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.14354846281907
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 270
  episodes_total: 19579
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2210515650610129
        entropy_coeff: 0.0005000000000000001
        kl: 0.004453400227551659
        model: {}
        policy_loss: -0.00836365584594508
        total_loss: 4.5613594849904375
        vf_explained_var: 0.9907524585723877
        vf_loss: 4.569833596547444
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4
    gpu_util_percent0: 0.3266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14755280654915498
    mean_env_wait_ms: 1.2003348470508388
    mean_inference_ms: 4.371281108113632
    mean_raw_obs_processing_ms: 0.38121022060745297
  time_since_restore: 2409.3142652511597
  time_this_iter_s: 25.53666114807129
  time_total_s: 2409.3142652511597
  timers:
    learn_throughput: 8698.748
    learn_time_ms: 18599.459
    sample_throughput: 23307.585
    sample_time_ms: 6941.603
    update_time_ms: 30.491
  timestamp: 1602734170
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     94 |          2409.31 | 15208448 |  280.144 |              325.475 |              160.172 |            774.894 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3134.9877720838194
    time_step_min: 2839
  date: 2020-10-15_03-56-37
  done: false
  episode_len_mean: 774.8060351374614
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.3083040418872
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 172
  episodes_total: 19751
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.2316630259156227
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040806956045950455
        model: {}
        policy_loss: -0.008456611190922558
        total_loss: 4.0203032692273455
        vf_explained_var: 0.9889214634895325
        vf_loss: 4.028875728448232
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39333333333333
    gpu_util_percent0: 0.33666666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753961821047093
    mean_env_wait_ms: 1.200410229830588
    mean_inference_ms: 4.37044690229048
    mean_raw_obs_processing_ms: 0.38116381914872705
  time_since_restore: 2435.2629051208496
  time_this_iter_s: 25.94863986968994
  time_total_s: 2435.2629051208496
  timers:
    learn_throughput: 8698.034
    learn_time_ms: 18600.985
    sample_throughput: 23272.106
    sample_time_ms: 6952.186
    update_time_ms: 31.184
  timestamp: 1602734197
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     95 |          2435.26 | 15370240 |  280.308 |              325.475 |              160.172 |            774.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3133.665226564855
    time_step_min: 2839
  date: 2020-10-15_03-57-03
  done: false
  episode_len_mean: 774.693503108081
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.4952325541357
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 19948
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.24208191906412443
        entropy_coeff: 0.0005000000000000001
        kl: 0.00469647313002497
        model: {}
        policy_loss: -0.008889193115464877
        total_loss: 4.746521472930908
        vf_explained_var: 0.988551139831543
        vf_loss: 4.755531708399455
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85666666666667
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752478253235415
    mean_env_wait_ms: 1.2004917730183546
    mean_inference_ms: 4.369482489034079
    mean_raw_obs_processing_ms: 0.38111069368295364
  time_since_restore: 2461.1342754364014
  time_this_iter_s: 25.871370315551758
  time_total_s: 2461.1342754364014
  timers:
    learn_throughput: 8698.626
    learn_time_ms: 18599.72
    sample_throughput: 23293.064
    sample_time_ms: 6945.93
    update_time_ms: 29.646
  timestamp: 1602734223
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     96 |          2461.13 | 15532032 |  280.495 |              325.475 |              160.172 |            774.694 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3131.9620265714852
    time_step_min: 2839
  date: 2020-10-15_03-57-29
  done: false
  episode_len_mean: 774.5555060848917
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.76618215398264
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 266
  episodes_total: 20214
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.21615375702579817
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043829032219946384
        model: {}
        policy_loss: -0.009730958918225951
        total_loss: 4.789487361907959
        vf_explained_var: 0.9898057579994202
        vf_loss: 4.799326459566752
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.163333333333338
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750519254654412
    mean_env_wait_ms: 1.2005986392066315
    mean_inference_ms: 4.3682225728482305
    mean_raw_obs_processing_ms: 0.38104335381678645
  time_since_restore: 2487.03395318985
  time_this_iter_s: 25.899677753448486
  time_total_s: 2487.03395318985
  timers:
    learn_throughput: 8699.941
    learn_time_ms: 18596.908
    sample_throughput: 23306.506
    sample_time_ms: 6941.924
    update_time_ms: 27.652
  timestamp: 1602734249
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     97 |          2487.03 | 15693824 |  280.766 |              325.475 |              160.172 |            774.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3130.9395309965093
    time_step_min: 2839
  date: 2020-10-15_03-57-56
  done: false
  episode_len_mean: 774.5000245302458
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 280.92261772907403
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 169
  episodes_total: 20383
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.22247247149546942
        entropy_coeff: 0.0005000000000000001
        kl: 0.004486261362520357
        model: {}
        policy_loss: -0.01045249072679629
        total_loss: 3.540562887986501
        vf_explained_var: 0.9903184771537781
        vf_loss: 3.5511265794436135
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.71
    gpu_util_percent0: 0.33966666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474929597440828
    mean_env_wait_ms: 1.2006649541031174
    mean_inference_ms: 4.367444572040504
    mean_raw_obs_processing_ms: 0.3809997565326725
  time_since_restore: 2512.9816195964813
  time_this_iter_s: 25.94766640663147
  time_total_s: 2512.9816195964813
  timers:
    learn_throughput: 8694.447
    learn_time_ms: 18608.659
    sample_throughput: 23348.328
    sample_time_ms: 6929.49
    update_time_ms: 27.518
  timestamp: 1602734276
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     98 |          2512.98 | 15855616 |  280.923 |              325.475 |              160.172 |              774.5 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3129.715092227576
    time_step_min: 2839
  date: 2020-10-15_03-58-21
  done: false
  episode_len_mean: 774.4038564281898
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.0948711948275
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 20589
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.23181903610626856
        entropy_coeff: 0.0005000000000000001
        kl: 0.004677652070919673
        model: {}
        policy_loss: -0.010011315243900754
        total_loss: 5.311827023824056
        vf_explained_var: 0.9880104064941406
        vf_loss: 5.321954131126404
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.334482758620688
    gpu_util_percent0: 0.3437931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474789418195766
    mean_env_wait_ms: 1.2007413715468234
    mean_inference_ms: 4.3664976617379025
    mean_raw_obs_processing_ms: 0.38094916254117783
  time_since_restore: 2538.441076040268
  time_this_iter_s: 25.45945644378662
  time_total_s: 2538.441076040268
  timers:
    learn_throughput: 8695.982
    learn_time_ms: 18605.375
    sample_throughput: 23425.418
    sample_time_ms: 6906.686
    update_time_ms: 26.046
  timestamp: 1602734301
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |     99 |          2538.44 | 16017408 |  281.095 |              325.475 |              160.172 |            774.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3128.1833301288216
    time_step_min: 2839
  date: 2020-10-15_03-58-47
  done: false
  episode_len_mean: 774.2745850522882
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.3305253436214
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 257
  episodes_total: 20846
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.2023782953619957
        entropy_coeff: 0.0005000000000000001
        kl: 0.00405684191112717
        model: {}
        policy_loss: -0.009388180328339027
        total_loss: 4.744685212771098
        vf_explained_var: 0.9900166392326355
        vf_loss: 4.754174590110779
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.88
    gpu_util_percent0: 0.4123333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14746065972837433
    mean_env_wait_ms: 1.2008325873757897
    mean_inference_ms: 4.365343311661565
    mean_raw_obs_processing_ms: 0.3808862150400868
  time_since_restore: 2564.105705976486
  time_this_iter_s: 25.66462993621826
  time_total_s: 2564.105705976486
  timers:
    learn_throughput: 8689.193
    learn_time_ms: 18619.911
    sample_throughput: 23459.379
    sample_time_ms: 6896.687
    update_time_ms: 27.468
  timestamp: 1602734327
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    100 |          2564.11 | 16179200 |  281.331 |              325.475 |              160.172 |            774.275 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3127.2248128546225
    time_step_min: 2839
  date: 2020-10-15_03-59-14
  done: false
  episode_len_mean: 774.1690221270521
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.4788498835608
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 169
  episodes_total: 21015
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.21184253816803297
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042800351123635965
        model: {}
        policy_loss: -0.010074328553552428
        total_loss: 3.7910202145576477
        vf_explained_var: 0.9894771575927734
        vf_loss: 3.8012004693349204
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666665
    gpu_util_percent0: 0.3976666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14744890541072272
    mean_env_wait_ms: 1.2008921052983133
    mean_inference_ms: 4.36460588748096
    mean_raw_obs_processing_ms: 0.38084554745207655
  time_since_restore: 2589.855261325836
  time_this_iter_s: 25.749555349349976
  time_total_s: 2589.855261325836
  timers:
    learn_throughput: 8650.228
    learn_time_ms: 18703.784
    sample_throughput: 23525.568
    sample_time_ms: 6877.284
    update_time_ms: 29.276
  timestamp: 1602734354
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    101 |          2589.86 | 16340992 |  281.479 |              325.475 |              160.172 |            774.169 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3125.8344186485465
    time_step_min: 2839
  date: 2020-10-15_03-59-40
  done: false
  episode_len_mean: 774.0408307431478
  episode_reward_max: 325.4747474747478
  episode_reward_mean: 281.6766720611027
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 219
  episodes_total: 21234
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.22323922937115034
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050403246423229575
        model: {}
        policy_loss: -0.009667658693312356
        total_loss: 4.959194461504619
        vf_explained_var: 0.9885984063148499
        vf_loss: 4.968973716100057
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09
    gpu_util_percent0: 0.35133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743497149205118
    mean_env_wait_ms: 1.2009655230841203
    mean_inference_ms: 4.363659973577514
    mean_raw_obs_processing_ms: 0.3807941017157931
  time_since_restore: 2615.4981729984283
  time_this_iter_s: 25.642911672592163
  time_total_s: 2615.4981729984283
  timers:
    learn_throughput: 8657.04
    learn_time_ms: 18689.066
    sample_throughput: 23578.682
    sample_time_ms: 6861.791
    update_time_ms: 30.063
  timestamp: 1602734380
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    102 |           2615.5 | 16502784 |  281.677 |              325.475 |              160.172 |            774.041 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3124.2488339552237
    time_step_min: 2831
  date: 2020-10-15_04-00-06
  done: false
  episode_len_mean: 773.8888371659995
  episode_reward_max: 326.2323232323229
  episode_reward_mean: 281.9130975521907
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 248
  episodes_total: 21482
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.1936775098244349
        entropy_coeff: 0.0005000000000000001
        kl: 0.004403773307179411
        model: {}
        policy_loss: -0.008988071019606044
        total_loss: 3.9723556439081826
        vf_explained_var: 0.9909936785697937
        vf_loss: 3.9814405838648477
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.730000000000008
    gpu_util_percent0: 0.3306666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741773082330828
    mean_env_wait_ms: 1.2010442494246978
    mean_inference_ms: 4.362603069437725
    mean_raw_obs_processing_ms: 0.3807373494209976
  time_since_restore: 2640.9548890590668
  time_this_iter_s: 25.456716060638428
  time_total_s: 2640.9548890590668
  timers:
    learn_throughput: 8660.651
    learn_time_ms: 18681.274
    sample_throughput: 23623.898
    sample_time_ms: 6848.658
    update_time_ms: 30.68
  timestamp: 1602734406
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    103 |          2640.95 | 16664576 |  281.913 |              326.232 |              160.172 |            773.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3123.276695209442
    time_step_min: 2831
  date: 2020-10-15_04-00-31
  done: false
  episode_len_mean: 773.7868988774426
  episode_reward_max: 326.2323232323229
  episode_reward_mean: 282.0585207178731
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 165
  episodes_total: 21647
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.20866921047369638
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041684470876740915
        model: {}
        policy_loss: -0.008656489400891587
        total_loss: 3.79058971007665
        vf_explained_var: 0.989311158657074
        vf_loss: 3.799350599447886
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.989655172413794
    gpu_util_percent0: 0.3589655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474071347944397
    mean_env_wait_ms: 1.2010964176121022
    mean_inference_ms: 4.361914389419006
    mean_raw_obs_processing_ms: 0.38069984458028433
  time_since_restore: 2666.2656672000885
  time_this_iter_s: 25.31077814102173
  time_total_s: 2666.2656672000885
  timers:
    learn_throughput: 8672.576
    learn_time_ms: 18655.588
    sample_throughput: 23611.833
    sample_time_ms: 6852.157
    update_time_ms: 29.263
  timestamp: 1602734431
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    104 |          2666.27 | 16826368 |  282.059 |              326.232 |              160.172 |            773.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3121.967527709078
    time_step_min: 2828
  date: 2020-10-15_04-00-57
  done: false
  episode_len_mean: 773.6560614371914
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.25733611485117
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 21876
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21525794391830763
        entropy_coeff: 0.0005000000000000001
        kl: 0.004697754746302962
        model: {}
        policy_loss: -0.008619171579198337
        total_loss: 4.619357625643413
        vf_explained_var: 0.9897849559783936
        vf_loss: 4.628084301948547
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84333333333333
    gpu_util_percent0: 0.328
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14739271619752461
    mean_env_wait_ms: 1.2011649488298584
    mean_inference_ms: 4.360973990075465
    mean_raw_obs_processing_ms: 0.38064881133024014
  time_since_restore: 2691.8085718154907
  time_this_iter_s: 25.54290461540222
  time_total_s: 2691.8085718154907
  timers:
    learn_throughput: 8685.417
    learn_time_ms: 18628.005
    sample_throughput: 23629.868
    sample_time_ms: 6846.928
    update_time_ms: 28.713
  timestamp: 1602734457
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    105 |          2691.81 | 16988160 |  282.257 |              326.687 |              160.172 |            773.656 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3120.7082861414397
    time_step_min: 2828
  date: 2020-10-15_04-01-24
  done: false
  episode_len_mean: 773.5319466425503
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.4568451871188
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 239
  episodes_total: 22115
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.18876768772800764
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037186899959730604
        model: {}
        policy_loss: -0.008131141478467422
        total_loss: 4.283570170402527
        vf_explained_var: 0.9903204441070557
        vf_loss: 4.291795651117961
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.2933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737752432111179
    mean_env_wait_ms: 1.2012338046354094
    mean_inference_ms: 4.360005399648566
    mean_raw_obs_processing_ms: 0.3805967819780292
  time_since_restore: 2717.8512711524963
  time_this_iter_s: 26.042699337005615
  time_total_s: 2717.8512711524963
  timers:
    learn_throughput: 8684.189
    learn_time_ms: 18630.64
    sample_throughput: 23587.169
    sample_time_ms: 6859.323
    update_time_ms: 29.899
  timestamp: 1602734484
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    106 |          2717.85 | 17149952 |  282.457 |              326.687 |              160.172 |            773.532 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3119.7741108763094
    time_step_min: 2828
  date: 2020-10-15_04-01-50
  done: false
  episode_len_mean: 773.4342323744559
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.6014672597718
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 168
  episodes_total: 22283
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.20253701011339822
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041220731412371
        model: {}
        policy_loss: -0.009113873481207216
        total_loss: 3.457788904507955
        vf_explained_var: 0.9901456236839294
        vf_loss: 3.4670040011405945
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48666666666667
    gpu_util_percent0: 0.37533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736764317363504
    mean_env_wait_ms: 1.2012823515130429
    mean_inference_ms: 4.359353070386949
    mean_raw_obs_processing_ms: 0.3805612269415839
  time_since_restore: 2743.759480237961
  time_this_iter_s: 25.908209085464478
  time_total_s: 2743.759480237961
  timers:
    learn_throughput: 8683.088
    learn_time_ms: 18633.003
    sample_throughput: 23566.745
    sample_time_ms: 6865.267
    update_time_ms: 29.886
  timestamp: 1602734510
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    107 |          2743.76 | 17311744 |  282.601 |              326.687 |              160.172 |            773.434 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3118.335631365921
    time_step_min: 2828
  date: 2020-10-15_04-02-16
  done: false
  episode_len_mean: 773.290166481687
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 282.8173028845615
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 242
  episodes_total: 22525
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2062460370361805
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043828646109128995
        model: {}
        policy_loss: -0.006959196558455005
        total_loss: 4.403973579406738
        vf_explained_var: 0.9902663230895996
        vf_loss: 4.411036094029744
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.566666666666666
    gpu_util_percent0: 0.327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735315923006986
    mean_env_wait_ms: 1.2013454902573701
    mean_inference_ms: 4.35839626419083
    mean_raw_obs_processing_ms: 0.380509263803259
  time_since_restore: 2769.593340396881
  time_this_iter_s: 25.833860158920288
  time_total_s: 2769.593340396881
  timers:
    learn_throughput: 8691.701
    learn_time_ms: 18614.539
    sample_throughput: 23543.276
    sample_time_ms: 6872.111
    update_time_ms: 30.639
  timestamp: 1602734536
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    108 |          2769.59 | 17473536 |  282.817 |              326.687 |              160.172 |             773.29 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3116.9465386647876
    time_step_min: 2828
  date: 2020-10-15_04-02-42
  done: false
  episode_len_mean: 773.1590769230769
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 283.0131224331224
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 225
  episodes_total: 22750
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.18024898320436478
        entropy_coeff: 0.0005000000000000001
        kl: 0.003887702633316318
        model: {}
        policy_loss: -0.010001509741414338
        total_loss: 3.7521414359410605
        vf_explained_var: 0.9911050200462341
        vf_loss: 3.762233078479767
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.59
    gpu_util_percent0: 0.296
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733915726412977
    mean_env_wait_ms: 1.2014027481520104
    mean_inference_ms: 4.3575398464482
    mean_raw_obs_processing_ms: 0.3804621616444683
  time_since_restore: 2795.1623928546906
  time_this_iter_s: 25.56905245780945
  time_total_s: 2795.1623928546906
  timers:
    learn_throughput: 8689.691
    learn_time_ms: 18618.845
    sample_throughput: 23524.911
    sample_time_ms: 6877.475
    update_time_ms: 30.422
  timestamp: 1602734562
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    109 |          2795.16 | 17635328 |  283.013 |              326.687 |              160.172 |            773.159 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3115.949989071038
    time_step_min: 2828
  date: 2020-10-15_04-03-09
  done: false
  episode_len_mean: 773.0685080944277
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 283.1586423205745
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 167
  episodes_total: 22917
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.2003182793656985
        entropy_coeff: 0.0005000000000000001
        kl: 0.004819937632419169
        model: {}
        policy_loss: -0.010830288670452623
        total_loss: 3.2065762281417847
        vf_explained_var: 0.9909068942070007
        vf_loss: 3.2175065875053406
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.251724137931042
    gpu_util_percent0: 0.3206896551724138
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147330083982694
    mean_env_wait_ms: 1.201446083741759
    mean_inference_ms: 4.356927090584736
    mean_raw_obs_processing_ms: 0.3804297825904849
  time_since_restore: 2820.7132375240326
  time_this_iter_s: 25.55084466934204
  time_total_s: 2820.7132375240326
  timers:
    learn_throughput: 8687.417
    learn_time_ms: 18623.718
    sample_throughput: 23580.821
    sample_time_ms: 6861.169
    update_time_ms: 28.836
  timestamp: 1602734589
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    110 |          2820.71 | 17797120 |  283.159 |              326.687 |              160.172 |            773.069 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3114.579964546673
    time_step_min: 2828
  date: 2020-10-15_04-03-35
  done: false
  episode_len_mean: 772.9293081869579
  episode_reward_max: 326.6868686868688
  episode_reward_mean: 283.3710533325138
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 254
  episodes_total: 23171
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.19308664898077646
        entropy_coeff: 0.0005000000000000001
        kl: 0.004499280165570478
        model: {}
        policy_loss: -0.006127382007737954
        total_loss: 4.901983181635539
        vf_explained_var: 0.9898405075073242
        vf_loss: 4.908207217852275
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78666666666667
    gpu_util_percent0: 0.30233333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731493022239261
    mean_env_wait_ms: 1.2015032163587243
    mean_inference_ms: 4.355961265242673
    mean_raw_obs_processing_ms: 0.3803767759487892
  time_since_restore: 2846.380576610565
  time_this_iter_s: 25.667339086532593
  time_total_s: 2846.380576610565
  timers:
    learn_throughput: 8694.143
    learn_time_ms: 18609.311
    sample_throughput: 23559.262
    sample_time_ms: 6867.448
    update_time_ms: 27.172
  timestamp: 1602734615
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    111 |          2846.38 | 17958912 |  283.371 |              326.687 |              160.172 |            772.929 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3113.332176520994
    time_step_min: 2809
  date: 2020-10-15_04-04-01
  done: false
  episode_len_mean: 772.8234539389274
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 283.55343659847125
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 211
  episodes_total: 23382
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.17463851099212965
        entropy_coeff: 0.0005000000000000001
        kl: 0.004277046498221655
        model: {}
        policy_loss: -0.007814210703751693
        total_loss: 2.9094037612279258
        vf_explained_var: 0.9926970601081848
        vf_loss: 2.917305290699005
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.440000000000005
    gpu_util_percent0: 0.335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730286980872206
    mean_env_wait_ms: 1.2015534310055453
    mean_inference_ms: 4.355206424714107
    mean_raw_obs_processing_ms: 0.3803353039032961
  time_since_restore: 2872.1937186717987
  time_this_iter_s: 25.81314206123352
  time_total_s: 2872.1937186717987
  timers:
    learn_throughput: 8692.881
    learn_time_ms: 18612.011
    sample_throughput: 23504.742
    sample_time_ms: 6883.377
    update_time_ms: 26.322
  timestamp: 1602734641
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    112 |          2872.19 | 18120704 |  283.553 |              329.566 |              160.172 |            772.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3112.4548509208457
    time_step_min: 2809
  date: 2020-10-15_04-04-27
  done: false
  episode_len_mean: 772.739693457309
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 283.68228457032427
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 23553
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.19276007761557898
        entropy_coeff: 0.0005000000000000001
        kl: 0.004376984162566562
        model: {}
        policy_loss: -0.01104148793577527
        total_loss: 3.180401782194773
        vf_explained_var: 0.9914665222167969
        vf_loss: 3.191539704799652
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.743333333333336
    gpu_util_percent0: 0.2996666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729413734602842
    mean_env_wait_ms: 1.2015926039579794
    mean_inference_ms: 4.354608675869298
    mean_raw_obs_processing_ms: 0.38030342744386497
  time_since_restore: 2897.8294565677643
  time_this_iter_s: 25.635737895965576
  time_total_s: 2897.8294565677643
  timers:
    learn_throughput: 8685.897
    learn_time_ms: 18626.976
    sample_throughput: 23496.23
    sample_time_ms: 6885.871
    update_time_ms: 26.144
  timestamp: 1602734667
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    113 |          2897.83 | 18282496 |  283.682 |              329.566 |              160.172 |             772.74 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3110.940731081479
    time_step_min: 2809
  date: 2020-10-15_04-04-53
  done: false
  episode_len_mean: 772.6221709006928
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 283.916644929242
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 262
  episodes_total: 23815
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.18339999889334044
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043749943530807895
        model: {}
        policy_loss: -0.006947610915328066
        total_loss: 4.638766368230184
        vf_explained_var: 0.9903349876403809
        vf_loss: 4.645805676778157
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57666666666667
    gpu_util_percent0: 0.341
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727898678375384
    mean_env_wait_ms: 1.2016471022624788
    mean_inference_ms: 4.353679946453307
    mean_raw_obs_processing_ms: 0.3802520046553198
  time_since_restore: 2923.6090185642242
  time_this_iter_s: 25.77956199645996
  time_total_s: 2923.6090185642242
  timers:
    learn_throughput: 8672.315
    learn_time_ms: 18656.149
    sample_throughput: 23442.364
    sample_time_ms: 6901.693
    update_time_ms: 26.092
  timestamp: 1602734693
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    114 |          2923.61 | 18444288 |  283.917 |              329.566 |              160.172 |            772.622 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3109.8491990655766
    time_step_min: 2809
  date: 2020-10-15_04-05-19
  done: false
  episode_len_mean: 772.5273173981844
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.0827185825104
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 199
  episodes_total: 24014
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.16781583055853844
        entropy_coeff: 0.0005000000000000001
        kl: 0.004889018988857667
        model: {}
        policy_loss: -0.009682772739324719
        total_loss: 3.5239917437235513
        vf_explained_var: 0.9909854531288147
        vf_loss: 3.5337583820025125
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58666666666667
    gpu_util_percent0: 0.3126666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726826810751212
    mean_env_wait_ms: 1.2016858226898077
    mean_inference_ms: 4.352984514961891
    mean_raw_obs_processing_ms: 0.3802145613524847
  time_since_restore: 2949.324979543686
  time_this_iter_s: 25.71596097946167
  time_total_s: 2949.324979543686
  timers:
    learn_throughput: 8665.302
    learn_time_ms: 18671.247
    sample_throughput: 23475.422
    sample_time_ms: 6891.974
    update_time_ms: 26.681
  timestamp: 1602734719
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    115 |          2949.32 | 18606080 |  284.083 |              329.566 |              160.172 |            772.527 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3108.83231871454
    time_step_min: 2809
  date: 2020-10-15_04-05-45
  done: false
  episode_len_mean: 772.4584728595643
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.24044613316596
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 24189
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.18804708371559778
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044516730510319276
        model: {}
        policy_loss: -0.01162540833077704
        total_loss: 3.059577524662018
        vf_explained_var: 0.9915698170661926
        vf_loss: 3.0712968508402505
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84827586206897
    gpu_util_percent0: 0.3475862068965518
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14725978836263276
    mean_env_wait_ms: 1.201719566322602
    mean_inference_ms: 4.352402171602601
    mean_raw_obs_processing_ms: 0.3801838126872616
  time_since_restore: 2974.8629446029663
  time_this_iter_s: 25.537965059280396
  time_total_s: 2974.8629446029663
  timers:
    learn_throughput: 8676.294
    learn_time_ms: 18647.592
    sample_throughput: 23566.851
    sample_time_ms: 6865.236
    update_time_ms: 25.719
  timestamp: 1602734745
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    116 |          2974.86 | 18767872 |   284.24 |              329.566 |              160.172 |            772.458 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3107.3032363785333
    time_step_min: 2809
  date: 2020-10-15_04-06-12
  done: false
  episode_len_mean: 772.3368231637494
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.46565854851474
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 263
  episodes_total: 24452
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.17432812228798866
        entropy_coeff: 0.0005000000000000001
        kl: 0.004144462815020233
        model: {}
        policy_loss: -0.008397253603713276
        total_loss: 4.37483274936676
        vf_explained_var: 0.9908452033996582
        vf_loss: 4.383317192395528
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63333333333333
    gpu_util_percent0: 0.3496666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472452329270902
    mean_env_wait_ms: 1.2017663675246097
    mean_inference_ms: 4.351507673599308
    mean_raw_obs_processing_ms: 0.38013390734952995
  time_since_restore: 3000.3928306102753
  time_this_iter_s: 25.52988600730896
  time_total_s: 3000.3928306102753
  timers:
    learn_throughput: 8682.454
    learn_time_ms: 18634.362
    sample_throughput: 23655.161
    sample_time_ms: 6839.607
    update_time_ms: 25.843
  timestamp: 1602734772
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    117 |          3000.39 | 18929664 |  284.466 |              329.566 |              160.172 |            772.337 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3106.191343222922
    time_step_min: 2809
  date: 2020-10-15_04-06-38
  done: false
  episode_len_mean: 772.2565423783827
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.6371509143449
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 195
  episodes_total: 24647
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.16233477865656218
        entropy_coeff: 0.0005000000000000001
        kl: 0.004291001319264372
        model: {}
        policy_loss: -0.008012217675665548
        total_loss: 3.577251593271891
        vf_explained_var: 0.9904074668884277
        vf_loss: 3.5853449503580728
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.849999999999998
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723518152124807
    mean_env_wait_ms: 1.2017991121849527
    mean_inference_ms: 4.350865623971163
    mean_raw_obs_processing_ms: 0.38009972982330464
  time_since_restore: 3025.9837934970856
  time_this_iter_s: 25.590962886810303
  time_total_s: 3025.9837934970856
  timers:
    learn_throughput: 8686.535
    learn_time_ms: 18625.609
    sample_throughput: 23734.567
    sample_time_ms: 6816.724
    update_time_ms: 23.719
  timestamp: 1602734798
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    118 |          3025.98 | 19091456 |  284.637 |              329.566 |              160.172 |            772.257 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3105.152325628303
    time_step_min: 2809
  date: 2020-10-15_04-07-04
  done: false
  episode_len_mean: 772.1902057911482
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 284.8015542644031
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 24831
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.17706493909160295
        entropy_coeff: 0.0005000000000000001
        kl: 0.004648963765551646
        model: {}
        policy_loss: -0.007830245094131291
        total_loss: 3.3011738061904907
        vf_explained_var: 0.9911198616027832
        vf_loss: 3.3090926011403403
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.910344827586208
    gpu_util_percent0: 0.3248275862068966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722623283783853
    mean_env_wait_ms: 1.2018302048157363
    mean_inference_ms: 4.350281854199701
    mean_raw_obs_processing_ms: 0.38006855701028625
  time_since_restore: 3051.549909353256
  time_this_iter_s: 25.566115856170654
  time_total_s: 3051.549909353256
  timers:
    learn_throughput: 8687.1
    learn_time_ms: 18624.398
    sample_throughput: 23732.544
    sample_time_ms: 6817.305
    update_time_ms: 23.738
  timestamp: 1602734824
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    119 |          3051.55 | 19253248 |  284.802 |              329.566 |              160.172 |             772.19 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3103.6541578506126
    time_step_min: 2809
  date: 2020-10-15_04-07-30
  done: false
  episode_len_mean: 772.1144633533936
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.0285111688404
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 260
  episodes_total: 25091
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.16731526081760725
        entropy_coeff: 0.0005000000000000001
        kl: 0.004477322955305378
        model: {}
        policy_loss: -0.009297993674408644
        total_loss: 3.5320072372754416
        vf_explained_var: 0.9924858212471008
        vf_loss: 3.541388968626658
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.49666666666667
    gpu_util_percent0: 0.3549999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721300233074744
    mean_env_wait_ms: 1.201866651996476
    mean_inference_ms: 4.349436268792504
    mean_raw_obs_processing_ms: 0.38002142689854956
  time_since_restore: 3077.3851408958435
  time_this_iter_s: 25.83523154258728
  time_total_s: 3077.3851408958435
  timers:
    learn_throughput: 8682.665
    learn_time_ms: 18633.91
    sample_throughput: 23672.342
    sample_time_ms: 6834.643
    update_time_ms: 25.573
  timestamp: 1602734850
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    120 |          3077.39 | 19415040 |  285.029 |              329.566 |              160.172 |            772.114 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3102.5843569221015
    time_step_min: 2809
  date: 2020-10-15_04-07-56
  done: false
  episode_len_mean: 772.0536392405063
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.1919451636619
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 189
  episodes_total: 25280
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.15328088526924452
        entropy_coeff: 0.0005000000000000001
        kl: 0.003679691484042754
        model: {}
        policy_loss: -0.010515831701923162
        total_loss: 2.6866751313209534
        vf_explained_var: 0.9926435947418213
        vf_loss: 2.697267552216848
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38666666666667
    gpu_util_percent0: 0.30299999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720385858507418
    mean_env_wait_ms: 1.2018940989779867
    mean_inference_ms: 4.348844229062376
    mean_raw_obs_processing_ms: 0.3799903067818294
  time_since_restore: 3102.8974509239197
  time_this_iter_s: 25.512310028076172
  time_total_s: 3102.8974509239197
  timers:
    learn_throughput: 8686.716
    learn_time_ms: 18625.22
    sample_throughput: 23699.195
    sample_time_ms: 6826.898
    update_time_ms: 25.646
  timestamp: 1602734876
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    121 |           3102.9 | 19576832 |  285.192 |              329.566 |              160.172 |            772.054 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3101.5473352999015
    time_step_min: 2809
  date: 2020-10-15_04-08-22
  done: false
  episode_len_mean: 771.9876310519496
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.3478869267537
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 25467
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.17071770379940668
        entropy_coeff: 0.0005000000000000001
        kl: 0.003886608600926896
        model: {}
        policy_loss: -0.00895706876569117
        total_loss: 3.2321362495422363
        vf_explained_var: 0.9915964603424072
        vf_loss: 3.2411786715189614
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.699999999999992
    gpu_util_percent0: 0.29333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719503562822758
    mean_env_wait_ms: 1.2019188706536652
    mean_inference_ms: 4.348268688594921
    mean_raw_obs_processing_ms: 0.37995986475119903
  time_since_restore: 3128.7914905548096
  time_this_iter_s: 25.894039630889893
  time_total_s: 3128.7914905548096
  timers:
    learn_throughput: 8676.587
    learn_time_ms: 18646.963
    sample_throughput: 23753.577
    sample_time_ms: 6811.269
    update_time_ms: 25.368
  timestamp: 1602734902
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    122 |          3128.79 | 19738624 |  285.348 |              329.566 |              160.172 |            771.988 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3100.0367947669665
    time_step_min: 2809
  date: 2020-10-15_04-08-48
  done: false
  episode_len_mean: 771.9261807580175
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.57558677150513
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 258
  episodes_total: 25725
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15733924259742102
        entropy_coeff: 0.0005000000000000001
        kl: 0.004441594860206048
        model: {}
        policy_loss: -0.008154638237707937
        total_loss: 3.6197384198506675
        vf_explained_var: 0.9923160672187805
        vf_loss: 3.6279717286427817
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.37586206896552
    gpu_util_percent0: 0.30241379310344835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718265064585104
    mean_env_wait_ms: 1.2019481815365167
    mean_inference_ms: 4.347469544491086
    mean_raw_obs_processing_ms: 0.3799147455888389
  time_since_restore: 3153.897025823593
  time_this_iter_s: 25.10553526878357
  time_total_s: 3153.897025823593
  timers:
    learn_throughput: 8698.726
    learn_time_ms: 18599.505
    sample_throughput: 23778.718
    sample_time_ms: 6804.067
    update_time_ms: 25.705
  timestamp: 1602734928
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    123 |           3153.9 | 19900416 |  285.576 |              329.566 |              160.172 |            771.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3098.8659012717926
    time_step_min: 2809
  date: 2020-10-15_04-09-14
  done: false
  episode_len_mean: 771.8986530817027
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.7483998255099
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 25911
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.1461001473168532
        entropy_coeff: 0.0005000000000000001
        kl: 0.004736314256054659
        model: {}
        policy_loss: -0.009588659126166021
        total_loss: 2.083389868338903
        vf_explained_var: 0.9941903948783875
        vf_loss: 2.093051532904307
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39333333333333
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717372479662857
    mean_env_wait_ms: 1.201969535719497
    mean_inference_ms: 4.34691458927581
    mean_raw_obs_processing_ms: 0.379884488808561
  time_since_restore: 3179.218817949295
  time_this_iter_s: 25.321792125701904
  time_total_s: 3179.218817949295
  timers:
    learn_throughput: 8715.362
    learn_time_ms: 18564.002
    sample_throughput: 23821.162
    sample_time_ms: 6791.944
    update_time_ms: 27.439
  timestamp: 1602734954
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    124 |          3179.22 | 20062208 |  285.748 |              329.566 |              160.172 |            771.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3097.7743235463445
    time_step_min: 2809
  date: 2020-10-15_04-09-40
  done: false
  episode_len_mean: 771.8722841705943
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 285.9107955053466
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 26097
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.1647727092107137
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038835582866643867
        model: {}
        policy_loss: -0.0076420325397824245
        total_loss: 2.931694527467092
        vf_explained_var: 0.99238520860672
        vf_loss: 2.939418931802114
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.330000000000002
    gpu_util_percent0: 0.35433333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716528988877567
    mean_env_wait_ms: 1.2019882724642543
    mean_inference_ms: 4.346365571385863
    mean_raw_obs_processing_ms: 0.3798550879998718
  time_since_restore: 3205.115535259247
  time_this_iter_s: 25.896717309951782
  time_total_s: 3205.115535259247
  timers:
    learn_throughput: 8710.417
    learn_time_ms: 18574.542
    sample_throughput: 23787.777
    sample_time_ms: 6801.476
    update_time_ms: 25.974
  timestamp: 1602734980
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    125 |          3205.12 | 20224000 |  285.911 |              329.566 |              160.172 |            771.872 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3096.1824157452793
    time_step_min: 2809
  date: 2020-10-15_04-10-06
  done: false
  episode_len_mean: 771.8131709722696
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.14718253434535
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 264
  episodes_total: 26361
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.1526032475133737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004989463758344452
        model: {}
        policy_loss: -0.007013317488599569
        total_loss: 3.4168810645739236
        vf_explained_var: 0.9925634264945984
        vf_loss: 3.423970659573873
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.84000000000001
    gpu_util_percent0: 0.3636666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715356413075717
    mean_env_wait_ms: 1.2020125889615627
    mean_inference_ms: 4.3455913328312255
    mean_raw_obs_processing_ms: 0.37981125365372
  time_since_restore: 3230.878668308258
  time_this_iter_s: 25.76313304901123
  time_total_s: 3230.878668308258
  timers:
    learn_throughput: 8702.845
    learn_time_ms: 18590.701
    sample_throughput: 23800.206
    sample_time_ms: 6797.924
    update_time_ms: 34.201
  timestamp: 1602735006
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    126 |          3230.88 | 20385792 |  286.147 |              329.566 |              160.172 |            771.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3095.1322214256065
    time_step_min: 2809
  date: 2020-10-15_04-10-33
  done: false
  episode_len_mean: 771.7946727950872
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.3014677536773
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 182
  episodes_total: 26543
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.14293036858240762
        entropy_coeff: 0.0005000000000000001
        kl: 0.003728000020297865
        model: {}
        policy_loss: -0.008170336106559262
        total_loss: 2.9872645139694214
        vf_explained_var: 0.9919014573097229
        vf_loss: 2.9955062667528787
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206666666666674
    gpu_util_percent0: 0.35899999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714496221024262
    mean_env_wait_ms: 1.2020271654599815
    mean_inference_ms: 4.3450700877315676
    mean_raw_obs_processing_ms: 0.3797822436178282
  time_since_restore: 3256.845372915268
  time_this_iter_s: 25.966704607009888
  time_total_s: 3256.845372915268
  timers:
    learn_throughput: 8689.425
    learn_time_ms: 18619.414
    sample_throughput: 23759.502
    sample_time_ms: 6809.57
    update_time_ms: 36.113
  timestamp: 1602735033
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    127 |          3256.85 | 20547584 |  286.301 |              329.566 |              160.172 |            771.795 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3094.0028848675584
    time_step_min: 2809
  date: 2020-10-15_04-10-59
  done: false
  episode_len_mean: 771.7503086073392
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.4713083024159
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 26733
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.15876951068639755
        entropy_coeff: 0.0005000000000000001
        kl: 0.004037241781285654
        model: {}
        policy_loss: -0.00954190470414081
        total_loss: 2.4400351444880166
        vf_explained_var: 0.9935087561607361
        vf_loss: 2.4496564467748008
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.26666666666667
    gpu_util_percent0: 0.3283333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713684769484786
    mean_env_wait_ms: 1.202042406232706
    mean_inference_ms: 4.344538823981054
    mean_raw_obs_processing_ms: 0.3797535301410589
  time_since_restore: 3282.4440557956696
  time_this_iter_s: 25.59868288040161
  time_total_s: 3282.4440557956696
  timers:
    learn_throughput: 8687.361
    learn_time_ms: 18623.837
    sample_throughput: 23751.715
    sample_time_ms: 6811.803
    update_time_ms: 37.864
  timestamp: 1602735059
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    128 |          3282.44 | 20709376 |  286.471 |              329.566 |              160.172 |             771.75 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3092.5000927678207
    time_step_min: 2809
  date: 2020-10-15_04-11-25
  done: false
  episode_len_mean: 771.7054573746805
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.6977252799193
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 258
  episodes_total: 26991
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.14370925227801004
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035917371860705316
        model: {}
        policy_loss: -0.008402867129613393
        total_loss: 3.3414038022359214
        vf_explained_var: 0.9927160739898682
        vf_loss: 3.349878489971161
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.82
    gpu_util_percent0: 0.31700000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712578001977014
    mean_env_wait_ms: 1.2020587506531604
    mean_inference_ms: 4.343814642890904
    mean_raw_obs_processing_ms: 0.37971210013748374
  time_since_restore: 3308.1741609573364
  time_this_iter_s: 25.73010516166687
  time_total_s: 3308.1741609573364
  timers:
    learn_throughput: 8682.526
    learn_time_ms: 18634.208
    sample_throughput: 23743.904
    sample_time_ms: 6814.044
    update_time_ms: 39.744
  timestamp: 1602735085
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    129 |          3308.17 | 20871168 |  286.698 |              329.566 |              160.172 |            771.705 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3091.4036044668856
    time_step_min: 2809
  date: 2020-10-15_04-11-51
  done: false
  episode_len_mean: 771.6939098436062
  episode_reward_max: 329.5656565656567
  episode_reward_mean: 286.8660942451191
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 27175
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.13174298902352652
        entropy_coeff: 0.0005000000000000001
        kl: 0.004347070857572059
        model: {}
        policy_loss: -0.008877743481813619
        total_loss: 2.0630916754404702
        vf_explained_var: 0.9942089915275574
        vf_loss: 2.072035312652588
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.396551724137936
    gpu_util_percent0: 0.3075862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711744849251673
    mean_env_wait_ms: 1.2020684547485712
    mean_inference_ms: 4.3433086532088225
    mean_raw_obs_processing_ms: 0.37968425124031646
  time_since_restore: 3333.4869916439056
  time_this_iter_s: 25.312830686569214
  time_total_s: 3333.4869916439056
  timers:
    learn_throughput: 8698.684
    learn_time_ms: 18599.595
    sample_throughput: 23803.973
    sample_time_ms: 6796.848
    update_time_ms: 37.964
  timestamp: 1602735111
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    130 |          3333.49 | 21032960 |  286.866 |              329.566 |              160.172 |            771.694 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3090.298817759233
    time_step_min: 2798
  date: 2020-10-15_04-12-17
  done: false
  episode_len_mean: 771.6669590322698
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.03212588554106
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 27363
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.1517644263803959
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039003227720968425
        model: {}
        policy_loss: -0.009901351989052879
        total_loss: 2.3748079538345337
        vf_explained_var: 0.9937583804130554
        vf_loss: 2.3847851554552713
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.25483870967742
    gpu_util_percent0: 0.3022580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710981623643773
    mean_env_wait_ms: 1.2020782076415049
    mean_inference_ms: 4.3428076547617716
    mean_raw_obs_processing_ms: 0.37965741500442973
  time_since_restore: 3359.3199837207794
  time_this_iter_s: 25.83299207687378
  time_total_s: 3359.3199837207794
  timers:
    learn_throughput: 8683.292
    learn_time_ms: 18632.565
    sample_throughput: 23806.786
    sample_time_ms: 6796.045
    update_time_ms: 37.698
  timestamp: 1602735137
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    131 |          3359.32 | 21194752 |  287.032 |              331.232 |              160.172 |            771.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3088.800899234925
    time_step_min: 2798
  date: 2020-10-15_04-12-44
  done: false
  episode_len_mean: 771.6471887332102
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.2579591944206
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 258
  episodes_total: 27621
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.13860419020056725
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040165691170841455
        model: {}
        policy_loss: -0.011186293248708049
        total_loss: 2.730085849761963
        vf_explained_var: 0.9940776824951172
        vf_loss: 2.741341511408488
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.41379310344828
    gpu_util_percent0: 0.29103448275862065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470991275262418
    mean_env_wait_ms: 1.2020873202013957
    mean_inference_ms: 4.3421057800390095
    mean_raw_obs_processing_ms: 0.3796171928785551
  time_since_restore: 3384.8633534908295
  time_this_iter_s: 25.54336977005005
  time_total_s: 3384.8633534908295
  timers:
    learn_throughput: 8697.787
    learn_time_ms: 18601.514
    sample_throughput: 23821.773
    sample_time_ms: 6791.77
    update_time_ms: 37.723
  timestamp: 1602735164
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    132 |          3384.86 | 21356544 |  287.258 |              331.232 |              160.172 |            771.647 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3087.696380334954
    time_step_min: 2798
  date: 2020-10-15_04-13-10
  done: false
  episode_len_mean: 771.638256554105
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.42040972896507
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 27807
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.12539237551391125
        entropy_coeff: 0.0005000000000000001
        kl: 0.003125751914922148
        model: {}
        policy_loss: -0.00843535780829067
        total_loss: 2.545394460360209
        vf_explained_var: 0.9930445551872253
        vf_loss: 2.553892433643341
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.163333333333338
    gpu_util_percent0: 0.2886666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709109262237358
    mean_env_wait_ms: 1.2020942154179948
    mean_inference_ms: 4.341623341398049
    mean_raw_obs_processing_ms: 0.37959041774133256
  time_since_restore: 3410.640928030014
  time_this_iter_s: 25.77757453918457
  time_total_s: 3410.640928030014
  timers:
    learn_throughput: 8667.577
    learn_time_ms: 18666.347
    sample_throughput: 23815.307
    sample_time_ms: 6793.614
    update_time_ms: 37.545
  timestamp: 1602735190
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    133 |          3410.64 | 21518336 |   287.42 |              331.232 |              160.172 |            771.638 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3086.6044148688775
    time_step_min: 2798
  date: 2020-10-15_04-13-36
  done: false
  episode_len_mean: 771.6435894687958
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.5825417393309
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 27993
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.143315768490235
        entropy_coeff: 0.0005000000000000001
        kl: 0.003707238104349623
        model: {}
        policy_loss: -0.009574485285459863
        total_loss: 2.5298949480056763
        vf_explained_var: 0.9933602213859558
        vf_loss: 2.539541184902191
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.246666666666663
    gpu_util_percent0: 0.37699999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14708396018894176
    mean_env_wait_ms: 1.2020998656796964
    mean_inference_ms: 4.341151392375738
    mean_raw_obs_processing_ms: 0.3795651163022009
  time_since_restore: 3436.418956041336
  time_this_iter_s: 25.77802801132202
  time_total_s: 3436.418956041336
  timers:
    learn_throughput: 8654.414
    learn_time_ms: 18694.739
    sample_throughput: 23757.02
    sample_time_ms: 6810.282
    update_time_ms: 35.763
  timestamp: 1602735216
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    134 |          3436.42 | 21680128 |  287.583 |              331.232 |              160.172 |            771.644 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3085.170932293513
    time_step_min: 2798
  date: 2020-10-15_04-14-03
  done: false
  episode_len_mean: 771.6557765821889
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.80680548941206
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 259
  episodes_total: 28252
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.13739688570300737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004209300541939835
        model: {}
        policy_loss: -0.00774512122249386
        total_loss: 3.0571309129397073
        vf_explained_var: 0.9934402108192444
        vf_loss: 3.064944644769033
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.046666666666674
    gpu_util_percent0: 0.325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707353306428422
    mean_env_wait_ms: 1.2021008038746845
    mean_inference_ms: 4.340475013989592
    mean_raw_obs_processing_ms: 0.37952591744984343
  time_since_restore: 3462.3177769184113
  time_this_iter_s: 25.898820877075195
  time_total_s: 3462.3177769184113
  timers:
    learn_throughput: 8655.05
    learn_time_ms: 18693.365
    sample_throughput: 23726.864
    sample_time_ms: 6818.937
    update_time_ms: 36.309
  timestamp: 1602735243
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    135 |          3462.32 | 21841920 |  287.807 |              331.232 |              160.172 |            771.656 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3084.0746205585097
    time_step_min: 2798
  date: 2020-10-15_04-14-29
  done: false
  episode_len_mean: 771.6721755335982
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 287.97063713544605
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 28439
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.12631800646583238
        entropy_coeff: 0.0005000000000000001
        kl: 0.005201198354673882
        model: {}
        policy_loss: -0.009248104861399042
        total_loss: 1.9554169476032257
        vf_explained_var: 0.994600772857666
        vf_loss: 1.964728186527888
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180000000000003
    gpu_util_percent0: 0.3306666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706571153088124
    mean_env_wait_ms: 1.2021021985583282
    mean_inference_ms: 4.340015275206162
    mean_raw_obs_processing_ms: 0.379499729002795
  time_since_restore: 3487.97540307045
  time_this_iter_s: 25.657626152038574
  time_total_s: 3487.97540307045
  timers:
    learn_throughput: 8661.395
    learn_time_ms: 18679.669
    sample_throughput: 23688.787
    sample_time_ms: 6829.898
    update_time_ms: 27.802
  timestamp: 1602735269
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    136 |          3487.98 | 22003712 |  287.971 |              331.232 |              160.172 |            771.672 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3083.043254593176
    time_step_min: 2798
  date: 2020-10-15_04-14-55
  done: false
  episode_len_mean: 771.7036726421358
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.1272765393742
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 178
  episodes_total: 28617
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.14297903329133987
        entropy_coeff: 0.0005000000000000001
        kl: 0.00392979724953572
        model: {}
        policy_loss: -0.00978231765596623
        total_loss: 2.057941049337387
        vf_explained_var: 0.9943475127220154
        vf_loss: 2.067794839541117
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.88666666666667
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14705918293258743
    mean_env_wait_ms: 1.2021027500368888
    mean_inference_ms: 4.339580886221096
    mean_raw_obs_processing_ms: 0.379475738466002
  time_since_restore: 3513.5832328796387
  time_this_iter_s: 25.607829809188843
  time_total_s: 3513.5832328796387
  timers:
    learn_throughput: 8675.517
    learn_time_ms: 18649.264
    sample_throughput: 23705.277
    sample_time_ms: 6825.147
    update_time_ms: 25.702
  timestamp: 1602735295
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    137 |          3513.58 | 22165504 |  288.127 |              331.232 |              160.172 |            771.704 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3081.633099764118
    time_step_min: 2798
  date: 2020-10-15_04-15-22
  done: false
  episode_len_mean: 771.7493591963977
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.3412493483501
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 253
  episodes_total: 28870
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.14113491649429002
        entropy_coeff: 0.0005000000000000001
        kl: 0.004532522909964125
        model: {}
        policy_loss: -0.010601744269176075
        total_loss: 2.8867114981015525
        vf_explained_var: 0.9938721656799316
        vf_loss: 2.897383769353231
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.83
    gpu_util_percent0: 0.2923333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704963151010986
    mean_env_wait_ms: 1.2020981260873884
    mean_inference_ms: 4.338969631913761
    mean_raw_obs_processing_ms: 0.3794391800661597
  time_since_restore: 3539.5742115974426
  time_this_iter_s: 25.990978717803955
  time_total_s: 3539.5742115974426
  timers:
    learn_throughput: 8663.559
    learn_time_ms: 18675.003
    sample_throughput: 23657.501
    sample_time_ms: 6838.93
    update_time_ms: 23.891
  timestamp: 1602735322
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    138 |          3539.57 | 22327296 |  288.341 |              331.232 |              160.172 |            771.749 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3080.5477347114556
    time_step_min: 2798
  date: 2020-10-15_04-15-48
  done: false
  episode_len_mean: 771.7780988750129
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.5081676502876
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 29067
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12842008223136267
        entropy_coeff: 0.0005000000000000001
        kl: 0.0077807044532770915
        model: {}
        policy_loss: -0.009057981437460208
        total_loss: 2.194779713948568
        vf_explained_var: 0.9943049550056458
        vf_loss: 2.2039019068082175
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36
    gpu_util_percent0: 0.3276666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470416843557686
    mean_env_wait_ms: 1.202092083908566
    mean_inference_ms: 4.33848137783018
    mean_raw_obs_processing_ms: 0.37941247439083087
  time_since_restore: 3565.18368601799
  time_this_iter_s: 25.609474420547485
  time_total_s: 3565.18368601799
  timers:
    learn_throughput: 8666.745
    learn_time_ms: 18668.139
    sample_throughput: 23669.831
    sample_time_ms: 6835.368
    update_time_ms: 21.755
  timestamp: 1602735348
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    139 |          3565.18 | 22489088 |  288.508 |              331.232 |              160.172 |            771.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3079.5207534246574
    time_step_min: 2798
  date: 2020-10-15_04-16-14
  done: false
  episode_len_mean: 771.8019971274194
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.66396990906253
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 29242
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.13905109713474909
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038545686402358115
        model: {}
        policy_loss: -0.008588418323294414
        total_loss: 2.069778402646383
        vf_explained_var: 0.9941542148590088
        vf_loss: 2.078436324993769
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.88
    gpu_util_percent0: 0.4026666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703536682036505
    mean_env_wait_ms: 1.2020880064668413
    mean_inference_ms: 4.3380844934466
    mean_raw_obs_processing_ms: 0.379390202723641
  time_since_restore: 3590.837648153305
  time_this_iter_s: 25.65396213531494
  time_total_s: 3590.837648153305
  timers:
    learn_throughput: 8659.418
    learn_time_ms: 18683.935
    sample_throughput: 23621.853
    sample_time_ms: 6849.251
    update_time_ms: 24.387
  timestamp: 1602735374
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    140 |          3590.84 | 22650880 |  288.664 |              331.232 |              160.172 |            771.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3078.193723678848
    time_step_min: 2798
  date: 2020-10-15_04-16-40
  done: false
  episode_len_mean: 771.8479956589568
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 288.86195845725797
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 244
  episodes_total: 29486
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.14169430236021677
        entropy_coeff: 0.0005000000000000001
        kl: 0.004087338127040614
        model: {}
        policy_loss: -0.010418552429958558
        total_loss: 3.404843032360077
        vf_explained_var: 0.9925181269645691
        vf_loss: 3.415332555770874
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.606666666666673
    gpu_util_percent0: 0.34500000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702650293567315
    mean_env_wait_ms: 1.2020776499725518
    mean_inference_ms: 4.3375163977061675
    mean_raw_obs_processing_ms: 0.3793566225080663
  time_since_restore: 3616.426659822464
  time_this_iter_s: 25.589011669158936
  time_total_s: 3616.426659822464
  timers:
    learn_throughput: 8676.685
    learn_time_ms: 18646.752
    sample_throughput: 23581.107
    sample_time_ms: 6861.086
    update_time_ms: 24.856
  timestamp: 1602735400
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    141 |          3616.43 | 22812672 |  288.862 |              331.232 |              160.172 |            771.848 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3077.0022928819503
    time_step_min: 2798
  date: 2020-10-15_04-17-06
  done: false
  episode_len_mean: 771.8828916798545
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 289.0347132730041
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 213
  episodes_total: 29699
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.12174105023344357
        entropy_coeff: 0.0005000000000000001
        kl: 0.003985141481583317
        model: {}
        policy_loss: -0.008610964735756474
        total_loss: 2.4176671902338662
        vf_explained_var: 0.9939587712287903
        vf_loss: 2.426339050134023
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.69666666666667
    gpu_util_percent0: 0.323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701822077609225
    mean_env_wait_ms: 1.2020659191762197
    mean_inference_ms: 4.337010214537786
    mean_raw_obs_processing_ms: 0.37932807436710064
  time_since_restore: 3641.8569645881653
  time_this_iter_s: 25.430304765701294
  time_total_s: 3641.8569645881653
  timers:
    learn_throughput: 8682.134
    learn_time_ms: 18635.05
    sample_throughput: 23585.207
    sample_time_ms: 6859.893
    update_time_ms: 25.155
  timestamp: 1602735426
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    142 |          3641.86 | 22974464 |  289.035 |              331.232 |              160.172 |            771.883 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3076.0884679852497
    time_step_min: 2798
  date: 2020-10-15_04-17-32
  done: false
  episode_len_mean: 771.907974022496
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 289.1771149497113
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 29872
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.12078638498981793
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035824499015385904
        model: {}
        policy_loss: -0.006959886949819823
        total_loss: 2.3264532883961997
        vf_explained_var: 0.9936222434043884
        vf_loss: 2.3334736426671348
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.137931034482758
    gpu_util_percent0: 0.35068965517241385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701233280098044
    mean_env_wait_ms: 1.202058149495795
    mean_inference_ms: 4.3366390019154135
    mean_raw_obs_processing_ms: 0.37930698529882506
  time_since_restore: 3667.174603700638
  time_this_iter_s: 25.317639112472534
  time_total_s: 3667.174603700638
  timers:
    learn_throughput: 8701.703
    learn_time_ms: 18593.142
    sample_throughput: 23600.236
    sample_time_ms: 6855.525
    update_time_ms: 24.712
  timestamp: 1602735452
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    143 |          3667.17 | 23136256 |  289.177 |              331.232 |              160.172 |            771.908 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3074.767560196887
    time_step_min: 2798
  date: 2020-10-15_04-17-58
  done: false
  episode_len_mean: 771.9446031218864
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 289.3754600136201
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 238
  episodes_total: 30110
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.13015408689777055
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052717971460272866
        model: {}
        policy_loss: -0.008923860349265548
        total_loss: 2.7079462011655173
        vf_explained_var: 0.993965208530426
        vf_loss: 2.716935157775879
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23
    gpu_util_percent0: 0.3243333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700350027488002
    mean_env_wait_ms: 1.2020390628352924
    mean_inference_ms: 4.336101514265683
    mean_raw_obs_processing_ms: 0.3792753466372956
  time_since_restore: 3692.7784893512726
  time_this_iter_s: 25.603885650634766
  time_total_s: 3692.7784893512726
  timers:
    learn_throughput: 8703.645
    learn_time_ms: 18588.994
    sample_throughput: 23643.763
    sample_time_ms: 6842.904
    update_time_ms: 24.902
  timestamp: 1602735478
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    144 |          3692.78 | 23298048 |  289.375 |              331.232 |              160.172 |            771.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3073.5813193343897
    time_step_min: 2798
  date: 2020-10-15_04-18-24
  done: false
  episode_len_mean: 771.9800857237059
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 289.5657764589515
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 220
  episodes_total: 30330
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.10993173470099767
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036828691178622344
        model: {}
        policy_loss: -0.007682798071376358
        total_loss: 1.8723307351271312
        vf_explained_var: 0.9953811764717102
        vf_loss: 1.8800684611002605
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92
    gpu_util_percent0: 0.34033333333333327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699565098116307
    mean_env_wait_ms: 1.2020233459391196
    mean_inference_ms: 4.335603759132035
    mean_raw_obs_processing_ms: 0.3792468634609439
  time_since_restore: 3718.4374318122864
  time_this_iter_s: 25.658942461013794
  time_total_s: 3718.4374318122864
  timers:
    learn_throughput: 8709.941
    learn_time_ms: 18575.557
    sample_throughput: 23681.926
    sample_time_ms: 6831.877
    update_time_ms: 25.616
  timestamp: 1602735504
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    145 |          3718.44 | 23459840 |  289.566 |              331.232 |              160.172 |             771.98 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3072.583308710069
    time_step_min: 2798
  date: 2020-10-15_04-18-51
  done: false
  episode_len_mean: 772.0048522999246
  episode_reward_max: 331.23232323232304
  episode_reward_mean: 289.7194865940809
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 30501
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.11126973666250706
        entropy_coeff: 0.0005000000000000001
        kl: 0.004640771386524041
        model: {}
        policy_loss: -0.008805120460844288
        total_loss: 1.761100749174754
        vf_explained_var: 0.994813859462738
        vf_loss: 1.7699615061283112
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.896666666666665
    gpu_util_percent0: 0.32533333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699002374530626
    mean_env_wait_ms: 1.2020112033682502
    mean_inference_ms: 4.33524945668496
    mean_raw_obs_processing_ms: 0.37922648245710533
  time_since_restore: 3743.9975821971893
  time_this_iter_s: 25.560150384902954
  time_total_s: 3743.9975821971893
  timers:
    learn_throughput: 8709.914
    learn_time_ms: 18575.614
    sample_throughput: 23745.787
    sample_time_ms: 6813.503
    update_time_ms: 25.463
  timestamp: 1602735531
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    146 |             3744 | 23621632 |  289.719 |              331.232 |              160.172 |            772.005 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3071.2489979470133
    time_step_min: 2798
  date: 2020-10-15_04-19-17
  done: false
  episode_len_mean: 772.0301994858277
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 289.91424150713425
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 228
  episodes_total: 30729
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.12445454920331638
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036275664848896363
        model: {}
        policy_loss: -0.00708467010796691
        total_loss: 2.5997930765151978
        vf_explained_var: 0.9938389658927917
        vf_loss: 2.606940050919851
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.61
    gpu_util_percent0: 0.36899999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698226693495267
    mean_env_wait_ms: 1.2019893407865876
    mean_inference_ms: 4.334755891102262
    mean_raw_obs_processing_ms: 0.3791979489045361
  time_since_restore: 3769.7016739845276
  time_this_iter_s: 25.704091787338257
  time_total_s: 3769.7016739845276
  timers:
    learn_throughput: 8703.861
    learn_time_ms: 18588.533
    sample_throughput: 23786.701
    sample_time_ms: 6801.784
    update_time_ms: 32.387
  timestamp: 1602735557
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    147 |           3769.7 | 23783424 |  289.914 |              333.202 |              160.172 |             772.03 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3069.9675572519086
    time_step_min: 2798
  date: 2020-10-15_04-19-43
  done: false
  episode_len_mean: 772.0565928031526
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 290.1046615127305
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 30958
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.11470815973977248
        entropy_coeff: 0.0005000000000000001
        kl: 0.003626958253638198
        model: {}
        policy_loss: -0.008745116627930353
        total_loss: 2.437072495619456
        vf_explained_var: 0.9941577315330505
        vf_loss: 2.4458749691645303
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.072413793103454
    gpu_util_percent0: 0.34413793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697373611830605
    mean_env_wait_ms: 1.2019672605165224
    mean_inference_ms: 4.3342539871331685
    mean_raw_obs_processing_ms: 0.3791679154310621
  time_since_restore: 3795.166284561157
  time_this_iter_s: 25.46461057662964
  time_total_s: 3795.166284561157
  timers:
    learn_throughput: 8718.616
    learn_time_ms: 18557.075
    sample_throughput: 23862.306
    sample_time_ms: 6780.233
    update_time_ms: 32.559
  timestamp: 1602735583
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    148 |          3795.17 | 23945216 |  290.105 |              333.202 |              160.172 |            772.057 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3068.990735677293
    time_step_min: 2798
  date: 2020-10-15_04-20-09
  done: false
  episode_len_mean: 772.0688746827717
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 290.25418923080264
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 31129
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.10691268183290958
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008838230879822126
        total_loss: .inf
        vf_explained_var: 0.9958763122558594
        vf_loss: 1.358661373456319
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.290000000000003
    gpu_util_percent0: 0.29766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696838173203616
    mean_env_wait_ms: 1.2019512543124409
    mean_inference_ms: 4.333907347156311
    mean_raw_obs_processing_ms: 0.3791486551700799
  time_since_restore: 3820.7119040489197
  time_this_iter_s: 25.54561948776245
  time_total_s: 3820.7119040489197
  timers:
    learn_throughput: 8720.331
    learn_time_ms: 18553.425
    sample_throughput: 23877.79
    sample_time_ms: 6775.836
    update_time_ms: 32.663
  timestamp: 1602735609
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    149 |          3820.71 | 24107008 |  290.254 |              333.202 |              160.172 |            772.069 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3067.7360889286397
    time_step_min: 2798
  date: 2020-10-15_04-20-35
  done: false
  episode_len_mean: 772.0865764961083
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 290.4409528486343
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 219
  episodes_total: 31348
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0164395367051604e-21
        cur_lr: 5.0e-05
        entropy: 0.11582487697402637
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040368668851442635
        model: {}
        policy_loss: -0.0064011469900530455
        total_loss: 2.3957329988479614
        vf_explained_var: 0.9940428137779236
        vf_loss: 2.4021921157836914
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.516666666666666
    gpu_util_percent0: 0.30300000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696142720818078
    mean_env_wait_ms: 1.2019261051620542
    mean_inference_ms: 4.33346835495555
    mean_raw_obs_processing_ms: 0.3791230529353508
  time_since_restore: 3846.495891571045
  time_this_iter_s: 25.783987522125244
  time_total_s: 3846.495891571045
  timers:
    learn_throughput: 8711.83
    learn_time_ms: 18571.528
    sample_throughput: 23887.063
    sample_time_ms: 6773.206
    update_time_ms: 30.429
  timestamp: 1602735635
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    150 |           3846.5 | 24268800 |  290.441 |              333.202 |              160.172 |            772.087 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3066.3633452747044
    time_step_min: 2798
  date: 2020-10-15_04-21-02
  done: false
  episode_len_mean: 772.1183789773627
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 290.647398474215
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 237
  episodes_total: 31585
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.082197683525802e-22
        cur_lr: 5.0e-05
        entropy: 0.10701155476272106
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037498693563975394
        model: {}
        policy_loss: -0.008422740550789362
        total_loss: 2.2420021096865335
        vf_explained_var: 0.9945583343505859
        vf_loss: 2.2504783272743225
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55333333333334
    gpu_util_percent0: 0.32899999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469533072133453
    mean_env_wait_ms: 1.2018998480269956
    mean_inference_ms: 4.332962021109189
    mean_raw_obs_processing_ms: 0.37909314497656205
  time_since_restore: 3872.139844417572
  time_this_iter_s: 25.6439528465271
  time_total_s: 3872.139844417572
  timers:
    learn_throughput: 8706.558
    learn_time_ms: 18582.774
    sample_throughput: 23913.208
    sample_time_ms: 6765.801
    update_time_ms: 30.224
  timestamp: 1602735662
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    151 |          3872.14 | 24430592 |  290.647 |              333.202 |              160.172 |            772.118 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3065.2804312872413
    time_step_min: 2798
  date: 2020-10-15_04-21-28
  done: false
  episode_len_mean: 772.136141809137
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 290.80627661330414
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 176
  episodes_total: 31761
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.541098841762901e-22
        cur_lr: 5.0e-05
        entropy: 0.09626413881778717
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031298703591649732
        model: {}
        policy_loss: -0.007317855895962566
        total_loss: 1.436600685119629
        vf_explained_var: 0.9956057667732239
        vf_loss: 1.443966656923294
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.436666666666664
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469476395114536
    mean_env_wait_ms: 1.2018791388285242
    mean_inference_ms: 4.332614746616426
    mean_raw_obs_processing_ms: 0.3790737549089129
  time_since_restore: 3897.6004848480225
  time_this_iter_s: 25.46064043045044
  time_total_s: 3897.6004848480225
  timers:
    learn_throughput: 8706.568
    learn_time_ms: 18582.752
    sample_throughput: 23902.468
    sample_time_ms: 6768.841
    update_time_ms: 29.988
  timestamp: 1602735688
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    152 |           3897.6 | 24592384 |  290.806 |              333.202 |              160.172 |            772.136 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3064.0255599060297
    time_step_min: 2788
  date: 2020-10-15_04-21-54
  done: false
  episode_len_mean: 772.1526261457128
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 290.9956492380242
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 31967
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2705494208814505e-22
        cur_lr: 5.0e-05
        entropy: 0.10917978174984455
        entropy_coeff: 0.0005000000000000001
        kl: 0.005010232833834986
        model: {}
        policy_loss: -0.008019551925826818
        total_loss: 1.6553577582041423
        vf_explained_var: 0.9955945014953613
        vf_loss: 1.663431892792384
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.436666666666667
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469414542194872
    mean_env_wait_ms: 1.2018527046831604
    mean_inference_ms: 4.332217824147232
    mean_raw_obs_processing_ms: 0.3790506081253644
  time_since_restore: 3923.1218554973602
  time_this_iter_s: 25.52137064933777
  time_total_s: 3923.1218554973602
  timers:
    learn_throughput: 8695.139
    learn_time_ms: 18607.178
    sample_throughput: 23922.12
    sample_time_ms: 6763.28
    update_time_ms: 30.091
  timestamp: 1602735714
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    153 |          3923.12 | 24754176 |  290.996 |              333.202 |              160.172 |            772.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3062.546257149963
    time_step_min: 2788
  date: 2020-10-15_04-22-20
  done: false
  episode_len_mean: 772.1701955914313
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 291.217783234393
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 243
  episodes_total: 32210
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2705494208814505e-22
        cur_lr: 5.0e-05
        entropy: 0.10672206245362759
        entropy_coeff: 0.0005000000000000001
        kl: 0.004046217965272565
        model: {}
        policy_loss: -0.008104845832955712
        total_loss: 2.194529930750529
        vf_explained_var: 0.9945572018623352
        vf_loss: 2.2026881178220115
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.78333333333334
    gpu_util_percent0: 0.37866666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693349354435387
    mean_env_wait_ms: 1.201819302455364
    mean_inference_ms: 4.331717841817708
    mean_raw_obs_processing_ms: 0.3790213437048282
  time_since_restore: 3948.9493849277496
  time_this_iter_s: 25.827529430389404
  time_total_s: 3948.9493849277496
  timers:
    learn_throughput: 8689.875
    learn_time_ms: 18618.45
    sample_throughput: 23885.023
    sample_time_ms: 6773.785
    update_time_ms: 29.561
  timestamp: 1602735740
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    154 |          3948.95 | 24915968 |  291.218 |              333.202 |              160.172 |             772.17 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3061.46114374034
    time_step_min: 2788
  date: 2020-10-15_04-22-47
  done: false
  episode_len_mean: 772.1862188194616
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 291.38583101950593
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 182
  episodes_total: 32392
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.352747104407252e-23
        cur_lr: 5.0e-05
        entropy: 0.09805548874040444
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033431422586242356
        model: {}
        policy_loss: -0.006408829930781697
        total_loss: 1.6036224861939747
        vf_explained_var: 0.9950172901153564
        vf_loss: 1.6100803414980571
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.879310344827587
    gpu_util_percent0: 0.35862068965517235
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692767593627973
    mean_env_wait_ms: 1.2017949020469265
    mean_inference_ms: 4.331370943659212
    mean_raw_obs_processing_ms: 0.37900166748408204
  time_since_restore: 3974.428224802017
  time_this_iter_s: 25.478839874267578
  time_total_s: 3974.428224802017
  timers:
    learn_throughput: 8697.207
    learn_time_ms: 18602.754
    sample_throughput: 23894.522
    sample_time_ms: 6771.092
    update_time_ms: 28.292
  timestamp: 1602735767
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    155 |          3974.43 | 25077760 |  291.386 |              333.202 |              160.172 |            772.186 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3060.2619676765194
    time_step_min: 2788
  date: 2020-10-15_04-23-13
  done: false
  episode_len_mean: 772.1936295568921
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 291.5697034168864
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 32588
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.176373552203626e-23
        cur_lr: 5.0e-05
        entropy: 0.10333130694925785
        entropy_coeff: 0.0005000000000000001
        kl: 0.00324533445139726
        model: {}
        policy_loss: -0.008561371937200116
        total_loss: 1.538722683986028
        vf_explained_var: 0.9956181645393372
        vf_loss: 1.5473356942335765
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.253333333333334
    gpu_util_percent0: 0.3156666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692237839534872
    mean_env_wait_ms: 1.2017672404209927
    mean_inference_ms: 4.331012488163938
    mean_raw_obs_processing_ms: 0.37898044172047035
  time_since_restore: 4000.198248386383
  time_this_iter_s: 25.770023584365845
  time_total_s: 4000.198248386383
  timers:
    learn_throughput: 8690.585
    learn_time_ms: 18616.929
    sample_throughput: 23848.32
    sample_time_ms: 6784.209
    update_time_ms: 28.447
  timestamp: 1602735793
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    156 |           4000.2 | 25239552 |   291.57 |              333.202 |              160.172 |            772.194 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3058.6828784875743
    time_step_min: 2788
  date: 2020-10-15_04-23-39
  done: false
  episode_len_mean: 772.2114078630813
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 291.8053741421893
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 249
  episodes_total: 32837
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.588186776101813e-23
        cur_lr: 5.0e-05
        entropy: 0.09915277113517125
        entropy_coeff: 0.0005000000000000001
        kl: 0.003716728688838581
        model: {}
        policy_loss: -0.008861705699625114
        total_loss: 1.5282563666502635
        vf_explained_var: 0.9961475729942322
        vf_loss: 1.5371676484743755
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040000000000003
    gpu_util_percent0: 0.34766666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469141968881943
    mean_env_wait_ms: 1.2017297546188586
    mean_inference_ms: 4.33051983919293
    mean_raw_obs_processing_ms: 0.37895130878580974
  time_since_restore: 4025.9851989746094
  time_this_iter_s: 25.78695058822632
  time_total_s: 4025.9851989746094
  timers:
    learn_throughput: 8686.613
    learn_time_ms: 18625.441
    sample_throughput: 23836.797
    sample_time_ms: 6787.489
    update_time_ms: 23.063
  timestamp: 1602735819
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    157 |          4025.99 | 25401344 |  291.805 |              333.202 |              160.172 |            772.211 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3057.527167980594
    time_step_min: 2787
  date: 2020-10-15_04-24-06
  done: false
  episode_len_mean: 772.231118648174
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 291.97811621147576
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 185
  episodes_total: 33022
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.940933880509065e-24
        cur_lr: 5.0e-05
        entropy: 0.0848009493201971
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032612585346214473
        model: {}
        policy_loss: -0.007904797013604062
        total_loss: 1.198360155026118
        vf_explained_var: 0.9962959885597229
        vf_loss: 1.2063073416550953
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.356666666666666
    gpu_util_percent0: 0.3393333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690851428007334
    mean_env_wait_ms: 1.2017000846614068
    mean_inference_ms: 4.330176042686375
    mean_raw_obs_processing_ms: 0.37893162587997126
  time_since_restore: 4051.656966686249
  time_this_iter_s: 25.671767711639404
  time_total_s: 4051.656966686249
  timers:
    learn_throughput: 8676.158
    learn_time_ms: 18647.885
    sample_throughput: 23848.254
    sample_time_ms: 6784.228
    update_time_ms: 23.102
  timestamp: 1602735846
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    158 |          4051.66 | 25563136 |  291.978 |              333.202 |              160.172 |            772.231 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3056.3690002411963
    time_step_min: 2787
  date: 2020-10-15_04-24-32
  done: false
  episode_len_mean: 772.2308943089431
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 292.15447154471536
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 33210
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.970466940254533e-24
        cur_lr: 5.0e-05
        entropy: 0.09625430839757125
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035594564978964627
        model: {}
        policy_loss: -0.009086302714422345
        total_loss: 1.712380051612854
        vf_explained_var: 0.9949207305908203
        vf_loss: 1.7215144832928975
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.37333333333333
    gpu_util_percent0: 0.32266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469036537685584
    mean_env_wait_ms: 1.201670926051852
    mean_inference_ms: 4.32984416593687
    mean_raw_obs_processing_ms: 0.3789126171601082
  time_since_restore: 4077.4826097488403
  time_this_iter_s: 25.825643062591553
  time_total_s: 4077.4826097488403
  timers:
    learn_throughput: 8664.5
    learn_time_ms: 18672.977
    sample_throughput: 23839.194
    sample_time_ms: 6786.807
    update_time_ms: 23.027
  timestamp: 1602735872
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    159 |          4077.48 | 25724928 |  292.154 |              333.202 |              160.172 |            772.231 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3054.7227275447312
    time_step_min: 2787
  date: 2020-10-15_04-24-58
  done: false
  episode_len_mean: 772.2368216590963
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 292.4002087574284
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 254
  episodes_total: 33464
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9852334701272663e-24
        cur_lr: 5.0e-05
        entropy: 0.09642826331158479
        entropy_coeff: 0.0005000000000000001
        kl: 0.006619200964147846
        model: {}
        policy_loss: -0.007994244420842733
        total_loss: 1.3013978203137715
        vf_explained_var: 0.9967036247253418
        vf_loss: 1.3094402352968852
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.766666666666666
    gpu_util_percent0: 0.295
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689576605968058
    mean_env_wait_ms: 1.2016268961004415
    mean_inference_ms: 4.329370307168621
    mean_raw_obs_processing_ms: 0.37888377921388916
  time_since_restore: 4102.884414434433
  time_this_iter_s: 25.40180468559265
  time_total_s: 4102.884414434433
  timers:
    learn_throughput: 8678.983
    learn_time_ms: 18641.816
    sample_throughput: 23866.466
    sample_time_ms: 6779.051
    update_time_ms: 22.682
  timestamp: 1602735898
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    160 |          4102.88 | 25886720 |    292.4 |              333.202 |              160.172 |            772.237 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3053.583142423611
    time_step_min: 2786
  date: 2020-10-15_04-25-24
  done: false
  episode_len_mean: 772.2349270495944
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 292.56318301428695
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 189
  episodes_total: 33653
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9852334701272663e-24
        cur_lr: 5.0e-05
        entropy: 0.1112257397423188
        entropy_coeff: 0.0005000000000000001
        kl: 0.004630551440641284
        model: {}
        policy_loss: -0.010297131802265843
        total_loss: 2.034514904022217
        vf_explained_var: 0.9938924908638
        vf_loss: 2.044867674509684
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65333333333334
    gpu_util_percent0: 0.32833333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688991075445648
    mean_env_wait_ms: 1.2015942429442434
    mean_inference_ms: 4.329022860536752
    mean_raw_obs_processing_ms: 0.3788643547062418
  time_since_restore: 4128.307551860809
  time_this_iter_s: 25.423137426376343
  time_total_s: 4128.307551860809
  timers:
    learn_throughput: 8684.563
    learn_time_ms: 18629.838
    sample_throughput: 23903.07
    sample_time_ms: 6768.67
    update_time_ms: 22.497
  timestamp: 1602735924
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    161 |          4128.31 | 26048512 |  292.563 |              333.202 |              160.172 |            772.235 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3052.8358584364087
    time_step_min: 2786
  date: 2020-10-15_04-25-51
  done: false
  episode_len_mean: 772.2427887457146
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 292.6676959929117
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 183
  episodes_total: 33836
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.926167350636332e-25
        cur_lr: 5.0e-05
        entropy: 0.12533406292398772
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008819864228523025
        total_loss: .inf
        vf_explained_var: 0.9894394278526306
        vf_loss: 4.058071633179982
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.97586206896552
    gpu_util_percent0: 0.32172413793103455
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688543952241528
    mean_env_wait_ms: 1.2015656550709726
    mean_inference_ms: 4.328719885188558
    mean_raw_obs_processing_ms: 0.3788466023241593
  time_since_restore: 4153.773095369339
  time_this_iter_s: 25.465543508529663
  time_total_s: 4153.773095369339
  timers:
    learn_throughput: 8684.897
    learn_time_ms: 18629.121
    sample_throughput: 23902.254
    sample_time_ms: 6768.901
    update_time_ms: 22.407
  timestamp: 1602735951
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    162 |          4153.77 | 26210304 |  292.668 |              333.202 |              160.172 |            772.243 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3051.579874878844
    time_step_min: 2786
  date: 2020-10-15_04-26-17
  done: false
  episode_len_mean: 772.2605532576491
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 292.87640641209236
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 253
  episodes_total: 34089
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4889251025954502e-24
        cur_lr: 5.0e-05
        entropy: 0.10353474132716656
        entropy_coeff: 0.0005000000000000001
        kl: 0.004417272362237175
        model: {}
        policy_loss: -0.008400073311349843
        total_loss: 2.445580303668976
        vf_explained_var: 0.9940967559814453
        vf_loss: 2.454032222429911
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.173333333333336
    gpu_util_percent0: 0.37999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687780803257533
    mean_env_wait_ms: 1.201517086738609
    mean_inference_ms: 4.328249644106313
    mean_raw_obs_processing_ms: 0.3788193432263218
  time_since_restore: 4179.216020584106
  time_this_iter_s: 25.442925214767456
  time_total_s: 4179.216020584106
  timers:
    learn_throughput: 8691.671
    learn_time_ms: 18614.601
    sample_throughput: 23879.498
    sample_time_ms: 6775.352
    update_time_ms: 22.21
  timestamp: 1602735977
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    163 |          4179.22 | 26372096 |  292.876 |              333.202 |              160.172 |            772.261 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3050.3955960516323
    time_step_min: 2786
  date: 2020-10-15_04-26-43
  done: false
  episode_len_mean: 772.2645840625364
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 293.0485428311819
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 195
  episodes_total: 34284
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.444625512977251e-25
        cur_lr: 5.0e-05
        entropy: 0.09280293931563695
        entropy_coeff: 0.0005000000000000001
        kl: 0.003889291392018398
        model: {}
        policy_loss: -0.008285428181504054
        total_loss: 1.129340519507726
        vf_explained_var: 0.9965343475341797
        vf_loss: 1.137672334909439
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65333333333334
    gpu_util_percent0: 0.30700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468721076703425
    mean_env_wait_ms: 1.2014816839485327
    mean_inference_ms: 4.327915835769155
    mean_raw_obs_processing_ms: 0.3787997979356231
  time_since_restore: 4204.581651926041
  time_this_iter_s: 25.365631341934204
  time_total_s: 4204.581651926041
  timers:
    learn_throughput: 8708.249
    learn_time_ms: 18579.165
    sample_throughput: 23919.123
    sample_time_ms: 6764.128
    update_time_ms: 22.299
  timestamp: 1602736003
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    164 |          4204.58 | 26533888 |  293.049 |              333.202 |              160.172 |            772.265 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3049.3732206147233
    time_step_min: 2786
  date: 2020-10-15_04-27-09
  done: false
  episode_len_mean: 772.2775359795729
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 293.20287309023377
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 180
  episodes_total: 34464
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7223127564886255e-25
        cur_lr: 5.0e-05
        entropy: 0.09191484128435452
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036966565918798247
        model: {}
        policy_loss: -0.009319409252687668
        total_loss: 1.1937608122825623
        vf_explained_var: 0.9963530898094177
        vf_loss: 1.2031261622905731
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.513793103448275
    gpu_util_percent0: 0.39999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686783999079403
    mean_env_wait_ms: 1.201450342707161
    mean_inference_ms: 4.32762714674689
    mean_raw_obs_processing_ms: 0.37878291519972423
  time_since_restore: 4229.715001821518
  time_this_iter_s: 25.133349895477295
  time_total_s: 4229.715001821518
  timers:
    learn_throughput: 8724.922
    learn_time_ms: 18543.661
    sample_throughput: 23947.134
    sample_time_ms: 6756.216
    update_time_ms: 22.201
  timestamp: 1602736029
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: af50e_00000
  
2020-10-15 04:27:09,693	WARNING util.py:136 -- The `process_trial` operation took 0.5062572956085205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    165 |          4229.72 | 26695680 |  293.203 |              333.202 |              160.172 |            772.278 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3047.8946518202274
    time_step_min: 2786
  date: 2020-10-15_04-27-35
  done: false
  episode_len_mean: 772.3005935231071
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 293.4271495058921
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 244
  episodes_total: 34708
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8611563782443128e-25
        cur_lr: 5.0e-05
        entropy: 0.0936073586344719
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035849504250412187
        model: {}
        policy_loss: -0.0090693198981171
        total_loss: 1.0880359212557476
        vf_explained_var: 0.9972798228263855
        vf_loss: 1.097152014573415
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.000000000000004
    gpu_util_percent0: 0.3476666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686103449382285
    mean_env_wait_ms: 1.2014002652721
    mean_inference_ms: 4.327199740049867
    mean_raw_obs_processing_ms: 0.37875722449095933
  time_since_restore: 4255.133347034454
  time_this_iter_s: 25.4183452129364
  time_total_s: 4255.133347034454
  timers:
    learn_throughput: 8743.179
    learn_time_ms: 18504.939
    sample_throughput: 23967.458
    sample_time_ms: 6750.486
    update_time_ms: 23.398
  timestamp: 1602736055
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    166 |          4255.13 | 26857472 |  293.427 |              333.202 |              160.172 |            772.301 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3046.6899893900727
    time_step_min: 2786
  date: 2020-10-15_04-28-01
  done: false
  episode_len_mean: 772.3145066590291
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 293.61229363663847
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 207
  episodes_total: 34915
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.305781891221564e-26
        cur_lr: 5.0e-05
        entropy: 0.08135740521053474
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039465878120002644
        model: {}
        policy_loss: -0.007984967394804698
        total_loss: 0.968469500541687
        vf_explained_var: 0.9970898628234863
        vf_loss: 0.97649518152078
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64666666666666
    gpu_util_percent0: 0.4043333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468548674256954
    mean_env_wait_ms: 1.2013596955458625
    mean_inference_ms: 4.326844093786393
    mean_raw_obs_processing_ms: 0.37873673946360426
  time_since_restore: 4281.1532735824585
  time_this_iter_s: 26.01992654800415
  time_total_s: 4281.1532735824585
  timers:
    learn_throughput: 8742.864
    learn_time_ms: 18505.607
    sample_throughput: 23885.822
    sample_time_ms: 6773.558
    update_time_ms: 24.188
  timestamp: 1602736081
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: af50e_00000
  
2020-10-15 04:28:02,531	WARNING util.py:136 -- The `process_trial` operation took 0.5090780258178711 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    167 |          4281.15 | 27019264 |  293.612 |              333.202 |              160.172 |            772.315 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3045.6404564907275
    time_step_min: 2786
  date: 2020-10-15_04-28-28
  done: false
  episode_len_mean: 772.3228656103955
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 293.77051289136654
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 177
  episodes_total: 35092
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.652890945610782e-26
        cur_lr: 5.0e-05
        entropy: 0.08573194965720177
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034817688283510506
        model: {}
        policy_loss: -0.0074267389718443155
        total_loss: 0.7616454660892487
        vf_explained_var: 0.997586727142334
        vf_loss: 0.769115075469017
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77741935483871
    gpu_util_percent0: 0.3158064516129033
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685068171651422
    mean_env_wait_ms: 1.2013259561737366
    mean_inference_ms: 4.326570353853657
    mean_raw_obs_processing_ms: 0.37872036027061473
  time_since_restore: 4307.017193078995
  time_this_iter_s: 25.863919496536255
  time_total_s: 4307.017193078995
  timers:
    learn_throughput: 8754.3
    learn_time_ms: 18481.432
    sample_throughput: 23736.206
    sample_time_ms: 6816.254
    update_time_ms: 24.493
  timestamp: 1602736108
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    168 |          4307.02 | 27181056 |  293.771 |              333.202 |              160.172 |            772.323 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3044.1721919981865
    time_step_min: 2786
  date: 2020-10-15_04-28-55
  done: false
  episode_len_mean: 772.3216731759778
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 293.99094185186897
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 242
  episodes_total: 35334
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.326445472805391e-26
        cur_lr: 5.0e-05
        entropy: 0.09519958992799123
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009443253880211463
        total_loss: .inf
        vf_explained_var: 0.9979766011238098
        vf_loss: 0.7674968789021174
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.826666666666668
    gpu_util_percent0: 0.35800000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684437936634734
    mean_env_wait_ms: 1.20127230787963
    mean_inference_ms: 4.326166200866933
    mean_raw_obs_processing_ms: 0.37869627825333646
  time_since_restore: 4333.041414737701
  time_this_iter_s: 26.024221658706665
  time_total_s: 4333.041414737701
  timers:
    learn_throughput: 8749.886
    learn_time_ms: 18490.755
    sample_throughput: 23710.003
    sample_time_ms: 6823.786
    update_time_ms: 24.805
  timestamp: 1602736135
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    169 |          4333.04 | 27342848 |  293.991 |              333.202 |              160.172 |            772.322 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3043.0794276701217
    time_step_min: 2786
  date: 2020-10-15_04-29-21
  done: false
  episode_len_mean: 772.3301637315028
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 294.1485956168902
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 35546
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.489668209208085e-26
        cur_lr: 5.0e-05
        entropy: 0.10897541294495265
        entropy_coeff: 0.0005000000000000001
        kl: 0.005357552242154877
        model: {}
        policy_loss: -0.011745498709691068
        total_loss: 1.9800259669621785
        vf_explained_var: 0.9946103096008301
        vf_loss: 1.9918259580930073
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.146666666666672
    gpu_util_percent0: 0.37133333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683839609128435
    mean_env_wait_ms: 1.2012288734477305
    mean_inference_ms: 4.325814391473832
    mean_raw_obs_processing_ms: 0.37867572633556257
  time_since_restore: 4358.609258413315
  time_this_iter_s: 25.567843675613403
  time_total_s: 4358.609258413315
  timers:
    learn_throughput: 8749.191
    learn_time_ms: 18492.225
    sample_throughput: 23663.356
    sample_time_ms: 6837.238
    update_time_ms: 24.897
  timestamp: 1602736161
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: af50e_00000
  
2020-10-15 04:29:22,120	WARNING util.py:136 -- The `process_trial` operation took 0.5030319690704346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    170 |          4358.61 | 27504640 |  294.149 |              333.202 |              160.172 |             772.33 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3042.501219204574
    time_step_min: 2786
  date: 2020-10-15_04-29-47
  done: false
  episode_len_mean: 772.3443632597072
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 294.2348648151117
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 35721
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.489668209208085e-26
        cur_lr: 5.0e-05
        entropy: 0.11515075154602528
        entropy_coeff: 0.0005000000000000001
        kl: 0.004569795370722811
        model: {}
        policy_loss: -0.010962411877699196
        total_loss: 2.9644157687822976
        vf_explained_var: 0.9922477602958679
        vf_loss: 2.975435813268026
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17333333333334
    gpu_util_percent0: 0.343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468343611012238
    mean_env_wait_ms: 1.201192913544042
    mean_inference_ms: 4.325548451401998
    mean_raw_obs_processing_ms: 0.37865997655468986
  time_since_restore: 4384.3112025260925
  time_this_iter_s: 25.70194411277771
  time_total_s: 4384.3112025260925
  timers:
    learn_throughput: 8740.585
    learn_time_ms: 18510.432
    sample_throughput: 23640.932
    sample_time_ms: 6843.723
    update_time_ms: 26.373
  timestamp: 1602736187
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: af50e_00000
  
2020-10-15 04:29:48,506	WARNING util.py:136 -- The `process_trial` operation took 0.520089864730835 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    171 |          4384.31 | 27666432 |  294.235 |              333.202 |              160.172 |            772.344 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3041.4052624251703
    time_step_min: 2786
  date: 2020-10-15_04-30-13
  done: false
  episode_len_mean: 772.3531162221542
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 294.4131638154776
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 236
  episodes_total: 35957
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7448341046040426e-26
        cur_lr: 5.0e-05
        entropy: 0.09907115002473195
        entropy_coeff: 0.0005000000000000001
        kl: 0.003360668934571246
        model: {}
        policy_loss: -0.007767040243682762
        total_loss: 1.7311718861262004
        vf_explained_var: 0.9957299828529358
        vf_loss: 1.7389883995056152
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666664
    gpu_util_percent0: 0.317
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682830649818376
    mean_env_wait_ms: 1.2011389045324197
    mean_inference_ms: 4.325172886648317
    mean_raw_obs_processing_ms: 0.37863754209128087
  time_since_restore: 4409.7394959926605
  time_this_iter_s: 25.428293466567993
  time_total_s: 4409.7394959926605
  timers:
    learn_throughput: 8747.433
    learn_time_ms: 18495.94
    sample_throughput: 23637.433
    sample_time_ms: 6844.737
    update_time_ms: 33.061
  timestamp: 1602736213
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: af50e_00000
  
2020-10-15 04:30:14,638	WARNING util.py:136 -- The `process_trial` operation took 0.5375373363494873 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    172 |          4409.74 | 27828224 |  294.413 |              333.202 |              160.172 |            772.353 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3040.167838166925
    time_step_min: 2786
  date: 2020-10-15_04-30-40
  done: false
  episode_len_mean: 772.3639228260269
  episode_reward_max: 333.2020202020205
  episode_reward_mean: 294.59772443881565
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 221
  episodes_total: 36178
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.724170523020213e-27
        cur_lr: 5.0e-05
        entropy: 0.08440733390549819
        entropy_coeff: 0.0005000000000000001
        kl: 0.002996920025907457
        model: {}
        policy_loss: -0.007704316958552226
        total_loss: 0.9919647673765818
        vf_explained_var: 0.9972257018089294
        vf_loss: 0.999711294968923
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14682232439674675
    mean_env_wait_ms: 1.2010907566641011
    mean_inference_ms: 4.324818655725142
    mean_raw_obs_processing_ms: 0.3786164550728025
  time_since_restore: 4435.388581037521
  time_this_iter_s: 25.64908504486084
  time_total_s: 4435.388581037521
  timers:
    learn_throughput: 8741.891
    learn_time_ms: 18507.667
    sample_throughput: 23614.611
    sample_time_ms: 6851.351
    update_time_ms: 34.735
  timestamp: 1602736240
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    173 |          4435.39 | 27990016 |  294.598 |              333.202 |              160.172 |            772.364 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3039.2065328155554
    time_step_min: 2786
  date: 2020-10-15_04-31-06
  done: false
  episode_len_mean: 772.3695084041705
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 294.7424513351722
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 36351
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3620852615101064e-27
        cur_lr: 5.0e-05
        entropy: 0.0840028381596009
        entropy_coeff: 0.0005000000000000001
        kl: 0.004139740427490324
        model: {}
        policy_loss: -0.008307150540834604
        total_loss: 0.8333884080251058
        vf_explained_var: 0.9973152279853821
        vf_loss: 0.8417375485102335
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.986666666666665
    gpu_util_percent0: 0.36300000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681838979057607
    mean_env_wait_ms: 1.201052610075797
    mean_inference_ms: 4.324562710480344
    mean_raw_obs_processing_ms: 0.3786011876405806
  time_since_restore: 4460.970606327057
  time_this_iter_s: 25.582025289535522
  time_total_s: 4460.970606327057
  timers:
    learn_throughput: 8731.43
    learn_time_ms: 18529.84
    sample_throughput: 23628.547
    sample_time_ms: 6847.311
    update_time_ms: 36.064
  timestamp: 1602736266
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    174 |          4460.97 | 28151808 |  294.742 |              333.354 |              160.172 |             772.37 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3038.0028463517433
    time_step_min: 2786
  date: 2020-10-15_04-31-32
  done: false
  episode_len_mean: 772.3629305631492
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 294.9225745701961
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 36580
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1810426307550532e-27
        cur_lr: 5.0e-05
        entropy: 0.09721827444930871
        entropy_coeff: 0.0005000000000000001
        kl: 0.004642712146354218
        model: {}
        policy_loss: -0.01025357328459601
        total_loss: 1.2717787126700084
        vf_explained_var: 0.9966995120048523
        vf_loss: 1.2820809384187062
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690000000000005
    gpu_util_percent0: 0.29433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681308659071005
    mean_env_wait_ms: 1.2009973075265712
    mean_inference_ms: 4.324215649630902
    mean_raw_obs_processing_ms: 0.3785803100378047
  time_since_restore: 4486.6039073467255
  time_this_iter_s: 25.63330101966858
  time_total_s: 4486.6039073467255
  timers:
    learn_throughput: 8703.002
    learn_time_ms: 18590.367
    sample_throughput: 23644.903
    sample_time_ms: 6842.574
    update_time_ms: 36.46
  timestamp: 1602736292
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: af50e_00000
  
2020-10-15 04:31:33,501	WARNING util.py:136 -- The `process_trial` operation took 0.5002725124359131 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    175 |           4486.6 | 28313600 |  294.923 |              333.354 |              160.172 |            772.363 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3036.8573487815493
    time_step_min: 2786
  date: 2020-10-15_04-31-59
  done: false
  episode_len_mean: 772.3602010323282
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 295.1020062071406
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 230
  episodes_total: 36810
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0905213153775266e-27
        cur_lr: 5.0e-05
        entropy: 0.08725414176781972
        entropy_coeff: 0.0005000000000000001
        kl: 0.004216978423452626
        model: {}
        policy_loss: -0.009772096246403331
        total_loss: 0.8887103994687399
        vf_explained_var: 0.9975417256355286
        vf_loss: 0.8985261172056198
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.709999999999997
    gpu_util_percent0: 0.3486666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680686058780307
    mean_env_wait_ms: 1.2009455752619917
    mean_inference_ms: 4.32385576907042
    mean_raw_obs_processing_ms: 0.3785587118893519
  time_since_restore: 4512.437553882599
  time_this_iter_s: 25.833646535873413
  time_total_s: 4512.437553882599
  timers:
    learn_throughput: 8682.642
    learn_time_ms: 18633.96
    sample_throughput: 23629.799
    sample_time_ms: 6846.948
    update_time_ms: 37.315
  timestamp: 1602736319
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: af50e_00000
  
2020-10-15 04:32:00,018	WARNING util.py:136 -- The `process_trial` operation took 0.515822172164917 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    176 |          4512.44 | 28475392 |  295.102 |              333.354 |              160.172 |             772.36 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3035.967541081242
    time_step_min: 2786
  date: 2020-10-15_04-32-25
  done: false
  episode_len_mean: 772.3574267867283
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 295.23795511700104
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 36981
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.452606576887633e-28
        cur_lr: 5.0e-05
        entropy: 0.08246223255991936
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034226336089583733
        model: {}
        policy_loss: -0.007694716541057763
        total_loss: 0.8591456611951193
        vf_explained_var: 0.9971842765808105
        vf_loss: 0.8668816288312277
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.780000000000005
    gpu_util_percent0: 0.3853333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680304453031487
    mean_env_wait_ms: 1.2009057987302125
    mean_inference_ms: 4.323609700302339
    mean_raw_obs_processing_ms: 0.3785440016188776
  time_since_restore: 4537.82053899765
  time_this_iter_s: 25.38298511505127
  time_total_s: 4537.82053899765
  timers:
    learn_throughput: 8696.864
    learn_time_ms: 18603.487
    sample_throughput: 23741.223
    sample_time_ms: 6814.813
    update_time_ms: 35.211
  timestamp: 1602736345
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    177 |          4537.82 | 28637184 |  295.238 |              333.354 |              160.172 |            772.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3034.725763487152
    time_step_min: 2786
  date: 2020-10-15_04-32-51
  done: false
  episode_len_mean: 772.3537237616578
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 295.42609664250745
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 37207
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7263032884438165e-28
        cur_lr: 5.0e-05
        entropy: 0.09107551040748756
        entropy_coeff: 0.0005000000000000001
        kl: 0.005250485070670645
        model: {}
        policy_loss: -0.009365405666661294
        total_loss: 0.5907025958100954
        vf_explained_var: 0.9983400702476501
        vf_loss: 0.6001135508219401
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57
    gpu_util_percent0: 0.34466666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679816149611347
    mean_env_wait_ms: 1.2008498497536262
    mean_inference_ms: 4.32328118817733
    mean_raw_obs_processing_ms: 0.3785236711540307
  time_since_restore: 4563.264427661896
  time_this_iter_s: 25.443888664245605
  time_total_s: 4563.264427661896
  timers:
    learn_throughput: 8698.373
    learn_time_ms: 18600.26
    sample_throughput: 23875.165
    sample_time_ms: 6776.581
    update_time_ms: 35.044
  timestamp: 1602736371
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    178 |          4563.26 | 28798976 |  295.426 |              333.354 |              160.172 |            772.354 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3033.5192533960853
    time_step_min: 2786
  date: 2020-10-15_04-33-17
  done: false
  episode_len_mean: 772.3560553448368
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 295.60527439035894
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 231
  episodes_total: 37438
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7263032884438165e-28
        cur_lr: 5.0e-05
        entropy: 0.08714201611777146
        entropy_coeff: 0.0005000000000000001
        kl: 0.003868526817920307
        model: {}
        policy_loss: -0.00782819208689034
        total_loss: 1.140407770872116
        vf_explained_var: 0.9969741702079773
        vf_loss: 1.1482795377572377
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62
    gpu_util_percent0: 0.2800000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679209548609626
    mean_env_wait_ms: 1.2007953775069988
    mean_inference_ms: 4.3229311379856865
    mean_raw_obs_processing_ms: 0.3785027256572141
  time_since_restore: 4588.880837202072
  time_this_iter_s: 25.61640954017639
  time_total_s: 4588.880837202072
  timers:
    learn_throughput: 8714.936
    learn_time_ms: 18564.91
    sample_throughput: 23922.732
    sample_time_ms: 6763.107
    update_time_ms: 35.094
  timestamp: 1602736397
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: af50e_00000
  
2020-10-15 04:33:18,442	WARNING util.py:136 -- The `process_trial` operation took 0.5135116577148438 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    179 |          4588.88 | 28960768 |  295.605 |              333.354 |              160.172 |            772.356 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3032.6039714650765
    time_step_min: 2786
  date: 2020-10-15_04-33-44
  done: false
  episode_len_mean: 772.3574847115129
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 295.74343407486185
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 172
  episodes_total: 37610
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3631516442219083e-28
        cur_lr: 5.0e-05
        entropy: 0.0820482149720192
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038630730705335736
        model: {}
        policy_loss: -0.008636291604489088
        total_loss: 0.5803056160608927
        vf_explained_var: 0.9980428218841553
        vf_loss: 0.5889829397201538
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.68333333333333
    gpu_util_percent0: 0.3390000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678819482362004
    mean_env_wait_ms: 1.200753594691898
    mean_inference_ms: 4.322693927207653
    mean_raw_obs_processing_ms: 0.37848857493205385
  time_since_restore: 4614.575366735458
  time_this_iter_s: 25.69452953338623
  time_total_s: 4614.575366735458
  timers:
    learn_throughput: 8706.226
    learn_time_ms: 18583.484
    sample_throughput: 23953.382
    sample_time_ms: 6754.453
    update_time_ms: 37.118
  timestamp: 1602736424
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: af50e_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    180 |          4614.58 | 29122560 |  295.743 |              333.354 |              160.172 |            772.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3031.3738621930565
    time_step_min: 2786
  date: 2020-10-15_04-34-10
  done: false
  episode_len_mean: 772.3590421314162
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 295.9268460360863
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 37834
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.815758221109541e-29
        cur_lr: 5.0e-05
        entropy: 0.09000694875915845
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007626680230714555
        total_loss: .inf
        vf_explained_var: 0.9981078505516052
        vf_loss: 0.713044802347819
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.789999999999996
    gpu_util_percent0: 0.31300000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678373122586189
    mean_env_wait_ms: 1.2006954682382347
    mean_inference_ms: 4.322378445080466
    mean_raw_obs_processing_ms: 0.3784691490032
  time_since_restore: 4640.332973480225
  time_this_iter_s: 25.757606744766235
  time_total_s: 4640.332973480225
  timers:
    learn_throughput: 8704.089
    learn_time_ms: 18588.045
    sample_throughput: 23948.479
    sample_time_ms: 6755.836
    update_time_ms: 36.206
  timestamp: 1602736450
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:34:11,242	WARNING util.py:136 -- The `process_trial` operation took 0.5030384063720703 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    181 |          4640.33 | 29284352 |  295.927 |              333.354 |              160.172 |            772.359 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3030.0932519854837
    time_step_min: 2786
  date: 2020-10-15_04-34-36
  done: false
  episode_len_mean: 772.3637438268362
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 296.11204007077174
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 234
  episodes_total: 38068
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0223637331664313e-28
        cur_lr: 5.0e-05
        entropy: 0.0928005650639534
        entropy_coeff: 0.0005000000000000001
        kl: 0.007390397251583636
        model: {}
        policy_loss: -0.00817735434975475
        total_loss: 0.48445215572913486
        vf_explained_var: 0.9986615180969238
        vf_loss: 0.49267590542634326
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.766666666666666
    gpu_util_percent0: 0.31500000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467775165144033
    mean_env_wait_ms: 1.2006382306737853
    mean_inference_ms: 4.322033546646762
    mean_raw_obs_processing_ms: 0.3784476504607364
  time_since_restore: 4665.979074954987
  time_this_iter_s: 25.646101474761963
  time_total_s: 4665.979074954987
  timers:
    learn_throughput: 8690.751
    learn_time_ms: 18616.573
    sample_throughput: 23945.787
    sample_time_ms: 6756.596
    update_time_ms: 29.434
  timestamp: 1602736476
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: af50e_00000
  
2020-10-15 04:34:37,645	WARNING util.py:136 -- The `process_trial` operation took 0.5778071880340576 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    182 |          4665.98 | 29446144 |  296.112 |              333.354 |              160.172 |            772.364 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3029.2437364190905
    time_step_min: 2786
  date: 2020-10-15_04-35-03
  done: false
  episode_len_mean: 772.3718978006747
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 296.2237685836106
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 171
  episodes_total: 38239
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0223637331664313e-28
        cur_lr: 5.0e-05
        entropy: 0.11588931518296401
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01228037488181144
        total_loss: .inf
        vf_explained_var: 0.995277464389801
        vf_loss: 1.5207386712233226
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.412903225806446
    gpu_util_percent0: 0.3570967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467738360765439
    mean_env_wait_ms: 1.2005947074995542
    mean_inference_ms: 4.321801978299038
    mean_raw_obs_processing_ms: 0.3784344066975884
  time_since_restore: 4691.8084127902985
  time_this_iter_s: 25.82933783531189
  time_total_s: 4691.8084127902985
  timers:
    learn_throughput: 8685.679
    learn_time_ms: 18627.445
    sample_throughput: 23953.46
    sample_time_ms: 6754.431
    update_time_ms: 29.428
  timestamp: 1602736503
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:35:04,153	WARNING util.py:136 -- The `process_trial` operation took 0.5052998065948486 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    183 |          4691.81 | 29607936 |  296.224 |              333.354 |              160.172 |            772.372 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3028.6432625221287
    time_step_min: 2786
  date: 2020-10-15_04-35-29
  done: false
  episode_len_mean: 772.3930150309461
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 296.3102518396635
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 215
  episodes_total: 38454
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5335455997496474e-28
        cur_lr: 5.0e-05
        entropy: 0.13525691131750742
        entropy_coeff: 0.0005000000000000001
        kl: 0.004792638394671182
        model: {}
        policy_loss: -0.012031563024114197
        total_loss: 3.540084958076477
        vf_explained_var: 0.9916236996650696
        vf_loss: 3.5521841446558633
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.929999999999996
    gpu_util_percent0: 0.3346666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676997475931025
    mean_env_wait_ms: 1.2005393881070876
    mean_inference_ms: 4.32151734368965
    mean_raw_obs_processing_ms: 0.37841665887678544
  time_since_restore: 4717.3430869579315
  time_this_iter_s: 25.534674167633057
  time_total_s: 4717.3430869579315
  timers:
    learn_throughput: 8686.418
    learn_time_ms: 18625.86
    sample_throughput: 23964.564
    sample_time_ms: 6751.302
    update_time_ms: 29.193
  timestamp: 1602736529
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: af50e_00000
  
2020-10-15 04:35:30,547	WARNING util.py:136 -- The `process_trial` operation took 0.5074517726898193 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    184 |          4717.34 | 29769728 |   296.31 |              333.354 |              160.172 |            772.393 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3027.7718218036944
    time_step_min: 2786
  date: 2020-10-15_04-35-56
  done: false
  episode_len_mean: 772.4173041141203
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 296.43902770729824
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 242
  episodes_total: 38696
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.667727998748237e-29
        cur_lr: 5.0e-05
        entropy: 0.10578482784330845
        entropy_coeff: 0.0005000000000000001
        kl: 0.003979989579723527
        model: {}
        policy_loss: -0.010506627382710576
        total_loss: 2.3240765730539956
        vf_explained_var: 0.9944825172424316
        vf_loss: 2.334636092185974
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64666666666667
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676373155376007
    mean_env_wait_ms: 1.2004772963446981
    mean_inference_ms: 4.321169037201316
    mean_raw_obs_processing_ms: 0.3783953206018639
  time_since_restore: 4742.805537223816
  time_this_iter_s: 25.4624502658844
  time_total_s: 4742.805537223816
  timers:
    learn_throughput: 8699.429
    learn_time_ms: 18598.003
    sample_throughput: 23932.088
    sample_time_ms: 6760.463
    update_time_ms: 30.1
  timestamp: 1602736556
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: af50e_00000
  
2020-10-15 04:35:56,927	WARNING util.py:136 -- The `process_trial` operation took 0.5184426307678223 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    185 |          4742.81 | 29931520 |  296.439 |              333.354 |              160.172 |            772.417 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3027.06168229113
    time_step_min: 2786
  date: 2020-10-15_04-36-22
  done: false
  episode_len_mean: 772.4304605093903
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 296.5522435572602
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 174
  episodes_total: 38870
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8338639993741186e-29
        cur_lr: 5.0e-05
        entropy: 0.09040066724022229
        entropy_coeff: 0.0005000000000000001
        kl: 0.003605869695699463
        model: {}
        policy_loss: -0.010318671651475597
        total_loss: 1.3141196568806965
        vf_explained_var: 0.9959505200386047
        vf_loss: 1.3244835038979847
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.11666666666667
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675986645267444
    mean_env_wait_ms: 1.2004319079065637
    mean_inference_ms: 4.320938477077333
    mean_raw_obs_processing_ms: 0.3783818865078463
  time_since_restore: 4768.44868183136
  time_this_iter_s: 25.643144607543945
  time_total_s: 4768.44868183136
  timers:
    learn_throughput: 8703.916
    learn_time_ms: 18588.414
    sample_throughput: 23962.559
    sample_time_ms: 6751.867
    update_time_ms: 27.994
  timestamp: 1602736582
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: af50e_00000
  
2020-10-15 04:36:23,451	WARNING util.py:136 -- The `process_trial` operation took 0.5095653533935547 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    186 |          4768.45 | 30093312 |  296.552 |              333.354 |              160.172 |             772.43 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3026.173878513053
    time_step_min: 2786
  date: 2020-10-15_04-36-49
  done: false
  episode_len_mean: 772.441279590531
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 296.69214602842237
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 205
  episodes_total: 39075
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9169319996870593e-29
        cur_lr: 5.0e-05
        entropy: 0.09426305505136649
        entropy_coeff: 0.0005000000000000001
        kl: 0.003493090315411488
        model: {}
        policy_loss: -0.009254872265349453
        total_loss: 1.2661160131295521
        vf_explained_var: 0.9965309500694275
        vf_loss: 1.2754180431365967
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.523333333333333
    gpu_util_percent0: 0.30366666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675621373777437
    mean_env_wait_ms: 1.200378674562448
    mean_inference_ms: 4.320682065880406
    mean_raw_obs_processing_ms: 0.3783659507884741
  time_since_restore: 4794.063989877701
  time_this_iter_s: 25.615308046340942
  time_total_s: 4794.063989877701
  timers:
    learn_throughput: 8699.082
    learn_time_ms: 18598.744
    sample_throughput: 23947.948
    sample_time_ms: 6755.986
    update_time_ms: 28.19
  timestamp: 1602736609
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: af50e_00000
  
2020-10-15 04:36:49,836	WARNING util.py:136 -- The `process_trial` operation took 0.509608268737793 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    187 |          4794.06 | 30255104 |  296.692 |              333.354 |              160.172 |            772.441 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3024.9663696537677
    time_step_min: 2786
  date: 2020-10-15_04-37-15
  done: false
  episode_len_mean: 772.4508417679671
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 296.8758561146791
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 247
  episodes_total: 39322
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.584659998435297e-30
        cur_lr: 5.0e-05
        entropy: 0.08717781988282998
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0074686968776707845
        total_loss: .inf
        vf_explained_var: 0.9983541965484619
        vf_loss: 0.6239821761846542
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.25517241379311
    gpu_util_percent0: 0.3237931034482759
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675030764568506
    mean_env_wait_ms: 1.2003130445874948
    mean_inference_ms: 4.320336075319402
    mean_raw_obs_processing_ms: 0.37834471761703875
  time_since_restore: 4819.454191446304
  time_this_iter_s: 25.390201568603516
  time_total_s: 4819.454191446304
  timers:
    learn_throughput: 8698.987
    learn_time_ms: 18598.947
    sample_throughput: 23974.752
    sample_time_ms: 6748.433
    update_time_ms: 27.779
  timestamp: 1602736635
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:37:16,065	WARNING util.py:136 -- The `process_trial` operation took 0.5126175880432129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    188 |          4819.45 | 30416896 |  296.876 |              333.354 |              160.172 |            772.451 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3024.0864419665486
    time_step_min: 2786
  date: 2020-10-15_04-37-41
  done: false
  episode_len_mean: 772.4553946635614
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 297.01419004996035
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 180
  episodes_total: 39502
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4376989997652938e-29
        cur_lr: 5.0e-05
        entropy: 0.07757772256930669
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007750394298151757
        total_loss: .inf
        vf_explained_var: 0.9989502429962158
        vf_loss: 0.3270198777318001
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.86
    gpu_util_percent0: 0.2753333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674633452689617
    mean_env_wait_ms: 1.200264820400488
    mean_inference_ms: 4.32010315285746
    mean_raw_obs_processing_ms: 0.37833076599937027
  time_since_restore: 4845.0997495651245
  time_this_iter_s: 25.64555811882019
  time_total_s: 4845.0997495651245
  timers:
    learn_throughput: 8694.3
    learn_time_ms: 18608.974
    sample_throughput: 23977.427
    sample_time_ms: 6747.68
    update_time_ms: 29.695
  timestamp: 1602736661
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:37:42,550	WARNING util.py:136 -- The `process_trial` operation took 0.5584230422973633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    189 |           4845.1 | 30578688 |  297.014 |              333.354 |              160.172 |            772.455 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3023.1296936073636
    time_step_min: 2786
  date: 2020-10-15_04-38-08
  done: false
  episode_len_mean: 772.4621507922514
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 297.155604970276
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 195
  episodes_total: 39697
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1565484996479406e-29
        cur_lr: 5.0e-05
        entropy: 0.08554664440453053
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036550116686460874
        model: {}
        policy_loss: -0.01078832967323251
        total_loss: 0.6035386621952057
        vf_explained_var: 0.9982423782348633
        vf_loss: 0.614369735121727
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73333333333334
    gpu_util_percent0: 0.3266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467430932342073
    mean_env_wait_ms: 1.2002135801946312
    mean_inference_ms: 4.319867591841963
    mean_raw_obs_processing_ms: 0.3783161379623128
  time_since_restore: 4870.864607334137
  time_this_iter_s: 25.76485776901245
  time_total_s: 4870.864607334137
  timers:
    learn_throughput: 8688.923
    learn_time_ms: 18620.49
    sample_throughput: 23989.059
    sample_time_ms: 6744.408
    update_time_ms: 28.086
  timestamp: 1602736688
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: af50e_00000
  
2020-10-15 04:38:09,114	WARNING util.py:136 -- The `process_trial` operation took 0.5167891979217529 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    190 |          4870.86 | 30740480 |  297.156 |              333.354 |              160.172 |            772.462 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3021.9348136638177
    time_step_min: 2786
  date: 2020-10-15_04-38-34
  done: false
  episode_len_mean: 772.4698695641289
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 297.3339309020404
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 246
  episodes_total: 39943
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0782742498239703e-29
        cur_lr: 5.0e-05
        entropy: 0.08376993673543136
        entropy_coeff: 0.0005000000000000001
        kl: 0.003143497296453764
        model: {}
        policy_loss: -0.009355487418361008
        total_loss: 0.9025970200697581
        vf_explained_var: 0.9976655840873718
        vf_loss: 0.9119944075743357
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193333333333335
    gpu_util_percent0: 0.2866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673732587722824
    mean_env_wait_ms: 1.2001463319432255
    mean_inference_ms: 4.3195288482583685
    mean_raw_obs_processing_ms: 0.3782948953702855
  time_since_restore: 4896.560076713562
  time_this_iter_s: 25.69546937942505
  time_total_s: 4896.560076713562
  timers:
    learn_throughput: 8697.794
    learn_time_ms: 18601.499
    sample_throughput: 23945.936
    sample_time_ms: 6756.554
    update_time_ms: 27.655
  timestamp: 1602736714
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: af50e_00000
  
2020-10-15 04:38:35,658	WARNING util.py:136 -- The `process_trial` operation took 0.5771820545196533 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    191 |          4896.56 | 30902272 |  297.334 |              333.354 |              160.172 |             772.47 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3021.019231248909
    time_step_min: 2786
  date: 2020-10-15_04-39-01
  done: false
  episode_len_mean: 772.4832930506067
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 297.472993458367
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 40133
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.3913712491198516e-30
        cur_lr: 5.0e-05
        entropy: 0.07657321666677792
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007446435163122563
        total_loss: .inf
        vf_explained_var: 0.9984707832336426
        vf_loss: 0.49030252049366635
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91333333333333
    gpu_util_percent0: 0.3556666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673316094919478
    mean_env_wait_ms: 1.2000935665251062
    mean_inference_ms: 4.319294398368607
    mean_raw_obs_processing_ms: 0.3782809743354554
  time_since_restore: 4922.038401842117
  time_this_iter_s: 25.478325128555298
  time_total_s: 4922.038401842117
  timers:
    learn_throughput: 8713.766
    learn_time_ms: 18567.402
    sample_throughput: 23898.219
    sample_time_ms: 6770.044
    update_time_ms: 29.832
  timestamp: 1602736741
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:39:01,872	WARNING util.py:136 -- The `process_trial` operation took 0.5532772541046143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    192 |          4922.04 | 31064064 |  297.473 |              333.354 |              160.172 |            772.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3020.1131550855243
    time_step_min: 2786
  date: 2020-10-15_04-39-27
  done: false
  episode_len_mean: 772.4835205713861
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 297.60950175815134
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 40323
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.08705687367978e-30
        cur_lr: 5.0e-05
        entropy: 0.08743871313830216
        entropy_coeff: 0.0005000000000000001
        kl: 0.003790839090167234
        model: {}
        policy_loss: -0.010780099856977662
        total_loss: 0.534661720196406
        vf_explained_var: 0.9983654618263245
        vf_loss: 0.5454855561256409
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180000000000003
    gpu_util_percent0: 0.3453333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467299710545683
    mean_env_wait_ms: 1.2000417702397845
    mean_inference_ms: 4.319073470504253
    mean_raw_obs_processing_ms: 0.3782673671687263
  time_since_restore: 4947.877090930939
  time_this_iter_s: 25.83868908882141
  time_total_s: 4947.877090930939
  timers:
    learn_throughput: 8714.496
    learn_time_ms: 18565.847
    sample_throughput: 23863.306
    sample_time_ms: 6779.949
    update_time_ms: 29.75
  timestamp: 1602736767
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: af50e_00000
  
2020-10-15 04:39:28,534	WARNING util.py:136 -- The `process_trial` operation took 0.5463385581970215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    193 |          4947.88 | 31225856 |   297.61 |              333.354 |              160.172 |            772.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3018.954151758174
    time_step_min: 2786
  date: 2020-10-15_04-39-54
  done: false
  episode_len_mean: 772.4796509478148
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 297.7815186897444
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 244
  episodes_total: 40567
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.04352843683989e-30
        cur_lr: 5.0e-05
        entropy: 0.08816651130716006
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009017098530118043
        total_loss: .inf
        vf_explained_var: 0.9977190494537354
        vf_loss: 0.8810412436723709
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.666666666666668
    gpu_util_percent0: 0.3253333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672467024489186
    mean_env_wait_ms: 1.1999742783438434
    mean_inference_ms: 4.318754783715351
    mean_raw_obs_processing_ms: 0.37824701278103756
  time_since_restore: 4973.577870607376
  time_this_iter_s: 25.700779676437378
  time_total_s: 4973.577870607376
  timers:
    learn_throughput: 8707.355
    learn_time_ms: 18581.072
    sample_throughput: 23861.878
    sample_time_ms: 6780.355
    update_time_ms: 29.414
  timestamp: 1602736794
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:39:55,021	WARNING util.py:136 -- The `process_trial` operation took 0.5728285312652588 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    194 |          4973.58 | 31387648 |  297.782 |              333.354 |              160.172 |             772.48 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3018.057708364029
    time_step_min: 2786
  date: 2020-10-15_04-40-20
  done: false
  episode_len_mean: 772.4784613874988
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 297.9170167973523
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 40764
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.065292655259834e-30
        cur_lr: 5.0e-05
        entropy: 0.07970171545942624
        entropy_coeff: 0.0005000000000000001
        kl: 0.006157153945726653
        model: {}
        policy_loss: -0.010249097668444543
        total_loss: 0.5526012927293777
        vf_explained_var: 0.9983204007148743
        vf_loss: 0.5628902390599251
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.74
    gpu_util_percent0: 0.3456666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146720340005889
    mean_env_wait_ms: 1.1999185283884586
    mean_inference_ms: 4.318513394696008
    mean_raw_obs_processing_ms: 0.3782323368029149
  time_since_restore: 4999.098117828369
  time_this_iter_s: 25.520247220993042
  time_total_s: 4999.098117828369
  timers:
    learn_throughput: 8705.626
    learn_time_ms: 18584.763
    sample_throughput: 23851.234
    sample_time_ms: 6783.381
    update_time_ms: 28.452
  timestamp: 1602736820
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: af50e_00000
  
2020-10-15 04:40:21,286	WARNING util.py:136 -- The `process_trial` operation took 0.5573253631591797 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    195 |           4999.1 | 31549440 |  297.917 |              333.354 |              160.172 |            772.478 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3017.1875244427065
    time_step_min: 2786
  date: 2020-10-15_04-40-46
  done: false
  episode_len_mean: 772.4820774527519
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 298.04629115790414
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 40954
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.065292655259834e-30
        cur_lr: 5.0e-05
        entropy: 0.090177691852053
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009806966122899516
        total_loss: .inf
        vf_explained_var: 0.9982088208198547
        vf_loss: 0.6185091932614645
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.730000000000004
    gpu_util_percent0: 0.31299999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671723523236366
    mean_env_wait_ms: 1.1998658193748832
    mean_inference_ms: 4.318298820002787
    mean_raw_obs_processing_ms: 0.3782192262122036
  time_since_restore: 5024.797885417938
  time_this_iter_s: 25.699767589569092
  time_total_s: 5024.797885417938
  timers:
    learn_throughput: 8706.93
    learn_time_ms: 18581.98
    sample_throughput: 23826.436
    sample_time_ms: 6790.441
    update_time_ms: 28.374
  timestamp: 1602736846
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:40:47,856	WARNING util.py:136 -- The `process_trial` operation took 0.5639126300811768 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    196 |           5024.8 | 31711232 |  298.046 |              333.354 |              160.172 |            772.482 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3016.18657206036
    time_step_min: 2786
  date: 2020-10-15_04-41-13
  done: false
  episode_len_mean: 772.4867581017114
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 298.19626781224054
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 241
  episodes_total: 41195
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.097938982889752e-30
        cur_lr: 5.0e-05
        entropy: 0.09564263684054215
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037072696917069456
        model: {}
        policy_loss: -0.01183268348298346
        total_loss: 1.0456050237019856
        vf_explained_var: 0.9973759055137634
        vf_loss: 1.0574855109055836
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.846666666666668
    gpu_util_percent0: 0.3376666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671217841206438
    mean_env_wait_ms: 1.1997986732898824
    mean_inference_ms: 4.3179997869901685
    mean_raw_obs_processing_ms: 0.37820006311093396
  time_since_restore: 5050.271535873413
  time_this_iter_s: 25.473650455474854
  time_total_s: 5050.271535873413
  timers:
    learn_throughput: 8713.328
    learn_time_ms: 18568.336
    sample_throughput: 23804.944
    sample_time_ms: 6796.571
    update_time_ms: 28.093
  timestamp: 1602736873
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: af50e_00000
  
2020-10-15 04:41:14,086	WARNING util.py:136 -- The `process_trial` operation took 0.5665247440338135 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    197 |          5050.27 | 31873024 |  298.196 |              333.354 |              160.172 |            772.487 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3015.3844554072352
    time_step_min: 2786
  date: 2020-10-15_04-41-39
  done: false
  episode_len_mean: 772.4833309175243
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 298.318063223919
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 199
  episodes_total: 41394
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.548969491444876e-30
        cur_lr: 5.0e-05
        entropy: 0.08404867226878802
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039014091598801315
        model: {}
        policy_loss: -0.008474887581542134
        total_loss: 0.936928595105807
        vf_explained_var: 0.9972718358039856
        vf_loss: 0.9454454978307089
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.170000000000005
    gpu_util_percent0: 0.3826666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670800445671162
    mean_env_wait_ms: 1.1997411891691916
    mean_inference_ms: 4.317757406791296
    mean_raw_obs_processing_ms: 0.3781851411532497
  time_since_restore: 5075.734868526459
  time_this_iter_s: 25.463332653045654
  time_total_s: 5075.734868526459
  timers:
    learn_throughput: 8714.747
    learn_time_ms: 18565.311
    sample_throughput: 23778.353
    sample_time_ms: 6804.172
    update_time_ms: 28.666
  timestamp: 1602736899
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: af50e_00000
  
2020-10-15 04:41:40,348	WARNING util.py:136 -- The `process_trial` operation took 0.6111702919006348 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    198 |          5075.73 | 32034816 |  298.318 |              333.354 |              160.172 |            772.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3014.6001637094637
    time_step_min: 2786
  date: 2020-10-15_04-42-06
  done: false
  episode_len_mean: 772.4846917915294
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 298.43727517849055
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 185
  episodes_total: 41579
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.274484745722438e-30
        cur_lr: 5.0e-05
        entropy: 0.08219874029358228
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036169000474425652
        model: {}
        policy_loss: -0.008744579919342263
        total_loss: 0.6770819276571274
        vf_explained_var: 0.9979226589202881
        vf_loss: 0.685867597659429
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.280000000000005
    gpu_util_percent0: 0.3443333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670492940958363
    mean_env_wait_ms: 1.1996901101429107
    mean_inference_ms: 4.317559582084262
    mean_raw_obs_processing_ms: 0.37817241152309233
  time_since_restore: 5101.473732471466
  time_this_iter_s: 25.738863945007324
  time_total_s: 5101.473732471466
  timers:
    learn_throughput: 8715.285
    learn_time_ms: 18564.167
    sample_throughput: 23737.508
    sample_time_ms: 6815.88
    update_time_ms: 26.238
  timestamp: 1602736926
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: af50e_00000
  
2020-10-15 04:42:06,841	WARNING util.py:136 -- The `process_trial` operation took 0.5616412162780762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    199 |          5101.47 | 32196608 |  298.437 |              333.354 |              160.172 |            772.485 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3013.5066539013883
    time_step_min: 2786
  date: 2020-10-15_04-42-32
  done: false
  episode_len_mean: 772.4870403137105
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 298.6015743007039
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 243
  episodes_total: 41822
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.137242372861219e-30
        cur_lr: 5.0e-05
        entropy: 0.0813807708521684
        entropy_coeff: 0.0005000000000000001
        kl: 0.003956505757135649
        model: {}
        policy_loss: -0.008534697842454383
        total_loss: 0.3513869022329648
        vf_explained_var: 0.999087393283844
        vf_loss: 0.35996229698260623
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.419999999999998
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670008529481984
    mean_env_wait_ms: 1.1996199149551068
    mean_inference_ms: 4.317269827923662
    mean_raw_obs_processing_ms: 0.37815336386565823
  time_since_restore: 5126.842403173447
  time_this_iter_s: 25.36867070198059
  time_total_s: 5126.842403173447
  timers:
    learn_throughput: 8731.577
    learn_time_ms: 18529.528
    sample_throughput: 23755.778
    sample_time_ms: 6810.638
    update_time_ms: 25.333
  timestamp: 1602736952
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: af50e_00000
  
2020-10-15 04:42:33,017	WARNING util.py:136 -- The `process_trial` operation took 0.6179044246673584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    200 |          5126.84 | 32358400 |  298.602 |              333.354 |              160.172 |            772.487 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3012.622013672201
    time_step_min: 2786
  date: 2020-10-15_04-42-58
  done: false
  episode_len_mean: 772.48749553837
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 298.73569604432174
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 203
  episodes_total: 42025
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.686211864306095e-31
        cur_lr: 5.0e-05
        entropy: 0.07549907080829144
        entropy_coeff: 0.0005000000000000001
        kl: 0.004830047604627907
        model: {}
        policy_loss: -0.008196346042192696
        total_loss: 0.501550130546093
        vf_explained_var: 0.9984626770019531
        vf_loss: 0.5097842266162237
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.60333333333333
    gpu_util_percent0: 0.3716666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466958987182289
    mean_env_wait_ms: 1.1995604696318158
    mean_inference_ms: 4.317023500567509
    mean_raw_obs_processing_ms: 0.37813871618037564
  time_since_restore: 5152.296957015991
  time_this_iter_s: 25.454553842544556
  time_total_s: 5152.296957015991
  timers:
    learn_throughput: 8738.506
    learn_time_ms: 18514.834
    sample_throughput: 23820.962
    sample_time_ms: 6792.001
    update_time_ms: 25.426
  timestamp: 1602736978
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: af50e_00000
  
2020-10-15 04:42:59,250	WARNING util.py:136 -- The `process_trial` operation took 0.5864148139953613 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    201 |           5152.3 | 32520192 |  298.736 |              333.354 |              160.172 |            772.487 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3011.8319656579074
    time_step_min: 2786
  date: 2020-10-15_04-43-24
  done: false
  episode_len_mean: 772.4902146614226
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 298.8522760180106
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 42206
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8431059321530475e-31
        cur_lr: 5.0e-05
        entropy: 0.07760083737472694
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032177720374117294
        model: {}
        policy_loss: -0.0055182093832020955
        total_loss: 0.7111246436834335
        vf_explained_var: 0.9979104995727539
        vf_loss: 0.7166816641887029
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.183333333333337
    gpu_util_percent0: 0.377
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669283505887393
    mean_env_wait_ms: 1.19950986493234
    mean_inference_ms: 4.31683514136237
    mean_raw_obs_processing_ms: 0.37812638277596033
  time_since_restore: 5178.014274358749
  time_this_iter_s: 25.71731734275818
  time_total_s: 5178.014274358749
  timers:
    learn_throughput: 8717.483
    learn_time_ms: 18559.486
    sample_throughput: 23891.452
    sample_time_ms: 6771.962
    update_time_ms: 24.562
  timestamp: 1602737004
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: af50e_00000
  
2020-10-15 04:43:25,719	WARNING util.py:136 -- The `process_trial` operation took 0.5595777034759521 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    202 |          5178.01 | 32681984 |  298.852 |              333.354 |              160.172 |             772.49 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3010.791968307119
    time_step_min: 2786
  date: 2020-10-15_04-43-51
  done: false
  episode_len_mean: 772.4962896652454
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.01019143352283
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 243
  episodes_total: 42449
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4215529660765238e-31
        cur_lr: 5.0e-05
        entropy: 0.08187570422887802
        entropy_coeff: 0.0005000000000000001
        kl: 0.004463149273457627
        model: {}
        policy_loss: -0.007938997451371202
        total_loss: 0.7481353729963303
        vf_explained_var: 0.998030424118042
        vf_loss: 0.7561153123776118
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.553333333333335
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668830041635794
    mean_env_wait_ms: 1.1994376375032172
    mean_inference_ms: 4.316552483231219
    mean_raw_obs_processing_ms: 0.3781079046680781
  time_since_restore: 5203.830141782761
  time_this_iter_s: 25.81586742401123
  time_total_s: 5203.830141782761
  timers:
    learn_throughput: 8715.644
    learn_time_ms: 18563.402
    sample_throughput: 23920.471
    sample_time_ms: 6763.747
    update_time_ms: 24.901
  timestamp: 1602737031
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: af50e_00000
  
2020-10-15 04:43:52,356	WARNING util.py:136 -- The `process_trial` operation took 0.5656173229217529 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    203 |          5203.83 | 32843776 |   299.01 |              333.354 |              160.172 |            772.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3009.980147369409
    time_step_min: 2786
  date: 2020-10-15_04-44-17
  done: false
  episode_len_mean: 772.509986871718
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.1402696791621
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 207
  episodes_total: 42656
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.107764830382619e-32
        cur_lr: 5.0e-05
        entropy: 0.0739387609064579
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00889684603730719
        total_loss: .inf
        vf_explained_var: 0.9972040057182312
        vf_loss: 0.9694284995396932
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683333333333334
    gpu_util_percent0: 0.33966666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668414906374153
    mean_env_wait_ms: 1.199376685460453
    mean_inference_ms: 4.316314789695307
    mean_raw_obs_processing_ms: 0.3780934489780017
  time_since_restore: 5229.431935071945
  time_this_iter_s: 25.60179328918457
  time_total_s: 5229.431935071945
  timers:
    learn_throughput: 8724.131
    learn_time_ms: 18545.342
    sample_throughput: 23900.04
    sample_time_ms: 6769.529
    update_time_ms: 26.331
  timestamp: 1602737057
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:44:18,768	WARNING util.py:136 -- The `process_trial` operation took 0.612558126449585 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    204 |          5229.43 | 33005568 |   299.14 |              333.354 |              160.172 |             772.51 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3009.2689224873225
    time_step_min: 2786
  date: 2020-10-15_04-44-44
  done: false
  episode_len_mean: 772.5200420217112
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.24629509758483
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 179
  episodes_total: 42835
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.066164724557393e-31
        cur_lr: 5.0e-05
        entropy: 0.07682287506759167
        entropy_coeff: 0.0005000000000000001
        kl: 0.003175856875410924
        model: {}
        policy_loss: -0.006828977687594791
        total_loss: 1.043194775780042
        vf_explained_var: 0.9968246817588806
        vf_loss: 1.0500621894995372
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.000000000000004
    gpu_util_percent0: 0.3766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146681281470317
    mean_env_wait_ms: 1.1993254870632706
    mean_inference_ms: 4.316131713737638
    mean_raw_obs_processing_ms: 0.37808133628835244
  time_since_restore: 5255.095628023148
  time_this_iter_s: 25.663692951202393
  time_total_s: 5255.095628023148
  timers:
    learn_throughput: 8717.453
    learn_time_ms: 18559.549
    sample_throughput: 23903.928
    sample_time_ms: 6768.427
    update_time_ms: 26.796
  timestamp: 1602737084
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: af50e_00000
  
2020-10-15 04:44:45,220	WARNING util.py:136 -- The `process_trial` operation took 0.5943286418914795 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    205 |           5255.1 | 33167360 |  299.246 |              333.354 |              160.172 |             772.52 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3008.3292739611416
    time_step_min: 2786
  date: 2020-10-15_04-45-10
  done: false
  episode_len_mean: 772.5325516600882
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.39078619958576
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 235
  episodes_total: 43070
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.330823622786965e-32
        cur_lr: 5.0e-05
        entropy: 0.08357197853426139
        entropy_coeff: 0.0005000000000000001
        kl: 0.005337033420801163
        model: {}
        policy_loss: -0.00907587504965098
        total_loss: 0.7073910584052404
        vf_explained_var: 0.9981546998023987
        vf_loss: 0.7165087511142095
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.960000000000004
    gpu_util_percent0: 0.3466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466771656111108
    mean_env_wait_ms: 1.1992537803843812
    mean_inference_ms: 4.315872382152374
    mean_raw_obs_processing_ms: 0.37806434938727085
  time_since_restore: 5280.629150867462
  time_this_iter_s: 25.533522844314575
  time_total_s: 5280.629150867462
  timers:
    learn_throughput: 8723.241
    learn_time_ms: 18547.236
    sample_throughput: 23922.165
    sample_time_ms: 6763.268
    update_time_ms: 27.332
  timestamp: 1602737110
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: af50e_00000
  
2020-10-15 04:45:11,523	WARNING util.py:136 -- The `process_trial` operation took 0.569554328918457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    206 |          5280.63 | 33329152 |  299.391 |              333.354 |              160.172 |            772.533 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3007.466817740369
    time_step_min: 2786
  date: 2020-10-15_04-45-37
  done: false
  episode_len_mean: 772.5378626871188
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.521011958431
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 218
  episodes_total: 43288
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.330823622786965e-32
        cur_lr: 5.0e-05
        entropy: 0.0802230400343736
        entropy_coeff: 0.0005000000000000001
        kl: 0.005888586242993672
        model: {}
        policy_loss: -0.008458975925653553
        total_loss: 0.4825522502263387
        vf_explained_var: 0.9986522197723389
        vf_loss: 0.4910513535141945
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.296666666666674
    gpu_util_percent0: 0.37000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667278736842512
    mean_env_wait_ms: 1.1991895071857444
    mean_inference_ms: 4.315624637978664
    mean_raw_obs_processing_ms: 0.378048908599477
  time_since_restore: 5306.223612546921
  time_this_iter_s: 25.594461679458618
  time_total_s: 5306.223612546921
  timers:
    learn_throughput: 8720.89
    learn_time_ms: 18552.234
    sample_throughput: 23917.575
    sample_time_ms: 6764.565
    update_time_ms: 29.187
  timestamp: 1602737137
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: af50e_00000
  
2020-10-15 04:45:37,937	WARNING util.py:136 -- The `process_trial` operation took 0.6046435832977295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    207 |          5306.22 | 33490944 |  299.521 |              333.354 |              160.172 |            772.538 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3006.7753442955186
    time_step_min: 2786
  date: 2020-10-15_04-46-03
  done: false
  episode_len_mean: 772.5426559911651
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.6285473453474
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 176
  episodes_total: 43464
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.330823622786965e-32
        cur_lr: 5.0e-05
        entropy: 0.08125214589138825
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009541347969085715
        total_loss: .inf
        vf_explained_var: 0.9982423782348633
        vf_loss: 0.5537614127000173
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.196666666666673
    gpu_util_percent0: 0.2780000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667011548521697
    mean_env_wait_ms: 1.1991378779801458
    mean_inference_ms: 4.315450941734174
    mean_raw_obs_processing_ms: 0.37803737575603386
  time_since_restore: 5331.810578584671
  time_this_iter_s: 25.586966037750244
  time_total_s: 5331.810578584671
  timers:
    learn_throughput: 8713.076
    learn_time_ms: 18568.874
    sample_throughput: 23934.473
    sample_time_ms: 6759.789
    update_time_ms: 29.392
  timestamp: 1602737163
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:46:04,380	WARNING util.py:136 -- The `process_trial` operation took 0.570622444152832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    208 |          5331.81 | 33652736 |  299.629 |              333.354 |              160.172 |            772.543 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3005.840999679296
    time_step_min: 2786
  date: 2020-10-15_04-46-29
  done: false
  episode_len_mean: 772.549295129989
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.7671450868996
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 232
  episodes_total: 43696
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.996235434180445e-32
        cur_lr: 5.0e-05
        entropy: 0.08646308692793052
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008480735680980919
        total_loss: .inf
        vf_explained_var: 0.9979528784751892
        vf_loss: 0.8046826720237732
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.923333333333336
    gpu_util_percent0: 0.348
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666597759779895
    mean_env_wait_ms: 1.1990661023882196
    mean_inference_ms: 4.315199749305521
    mean_raw_obs_processing_ms: 0.3780210736675929
  time_since_restore: 5357.410281181335
  time_this_iter_s: 25.59970259666443
  time_total_s: 5357.410281181335
  timers:
    learn_throughput: 8717.015
    learn_time_ms: 18560.482
    sample_throughput: 23962.271
    sample_time_ms: 6751.948
    update_time_ms: 30.777
  timestamp: 1602737189
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:46:30,850	WARNING util.py:136 -- The `process_trial` operation took 0.6093451976776123 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    209 |          5357.41 | 33814528 |  299.767 |              333.354 |              160.172 |            772.549 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3004.920299929348
    time_step_min: 2786
  date: 2020-10-15_04-46-56
  done: false
  episode_len_mean: 772.5537466700061
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 299.9038963141742
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 223
  episodes_total: 43919
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.199435315127067e-31
        cur_lr: 5.0e-05
        entropy: 0.08332967385649681
        entropy_coeff: 0.0005000000000000001
        kl: 0.008367107948288321
        model: {}
        policy_loss: -0.00904320205639427
        total_loss: 0.39955837776263553
        vf_explained_var: 0.9988482594490051
        vf_loss: 0.4086432432134946
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.783333333333335
    gpu_util_percent0: 0.3203333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466617887591208
    mean_env_wait_ms: 1.1990004888444135
    mean_inference_ms: 4.314958735865519
    mean_raw_obs_processing_ms: 0.3780058064949176
  time_since_restore: 5383.154265642166
  time_this_iter_s: 25.74398446083069
  time_total_s: 5383.154265642166
  timers:
    learn_throughput: 8703.388
    learn_time_ms: 18589.543
    sample_throughput: 23944.413
    sample_time_ms: 6756.983
    update_time_ms: 33.301
  timestamp: 1602737216
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: af50e_00000
  
2020-10-15 04:46:57,495	WARNING util.py:136 -- The `process_trial` operation took 0.6137845516204834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    210 |          5383.15 | 33976320 |  299.904 |              333.354 |              160.172 |            772.554 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3004.2441886860984
    time_step_min: 2786
  date: 2020-10-15_04-47-23
  done: false
  episode_len_mean: 772.5559940127908
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.003533085653
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 175
  episodes_total: 44094
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.199435315127067e-31
        cur_lr: 5.0e-05
        entropy: 0.08086940087378025
        entropy_coeff: 0.0005000000000000001
        kl: 0.004318808399451275
        model: {}
        policy_loss: -0.009590542298004342
        total_loss: 0.7246356308460236
        vf_explained_var: 0.9977390170097351
        vf_loss: 0.7342665940523148
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.845161290322583
    gpu_util_percent0: 0.3277419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466589616229178
    mean_env_wait_ms: 1.1989483560476424
    mean_inference_ms: 4.314786016774545
    mean_raw_obs_processing_ms: 0.37799429861874556
  time_since_restore: 5409.043840408325
  time_this_iter_s: 25.889574766159058
  time_total_s: 5409.043840408325
  timers:
    learn_throughput: 8687.211
    learn_time_ms: 18624.158
    sample_throughput: 23889.998
    sample_time_ms: 6772.374
    update_time_ms: 33.845
  timestamp: 1602737243
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: af50e_00000
  
2020-10-15 04:47:24,187	WARNING util.py:136 -- The `process_trial` operation took 0.6035542488098145 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    211 |          5409.04 | 34138112 |  300.004 |              333.354 |              160.172 |            772.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3003.3836164061795
    time_step_min: 2786
  date: 2020-10-15_04-47-49
  done: false
  episode_len_mean: 772.55593663974
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.1343084712369
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 44318
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.997176575635335e-32
        cur_lr: 5.0e-05
        entropy: 0.08912884630262852
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010226362520673623
        total_loss: .inf
        vf_explained_var: 0.9983426928520203
        vf_loss: 0.6306500931580862
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02333333333333
    gpu_util_percent0: 0.2803333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466555851829792
    mean_env_wait_ms: 1.198879124971271
    mean_inference_ms: 4.314555716339085
    mean_raw_obs_processing_ms: 0.3779791389245714
  time_since_restore: 5434.470282077789
  time_this_iter_s: 25.42644166946411
  time_total_s: 5434.470282077789
  timers:
    learn_throughput: 8699.098
    learn_time_ms: 18598.711
    sample_throughput: 23901.526
    sample_time_ms: 6769.108
    update_time_ms: 32.534
  timestamp: 1602737269
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:47:50,584	WARNING util.py:136 -- The `process_trial` operation took 0.6069498062133789 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    212 |          5434.47 | 34299904 |  300.134 |              333.354 |              160.172 |            772.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3002.521255561048
    time_step_min: 2786
  date: 2020-10-15_04-48-16
  done: false
  episode_len_mean: 772.5598904552393
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.26364049038455
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 230
  episodes_total: 44548
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.995764863453004e-32
        cur_lr: 5.0e-05
        entropy: 0.08581067373355229
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007931636178909685
        total_loss: .inf
        vf_explained_var: 0.9984632134437561
        vf_loss: 0.5607765962680181
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.933333333333337
    gpu_util_percent0: 0.33566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466511283025349
    mean_env_wait_ms: 1.1988106284566102
    mean_inference_ms: 4.314309263750162
    mean_raw_obs_processing_ms: 0.37796370984433214
  time_since_restore: 5460.103731632233
  time_this_iter_s: 25.63344955444336
  time_total_s: 5460.103731632233
  timers:
    learn_throughput: 8704.49
    learn_time_ms: 18587.19
    sample_throughput: 23920.358
    sample_time_ms: 6763.778
    update_time_ms: 30.47
  timestamp: 1602737296
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:48:17,042	WARNING util.py:136 -- The `process_trial` operation took 0.6203231811523438 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    213 |           5460.1 | 34461696 |  300.264 |              333.354 |              160.172 |             772.56 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3001.889794091316
    time_step_min: 2786
  date: 2020-10-15_04-48-42
  done: false
  episode_len_mean: 772.5652698895399
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.3632167116357
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 174
  episodes_total: 44722
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3493647295179501e-31
        cur_lr: 5.0e-05
        entropy: 0.07917071754733722
        entropy_coeff: 0.0005000000000000001
        kl: 0.004374430126821001
        model: {}
        policy_loss: -0.00800066157777716
        total_loss: 0.4681067441900571
        vf_explained_var: 0.9984709620475769
        vf_loss: 0.47614700595537823
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48709677419355
    gpu_util_percent0: 0.34516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466482242126467
    mean_env_wait_ms: 1.1987588766721746
    mean_inference_ms: 4.314142459267215
    mean_raw_obs_processing_ms: 0.37795281546665843
  time_since_restore: 5485.8543612957
  time_this_iter_s: 25.750629663467407
  time_total_s: 5485.8543612957
  timers:
    learn_throughput: 8698.114
    learn_time_ms: 18600.815
    sample_throughput: 23914.448
    sample_time_ms: 6765.45
    update_time_ms: 29.801
  timestamp: 1602737322
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: af50e_00000
  
2020-10-15 04:48:43,790	WARNING util.py:136 -- The `process_trial` operation took 0.6307756900787354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    214 |          5485.85 | 34623488 |  300.363 |              333.354 |              160.172 |            772.565 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3001.0869187600215
    time_step_min: 2786
  date: 2020-10-15_04-49-09
  done: false
  episode_len_mean: 772.5743336448182
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.4843145107461
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 44946
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.746823647589751e-32
        cur_lr: 5.0e-05
        entropy: 0.08464421952764194
        entropy_coeff: 0.0005000000000000001
        kl: 0.004289009841158986
        model: {}
        policy_loss: -0.009985922360404706
        total_loss: 0.6373470028241476
        vf_explained_var: 0.9982917904853821
        vf_loss: 0.6473752558231354
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.886666666666674
    gpu_util_percent0: 0.3366666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664502813203428
    mean_env_wait_ms: 1.1986898896750506
    mean_inference_ms: 4.313918143830104
    mean_raw_obs_processing_ms: 0.37793836151464155
  time_since_restore: 5511.444545030594
  time_this_iter_s: 25.5901837348938
  time_total_s: 5511.444545030594
  timers:
    learn_throughput: 8705.196
    learn_time_ms: 18585.682
    sample_throughput: 23896.994
    sample_time_ms: 6770.391
    update_time_ms: 31.089
  timestamp: 1602737349
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: af50e_00000
  
2020-10-15 04:49:10,169	WARNING util.py:136 -- The `process_trial` operation took 0.586113452911377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    215 |          5511.44 | 34785280 |  300.484 |              333.354 |              160.172 |            772.574 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 3000.2371657102344
    time_step_min: 2786
  date: 2020-10-15_04-49-35
  done: false
  episode_len_mean: 772.5836192584394
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.61488934726333
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 45175
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3734118237948753e-32
        cur_lr: 5.0e-05
        entropy: 0.0774636622518301
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040430462880370515
        model: {}
        policy_loss: -0.006858938065609739
        total_loss: 0.5665021439393362
        vf_explained_var: 0.9985322952270508
        vf_loss: 0.5733998119831085
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21666666666666
    gpu_util_percent0: 0.33766666666666656
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664086722105288
    mean_env_wait_ms: 1.198621307227354
    mean_inference_ms: 4.313679950245975
    mean_raw_obs_processing_ms: 0.3779233488460608
  time_since_restore: 5536.944550514221
  time_this_iter_s: 25.50000548362732
  time_total_s: 5536.944550514221
  timers:
    learn_throughput: 8707.448
    learn_time_ms: 18580.874
    sample_throughput: 23899.88
    sample_time_ms: 6769.574
    update_time_ms: 30.598
  timestamp: 1602737375
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: af50e_00000
  
2020-10-15 04:49:36,519	WARNING util.py:136 -- The `process_trial` operation took 0.6400728225708008 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    216 |          5536.94 | 34947072 |  300.615 |              333.354 |              160.172 |            772.584 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2999.59833587146
    time_step_min: 2786
  date: 2020-10-15_04-50-02
  done: false
  episode_len_mean: 772.5948490661727
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.7133677183289
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 176
  episodes_total: 45351
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6867059118974376e-32
        cur_lr: 5.0e-05
        entropy: 0.07219382251302402
        entropy_coeff: 0.0005000000000000001
        kl: 0.00311658822465688
        model: {}
        policy_loss: -0.007339941774262115
        total_loss: 0.38263236979643506
        vf_explained_var: 0.9987851977348328
        vf_loss: 0.3900084023674329
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.299999999999997
    gpu_util_percent0: 0.3443333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466377094875365
    mean_env_wait_ms: 1.19856789544061
    mean_inference_ms: 4.313513362429159
    mean_raw_obs_processing_ms: 0.3779124885207321
  time_since_restore: 5562.439532518387
  time_this_iter_s: 25.49498200416565
  time_total_s: 5562.439532518387
  timers:
    learn_throughput: 8708.169
    learn_time_ms: 18579.335
    sample_throughput: 23918.208
    sample_time_ms: 6764.386
    update_time_ms: 29.015
  timestamp: 1602737402
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: af50e_00000
  
2020-10-15 04:50:02,893	WARNING util.py:136 -- The `process_trial` operation took 0.6668505668640137 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    217 |          5562.44 | 35108864 |  300.713 |              333.354 |              160.172 |            772.595 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2998.834142483689
    time_step_min: 2786
  date: 2020-10-15_04-50-28
  done: false
  episode_len_mean: 772.6053376643329
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.8306682477829
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 45563
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.433529559487188e-33
        cur_lr: 5.0e-05
        entropy: 0.07769691323240598
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009281944890972227
        total_loss: .inf
        vf_explained_var: 0.9986528754234314
        vf_loss: 0.48352737973133725
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.083333333333332
    gpu_util_percent0: 0.35966666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663500441009752
    mean_env_wait_ms: 1.1985024787769112
    mean_inference_ms: 4.313313937093346
    mean_raw_obs_processing_ms: 0.3778995627827202
  time_since_restore: 5587.871110916138
  time_this_iter_s: 25.431578397750854
  time_total_s: 5587.871110916138
  timers:
    learn_throughput: 8716.126
    learn_time_ms: 18562.375
    sample_throughput: 23916.896
    sample_time_ms: 6764.757
    update_time_ms: 29.384
  timestamp: 1602737428
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:50:29,164	WARNING util.py:136 -- The `process_trial` operation took 0.6281464099884033 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    218 |          5587.87 | 35270656 |  300.831 |              333.354 |              160.172 |            772.605 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2998.033567900695
    time_step_min: 2786
  date: 2020-10-15_04-50-54
  done: false
  episode_len_mean: 772.6254148471616
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 300.9594570155704
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 237
  episodes_total: 45800
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2650294339230784e-32
        cur_lr: 5.0e-05
        entropy: 0.0741692055016756
        entropy_coeff: 0.0005000000000000001
        kl: 0.004079193051438779
        model: {}
        policy_loss: -0.006294836998373891
        total_loss: 0.7776241352160772
        vf_explained_var: 0.9980457425117493
        vf_loss: 0.7839560558398565
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.253333333333337
    gpu_util_percent0: 0.323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663087282150603
    mean_env_wait_ms: 1.1984310272482652
    mean_inference_ms: 4.313068498701238
    mean_raw_obs_processing_ms: 0.37788420252651916
  time_since_restore: 5613.579713582993
  time_this_iter_s: 25.70860266685486
  time_total_s: 5613.579713582993
  timers:
    learn_throughput: 8711.061
    learn_time_ms: 18573.167
    sample_throughput: 23916.507
    sample_time_ms: 6764.868
    update_time_ms: 28.366
  timestamp: 1602737454
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: af50e_00000
  
2020-10-15 04:50:55,717	WARNING util.py:136 -- The `process_trial` operation took 0.6155352592468262 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    219 |          5613.58 | 35432448 |  300.959 |              333.354 |              160.172 |            772.625 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2997.350268834759
    time_step_min: 2786
  date: 2020-10-15_04-51-21
  done: false
  episode_len_mean: 772.6336095343729
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.0595806041097
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 45981
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.325147169615392e-33
        cur_lr: 5.0e-05
        entropy: 0.07033694845934708
        entropy_coeff: 0.0005000000000000001
        kl: 0.00400311224317799
        model: {}
        policy_loss: -0.0071182446942354245
        total_loss: 0.3822120229403178
        vf_explained_var: 0.9988234043121338
        vf_loss: 0.38936542471249896
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81290322580646
    gpu_util_percent0: 0.3177419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662753931504335
    mean_env_wait_ms: 1.1983756258818203
    mean_inference_ms: 4.3129006775074705
    mean_raw_obs_processing_ms: 0.3778729723863431
  time_since_restore: 5639.140555143356
  time_this_iter_s: 25.56084156036377
  time_total_s: 5639.140555143356
  timers:
    learn_throughput: 8718.538
    learn_time_ms: 18557.239
    sample_throughput: 23919.686
    sample_time_ms: 6763.968
    update_time_ms: 25.9
  timestamp: 1602737481
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: af50e_00000
  
2020-10-15 04:51:22,309	WARNING util.py:136 -- The `process_trial` operation took 0.6456975936889648 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    220 |          5639.14 | 35594240 |   301.06 |              333.354 |              160.172 |            772.634 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2996.5701685953277
    time_step_min: 2786
  date: 2020-10-15_04-51-47
  done: false
  episode_len_mean: 772.6518576253573
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.1794923776606
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 207
  episodes_total: 46188
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.162573584807696e-33
        cur_lr: 5.0e-05
        entropy: 0.07101757079362869
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00798392220531241
        total_loss: .inf
        vf_explained_var: 0.9989599585533142
        vf_loss: 0.3645216077566147
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.393333333333334
    gpu_util_percent0: 0.313
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662466415157596
    mean_env_wait_ms: 1.198311395953566
    mean_inference_ms: 4.312717894502
    mean_raw_obs_processing_ms: 0.37786068869949685
  time_since_restore: 5664.803081989288
  time_this_iter_s: 25.662526845932007
  time_total_s: 5664.803081989288
  timers:
    learn_throughput: 8724.073
    learn_time_ms: 18545.465
    sample_throughput: 23959.916
    sample_time_ms: 6752.611
    update_time_ms: 25.064
  timestamp: 1602737507
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:51:48,828	WARNING util.py:136 -- The `process_trial` operation took 0.6439900398254395 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    221 |           5664.8 | 35756032 |  301.179 |              333.354 |              160.172 |            772.652 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2995.6901183725395
    time_step_min: 2786
  date: 2020-10-15_04-52-14
  done: false
  episode_len_mean: 772.6702569957562
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.3070183970637
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 233
  episodes_total: 46421
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.7438603772115445e-33
        cur_lr: 5.0e-05
        entropy: 0.06838048063218594
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008361432128973926
        total_loss: .inf
        vf_explained_var: 0.9988391995429993
        vf_loss: 0.4439869523048401
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.910000000000004
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466209259730835
    mean_env_wait_ms: 1.1982412757267604
    mean_inference_ms: 4.312483679225364
    mean_raw_obs_processing_ms: 0.3778457675800035
  time_since_restore: 5690.518139362335
  time_this_iter_s: 25.715057373046875
  time_total_s: 5690.518139362335
  timers:
    learn_throughput: 8718.58
    learn_time_ms: 18557.15
    sample_throughput: 23910.349
    sample_time_ms: 6766.61
    update_time_ms: 26.574
  timestamp: 1602737534
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:52:15,388	WARNING util.py:136 -- The `process_trial` operation took 0.6301002502441406 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    222 |          5690.52 | 35917824 |  301.307 |              333.354 |              160.172 |             772.67 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2994.981081835556
    time_step_min: 2786
  date: 2020-10-15_04-52-40
  done: false
  episode_len_mean: 772.6890862671902
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.4157335730997
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 46611
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.115790565817315e-33
        cur_lr: 5.0e-05
        entropy: 0.0671351036677758
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007749165204586461
        total_loss: .inf
        vf_explained_var: 0.9993907809257507
        vf_loss: 0.19883855059742928
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58
    gpu_util_percent0: 0.3163333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661773036500766
    mean_env_wait_ms: 1.1981820310869467
    mean_inference_ms: 4.312308087925065
    mean_raw_obs_processing_ms: 0.3778343365162783
  time_since_restore: 5715.937300920486
  time_this_iter_s: 25.419161558151245
  time_total_s: 5715.937300920486
  timers:
    learn_throughput: 8729.3
    learn_time_ms: 18534.362
    sample_throughput: 23907.172
    sample_time_ms: 6767.509
    update_time_ms: 26.603
  timestamp: 1602737560
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:52:41,664	WARNING util.py:136 -- The `process_trial` operation took 0.6516921520233154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    223 |          5715.94 | 36079616 |  301.416 |              333.354 |              160.172 |            772.689 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2994.277515235753
    time_step_min: 2786
  date: 2020-10-15_04-53-07
  done: false
  episode_len_mean: 772.7027581344671
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.5207740014712
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 46807
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0673685848725976e-32
        cur_lr: 5.0e-05
        entropy: 0.07181602281828721
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009220608723505089
        total_loss: .inf
        vf_explained_var: 0.9985305666923523
        vf_loss: 0.5067199890812238
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.050000000000004
    gpu_util_percent0: 0.3100000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466150887764172
    mean_env_wait_ms: 1.1981217208757882
    mean_inference_ms: 4.3121414014677795
    mean_raw_obs_processing_ms: 0.3778236558494031
  time_since_restore: 5741.628577709198
  time_this_iter_s: 25.691276788711548
  time_total_s: 5741.628577709198
  timers:
    learn_throughput: 8730.896
    learn_time_ms: 18530.972
    sample_throughput: 23920.408
    sample_time_ms: 6763.764
    update_time_ms: 25.641
  timestamp: 1602737587
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:53:08,265	WARNING util.py:136 -- The `process_trial` operation took 0.6900701522827148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    224 |          5741.63 | 36241408 |  301.521 |              333.354 |              160.172 |            772.703 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2993.4396434346013
    time_step_min: 2786
  date: 2020-10-15_04-53-33
  done: false
  episode_len_mean: 772.7264592101347
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.6506823109296
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 239
  episodes_total: 47046
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6010528773088958e-32
        cur_lr: 5.0e-05
        entropy: 0.0702562319735686
        entropy_coeff: 0.0005000000000000001
        kl: 0.004128685609127085
        model: {}
        policy_loss: -0.006172163334364693
        total_loss: 0.3267214472095172
        vf_explained_var: 0.9991336464881897
        vf_loss: 0.33292875190575916
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.554838709677416
    gpu_util_percent0: 0.3274193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661133243250515
    mean_env_wait_ms: 1.1980481309550504
    mean_inference_ms: 4.311912042703546
    mean_raw_obs_processing_ms: 0.37780813738903773
  time_since_restore: 5767.33233165741
  time_this_iter_s: 25.70375394821167
  time_total_s: 5767.33233165741
  timers:
    learn_throughput: 8725.32
    learn_time_ms: 18542.815
    sample_throughput: 23926.47
    sample_time_ms: 6762.05
    update_time_ms: 25.604
  timestamp: 1602737613
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: af50e_00000
  
2020-10-15 04:53:34,991	WARNING util.py:136 -- The `process_trial` operation took 0.6314067840576172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    225 |          5767.33 | 36403200 |  301.651 |              333.354 |              160.172 |            772.726 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2992.692775423729
    time_step_min: 2786
  date: 2020-10-15_04-54-00
  done: false
  episode_len_mean: 772.7461580796748
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.75842160652275
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 47242
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.005264386544479e-33
        cur_lr: 5.0e-05
        entropy: 0.07072540062169234
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006854162809759146
        total_loss: .inf
        vf_explained_var: 0.9990424513816833
        vf_loss: 0.31508712470531464
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58
    gpu_util_percent0: 0.3063333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660806492969813
    mean_env_wait_ms: 1.1979863501019896
    mean_inference_ms: 4.311729405187244
    mean_raw_obs_processing_ms: 0.3777967085807678
  time_since_restore: 5793.006418228149
  time_this_iter_s: 25.674086570739746
  time_total_s: 5793.006418228149
  timers:
    learn_throughput: 8719.082
    learn_time_ms: 18556.081
    sample_throughput: 23912.992
    sample_time_ms: 6765.862
    update_time_ms: 26.608
  timestamp: 1602737640
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:54:01,532	WARNING util.py:136 -- The `process_trial` operation took 0.6530807018280029 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    226 |          5793.01 | 36564992 |  301.758 |              333.354 |              160.172 |            772.746 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2992.051175452665
    time_step_min: 2786
  date: 2020-10-15_04-54-27
  done: false
  episode_len_mean: 772.7587290208315
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.8560853112382
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 47428
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2007896579816718e-32
        cur_lr: 5.0e-05
        entropy: 0.07413714751601219
        entropy_coeff: 0.0005000000000000001
        kl: 0.003713706935135027
        model: {}
        policy_loss: -0.00908201394486241
        total_loss: 0.7615623623132706
        vf_explained_var: 0.9976680278778076
        vf_loss: 0.770681435863177
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82333333333333
    gpu_util_percent0: 0.3423333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660581904956357
    mean_env_wait_ms: 1.1979292171225155
    mean_inference_ms: 4.311577601248418
    mean_raw_obs_processing_ms: 0.3777868846089709
  time_since_restore: 5818.698605298996
  time_this_iter_s: 25.692187070846558
  time_total_s: 5818.698605298996
  timers:
    learn_throughput: 8711.875
    learn_time_ms: 18571.432
    sample_throughput: 23898.943
    sample_time_ms: 6769.839
    update_time_ms: 26.304
  timestamp: 1602737667
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: af50e_00000
  
2020-10-15 04:54:28,097	WARNING util.py:136 -- The `process_trial` operation took 0.6525149345397949 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    227 |           5818.7 | 36726784 |  301.856 |              333.354 |              160.172 |            772.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2991.2457009679383
    time_step_min: 2786
  date: 2020-10-15_04-54-53
  done: false
  episode_len_mean: 772.7749900354528
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 301.98034764562266
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 241
  episodes_total: 47669
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.003948289908359e-33
        cur_lr: 5.0e-05
        entropy: 0.07517131107548873
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008168107669916935
        total_loss: .inf
        vf_explained_var: 0.9986891746520996
        vf_loss: 0.5096019233266512
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380000000000003
    gpu_util_percent0: 0.30733333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466020797443651
    mean_env_wait_ms: 1.1978539666610133
    mean_inference_ms: 4.31135515554841
    mean_raw_obs_processing_ms: 0.3777716217408568
  time_since_restore: 5844.365365028381
  time_this_iter_s: 25.666759729385376
  time_total_s: 5844.365365028381
  timers:
    learn_throughput: 8701.693
    learn_time_ms: 18593.163
    sample_throughput: 23884.171
    sample_time_ms: 6774.026
    update_time_ms: 24.945
  timestamp: 1602737693
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:54:54,641	WARNING util.py:136 -- The `process_trial` operation took 0.658306360244751 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    228 |          5844.37 | 36888576 |   301.98 |              333.354 |              160.172 |            772.775 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2990.5452036463994
    time_step_min: 2786
  date: 2020-10-15_04-55-20
  done: false
  episode_len_mean: 772.786505118028
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.08473285181026
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 201
  episodes_total: 47870
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.00592243486254e-33
        cur_lr: 5.0e-05
        entropy: 0.06831942809124787
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034743903282408914
        model: {}
        policy_loss: -0.006495478650322184
        total_loss: 0.6223229815562566
        vf_explained_var: 0.9982033371925354
        vf_loss: 0.6288526356220245
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.296666666666674
    gpu_util_percent0: 0.3166666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659872544334077
    mean_env_wait_ms: 1.1977906113028418
    mean_inference_ms: 4.311169444608925
    mean_raw_obs_processing_ms: 0.37775999339167643
  time_since_restore: 5869.83148765564
  time_this_iter_s: 25.4661226272583
  time_total_s: 5869.83148765564
  timers:
    learn_throughput: 8711.917
    learn_time_ms: 18571.343
    sample_throughput: 23893.582
    sample_time_ms: 6771.358
    update_time_ms: 24.451
  timestamp: 1602737720
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: af50e_00000
  
2020-10-15 04:55:21,027	WARNING util.py:136 -- The `process_trial` operation took 0.6403079032897949 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    229 |          5869.83 | 37050368 |  302.085 |              333.354 |              160.172 |            772.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2989.92560193285
    time_step_min: 2786
  date: 2020-10-15_04-55-46
  done: false
  episode_len_mean: 772.800162317393
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.1783738243969
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 48054
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.50296121743127e-33
        cur_lr: 5.0e-05
        entropy: 0.07100805391867955
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037931738576541343
        model: {}
        policy_loss: -0.006210955170293649
        total_loss: 0.36073973526557285
        vf_explained_var: 0.9988751411437988
        vf_loss: 0.36698619027932483
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.91612903225807
    gpu_util_percent0: 0.37193548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465965764015943
    mean_env_wait_ms: 1.1977337625579474
    mean_inference_ms: 4.311021468115131
    mean_raw_obs_processing_ms: 0.3777501827255827
  time_since_restore: 5895.586373329163
  time_this_iter_s: 25.75488567352295
  time_total_s: 5895.586373329163
  timers:
    learn_throughput: 8702.989
    learn_time_ms: 18590.395
    sample_throughput: 23895.374
    sample_time_ms: 6770.85
    update_time_ms: 24.64
  timestamp: 1602737746
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: af50e_00000
  
2020-10-15 04:55:47,826	WARNING util.py:136 -- The `process_trial` operation took 0.625424861907959 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    230 |          5895.59 | 37212160 |  302.178 |              333.354 |              160.172 |              772.8 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2989.1120414507773
    time_step_min: 2786
  date: 2020-10-15_04-56-13
  done: false
  episode_len_mean: 772.8129296777935
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.2974315757591
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 238
  episodes_total: 48292
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.251480608715635e-33
        cur_lr: 5.0e-05
        entropy: 0.07862081378698349
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008906708929013499
        total_loss: .inf
        vf_explained_var: 0.9987154006958008
        vf_loss: 0.5064521282911301
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02
    gpu_util_percent0: 0.28
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659310895849786
    mean_env_wait_ms: 1.1976583522102169
    mean_inference_ms: 4.310812471434603
    mean_raw_obs_processing_ms: 0.37773580692064385
  time_since_restore: 5921.062291145325
  time_this_iter_s: 25.47591781616211
  time_total_s: 5921.062291145325
  timers:
    learn_throughput: 8708.152
    learn_time_ms: 18579.373
    sample_throughput: 23922.133
    sample_time_ms: 6763.277
    update_time_ms: 24.56
  timestamp: 1602737773
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:56:14,355	WARNING util.py:136 -- The `process_trial` operation took 0.6601169109344482 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    231 |          5921.06 | 37373952 |  302.297 |              333.354 |              160.172 |            772.813 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2988.4073340349573
    time_step_min: 2786
  date: 2020-10-15_04-56-40
  done: false
  episode_len_mean: 772.8306839034246
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.40425470765035
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 209
  episodes_total: 48501
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.377220913073452e-33
        cur_lr: 5.0e-05
        entropy: 0.0795582290738821
        entropy_coeff: 0.0005000000000000001
        kl: 0.006242285404975216
        model: {}
        policy_loss: -0.008458546624751762
        total_loss: 0.37692944208780926
        vf_explained_var: 0.9988725781440735
        vf_loss: 0.38542776306470233
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.406666666666673
    gpu_util_percent0: 0.2923333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465895941137476
    mean_env_wait_ms: 1.1975922950033568
    mean_inference_ms: 4.31062430612409
    mean_raw_obs_processing_ms: 0.37772410566973036
  time_since_restore: 5946.785794973373
  time_this_iter_s: 25.723503828048706
  time_total_s: 5946.785794973373
  timers:
    learn_throughput: 8703.438
    learn_time_ms: 18589.436
    sample_throughput: 23951.934
    sample_time_ms: 6754.862
    update_time_ms: 23.585
  timestamp: 1602737800
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: af50e_00000
  
2020-10-15 04:56:40,987	WARNING util.py:136 -- The `process_trial` operation took 0.6818637847900391 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    232 |          5946.79 | 37535744 |  302.404 |              333.354 |              160.172 |            772.831 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2987.831859862659
    time_step_min: 2786
  date: 2020-10-15_04-57-06
  done: false
  episode_len_mean: 772.845377978636
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.4931328486175
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 179
  episodes_total: 48680
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.377220913073452e-33
        cur_lr: 5.0e-05
        entropy: 0.07835447912414868
        entropy_coeff: 0.0005000000000000001
        kl: 0.004463135963305831
        model: {}
        policy_loss: -0.007818976853741333
        total_loss: 0.6334934681653976
        vf_explained_var: 0.9981005191802979
        vf_loss: 0.6413516153891882
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.67
    gpu_util_percent0: 0.3336666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658737336590003
    mean_env_wait_ms: 1.197535982577299
    mean_inference_ms: 4.310479539247337
    mean_raw_obs_processing_ms: 0.3777143328943355
  time_since_restore: 5972.306274652481
  time_this_iter_s: 25.520479679107666
  time_total_s: 5972.306274652481
  timers:
    learn_throughput: 8697.964
    learn_time_ms: 18601.134
    sample_throughput: 23963.105
    sample_time_ms: 6751.713
    update_time_ms: 23.797
  timestamp: 1602737826
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: af50e_00000
  
2020-10-15 04:57:07,415	WARNING util.py:136 -- The `process_trial` operation took 0.6833858489990234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    233 |          5972.31 | 37697536 |  302.493 |              333.354 |              160.172 |            772.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2987.0763485307357
    time_step_min: 2786
  date: 2020-10-15_04-57-32
  done: false
  episode_len_mean: 772.8670619505214
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.6069507175619
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 230
  episodes_total: 48910
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.688610456536726e-33
        cur_lr: 5.0e-05
        entropy: 0.07923244498670101
        entropy_coeff: 0.0005000000000000001
        kl: 0.005181250317643086
        model: {}
        policy_loss: -0.010029669482416162
        total_loss: 0.3374100476503372
        vf_explained_var: 0.9991076588630676
        vf_loss: 0.34747932602961856
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.770967741935483
    gpu_util_percent0: 0.2787096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465841430067257
    mean_env_wait_ms: 1.197462761656933
    mean_inference_ms: 4.31028513122148
    mean_raw_obs_processing_ms: 0.37770152688019293
  time_since_restore: 5997.837114810944
  time_this_iter_s: 25.530840158462524
  time_total_s: 5997.837114810944
  timers:
    learn_throughput: 8702.681
    learn_time_ms: 18591.052
    sample_throughput: 23979.828
    sample_time_ms: 6747.004
    update_time_ms: 23.494
  timestamp: 1602737852
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: af50e_00000
  
2020-10-15 04:57:34,003	WARNING util.py:136 -- The `process_trial` operation took 0.6659932136535645 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    234 |          5997.84 | 37859328 |  302.607 |              333.354 |              160.172 |            772.867 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2986.39855367692
    time_step_min: 2786
  date: 2020-10-15_04-57-59
  done: false
  episode_len_mean: 772.8885247903606
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.7102774878968
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 49132
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.688610456536726e-33
        cur_lr: 5.0e-05
        entropy: 0.08112906043728192
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00864764271924893
        total_loss: .inf
        vf_explained_var: 0.9976688027381897
        vf_loss: 0.8648420969645182
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01666666666667
    gpu_util_percent0: 0.33833333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658063627027668
    mean_env_wait_ms: 1.1973927262644328
    mean_inference_ms: 4.310090953693529
    mean_raw_obs_processing_ms: 0.37768870853258785
  time_since_restore: 6023.38121509552
  time_this_iter_s: 25.544100284576416
  time_total_s: 6023.38121509552
  timers:
    learn_throughput: 8707.697
    learn_time_ms: 18580.344
    sample_throughput: 24018.61
    sample_time_ms: 6736.11
    update_time_ms: 21.688
  timestamp: 1602737879
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 04:58:00,440	WARNING util.py:136 -- The `process_trial` operation took 0.6710445880889893 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    235 |          6023.38 | 38021120 |   302.71 |              333.354 |              160.172 |            772.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2985.8766213994277
    time_step_min: 2786
  date: 2020-10-15_04-58-26
  done: false
  episode_len_mean: 772.9064597910963
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.791400466484
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 173
  episodes_total: 49305
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5329156848050888e-33
        cur_lr: 5.0e-05
        entropy: 0.07536938103536765
        entropy_coeff: 0.0005000000000000001
        kl: 0.004257334415645649
        model: {}
        policy_loss: -0.00866792273397247
        total_loss: 0.5591233844558398
        vf_explained_var: 0.9982500672340393
        vf_loss: 0.567828985551993
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39666666666667
    gpu_util_percent0: 0.3486666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657820970628144
    mean_env_wait_ms: 1.1973371406892053
    mean_inference_ms: 4.3099523687441295
    mean_raw_obs_processing_ms: 0.37767908210081397
  time_since_restore: 6049.047461509705
  time_this_iter_s: 25.66624641418457
  time_total_s: 6049.047461509705
  timers:
    learn_throughput: 8707.547
    learn_time_ms: 18580.663
    sample_throughput: 24022.373
    sample_time_ms: 6735.055
    update_time_ms: 20.796
  timestamp: 1602737906
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: af50e_00000
  
2020-10-15 04:58:26,977	WARNING util.py:136 -- The `process_trial` operation took 0.6434762477874756 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    236 |          6049.05 | 38182912 |  302.791 |              333.354 |              160.172 |            772.906 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2985.1771778446864
    time_step_min: 2786
  date: 2020-10-15_04-58-52
  done: false
  episode_len_mean: 772.9183306749582
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 302.8977540145339
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 49529
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2664578424025444e-33
        cur_lr: 5.0e-05
        entropy: 0.08086068866153558
        entropy_coeff: 0.0005000000000000001
        kl: 0.003999030275736004
        model: {}
        policy_loss: -0.007560527187403447
        total_loss: 0.5767252693573633
        vf_explained_var: 0.9984574913978577
        vf_loss: 0.5843262275060018
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.8
    gpu_util_percent0: 0.35933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465757396359638
    mean_env_wait_ms: 1.1972656181182921
    mean_inference_ms: 4.309773660185794
    mean_raw_obs_processing_ms: 0.3776670320844805
  time_since_restore: 6074.75541472435
  time_this_iter_s: 25.707953214645386
  time_total_s: 6074.75541472435
  timers:
    learn_throughput: 8707.504
    learn_time_ms: 18580.754
    sample_throughput: 24018.169
    sample_time_ms: 6736.234
    update_time_ms: 20.768
  timestamp: 1602737932
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: af50e_00000
  
2020-10-15 04:58:53,561	WARNING util.py:136 -- The `process_trial` operation took 0.6458237171173096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    237 |          6074.76 | 38344704 |  302.898 |              333.354 |              160.172 |            772.918 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2984.462092410436
    time_step_min: 2786
  date: 2020-10-15_04-59-19
  done: false
  episode_len_mean: 772.9338156969148
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.0022199687558
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 49755
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.332289212012722e-34
        cur_lr: 5.0e-05
        entropy: 0.07609153476854165
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038493874211174748
        model: {}
        policy_loss: -0.010079138407794138
        total_loss: 0.45993806173404056
        vf_explained_var: 0.9987812638282776
        vf_loss: 0.4700552523136139
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65161290322581
    gpu_util_percent0: 0.35387096774193544
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465721072883438
    mean_env_wait_ms: 1.1971942593943508
    mean_inference_ms: 4.309574522053545
    mean_raw_obs_processing_ms: 0.37765408459173766
  time_since_restore: 6100.609488487244
  time_this_iter_s: 25.854073762893677
  time_total_s: 6100.609488487244
  timers:
    learn_throughput: 8704.339
    learn_time_ms: 18587.511
    sample_throughput: 24017.99
    sample_time_ms: 6736.284
    update_time_ms: 21.211
  timestamp: 1602737959
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: af50e_00000
  
2020-10-15 04:59:20,300	WARNING util.py:136 -- The `process_trial` operation took 0.6402730941772461 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    238 |          6100.61 | 38506496 |  303.002 |              333.354 |              160.172 |            772.934 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2983.9165196825143
    time_step_min: 2786
  date: 2020-10-15_04-59-46
  done: false
  episode_len_mean: 772.9464493130932
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.08342122713077
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 179
  episodes_total: 49934
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.166144606006361e-34
        cur_lr: 5.0e-05
        entropy: 0.07356768225630124
        entropy_coeff: 0.0005000000000000001
        kl: 0.004352947949276616
        model: {}
        policy_loss: -0.006625737310969271
        total_loss: 0.8248907725016276
        vf_explained_var: 0.9975171685218811
        vf_loss: 0.8315532704194387
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74
    gpu_util_percent0: 0.294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656949272628597
    mean_env_wait_ms: 1.197136435262099
    mean_inference_ms: 4.309437133946146
    mean_raw_obs_processing_ms: 0.3776447050407368
  time_since_restore: 6126.517676591873
  time_this_iter_s: 25.908188104629517
  time_total_s: 6126.517676591873
  timers:
    learn_throughput: 8689.567
    learn_time_ms: 18619.11
    sample_throughput: 23989.205
    sample_time_ms: 6744.367
    update_time_ms: 22.235
  timestamp: 1602737986
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: af50e_00000
  
2020-10-15 04:59:47,241	WARNING util.py:136 -- The `process_trial` operation took 0.6897585391998291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    239 |          6126.52 | 38668288 |  303.083 |              333.354 |              160.172 |            772.946 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2983.3008042467422
    time_step_min: 2786
  date: 2020-10-15_05-00-13
  done: false
  episode_len_mean: 772.9548762736536
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.17584772774086
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 217
  episodes_total: 50151
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5830723030031805e-34
        cur_lr: 5.0e-05
        entropy: 0.08392687452336152
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00800275100239863
        total_loss: .inf
        vf_explained_var: 0.9975083470344543
        vf_loss: 0.9535115311543146
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.30322580645161
    gpu_util_percent0: 0.29419354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656712390653018
    mean_env_wait_ms: 1.1970651997466633
    mean_inference_ms: 4.30926902253637
    mean_raw_obs_processing_ms: 0.3776332714983622
  time_since_restore: 6152.624411344528
  time_this_iter_s: 26.10673475265503
  time_total_s: 6152.624411344528
  timers:
    learn_throughput: 8693.188
    learn_time_ms: 18611.354
    sample_throughput: 23845.387
    sample_time_ms: 6785.044
    update_time_ms: 23.043
  timestamp: 1602738013
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:00:14,248	WARNING util.py:136 -- The `process_trial` operation took 0.6687443256378174 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    240 |          6152.62 | 38830080 |  303.176 |              333.354 |              160.172 |            772.955 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2982.655508760777
    time_step_min: 2786
  date: 2020-10-15_05-00-40
  done: false
  episode_len_mean: 772.9691345772131
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.2728034613702
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 50380
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3746084545047717e-34
        cur_lr: 5.0e-05
        entropy: 0.08214847308893998
        entropy_coeff: 0.0005000000000000001
        kl: 0.004595147833848993
        model: {}
        policy_loss: -0.009231351075868588
        total_loss: 0.7616561005512873
        vf_explained_var: 0.9979851841926575
        vf_loss: 0.7709285020828247
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.141935483870967
    gpu_util_percent0: 0.3312903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656376549166977
    mean_env_wait_ms: 1.1969933786615334
    mean_inference_ms: 4.309071874903395
    mean_raw_obs_processing_ms: 0.37762052069092417
  time_since_restore: 6178.489985704422
  time_this_iter_s: 25.8655743598938
  time_total_s: 6178.489985704422
  timers:
    learn_throughput: 8683.702
    learn_time_ms: 18631.684
    sample_throughput: 23791.085
    sample_time_ms: 6800.53
    update_time_ms: 24.961
  timestamp: 1602738040
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: af50e_00000
  
2020-10-15 05:00:41,193	WARNING util.py:136 -- The `process_trial` operation took 0.6788663864135742 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    241 |          6178.49 | 38991872 |  303.273 |              333.354 |              160.172 |            772.969 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2982.1576176243543
    time_step_min: 2786
  date: 2020-10-15_05-01-07
  done: false
  episode_len_mean: 772.9841979312936
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.3501252662693
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 183
  episodes_total: 50563
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1873042272523859e-34
        cur_lr: 5.0e-05
        entropy: 0.07589915643135707
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037761670537292957
        model: {}
        policy_loss: -0.009361285037205866
        total_loss: 0.7381498515605927
        vf_explained_var: 0.9977555274963379
        vf_loss: 0.7475490768750509
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.256666666666664
    gpu_util_percent0: 0.3826666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656098289982997
    mean_env_wait_ms: 1.1969342906829032
    mean_inference_ms: 4.3089323625030636
    mean_raw_obs_processing_ms: 0.3776110749085328
  time_since_restore: 6204.524666309357
  time_this_iter_s: 26.034680604934692
  time_total_s: 6204.524666309357
  timers:
    learn_throughput: 8676.821
    learn_time_ms: 18646.46
    sample_throughput: 23741.132
    sample_time_ms: 6814.839
    update_time_ms: 26.171
  timestamp: 1602738067
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: af50e_00000
  
2020-10-15 05:01:08,112	WARNING util.py:136 -- The `process_trial` operation took 0.6534252166748047 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    242 |          6204.52 | 39153664 |   303.35 |              333.354 |              160.172 |            772.984 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2981.5798986852737
    time_step_min: 2786
  date: 2020-10-15_05-01-33
  done: false
  episode_len_mean: 772.9987592319054
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.4375274159615
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 50775
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.936521136261929e-35
        cur_lr: 5.0e-05
        entropy: 0.07962787027160327
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056205387227237225
        model: {}
        policy_loss: -0.010118481625492374
        total_loss: 0.8527446935574213
        vf_explained_var: 0.99765545129776
        vf_loss: 0.8629029641548792
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.026666666666664
    gpu_util_percent0: 0.3390000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655870704880192
    mean_env_wait_ms: 1.1968658631478502
    mean_inference_ms: 4.308783320276244
    mean_raw_obs_processing_ms: 0.3776004964856641
  time_since_restore: 6230.245897054672
  time_this_iter_s: 25.72123074531555
  time_total_s: 6230.245897054672
  timers:
    learn_throughput: 8673.964
    learn_time_ms: 18652.601
    sample_throughput: 23696.015
    sample_time_ms: 6827.815
    update_time_ms: 26.057
  timestamp: 1602738093
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: af50e_00000
  
2020-10-15 05:01:34,846	WARNING util.py:136 -- The `process_trial` operation took 0.6925785541534424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    243 |          6230.25 | 39315456 |  303.438 |              333.354 |              160.172 |            772.999 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2980.899185715687
    time_step_min: 2786
  date: 2020-10-15_05-02-00
  done: false
  episode_len_mean: 773.0165271433333
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.5374851500872
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 232
  episodes_total: 51007
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.936521136261929e-35
        cur_lr: 5.0e-05
        entropy: 0.07907192036509514
        entropy_coeff: 0.0005000000000000001
        kl: 0.003627125833493968
        model: {}
        policy_loss: -0.007769946290257697
        total_loss: 0.8631875415643057
        vf_explained_var: 0.9978606700897217
        vf_loss: 0.8709970364967982
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76451612903226
    gpu_util_percent0: 0.33741935483870966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655544803543877
    mean_env_wait_ms: 1.1967920034889712
    mean_inference_ms: 4.308583801585395
    mean_raw_obs_processing_ms: 0.3775875055160571
  time_since_restore: 6256.090820074081
  time_this_iter_s: 25.84492301940918
  time_total_s: 6256.090820074081
  timers:
    learn_throughput: 8667.368
    learn_time_ms: 18666.797
    sample_throughput: 23657.288
    sample_time_ms: 6838.992
    update_time_ms: 29.167
  timestamp: 1602738120
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: af50e_00000
  
2020-10-15 05:02:01,613	WARNING util.py:136 -- The `process_trial` operation took 0.6913185119628906 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    244 |          6256.09 | 39477248 |  303.537 |              333.354 |              160.172 |            773.017 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2980.3942290774735
    time_step_min: 2786
  date: 2020-10-15_05-02-27
  done: false
  episode_len_mean: 773.0327571051861
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.6187985924286
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 51195
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9682605681309646e-35
        cur_lr: 5.0e-05
        entropy: 0.07411876631279786
        entropy_coeff: 0.0005000000000000001
        kl: 0.003938887210097164
        model: {}
        policy_loss: -0.009776176055311225
        total_loss: 0.6976524790128072
        vf_explained_var: 0.9978530406951904
        vf_loss: 0.7074656883875529
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.610000000000003
    gpu_util_percent0: 0.3803333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655269823943262
    mean_env_wait_ms: 1.1967309231533796
    mean_inference_ms: 4.308444258010431
    mean_raw_obs_processing_ms: 0.377578075608855
  time_since_restore: 6281.7275223731995
  time_this_iter_s: 25.636702299118042
  time_total_s: 6281.7275223731995
  timers:
    learn_throughput: 8668.605
    learn_time_ms: 18664.133
    sample_throughput: 23598.988
    sample_time_ms: 6855.887
    update_time_ms: 30.566
  timestamp: 1602738147
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: af50e_00000
  
2020-10-15 05:02:28,163	WARNING util.py:136 -- The `process_trial` operation took 0.6808116436004639 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    245 |          6281.73 | 39639040 |  303.619 |              333.354 |              160.172 |            773.033 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2979.830292290616
    time_step_min: 2786
  date: 2020-10-15_05-02-53
  done: false
  episode_len_mean: 773.0519505788501
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.7030151696947
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 200
  episodes_total: 51395
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4841302840654823e-35
        cur_lr: 5.0e-05
        entropy: 0.0756031684577465
        entropy_coeff: 0.0005000000000000001
        kl: 0.003591393023574104
        model: {}
        policy_loss: -0.009181923669530079
        total_loss: 0.8003027737140656
        vf_explained_var: 0.9978101849555969
        vf_loss: 0.8095225145419439
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.27
    gpu_util_percent0: 0.3496666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655063168235422
    mean_env_wait_ms: 1.1966671609905593
    mean_inference_ms: 4.308308797517075
    mean_raw_obs_processing_ms: 0.37756850514578544
  time_since_restore: 6307.3083572387695
  time_this_iter_s: 25.58083486557007
  time_total_s: 6307.3083572387695
  timers:
    learn_throughput: 8675.566
    learn_time_ms: 18649.157
    sample_throughput: 23579.782
    sample_time_ms: 6861.471
    update_time_ms: 30.203
  timestamp: 1602738173
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: af50e_00000
  
2020-10-15 05:02:54,664	WARNING util.py:136 -- The `process_trial` operation took 0.6896545886993408 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    246 |          6307.31 | 39800832 |  303.703 |              333.354 |              160.172 |            773.052 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2979.126734899589
    time_step_min: 2786
  date: 2020-10-15_05-03-20
  done: false
  episode_len_mean: 773.0710827038544
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.80670153011795
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 235
  episodes_total: 51630
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.420651420327412e-36
        cur_lr: 5.0e-05
        entropy: 0.0750741691639026
        entropy_coeff: 0.0005000000000000001
        kl: 0.004780756348433594
        model: {}
        policy_loss: -0.007749457712634467
        total_loss: 0.43663427978754044
        vf_explained_var: 0.9988505840301514
        vf_loss: 0.44442127148310345
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.053333333333338
    gpu_util_percent0: 0.348
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654740440197112
    mean_env_wait_ms: 1.1965905746963703
    mean_inference_ms: 4.308114626245936
    mean_raw_obs_processing_ms: 0.3775556891441287
  time_since_restore: 6332.853059053421
  time_this_iter_s: 25.54470181465149
  time_total_s: 6332.853059053421
  timers:
    learn_throughput: 8680.036
    learn_time_ms: 18639.553
    sample_throughput: 23606.864
    sample_time_ms: 6853.6
    update_time_ms: 30.105
  timestamp: 1602738200
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: af50e_00000
  
2020-10-15 05:03:21,169	WARNING util.py:136 -- The `process_trial` operation took 0.7236509323120117 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    247 |          6332.85 | 39962624 |  303.807 |              333.354 |              160.172 |            773.071 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2978.583233556062
    time_step_min: 2786
  date: 2020-10-15_05-03-46
  done: false
  episode_len_mean: 773.0816995986415
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.8907878959398
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 194
  episodes_total: 51824
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.710325710163706e-36
        cur_lr: 5.0e-05
        entropy: 0.07177946157753468
        entropy_coeff: 0.0005000000000000001
        kl: 0.003019021785197159
        model: {}
        policy_loss: -0.007638281007530168
        total_loss: 0.49152250587940216
        vf_explained_var: 0.9985391497612
        vf_loss: 0.4991966634988785
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.32
    gpu_util_percent0: 0.303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654466037487804
    mean_env_wait_ms: 1.1965276601380765
    mean_inference_ms: 4.307971138268341
    mean_raw_obs_processing_ms: 0.3775455833455323
  time_since_restore: 6358.211980581284
  time_this_iter_s: 25.35892152786255
  time_total_s: 6358.211980581284
  timers:
    learn_throughput: 8698.778
    learn_time_ms: 18599.395
    sample_throughput: 23614.88
    sample_time_ms: 6851.274
    update_time_ms: 31.298
  timestamp: 1602738226
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: af50e_00000
  
2020-10-15 05:03:47,501	WARNING util.py:136 -- The `process_trial` operation took 0.7273025512695312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    248 |          6358.21 | 40124416 |  303.891 |              333.354 |              160.172 |            773.082 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2978.033765584116
    time_step_min: 2786
  date: 2020-10-15_05-04-12
  done: false
  episode_len_mean: 773.0916029066861
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 303.9717799704918
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 194
  episodes_total: 52018
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.855162855081853e-36
        cur_lr: 5.0e-05
        entropy: 0.07828318141400814
        entropy_coeff: 0.0005000000000000001
        kl: 0.006661874280932049
        model: {}
        policy_loss: -0.010096385007879386
        total_loss: 0.639169861872991
        vf_explained_var: 0.9981474280357361
        vf_loss: 0.649305393298467
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.633333333333336
    gpu_util_percent0: 0.33399999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465425371768412
    mean_env_wait_ms: 1.1964654437004274
    mean_inference_ms: 4.307842588904444
    mean_raw_obs_processing_ms: 0.3775368509975064
  time_since_restore: 6383.1821982860565
  time_this_iter_s: 24.97021770477295
  time_total_s: 6383.1821982860565
  timers:
    learn_throughput: 8738.008
    learn_time_ms: 18515.89
    sample_throughput: 23647.136
    sample_time_ms: 6841.928
    update_time_ms: 30.459
  timestamp: 1602738252
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: af50e_00000
  
2020-10-15 05:04:13,559	WARNING util.py:136 -- The `process_trial` operation took 0.6836347579956055 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    249 |          6383.18 | 40286208 |  303.972 |              333.354 |              160.172 |            773.092 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2977.385845623444
    time_step_min: 2786
  date: 2020-10-15_05-04-39
  done: false
  episode_len_mean: 773.1055461991886
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.0709154625176
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 234
  episodes_total: 52252
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.855162855081853e-36
        cur_lr: 5.0e-05
        entropy: 0.07926845923066139
        entropy_coeff: 0.0005000000000000001
        kl: 0.004908649832941592
        model: {}
        policy_loss: -0.010789210956621295
        total_loss: 0.6974678883949915
        vf_explained_var: 0.9982350468635559
        vf_loss: 0.7082967311143875
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803333333333338
    gpu_util_percent0: 0.3226666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653946437145368
    mean_env_wait_ms: 1.1963889341621619
    mean_inference_ms: 4.307654395700311
    mean_raw_obs_processing_ms: 0.37752385202744826
  time_since_restore: 6408.76756978035
  time_this_iter_s: 25.585371494293213
  time_total_s: 6408.76756978035
  timers:
    learn_throughput: 8746.23
    learn_time_ms: 18498.484
    sample_throughput: 23771.522
    sample_time_ms: 6806.127
    update_time_ms: 29.745
  timestamp: 1602738279
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: af50e_00000
  
2020-10-15 05:04:40,090	WARNING util.py:136 -- The `process_trial` operation took 0.701702356338501 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    250 |          6408.77 | 40448000 |  304.071 |              333.354 |              160.172 |            773.106 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2976.8101125739363
    time_step_min: 2786
  date: 2020-10-15_05-05-05
  done: false
  episode_len_mean: 773.1251239228246
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.1553246951324
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 200
  episodes_total: 52452
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.275814275409265e-37
        cur_lr: 5.0e-05
        entropy: 0.07405648442606132
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040343017705405755
        model: {}
        policy_loss: -0.008625380859787887
        total_loss: 0.5631585046648979
        vf_explained_var: 0.9983394145965576
        vf_loss: 0.5718208899100622
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79
    gpu_util_percent0: 0.37
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653680478348194
    mean_env_wait_ms: 1.1963240478909853
    mean_inference_ms: 4.307510465821018
    mean_raw_obs_processing_ms: 0.3775140064759169
  time_since_restore: 6434.491745710373
  time_this_iter_s: 25.724175930023193
  time_total_s: 6434.491745710373
  timers:
    learn_throughput: 8749.608
    learn_time_ms: 18491.343
    sample_throughput: 23796.699
    sample_time_ms: 6798.926
    update_time_ms: 28.622
  timestamp: 1602738305
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: af50e_00000
  
2020-10-15 05:05:06,835	WARNING util.py:136 -- The `process_trial` operation took 0.7044250965118408 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    251 |          6434.49 | 40609792 |  304.155 |              333.354 |              160.172 |            773.125 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2976.3084001977413
    time_step_min: 2786
  date: 2020-10-15_05-05-32
  done: false
  episode_len_mean: 773.1429819895129
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.2296870598222
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 52636
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.637907137704632e-37
        cur_lr: 5.0e-05
        entropy: 0.07820781506597996
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006669755952316336
        total_loss: .inf
        vf_explained_var: 0.9982597827911377
        vf_loss: 0.5981282393137614
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.213333333333335
    gpu_util_percent0: 0.364
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653491252329393
    mean_env_wait_ms: 1.1962640686305615
    mean_inference_ms: 4.307387792122716
    mean_raw_obs_processing_ms: 0.3775056708958956
  time_since_restore: 6460.023387908936
  time_this_iter_s: 25.531642198562622
  time_total_s: 6460.023387908936
  timers:
    learn_throughput: 8767.913
    learn_time_ms: 18452.737
    sample_throughput: 23839.708
    sample_time_ms: 6786.66
    update_time_ms: 26.841
  timestamp: 1602738332
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:05:33,336	WARNING util.py:136 -- The `process_trial` operation took 0.7273869514465332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    252 |          6460.02 | 40771584 |   304.23 |              333.354 |              160.172 |            773.143 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2975.655473955179
    time_step_min: 2786
  date: 2020-10-15_05-05-58
  done: false
  episode_len_mean: 773.1739796497334
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.32762928295693
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 238
  episodes_total: 52874
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.956860706556947e-37
        cur_lr: 5.0e-05
        entropy: 0.07723917253315449
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035892793675884604
        model: {}
        policy_loss: -0.007210227270358398
        total_loss: 0.8192129582166672
        vf_explained_var: 0.9979068636894226
        vf_loss: 0.8264618217945099
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97000000000001
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465320356779314
    mean_env_wait_ms: 1.1961866928407097
    mean_inference_ms: 4.3072177224408446
    mean_raw_obs_processing_ms: 0.37749310080441884
  time_since_restore: 6485.544056415558
  time_this_iter_s: 25.520668506622314
  time_total_s: 6485.544056415558
  timers:
    learn_throughput: 8769.672
    learn_time_ms: 18449.036
    sample_throughput: 23892.917
    sample_time_ms: 6771.547
    update_time_ms: 26.641
  timestamp: 1602738358
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: af50e_00000
  
2020-10-15 05:05:59,847	WARNING util.py:136 -- The `process_trial` operation took 0.7491772174835205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    253 |          6485.54 | 40933376 |  304.328 |              333.354 |              160.172 |            773.174 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2975.0898772601295
    time_step_min: 2786
  date: 2020-10-15_05-06-25
  done: false
  episode_len_mean: 773.1946647576345
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.41147824584436
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 207
  episodes_total: 53081
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4784303532784734e-37
        cur_lr: 5.0e-05
        entropy: 0.07506304606795311
        entropy_coeff: 0.0005000000000000001
        kl: 0.004275119203763704
        model: {}
        policy_loss: -0.009237821836601748
        total_loss: 0.6980894257624944
        vf_explained_var: 0.9980065822601318
        vf_loss: 0.7073647777239481
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.429032258064517
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652911937326324
    mean_env_wait_ms: 1.196118893264658
    mean_inference_ms: 4.307062123422146
    mean_raw_obs_processing_ms: 0.3774829360778588
  time_since_restore: 6511.32613825798
  time_this_iter_s: 25.782081842422485
  time_total_s: 6511.32613825798
  timers:
    learn_throughput: 8769.123
    learn_time_ms: 18450.192
    sample_throughput: 23939.807
    sample_time_ms: 6758.283
    update_time_ms: 23.786
  timestamp: 1602738385
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: af50e_00000
  
2020-10-15 05:06:26,751	WARNING util.py:136 -- The `process_trial` operation took 0.7869255542755127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    254 |          6511.33 | 41095168 |  304.411 |              333.354 |              160.172 |            773.195 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2974.5896390386893
    time_step_min: 2786
  date: 2020-10-15_05-06-52
  done: false
  episode_len_mean: 773.2216068042283
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.48935990649426
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 180
  episodes_total: 53261
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7392151766392367e-37
        cur_lr: 5.0e-05
        entropy: 0.07066610269248486
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045052829664200544
        model: {}
        policy_loss: -0.006693208122063273
        total_loss: 0.35274942219257355
        vf_explained_var: 0.9989063143730164
        vf_loss: 0.3594779645403226
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95333333333333
    gpu_util_percent0: 0.3483333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652727978982114
    mean_env_wait_ms: 1.196059502046995
    mean_inference_ms: 4.306945647130549
    mean_raw_obs_processing_ms: 0.3774747703059309
  time_since_restore: 6537.307283401489
  time_this_iter_s: 25.98114514350891
  time_total_s: 6537.307283401489
  timers:
    learn_throughput: 8756.965
    learn_time_ms: 18475.808
    sample_throughput: 23914.717
    sample_time_ms: 6765.374
    update_time_ms: 24.015
  timestamp: 1602738412
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: af50e_00000
  
2020-10-15 05:06:53,798	WARNING util.py:136 -- The `process_trial` operation took 0.7326712608337402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    255 |          6537.31 | 41256960 |  304.489 |              333.354 |              160.172 |            773.222 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2973.9774530349523
    time_step_min: 2786
  date: 2020-10-15_05-07-19
  done: false
  episode_len_mean: 773.2523090154433
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.5831066148905
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 225
  episodes_total: 53486
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.696075883196183e-38
        cur_lr: 5.0e-05
        entropy: 0.07179772357145946
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007970351037026072
        total_loss: .inf
        vf_explained_var: 0.9982807040214539
        vf_loss: 0.6513164242108663
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.751612903225812
    gpu_util_percent0: 0.3119354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465248745451109
    mean_env_wait_ms: 1.1959858555654348
    mean_inference_ms: 4.306793312671812
    mean_raw_obs_processing_ms: 0.37746340280816093
  time_since_restore: 6562.9110951423645
  time_this_iter_s: 25.603811740875244
  time_total_s: 6562.9110951423645
  timers:
    learn_throughput: 8756.187
    learn_time_ms: 18477.448
    sample_throughput: 23924.152
    sample_time_ms: 6762.706
    update_time_ms: 25.069
  timestamp: 1602738439
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:07:20,538	WARNING util.py:136 -- The `process_trial` operation took 0.6987643241882324 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    256 |          6562.91 | 41418752 |  304.583 |              333.354 |              160.172 |            773.252 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2973.369700922389
    time_step_min: 2786
  date: 2020-10-15_05-07-46
  done: false
  episode_len_mean: 773.2833522632059
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.67426776751427
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 221
  episodes_total: 53707
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3044113824794277e-37
        cur_lr: 5.0e-05
        entropy: 0.07298139855265617
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00739886596420547
        total_loss: .inf
        vf_explained_var: 0.9988391399383545
        vf_loss: 0.42214469114939374
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.060000000000002
    gpu_util_percent0: 0.33299999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652176469266506
    mean_env_wait_ms: 1.195912954538396
    mean_inference_ms: 4.306626300747809
    mean_raw_obs_processing_ms: 0.3774523769386464
  time_since_restore: 6588.665770292282
  time_this_iter_s: 25.754675149917603
  time_total_s: 6588.665770292282
  timers:
    learn_throughput: 8758.038
    learn_time_ms: 18473.544
    sample_throughput: 23885.155
    sample_time_ms: 6773.747
    update_time_ms: 27.833
  timestamp: 1602738466
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:07:47,260	WARNING util.py:136 -- The `process_trial` operation took 0.7205867767333984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    257 |          6588.67 | 41580544 |  304.674 |              333.354 |              160.172 |            773.283 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2972.919245198915
    time_step_min: 2786
  date: 2020-10-15_05-08-12
  done: false
  episode_len_mean: 773.3090713384307
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.74494180915366
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 177
  episodes_total: 53884
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.956617073719142e-37
        cur_lr: 5.0e-05
        entropy: 0.07542371687789758
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00851766787430582
        total_loss: .inf
        vf_explained_var: 0.9986533522605896
        vf_loss: 0.4431445350249608
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.823333333333334
    gpu_util_percent0: 0.3336666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651951995940332
    mean_env_wait_ms: 1.1958546808812385
    mean_inference_ms: 4.306513191853602
    mean_raw_obs_processing_ms: 0.37744437971468675
  time_since_restore: 6614.1934378147125
  time_this_iter_s: 25.52766752243042
  time_total_s: 6614.1934378147125
  timers:
    learn_throughput: 8753.61
    learn_time_ms: 18482.89
    sample_throughput: 23865.85
    sample_time_ms: 6779.226
    update_time_ms: 28.675
  timestamp: 1602738492
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:08:13,774	WARNING util.py:136 -- The `process_trial` operation took 0.7460489273071289 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    258 |          6614.19 | 41742336 |  304.745 |              333.354 |              160.172 |            773.309 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2972.358507666981
    time_step_min: 2786
  date: 2020-10-15_05-08-39
  done: false
  episode_len_mean: 773.3359763422974
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.8322313795004
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 221
  episodes_total: 54105
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.934925610578712e-37
        cur_lr: 5.0e-05
        entropy: 0.07376385604341824
        entropy_coeff: 0.0005000000000000001
        kl: 0.004188123993420352
        model: {}
        policy_loss: -0.009314790675489348
        total_loss: 0.4880044385790825
        vf_explained_var: 0.9986703991889954
        vf_loss: 0.4973560969034831
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.983333333333334
    gpu_util_percent0: 0.316
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651737929125952
    mean_env_wait_ms: 1.195780904563027
    mean_inference_ms: 4.306369954803781
    mean_raw_obs_processing_ms: 0.3774340580255542
  time_since_restore: 6639.880469799042
  time_this_iter_s: 25.687031984329224
  time_total_s: 6639.880469799042
  timers:
    learn_throughput: 8722.861
    learn_time_ms: 18548.043
    sample_throughput: 23845.546
    sample_time_ms: 6784.999
    update_time_ms: 29.105
  timestamp: 1602738519
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: af50e_00000
  
2020-10-15 05:08:40,465	WARNING util.py:136 -- The `process_trial` operation took 0.7596304416656494 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    259 |          6639.88 | 41904128 |  304.832 |              333.354 |              160.172 |            773.336 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2971.757829194606
    time_step_min: 2786
  date: 2020-10-15_05-09-06
  done: false
  episode_len_mean: 773.3647977027574
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.92411543182794
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 221
  episodes_total: 54326
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.467462805289356e-37
        cur_lr: 5.0e-05
        entropy: 0.0696639201293389
        entropy_coeff: 0.0005000000000000001
        kl: 0.005259278773640593
        model: {}
        policy_loss: -0.010400129462747524
        total_loss: 0.27731024225552875
        vf_explained_var: 0.9992353320121765
        vf_loss: 0.28774520258108777
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69354838709678
    gpu_util_percent0: 0.3409677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465146557407376
    mean_env_wait_ms: 1.1957089547785016
    mean_inference_ms: 4.306208324783024
    mean_raw_obs_processing_ms: 0.3774232736829035
  time_since_restore: 6665.54772233963
  time_this_iter_s: 25.66725254058838
  time_total_s: 6665.54772233963
  timers:
    learn_throughput: 8716.194
    learn_time_ms: 18562.23
    sample_throughput: 23872.866
    sample_time_ms: 6777.234
    update_time_ms: 31.262
  timestamp: 1602738546
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: af50e_00000
  
2020-10-15 05:09:07,281	WARNING util.py:136 -- The `process_trial` operation took 0.7332870960235596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    260 |          6665.55 | 42065920 |  304.924 |              333.354 |              160.172 |            773.365 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2971.285454645591
    time_step_min: 2786
  date: 2020-10-15_05-09-32
  done: false
  episode_len_mean: 773.3894850769541
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 304.9960765544386
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 54513
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.467462805289356e-37
        cur_lr: 5.0e-05
        entropy: 0.07220410369336605
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009687999554444104
        total_loss: .inf
        vf_explained_var: 0.9985723495483398
        vf_loss: 0.4680771678686142
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.41666666666667
    gpu_util_percent0: 0.386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651215462404438
    mean_env_wait_ms: 1.1956462348519876
    mean_inference_ms: 4.306087224205072
    mean_raw_obs_processing_ms: 0.3774148940864254
  time_since_restore: 6691.20562005043
  time_this_iter_s: 25.65789771080017
  time_total_s: 6691.20562005043
  timers:
    learn_throughput: 8719.635
    learn_time_ms: 18554.905
    sample_throughput: 23877.837
    sample_time_ms: 6775.823
    update_time_ms: 32.904
  timestamp: 1602738572
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:09:33,922	WARNING util.py:136 -- The `process_trial` operation took 0.7373192310333252 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    261 |          6691.21 | 42227712 |  304.996 |              333.354 |              160.172 |            773.389 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2970.765833318093
    time_step_min: 2786
  date: 2020-10-15_05-09-59
  done: false
  episode_len_mean: 773.422525173151
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.0797603416706
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 208
  episodes_total: 54721
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2011942079340342e-37
        cur_lr: 5.0e-05
        entropy: 0.07268022497495015
        entropy_coeff: 0.0005000000000000001
        kl: 0.004213126473284016
        model: {}
        policy_loss: -0.008453542162897065
        total_loss: 0.38518522431453067
        vf_explained_var: 0.9989925026893616
        vf_loss: 0.3936751112341881
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.229032258064525
    gpu_util_percent0: 0.31741935483870976
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651009446748414
    mean_env_wait_ms: 1.1955772075443205
    mean_inference_ms: 4.305959435210006
    mean_raw_obs_processing_ms: 0.37740542388113746
  time_since_restore: 6717.0665555000305
  time_this_iter_s: 25.86093544960022
  time_total_s: 6717.0665555000305
  timers:
    learn_throughput: 8703.721
    learn_time_ms: 18588.83
    sample_throughput: 23885.419
    sample_time_ms: 6773.672
    update_time_ms: 33.812
  timestamp: 1602738599
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: af50e_00000
  
2020-10-15 05:10:00,932	WARNING util.py:136 -- The `process_trial` operation took 0.718682050704956 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    262 |          6717.07 | 42389504 |   305.08 |              333.354 |              160.172 |            773.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2970.1382833415287
    time_step_min: 2786
  date: 2020-10-15_05-10-26
  done: false
  episode_len_mean: 773.456424814835
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.17460404117594
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 230
  episodes_total: 54951
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1005971039670171e-37
        cur_lr: 5.0e-05
        entropy: 0.07045281864702702
        entropy_coeff: 0.0005000000000000001
        kl: 0.004652956112598379
        model: {}
        policy_loss: -0.007912964618299156
        total_loss: 0.32986704011758167
        vf_explained_var: 0.9991464614868164
        vf_loss: 0.3378152400255203
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.530000000000005
    gpu_util_percent0: 0.295
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650735733324466
    mean_env_wait_ms: 1.1955014897923522
    mean_inference_ms: 4.305796986024191
    mean_raw_obs_processing_ms: 0.37739473455108735
  time_since_restore: 6742.884254217148
  time_this_iter_s: 25.81769871711731
  time_total_s: 6742.884254217148
  timers:
    learn_throughput: 8694.106
    learn_time_ms: 18609.388
    sample_throughput: 23865.724
    sample_time_ms: 6779.262
    update_time_ms: 34.012
  timestamp: 1602738626
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: af50e_00000
  
2020-10-15 05:10:27,748	WARNING util.py:136 -- The `process_trial` operation took 0.7455449104309082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    263 |          6742.88 | 42551296 |  305.175 |              333.354 |              160.172 |            773.456 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2969.6129149349354
    time_step_min: 2786
  date: 2020-10-15_05-10-53
  done: false
  episode_len_mean: 773.4894361727208
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.2513422430905
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 190
  episodes_total: 55141
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.502985519835086e-38
        cur_lr: 5.0e-05
        entropy: 0.07218968619902928
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008945159323047847
        total_loss: .inf
        vf_explained_var: 0.9986748099327087
        vf_loss: 0.46449292699495953
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55483870967742
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650495567827584
    mean_env_wait_ms: 1.1954374824422516
    mean_inference_ms: 4.305674482895831
    mean_raw_obs_processing_ms: 0.3773857026945095
  time_since_restore: 6768.692822217941
  time_this_iter_s: 25.808568000793457
  time_total_s: 6768.692822217941
  timers:
    learn_throughput: 8690.474
    learn_time_ms: 18617.167
    sample_throughput: 23856.631
    sample_time_ms: 6781.846
    update_time_ms: 33.698
  timestamp: 1602738653
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:10:54,701	WARNING util.py:136 -- The `process_trial` operation took 0.7237555980682373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    264 |          6768.69 | 42713088 |  305.251 |              333.354 |              160.172 |            773.489 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2969.109742643734
    time_step_min: 2786
  date: 2020-10-15_05-11-20
  done: false
  episode_len_mean: 773.5166892563477
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.3262141611285
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 194
  episodes_total: 55335
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.254478279752629e-38
        cur_lr: 5.0e-05
        entropy: 0.07613727015753587
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009350820725861317
        total_loss: .inf
        vf_explained_var: 0.9990187287330627
        vf_loss: 0.346089169383049
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.300000000000004
    gpu_util_percent0: 0.3656666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650306665473528
    mean_env_wait_ms: 1.1953733956170216
    mean_inference_ms: 4.305559833487489
    mean_raw_obs_processing_ms: 0.37737788072968337
  time_since_restore: 6794.449730396271
  time_this_iter_s: 25.756908178329468
  time_total_s: 6794.449730396271
  timers:
    learn_throughput: 8694.659
    learn_time_ms: 18608.206
    sample_throughput: 23902.199
    sample_time_ms: 6768.917
    update_time_ms: 32.097
  timestamp: 1602738680
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:11:21,441	WARNING util.py:136 -- The `process_trial` operation took 0.7305757999420166 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    265 |          6794.45 | 42874880 |  305.326 |              333.354 |              160.172 |            773.517 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2968.519323584498
    time_step_min: 2786
  date: 2020-10-15_05-11-46
  done: false
  episode_len_mean: 773.5417491452223
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.41318166367637
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 235
  episodes_total: 55570
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.238171741962894e-37
        cur_lr: 5.0e-05
        entropy: 0.08018573559820652
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010125683969818056
        total_loss: .inf
        vf_explained_var: 0.998830258846283
        vf_loss: 0.4554699982206027
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.806666666666665
    gpu_util_percent0: 0.396
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465004749163819
    mean_env_wait_ms: 1.1952953719606116
    mean_inference_ms: 4.305401704741007
    mean_raw_obs_processing_ms: 0.3773668129824555
  time_since_restore: 6819.890542984009
  time_this_iter_s: 25.440812587738037
  time_total_s: 6819.890542984009
  timers:
    learn_throughput: 8697.804
    learn_time_ms: 18601.476
    sample_throughput: 23939.47
    sample_time_ms: 6758.378
    update_time_ms: 33.343
  timestamp: 1602738706
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:11:48,003	WARNING util.py:136 -- The `process_trial` operation took 0.7705826759338379 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    266 |          6819.89 | 43036672 |  305.413 |              333.354 |              160.172 |            773.542 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2968.0063702268158
    time_step_min: 2786
  date: 2020-10-15_05-12-13
  done: false
  episode_len_mean: 773.5692307692308
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.4915018935996
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 200
  episodes_total: 55770
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.857257612944341e-37
        cur_lr: 5.0e-05
        entropy: 0.07111637853085995
        entropy_coeff: 0.0005000000000000001
        kl: 0.005416372752127548
        model: {}
        policy_loss: -0.008417334373613508
        total_loss: 0.24332204212745032
        vf_explained_var: 0.9992991089820862
        vf_loss: 0.2517749381562074
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.212903225806453
    gpu_util_percent0: 0.27548387096774196
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649794241119005
    mean_env_wait_ms: 1.19522833234358
    mean_inference_ms: 4.305272921013939
    mean_raw_obs_processing_ms: 0.37735774903552166
  time_since_restore: 6845.5250816345215
  time_this_iter_s: 25.634538650512695
  time_total_s: 6845.5250816345215
  timers:
    learn_throughput: 8699.166
    learn_time_ms: 18598.564
    sample_throughput: 23960.602
    sample_time_ms: 6752.418
    update_time_ms: 30.623
  timestamp: 1602738733
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: af50e_00000
  
2020-10-15 05:12:14,644	WARNING util.py:136 -- The `process_trial` operation took 0.7481060028076172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    267 |          6845.53 | 43198464 |  305.492 |              333.354 |              160.172 |            773.569 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2967.560390620808
    time_step_min: 2786
  date: 2020-10-15_05-12-40
  done: false
  episode_len_mean: 773.5916930280771
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.55711368145
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 183
  episodes_total: 55953
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.857257612944341e-37
        cur_lr: 5.0e-05
        entropy: 0.07377259681622188
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038848621188662946
        model: {}
        policy_loss: -0.009898960396337012
        total_loss: 0.4313003172477086
        vf_explained_var: 0.9987032413482666
        vf_loss: 0.44123617311318714
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.596666666666668
    gpu_util_percent0: 0.3636666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649632105395813
    mean_env_wait_ms: 1.1951673702645613
    mean_inference_ms: 4.305169083533474
    mean_raw_obs_processing_ms: 0.3773504916803801
  time_since_restore: 6871.184775590897
  time_this_iter_s: 25.659693956375122
  time_total_s: 6871.184775590897
  timers:
    learn_throughput: 8696.083
    learn_time_ms: 18605.159
    sample_throughput: 23936.016
    sample_time_ms: 6759.354
    update_time_ms: 28.653
  timestamp: 1602738760
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: af50e_00000
  
2020-10-15 05:12:41,303	WARNING util.py:136 -- The `process_trial` operation took 0.7443647384643555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    268 |          6871.18 | 43360256 |  305.557 |              333.354 |              160.172 |            773.592 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2966.9815289800863
    time_step_min: 2786
  date: 2020-10-15_05-13-07
  done: false
  episode_len_mean: 773.6282215577388
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.64553911606436
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 231
  episodes_total: 56184
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.286288064721705e-38
        cur_lr: 5.0e-05
        entropy: 0.0695076733827591
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034564423646467426
        model: {}
        policy_loss: -0.009033001270533228
        total_loss: 0.3394453600049019
        vf_explained_var: 0.9991360306739807
        vf_loss: 0.3485131139556567
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03870967741935
    gpu_util_percent0: 0.36806451612903224
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649365936393954
    mean_env_wait_ms: 1.1950897717595395
    mean_inference_ms: 4.305020187248164
    mean_raw_obs_processing_ms: 0.3773399446086623
  time_since_restore: 6897.161594867706
  time_this_iter_s: 25.976819276809692
  time_total_s: 6897.161594867706
  timers:
    learn_throughput: 8683.486
    learn_time_ms: 18632.148
    sample_throughput: 23935.612
    sample_time_ms: 6759.468
    update_time_ms: 30.002
  timestamp: 1602738787
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: af50e_00000
  
2020-10-15 05:13:08,270	WARNING util.py:136 -- The `process_trial` operation took 0.7333004474639893 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    269 |          6897.16 | 43522048 |  305.646 |              333.354 |              160.172 |            773.628 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2966.4927243043726
    time_step_min: 2786
  date: 2020-10-15_05-13-34
  done: false
  episode_len_mean: 773.6569493208498
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.7190083621618
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 210
  episodes_total: 56394
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6431440323608525e-38
        cur_lr: 5.0e-05
        entropy: 0.07120721538861592
        entropy_coeff: 0.0005000000000000001
        kl: 0.007042508339509368
        model: {}
        policy_loss: -0.009062879374444796
        total_loss: 0.6091997772455215
        vf_explained_var: 0.9982869029045105
        vf_loss: 0.618298257390658
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.919354838709676
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649129759522594
    mean_env_wait_ms: 1.1950198920721837
    mean_inference_ms: 4.304884085992116
    mean_raw_obs_processing_ms: 0.3773303774380991
  time_since_restore: 6923.163259267807
  time_this_iter_s: 26.001664400100708
  time_total_s: 6923.163259267807
  timers:
    learn_throughput: 8677.401
    learn_time_ms: 18645.215
    sample_throughput: 23893.772
    sample_time_ms: 6771.304
    update_time_ms: 28.021
  timestamp: 1602738814
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: af50e_00000
  
2020-10-15 05:13:35,361	WARNING util.py:136 -- The `process_trial` operation took 0.8037221431732178 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    270 |          6923.16 | 43683840 |  305.719 |              333.354 |              160.172 |            773.657 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2966.151964339412
    time_step_min: 2786
  date: 2020-10-15_05-14-01
  done: false
  episode_len_mean: 773.6793813521873
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.7753594272373
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 56575
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6431440323608525e-38
        cur_lr: 5.0e-05
        entropy: 0.08778755987683932
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049154567532241344
        model: {}
        policy_loss: -0.012605985626578331
        total_loss: 1.3923054337501526
        vf_explained_var: 0.9958968162536621
        vf_loss: 1.404955307642619
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.676666666666666
    gpu_util_percent0: 0.33533333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648930907686747
    mean_env_wait_ms: 1.1949591362158054
    mean_inference_ms: 4.304783475608719
    mean_raw_obs_processing_ms: 0.3773231891348873
  time_since_restore: 6948.842431306839
  time_this_iter_s: 25.679172039031982
  time_total_s: 6948.842431306839
  timers:
    learn_throughput: 8679.983
    learn_time_ms: 18639.667
    sample_throughput: 23870.546
    sample_time_ms: 6777.893
    update_time_ms: 27.128
  timestamp: 1602738841
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: af50e_00000
  
2020-10-15 05:14:02,101	WARNING util.py:136 -- The `process_trial` operation took 0.7290093898773193 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    271 |          6948.84 | 43845632 |  305.775 |              333.354 |              160.172 |            773.679 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2965.9906800563776
    time_step_min: 2786
  date: 2020-10-15_05-14-28
  done: false
  episode_len_mean: 773.6911552410127
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.8058577749608
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 227
  episodes_total: 56802
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3215720161804262e-38
        cur_lr: 5.0e-05
        entropy: 0.09346088021993637
        entropy_coeff: 0.0005000000000000001
        kl: 0.005510754495238264
        model: {}
        policy_loss: -0.01200256005540723
        total_loss: 3.3243016799290976
        vf_explained_var: 0.9923855662345886
        vf_loss: 3.336350997289022
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.632258064516126
    gpu_util_percent0: 0.3261290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648737083342242
    mean_env_wait_ms: 1.194884502228942
    mean_inference_ms: 4.304650408754978
    mean_raw_obs_processing_ms: 0.3773135389796183
  time_since_restore: 6974.765101909637
  time_this_iter_s: 25.922670602798462
  time_total_s: 6974.765101909637
  timers:
    learn_throughput: 8682.147
    learn_time_ms: 18635.021
    sample_throughput: 23862.629
    sample_time_ms: 6780.141
    update_time_ms: 26.496
  timestamp: 1602738868
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: af50e_00000
  
2020-10-15 05:14:29,031	WARNING util.py:136 -- The `process_trial` operation took 0.743027925491333 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    272 |          6974.77 | 44007424 |  305.806 |              333.354 |              160.172 |            773.691 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2965.696531994805
    time_step_min: 2786
  date: 2020-10-15_05-14-54
  done: false
  episode_len_mean: 773.7102770957558
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.85760622712553
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 218
  episodes_total: 57020
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3215720161804262e-38
        cur_lr: 5.0e-05
        entropy: 0.07981829345226288
        entropy_coeff: 0.0005000000000000001
        kl: 0.004376385516176621
        model: {}
        policy_loss: -0.010987848062844327
        total_loss: 1.27396226922671
        vf_explained_var: 0.9967644810676575
        vf_loss: 1.284990022579829
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.940000000000005
    gpu_util_percent0: 0.38133333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648474795267855
    mean_env_wait_ms: 1.1948115675060613
    mean_inference_ms: 4.304503242831518
    mean_raw_obs_processing_ms: 0.37730389510735024
  time_since_restore: 7000.481258153915
  time_this_iter_s: 25.716156244277954
  time_total_s: 7000.481258153915
  timers:
    learn_throughput: 8688.444
    learn_time_ms: 18621.517
    sample_throughput: 23854.355
    sample_time_ms: 6782.493
    update_time_ms: 26.748
  timestamp: 1602738894
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: af50e_00000
  
2020-10-15 05:14:55,793	WARNING util.py:136 -- The `process_trial` operation took 0.7832756042480469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    273 |          7000.48 | 44169216 |  305.858 |              333.354 |              160.172 |             773.71 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2965.3486415087737
    time_step_min: 2786
  date: 2020-10-15_05-15-21
  done: false
  episode_len_mean: 773.7372773203266
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.9140569167839
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 57201
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1607860080902131e-38
        cur_lr: 5.0e-05
        entropy: 0.07190689817070961
        entropy_coeff: 0.0005000000000000001
        kl: 0.004009690640183787
        model: {}
        policy_loss: -0.009385441554816984
        total_loss: 0.8684963136911392
        vf_explained_var: 0.9974064230918884
        vf_loss: 0.8779177119334539
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.029032258064515
    gpu_util_percent0: 0.31451612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648261319948547
    mean_env_wait_ms: 1.1947510793587743
    mean_inference_ms: 4.304401000511871
    mean_raw_obs_processing_ms: 0.3772966568956356
  time_since_restore: 7026.340382575989
  time_this_iter_s: 25.859124422073364
  time_total_s: 7026.340382575989
  timers:
    learn_throughput: 8692.785
    learn_time_ms: 18612.217
    sample_throughput: 23843.114
    sample_time_ms: 6785.691
    update_time_ms: 28.903
  timestamp: 1602738921
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: af50e_00000
  
2020-10-15 05:15:22,721	WARNING util.py:136 -- The `process_trial` operation took 0.7659165859222412 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    274 |          7026.34 | 44331008 |  305.914 |              333.354 |              160.172 |            773.737 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2964.851358653025
    time_step_min: 2786
  date: 2020-10-15_05-15-48
  done: false
  episode_len_mean: 773.7720456326743
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 305.98970106886134
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 214
  episodes_total: 57415
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.8039300404510656e-39
        cur_lr: 5.0e-05
        entropy: 0.071749497205019
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006828689297738795
        total_loss: .inf
        vf_explained_var: 0.998751699924469
        vf_loss: 0.4910421147942543
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.216666666666665
    gpu_util_percent0: 0.3403333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648085193821736
    mean_env_wait_ms: 1.1946794048434566
    mean_inference_ms: 4.304281400349034
    mean_raw_obs_processing_ms: 0.37728811163132936
  time_since_restore: 7052.002755403519
  time_this_iter_s: 25.662372827529907
  time_total_s: 7052.002755403519
  timers:
    learn_throughput: 8698.054
    learn_time_ms: 18600.942
    sample_throughput: 23846.675
    sample_time_ms: 6784.678
    update_time_ms: 30.32
  timestamp: 1602738948
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:15:49,397	WARNING util.py:136 -- The `process_trial` operation took 0.7505760192871094 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    275 |             7052 | 44492800 |   305.99 |              333.354 |              160.172 |            773.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2964.334079861111
    time_step_min: 2786
  date: 2020-10-15_05-16-15
  done: false
  episode_len_mean: 773.8071545053954
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.06789924854854
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 227
  episodes_total: 57642
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.705895060676598e-39
        cur_lr: 5.0e-05
        entropy: 0.07147789125641187
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010690314772849282
        total_loss: .inf
        vf_explained_var: 0.9988096356391907
        vf_loss: 0.4690316393971443
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.587096774193554
    gpu_util_percent0: 0.3858064516129031
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647832850809597
    mean_env_wait_ms: 1.194605053576633
    mean_inference_ms: 4.304134331044434
    mean_raw_obs_processing_ms: 0.37727829239827043
  time_since_restore: 7077.895989656448
  time_this_iter_s: 25.893234252929688
  time_total_s: 7077.895989656448
  timers:
    learn_throughput: 8678.029
    learn_time_ms: 18643.866
    sample_throughput: 23833.919
    sample_time_ms: 6788.309
    update_time_ms: 28.158
  timestamp: 1602738975
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:16:16,472	WARNING util.py:136 -- The `process_trial` operation took 0.7430603504180908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    276 |           7077.9 | 44654592 |  306.068 |              333.354 |              160.172 |            773.807 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2963.910586973074
    time_step_min: 2786
  date: 2020-10-15_05-16-42
  done: false
  episode_len_mean: 773.8260764309182
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.13055420188385
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 57830
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3058842591014897e-38
        cur_lr: 5.0e-05
        entropy: 0.06972832481066386
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008313094615004957
        total_loss: .inf
        vf_explained_var: 0.9984953999519348
        vf_loss: 0.5237497513492902
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69666666666667
    gpu_util_percent0: 0.3596666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647604965461397
    mean_env_wait_ms: 1.1945418759033934
    mean_inference_ms: 4.30402737255511
    mean_raw_obs_processing_ms: 0.37727055331773734
  time_since_restore: 7103.556335926056
  time_this_iter_s: 25.660346269607544
  time_total_s: 7103.556335926056
  timers:
    learn_throughput: 8675.491
    learn_time_ms: 18649.318
    sample_throughput: 23854.708
    sample_time_ms: 6782.393
    update_time_ms: 29.884
  timestamp: 1602739002
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:16:43,204	WARNING util.py:136 -- The `process_trial` operation took 0.7947471141815186 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    277 |          7103.56 | 44816384 |  306.131 |              333.354 |              160.172 |            773.826 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2963.427891496663
    time_step_min: 2786
  date: 2020-10-15_05-17-08
  done: false
  episode_len_mean: 773.85542210198
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.2040050345782
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 201
  episodes_total: 58031
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9588263886522345e-38
        cur_lr: 5.0e-05
        entropy: 0.06830848256746928
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008656364535757652
        total_loss: .inf
        vf_explained_var: 0.9993141293525696
        vf_loss: 0.26036406060059863
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.176666666666673
    gpu_util_percent0: 0.36066666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464742994892922
    mean_env_wait_ms: 1.1944754083531097
    mean_inference_ms: 4.3039209770830045
    mean_raw_obs_processing_ms: 0.3772631540515918
  time_since_restore: 7129.132391214371
  time_this_iter_s: 25.57605528831482
  time_total_s: 7129.132391214371
  timers:
    learn_throughput: 8676.504
    learn_time_ms: 18647.143
    sample_throughput: 23877.121
    sample_time_ms: 6776.026
    update_time_ms: 29.911
  timestamp: 1602739028
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:17:09,819	WARNING util.py:136 -- The `process_trial` operation took 0.7779762744903564 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    278 |          7129.13 | 44978176 |  306.204 |              333.354 |              160.172 |            773.855 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2962.860975609756
    time_step_min: 2786
  date: 2020-10-15_05-17-35
  done: false
  episode_len_mean: 773.8906491366586
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.2886752596854
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 231
  episodes_total: 58262
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9382395829783517e-38
        cur_lr: 5.0e-05
        entropy: 0.06987595806519191
        entropy_coeff: 0.0005000000000000001
        kl: 0.006968920351937413
        model: {}
        policy_loss: -0.007202171404060209
        total_loss: 0.2919377386569977
        vf_explained_var: 0.9992260932922363
        vf_loss: 0.29917485515276593
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12333333333334
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647190463688578
    mean_env_wait_ms: 1.194398605005402
    mean_inference_ms: 4.303775932972677
    mean_raw_obs_processing_ms: 0.3772529846740853
  time_since_restore: 7154.834670305252
  time_this_iter_s: 25.702279090881348
  time_total_s: 7154.834670305252
  timers:
    learn_throughput: 8686.759
    learn_time_ms: 18625.127
    sample_throughput: 23896.887
    sample_time_ms: 6770.422
    update_time_ms: 29.376
  timestamp: 1602739055
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: af50e_00000
  
2020-10-15 05:17:36,670	WARNING util.py:136 -- The `process_trial` operation took 0.7896404266357422 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    279 |          7154.83 | 45139968 |  306.289 |              333.354 |              160.172 |            773.891 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2962.3965009586414
    time_step_min: 2786
  date: 2020-10-15_05-18-02
  done: false
  episode_len_mean: 773.9212597078244
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.35968740053704
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 58458
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9382395829783517e-38
        cur_lr: 5.0e-05
        entropy: 0.07212307242055734
        entropy_coeff: 0.0005000000000000001
        kl: 0.007691092556342483
        model: {}
        policy_loss: -0.009497175749856979
        total_loss: 0.22592605153719583
        vf_explained_var: 0.9993116855621338
        vf_loss: 0.23545928547779718
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9
    gpu_util_percent0: 0.33096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464697533196305
    mean_env_wait_ms: 1.1943330004567276
    mean_inference_ms: 4.303665240988677
    mean_raw_obs_processing_ms: 0.37724499295715547
  time_since_restore: 7180.554638385773
  time_this_iter_s: 25.71996808052063
  time_total_s: 7180.554638385773
  timers:
    learn_throughput: 8694.395
    learn_time_ms: 18608.769
    sample_throughput: 23940.218
    sample_time_ms: 6758.167
    update_time_ms: 29.053
  timestamp: 1602739082
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: af50e_00000
  
2020-10-15 05:18:03,443	WARNING util.py:136 -- The `process_trial` operation took 0.7750332355499268 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    280 |          7180.55 | 45301760 |   306.36 |              333.354 |              160.172 |            773.921 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2961.9548502687485
    time_step_min: 2786
  date: 2020-10-15_05-18-28
  done: false
  episode_len_mean: 773.9511995498491
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.4251487197928
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 189
  episodes_total: 58647
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9382395829783517e-38
        cur_lr: 5.0e-05
        entropy: 0.0798992421478033
        entropy_coeff: 0.0005000000000000001
        kl: 0.005833944771438837
        model: {}
        policy_loss: -0.011638141479731226
        total_loss: 0.37683215985695523
        vf_explained_var: 0.9989094138145447
        vf_loss: 0.38851026197274524
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.456666666666663
    gpu_util_percent0: 0.38033333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464680621430998
    mean_env_wait_ms: 1.1942693782480094
    mean_inference_ms: 4.303564503665438
    mean_raw_obs_processing_ms: 0.3772382661320872
  time_since_restore: 7206.051055908203
  time_this_iter_s: 25.49641752243042
  time_total_s: 7206.051055908203
  timers:
    learn_throughput: 8698.04
    learn_time_ms: 18600.971
    sample_throughput: 23971.6
    sample_time_ms: 6749.32
    update_time_ms: 27.696
  timestamp: 1602739108
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: af50e_00000
  
2020-10-15 05:18:30,022	WARNING util.py:136 -- The `process_trial` operation took 0.8129417896270752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    281 |          7206.05 | 45463552 |  306.425 |              333.354 |              160.172 |            773.951 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2961.502668434292
    time_step_min: 2786
  date: 2020-10-15_05-18-55
  done: false
  episode_len_mean: 773.9818607969021
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.49538130721237
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 231
  episodes_total: 58878
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9382395829783517e-38
        cur_lr: 5.0e-05
        entropy: 0.07870030775666237
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007516250053110222
        total_loss: .inf
        vf_explained_var: 0.9975588321685791
        vf_loss: 0.987242524822553
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.049999999999994
    gpu_util_percent0: 0.4
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464657352476754
    mean_env_wait_ms: 1.1941925643919324
    mean_inference_ms: 4.303432714360822
    mean_raw_obs_processing_ms: 0.37722869112895435
  time_since_restore: 7231.629187583923
  time_this_iter_s: 25.578131675720215
  time_total_s: 7231.629187583923
  timers:
    learn_throughput: 8708.618
    learn_time_ms: 18578.379
    sample_throughput: 23988.082
    sample_time_ms: 6744.683
    update_time_ms: 27.983
  timestamp: 1602739135
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:18:56,633	WARNING util.py:136 -- The `process_trial` operation took 0.7552189826965332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    282 |          7231.63 | 45625344 |  306.495 |              333.354 |              160.172 |            773.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2961.0487263735517
    time_step_min: 2786
  date: 2020-10-15_05-19-22
  done: false
  episode_len_mean: 774.0133026436042
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.5633712475941
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 208
  episodes_total: 59086
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.4073593744675265e-38
        cur_lr: 5.0e-05
        entropy: 0.0710814173022906
        entropy_coeff: 0.0005000000000000001
        kl: 0.003940159687772393
        model: {}
        policy_loss: -0.007499456376535818
        total_loss: 0.42973912755648297
        vf_explained_var: 0.998770534992218
        vf_loss: 0.43727412074804306
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.848387096774196
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646360853675924
    mean_env_wait_ms: 1.1941231942965502
    mean_inference_ms: 4.303310176009908
    mean_raw_obs_processing_ms: 0.37722022331242067
  time_since_restore: 7257.45859837532
  time_this_iter_s: 25.829410791397095
  time_total_s: 7257.45859837532
  timers:
    learn_throughput: 8705.796
    learn_time_ms: 18584.402
    sample_throughput: 24004.604
    sample_time_ms: 6740.04
    update_time_ms: 27.684
  timestamp: 1602739162
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: af50e_00000
  
2020-10-15 05:19:23,509	WARNING util.py:136 -- The `process_trial` operation took 0.7634427547454834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    283 |          7257.46 | 45787136 |  306.563 |              333.354 |              160.172 |            774.013 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2960.636830285531
    time_step_min: 2786
  date: 2020-10-15_05-19-49
  done: false
  episode_len_mean: 774.0434320425209
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.6236983860369
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 179
  episodes_total: 59265
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2036796872337632e-38
        cur_lr: 5.0e-05
        entropy: 0.07242312716941039
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009157133002493842
        total_loss: .inf
        vf_explained_var: 0.9988318085670471
        vf_loss: 0.3990934044122696
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.51
    gpu_util_percent0: 0.32433333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464619316455704
    mean_env_wait_ms: 1.1940636273818839
    mean_inference_ms: 4.30322228497053
    mean_raw_obs_processing_ms: 0.37721377870694106
  time_since_restore: 7283.3317041397095
  time_this_iter_s: 25.873105764389038
  time_total_s: 7283.3317041397095
  timers:
    learn_throughput: 8701.769
    learn_time_ms: 18593.0
    sample_throughput: 23997.403
    sample_time_ms: 6742.063
    update_time_ms: 25.535
  timestamp: 1602739189
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:19:50,494	WARNING util.py:136 -- The `process_trial` operation took 0.8142871856689453 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    284 |          7283.33 | 45948928 |  306.624 |              333.354 |              160.172 |            774.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2960.145301014315
    time_step_min: 2786
  date: 2020-10-15_05-20-16
  done: false
  episode_len_mean: 774.0798440100183
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.7003950516917
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 59491
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3055195308506466e-38
        cur_lr: 5.0e-05
        entropy: 0.07376580126583576
        entropy_coeff: 0.0005000000000000001
        kl: 0.004999478890871008
        model: {}
        policy_loss: -0.008697275773253446
        total_loss: 0.5050692930817604
        vf_explained_var: 0.9986966252326965
        vf_loss: 0.5138034597039223
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56451612903226
    gpu_util_percent0: 0.27516129032258063
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464602772264265
    mean_env_wait_ms: 1.1939885558704035
    mean_inference_ms: 4.3031054060106415
    mean_raw_obs_processing_ms: 0.3772049026129903
  time_since_restore: 7309.3459367752075
  time_this_iter_s: 26.014232635498047
  time_total_s: 7309.3459367752075
  timers:
    learn_throughput: 8688.413
    learn_time_ms: 18621.582
    sample_throughput: 23975.522
    sample_time_ms: 6748.216
    update_time_ms: 25.857
  timestamp: 1602739216
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: af50e_00000
  
2020-10-15 05:20:17,566	WARNING util.py:136 -- The `process_trial` operation took 0.7866024971008301 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    285 |          7309.35 | 46110720 |    306.7 |              333.354 |              160.172 |             774.08 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2959.662532472974
    time_step_min: 2786
  date: 2020-10-15_05-20-43
  done: false
  episode_len_mean: 774.11546384846
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.7726157009488
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 216
  episodes_total: 59707
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6527597654253233e-38
        cur_lr: 5.0e-05
        entropy: 0.07259775201479594
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034857146286716065
        model: {}
        policy_loss: -0.008494651313715925
        total_loss: 0.6517341881990433
        vf_explained_var: 0.9982879757881165
        vf_loss: 0.6602651675542196
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.219354838709673
    gpu_util_percent0: 0.3951612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645774590952132
    mean_env_wait_ms: 1.1939161537289078
    mean_inference_ms: 4.302968189628493
    mean_raw_obs_processing_ms: 0.377195967073852
  time_since_restore: 7335.233179092407
  time_this_iter_s: 25.887242317199707
  time_total_s: 7335.233179092407
  timers:
    learn_throughput: 8692.157
    learn_time_ms: 18613.563
    sample_throughput: 23984.877
    sample_time_ms: 6745.584
    update_time_ms: 25.852
  timestamp: 1602739243
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: af50e_00000
  
2020-10-15 05:20:44,540	WARNING util.py:136 -- The `process_trial` operation took 0.7880568504333496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    286 |          7335.23 | 46272512 |  306.773 |              333.354 |              160.172 |            774.115 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2959.2778966012766
    time_step_min: 2786
  date: 2020-10-15_05-21-10
  done: false
  episode_len_mean: 774.1454214533796
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.8313479775039
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 59888
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.263798827126616e-39
        cur_lr: 5.0e-05
        entropy: 0.07191628403961658
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043137643951922655
        model: {}
        policy_loss: -0.010344261613984903
        total_loss: 0.48754285275936127
        vf_explained_var: 0.9986142516136169
        vf_loss: 0.4979230836033821
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12
    gpu_util_percent0: 0.368
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645588019593941
    mean_env_wait_ms: 1.1938558778662782
    mean_inference_ms: 4.302878932455769
    mean_raw_obs_processing_ms: 0.37718944345928823
  time_since_restore: 7360.910203456879
  time_this_iter_s: 25.677024364471436
  time_total_s: 7360.910203456879
  timers:
    learn_throughput: 8690.141
    learn_time_ms: 18617.879
    sample_throughput: 23959.181
    sample_time_ms: 6752.819
    update_time_ms: 24.052
  timestamp: 1602739270
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: af50e_00000
  
2020-10-15 05:21:11,270	WARNING util.py:136 -- The `process_trial` operation took 0.7704877853393555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    287 |          7360.91 | 46434304 |  306.831 |              333.354 |              160.172 |            774.145 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2958.799154122819
    time_step_min: 2786
  date: 2020-10-15_05-21-36
  done: false
  episode_len_mean: 774.1801890245931
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.9051182731503
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 210
  episodes_total: 60098
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.131899413563308e-39
        cur_lr: 5.0e-05
        entropy: 0.07510878704488277
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060388641043876605
        model: {}
        policy_loss: -0.007679429003170905
        total_loss: 0.23298160483439764
        vf_explained_var: 0.9993768334388733
        vf_loss: 0.24069859211643538
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03333333333334
    gpu_util_percent0: 0.3823333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645421585002333
    mean_env_wait_ms: 1.193785110268954
    mean_inference_ms: 4.3027697960841085
    mean_raw_obs_processing_ms: 0.37718196663415543
  time_since_restore: 7386.323945522308
  time_this_iter_s: 25.413742065429688
  time_total_s: 7386.323945522308
  timers:
    learn_throughput: 8694.825
    learn_time_ms: 18607.849
    sample_throughput: 23993.925
    sample_time_ms: 6743.04
    update_time_ms: 24.989
  timestamp: 1602739296
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: af50e_00000
  
2020-10-15 05:21:37,892	WARNING util.py:136 -- The `process_trial` operation took 0.841498851776123 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    288 |          7386.32 | 46596096 |  306.905 |              333.354 |              160.172 |             774.18 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2958.2735701963907
    time_step_min: 2786
  date: 2020-10-15_05-22-03
  done: false
  episode_len_mean: 774.2206530747555
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 306.98495396531183
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 232
  episodes_total: 60330
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.131899413563308e-39
        cur_lr: 5.0e-05
        entropy: 0.07500026623408
        entropy_coeff: 0.0005000000000000001
        kl: 0.006360095498772959
        model: {}
        policy_loss: -0.009221192798577249
        total_loss: 0.27070964003602666
        vf_explained_var: 0.9994023442268372
        vf_loss: 0.2799683374663194
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.951612903225808
    gpu_util_percent0: 0.342258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645207263918372
    mean_env_wait_ms: 1.193708441551609
    mean_inference_ms: 4.302634140190345
    mean_raw_obs_processing_ms: 0.3771726577838939
  time_since_restore: 7412.013117551804
  time_this_iter_s: 25.68917202949524
  time_total_s: 7412.013117551804
  timers:
    learn_throughput: 8701.695
    learn_time_ms: 18593.159
    sample_throughput: 23954.05
    sample_time_ms: 6754.265
    update_time_ms: 25.537
  timestamp: 1602739323
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: af50e_00000
  
2020-10-15 05:22:04,687	WARNING util.py:136 -- The `process_trial` operation took 0.8026695251464844 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    289 |          7412.01 | 46757888 |  306.985 |              333.354 |              160.172 |            774.221 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2957.914259256197
    time_step_min: 2786
  date: 2020-10-15_05-22-30
  done: false
  episode_len_mean: 774.2533421465753
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.03467376399686
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 185
  episodes_total: 60515
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.131899413563308e-39
        cur_lr: 5.0e-05
        entropy: 0.08830289480586846
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011412349180318415
        total_loss: .inf
        vf_explained_var: 0.9974440932273865
        vf_loss: 0.8930655320485433
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92666666666667
    gpu_util_percent0: 0.3203333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644992285102557
    mean_env_wait_ms: 1.193645853217943
    mean_inference_ms: 4.302538699892229
    mean_raw_obs_processing_ms: 0.3771658290240067
  time_since_restore: 7437.663213253021
  time_this_iter_s: 25.65009570121765
  time_total_s: 7437.663213253021
  timers:
    learn_throughput: 8703.698
    learn_time_ms: 18588.881
    sample_throughput: 23941.586
    sample_time_ms: 6757.781
    update_time_ms: 25.405
  timestamp: 1602739350
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:22:31,565	WARNING util.py:136 -- The `process_trial` operation took 0.835777759552002 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    290 |          7437.66 | 46919680 |  307.035 |              333.354 |              160.172 |            774.253 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2957.609552691433
    time_step_min: 2786
  date: 2020-10-15_05-22-57
  done: false
  episode_len_mean: 774.2894788853021
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.0777022148487
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 201
  episodes_total: 60716
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.197849120344961e-39
        cur_lr: 5.0e-05
        entropy: 0.08887688939770062
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012377543665934354
        total_loss: .inf
        vf_explained_var: 0.9973368048667908
        vf_loss: 1.0354344795147579
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77096774193549
    gpu_util_percent0: 0.3470967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464483069206285
    mean_env_wait_ms: 1.1935788173989008
    mean_inference_ms: 4.3024431079923735
    mean_raw_obs_processing_ms: 0.37715904351052076
  time_since_restore: 7463.46594786644
  time_this_iter_s: 25.80273461341858
  time_total_s: 7463.46594786644
  timers:
    learn_throughput: 8694.623
    learn_time_ms: 18608.281
    sample_throughput: 23914.251
    sample_time_ms: 6765.506
    update_time_ms: 27.112
  timestamp: 1602739377
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:22:58,448	WARNING util.py:136 -- The `process_trial` operation took 0.8008654117584229 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    291 |          7463.47 | 47081472 |  307.078 |              333.354 |              160.172 |            774.289 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2957.1804538438805
    time_step_min: 2786
  date: 2020-10-15_05-23-24
  done: false
  episode_len_mean: 774.3273496980835
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.1435447610787
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 228
  episodes_total: 60944
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.296773680517441e-39
        cur_lr: 5.0e-05
        entropy: 0.0808724295347929
        entropy_coeff: 0.0005000000000000001
        kl: 0.003746875076709936
        model: {}
        policy_loss: -0.009511189620146373
        total_loss: 0.7110746701558431
        vf_explained_var: 0.9982227683067322
        vf_loss: 0.720626304546992
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.35483870967742
    gpu_util_percent0: 0.31290322580645163
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644631081203277
    mean_env_wait_ms: 1.1935029987259524
    mean_inference_ms: 4.302315322050129
    mean_raw_obs_processing_ms: 0.3771502540057498
  time_since_restore: 7489.309958457947
  time_this_iter_s: 25.844010591506958
  time_total_s: 7489.309958457947
  timers:
    learn_throughput: 8687.817
    learn_time_ms: 18622.859
    sample_throughput: 23910.664
    sample_time_ms: 6766.52
    update_time_ms: 28.431
  timestamp: 1602739404
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: af50e_00000
  
2020-10-15 05:23:25,404	WARNING util.py:136 -- The `process_trial` operation took 0.8206894397735596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    292 |          7489.31 | 47243264 |  307.144 |              333.354 |              160.172 |            774.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2956.765719616379
    time_step_min: 2786
  date: 2020-10-15_05-23-50
  done: false
  episode_len_mean: 774.3580236818003
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.20679630929186
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 200
  episodes_total: 61144
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6483868402587205e-39
        cur_lr: 5.0e-05
        entropy: 0.07676281159122784
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010194810514803976
        total_loss: .inf
        vf_explained_var: 0.9993281364440918
        vf_loss: 0.23517602930466333
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.773333333333333
    gpu_util_percent0: 0.32266666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464442660261139
    mean_env_wait_ms: 1.193436206764041
    mean_inference_ms: 4.302210743397395
    mean_raw_obs_processing_ms: 0.3771427624397702
  time_since_restore: 7514.871305704117
  time_this_iter_s: 25.561347246170044
  time_total_s: 7514.871305704117
  timers:
    learn_throughput: 8703.103
    learn_time_ms: 18590.151
    sample_throughput: 23887.664
    sample_time_ms: 6773.036
    update_time_ms: 28.223
  timestamp: 1602739430
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:23:52,060	WARNING util.py:136 -- The `process_trial` operation took 0.808974027633667 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    293 |          7514.87 | 47405056 |  307.207 |              333.354 |              160.172 |            774.358 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2956.3869436104947
    time_step_min: 2786
  date: 2020-10-15_05-24-17
  done: false
  episode_len_mean: 774.389320071743
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.2649040544033
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 61330
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.972580260388082e-39
        cur_lr: 5.0e-05
        entropy: 0.07689749511579673
        entropy_coeff: 0.0005000000000000001
        kl: 0.004774824483320117
        model: {}
        policy_loss: -0.009120993638741007
        total_loss: 0.5030659784873327
        vf_explained_var: 0.9985814094543457
        vf_loss: 0.5122254267334938
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89666666666666
    gpu_util_percent0: 0.356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644277959337587
    mean_env_wait_ms: 1.1933735357634152
    mean_inference_ms: 4.302120193556599
    mean_raw_obs_processing_ms: 0.37713641213078336
  time_since_restore: 7540.444648265839
  time_this_iter_s: 25.5733425617218
  time_total_s: 7540.444648265839
  timers:
    learn_throughput: 8715.562
    learn_time_ms: 18563.575
    sample_throughput: 23901.041
    sample_time_ms: 6769.245
    update_time_ms: 28.375
  timestamp: 1602739457
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: af50e_00000
  
2020-10-15 05:24:18,735	WARNING util.py:136 -- The `process_trial` operation took 0.8150372505187988 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    294 |          7540.44 | 47566848 |  307.265 |              333.354 |              160.172 |            774.389 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2955.9149178275925
    time_step_min: 2786
  date: 2020-10-15_05-24-44
  done: false
  episode_len_mean: 774.4257216653941
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.3365632149561
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 61559
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.486290130194041e-39
        cur_lr: 5.0e-05
        entropy: 0.07453150115907192
        entropy_coeff: 0.0005000000000000001
        kl: 0.004138474837721636
        model: {}
        policy_loss: -0.007285423523474795
        total_loss: 0.4624177540342013
        vf_explained_var: 0.9988214373588562
        vf_loss: 0.46974043796459836
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16333333333334
    gpu_util_percent0: 0.2806666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644083606471323
    mean_env_wait_ms: 1.19329781608126
    mean_inference_ms: 4.302004659820709
    mean_raw_obs_processing_ms: 0.3771282666597
  time_since_restore: 7565.981273651123
  time_this_iter_s: 25.536625385284424
  time_total_s: 7565.981273651123
  timers:
    learn_throughput: 8732.827
    learn_time_ms: 18526.875
    sample_throughput: 23946.307
    sample_time_ms: 6756.449
    update_time_ms: 26.489
  timestamp: 1602739484
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: af50e_00000
  
2020-10-15 05:24:45,496	WARNING util.py:136 -- The `process_trial` operation took 0.8789608478546143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    295 |          7565.98 | 47728640 |  307.337 |              333.354 |              160.172 |            774.426 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2955.5046494297562
    time_step_min: 2786
  date: 2020-10-15_05-25-11
  done: false
  episode_len_mean: 774.4590092277805
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.39959085758
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 211
  episodes_total: 61770
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7431450650970205e-39
        cur_lr: 5.0e-05
        entropy: 0.07450422830879688
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045007171186928945
        model: {}
        policy_loss: -0.012340640377563735
        total_loss: 0.5033344502250353
        vf_explained_var: 0.9986127018928528
        vf_loss: 0.5157123332222303
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.532258064516125
    gpu_util_percent0: 0.3190322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464387373860624
    mean_env_wait_ms: 1.1932270570308197
    mean_inference_ms: 4.301888280852427
    mean_raw_obs_processing_ms: 0.3771202454240078
  time_since_restore: 7592.031647205353
  time_this_iter_s: 26.050373554229736
  time_total_s: 7592.031647205353
  timers:
    learn_throughput: 8725.327
    learn_time_ms: 18542.802
    sample_throughput: 23923.021
    sample_time_ms: 6763.025
    update_time_ms: 28.343
  timestamp: 1602739511
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: af50e_00000
  
2020-10-15 05:25:12,657	WARNING util.py:136 -- The `process_trial` operation took 0.8235251903533936 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    296 |          7592.03 | 47890432 |    307.4 |              333.354 |              160.172 |            774.459 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2955.1555720677793
    time_step_min: 2786
  date: 2020-10-15_05-25-38
  done: false
  episode_len_mean: 774.4844953106588
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.4534816925815
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 179
  episodes_total: 61949
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.715725325485103e-40
        cur_lr: 5.0e-05
        entropy: 0.0722575020045042
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00984539157555749
        total_loss: .inf
        vf_explained_var: 0.9988353252410889
        vf_loss: 0.40682928015788394
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.86129032258065
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643726629627027
    mean_env_wait_ms: 1.1931669780129583
    mean_inference_ms: 4.30180839976823
    mean_raw_obs_processing_ms: 0.3771144939770108
  time_since_restore: 7617.81472325325
  time_this_iter_s: 25.78307604789734
  time_total_s: 7617.81472325325
  timers:
    learn_throughput: 8728.804
    learn_time_ms: 18535.415
    sample_throughput: 23893.543
    sample_time_ms: 6771.369
    update_time_ms: 28.801
  timestamp: 1602739538
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:25:39,553	WARNING util.py:136 -- The `process_trial` operation took 0.8217108249664307 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    297 |          7617.81 | 48052224 |  307.453 |              333.354 |              160.172 |            774.484 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2954.6987412677463
    time_step_min: 2786
  date: 2020-10-15_05-26-05
  done: false
  episode_len_mean: 774.5135278599922
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.51998754758995
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 219
  episodes_total: 62168
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3073587988227651e-39
        cur_lr: 5.0e-05
        entropy: 0.07626582371691863
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009053761508160582
        total_loss: .inf
        vf_explained_var: 0.9987108707427979
        vf_loss: 0.50821486612161
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.869999999999997
    gpu_util_percent0: 0.30500000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643581235907463
    mean_env_wait_ms: 1.1930942300408987
    mean_inference_ms: 4.301707550666916
    mean_raw_obs_processing_ms: 0.37710699470178843
  time_since_restore: 7643.578538179398
  time_this_iter_s: 25.76381492614746
  time_total_s: 7643.578538179398
  timers:
    learn_throughput: 8721.308
    learn_time_ms: 18551.345
    sample_throughput: 23825.492
    sample_time_ms: 6790.71
    update_time_ms: 28.179
  timestamp: 1602739565
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:26:06,437	WARNING util.py:136 -- The `process_trial` operation took 0.8303067684173584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    298 |          7643.58 | 48214016 |   307.52 |              333.354 |              160.172 |            774.514 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2954.242076730609
    time_step_min: 2786
  date: 2020-10-15_05-26-32
  done: false
  episode_len_mean: 774.5408559063952
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.5862115302729
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 62390
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9610381982341478e-39
        cur_lr: 5.0e-05
        entropy: 0.07436256172756354
        entropy_coeff: 0.0005000000000000001
        kl: 0.00479010830167681
        model: {}
        policy_loss: -0.011798040672147181
        total_loss: 0.47290728489557904
        vf_explained_var: 0.9987495541572571
        vf_loss: 0.48474249492088956
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.919354838709673
    gpu_util_percent0: 0.3467741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464335413163104
    mean_env_wait_ms: 1.1930200709218108
    mean_inference_ms: 4.301578980680994
    mean_raw_obs_processing_ms: 0.37709842705171864
  time_since_restore: 7669.410721063614
  time_this_iter_s: 25.83218288421631
  time_total_s: 7669.410721063614
  timers:
    learn_throughput: 8719.492
    learn_time_ms: 18555.21
    sample_throughput: 23811.306
    sample_time_ms: 6794.755
    update_time_ms: 26.855
  timestamp: 1602739592
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: af50e_00000
  
2020-10-15 05:26:33,408	WARNING util.py:136 -- The `process_trial` operation took 0.8435788154602051 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    299 |          7669.41 | 48375808 |  307.586 |              333.354 |              160.172 |            774.541 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2953.88765751935
    time_step_min: 2786
  date: 2020-10-15_05-26-59
  done: false
  episode_len_mean: 774.5619106977339
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.6390842938928
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 62574
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.805190991170739e-40
        cur_lr: 5.0e-05
        entropy: 0.07366984089215596
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009321264612178007
        total_loss: .inf
        vf_explained_var: 0.9988253712654114
        vf_loss: 0.41137709965308505
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.843333333333337
    gpu_util_percent0: 0.3763333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643173717333546
    mean_env_wait_ms: 1.1929591380313906
    mean_inference_ms: 4.301496150053692
    mean_raw_obs_processing_ms: 0.3770925647095527
  time_since_restore: 7695.195334196091
  time_this_iter_s: 25.784613132476807
  time_total_s: 7695.195334196091
  timers:
    learn_throughput: 8721.404
    learn_time_ms: 18551.142
    sample_throughput: 23746.653
    sample_time_ms: 6813.255
    update_time_ms: 26.997
  timestamp: 1602739619
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:27:00,323	WARNING util.py:136 -- The `process_trial` operation took 0.8349597454071045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    300 |           7695.2 | 48537600 |  307.639 |              333.354 |              160.172 |            774.562 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2953.4797985432638
    time_step_min: 2786
  date: 2020-10-15_05-27-26
  done: false
  episode_len_mean: 774.5895516445011
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.7022482530166
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 211
  episodes_total: 62785
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.470778648675611e-39
        cur_lr: 5.0e-05
        entropy: 0.07353611290454865
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00905165687436238
        total_loss: .inf
        vf_explained_var: 0.9987204670906067
        vf_loss: 0.4683483416835467
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83870967741936
    gpu_util_percent0: 0.31354838709677424
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464301042184875
    mean_env_wait_ms: 1.1928882701961598
    mean_inference_ms: 4.3013956991477125
    mean_raw_obs_processing_ms: 0.37708539352656173
  time_since_restore: 7721.332389593124
  time_this_iter_s: 26.13705539703369
  time_total_s: 7721.332389593124
  timers:
    learn_throughput: 8711.449
    learn_time_ms: 18572.34
    sample_throughput: 23703.844
    sample_time_ms: 6825.56
    update_time_ms: 25.328
  timestamp: 1602739646
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:27:27,758	WARNING util.py:136 -- The `process_trial` operation took 0.8885242938995361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    301 |          7721.33 | 48699392 |  307.702 |              333.354 |              160.172 |             774.59 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2953.0105602489994
    time_step_min: 2786
  date: 2020-10-15_05-27-53
  done: false
  episode_len_mean: 774.6222744152093
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.77375317910736
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 63014
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2061679730134166e-39
        cur_lr: 5.0e-05
        entropy: 0.07146194577217102
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008889170926219473
        total_loss: .inf
        vf_explained_var: 0.9993236660957336
        vf_loss: 0.25698406373461086
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.722580645161294
    gpu_util_percent0: 0.34225806451612906
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642821988417154
    mean_env_wait_ms: 1.1928130530267933
    mean_inference_ms: 4.301274173679094
    mean_raw_obs_processing_ms: 0.37707729472874346
  time_since_restore: 7747.03403878212
  time_this_iter_s: 25.70164918899536
  time_total_s: 7747.03403878212
  timers:
    learn_throughput: 8718.126
    learn_time_ms: 18558.117
    sample_throughput: 23707.477
    sample_time_ms: 6824.514
    update_time_ms: 23.45
  timestamp: 1602739673
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:27:54,616	WARNING util.py:136 -- The `process_trial` operation took 0.8373875617980957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    302 |          7747.03 | 48861184 |  307.774 |              333.354 |              160.172 |            774.622 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2952.6397504710335
    time_step_min: 2786
  date: 2020-10-15_05-28-20
  done: false
  episode_len_mean: 774.6470309014098
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.83120472297844
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 63201
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3092519595201247e-39
        cur_lr: 5.0e-05
        entropy: 0.06920248890916507
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0076101059081944795
        total_loss: .inf
        vf_explained_var: 0.9992899894714355
        vf_loss: 0.2352579745153586
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.376666666666672
    gpu_util_percent0: 0.357
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642632938074068
    mean_env_wait_ms: 1.1927500548006207
    mean_inference_ms: 4.30118906499514
    mean_raw_obs_processing_ms: 0.37707100374837904
  time_since_restore: 7772.586872577667
  time_this_iter_s: 25.552833795547485
  time_total_s: 7772.586872577667
  timers:
    learn_throughput: 8713.197
    learn_time_ms: 18568.615
    sample_throughput: 23723.373
    sample_time_ms: 6819.941
    update_time_ms: 23.813
  timestamp: 1602739700
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:28:21,301	WARNING util.py:136 -- The `process_trial` operation took 0.8331253528594971 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    303 |          7772.59 | 49022976 |  307.831 |              333.354 |              160.172 |            774.647 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2952.2483388835403
    time_step_min: 2786
  date: 2020-10-15_05-28-46
  done: false
  episode_len_mean: 774.6756462627951
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.88980622113104
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 202
  episodes_total: 63403
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.9638779392801864e-39
        cur_lr: 5.0e-05
        entropy: 0.07034664849440257
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008432387961268736
        total_loss: .inf
        vf_explained_var: 0.9982314109802246
        vf_loss: 0.6409568736950556
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.47741935483871
    gpu_util_percent0: 0.3283870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464248405438823
    mean_env_wait_ms: 1.1926826610685075
    mean_inference_ms: 4.301100702102581
    mean_raw_obs_processing_ms: 0.377064760338476
  time_since_restore: 7798.248508453369
  time_this_iter_s: 25.661635875701904
  time_total_s: 7798.248508453369
  timers:
    learn_throughput: 8710.5
    learn_time_ms: 18574.364
    sample_throughput: 23723.574
    sample_time_ms: 6819.883
    update_time_ms: 25.568
  timestamp: 1602739726
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:28:48,296	WARNING util.py:136 -- The `process_trial` operation took 0.8628804683685303 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    304 |          7798.25 | 49184768 |   307.89 |              333.354 |              160.172 |            774.676 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2951.8222721982825
    time_step_min: 2786
  date: 2020-10-15_05-29-14
  done: false
  episode_len_mean: 774.70401395612
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 307.95375900197655
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 225
  episodes_total: 63628
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.445816908920279e-39
        cur_lr: 5.0e-05
        entropy: 0.06950649370749791
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009447466174606234
        total_loss: .inf
        vf_explained_var: 0.9989078640937805
        vf_loss: 0.4308674211303393
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.46
    gpu_util_percent0: 0.34900000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464231147573771
    mean_env_wait_ms: 1.1926078251875336
    mean_inference_ms: 4.30098788923995
    mean_raw_obs_processing_ms: 0.3770568242277437
  time_since_restore: 7824.116273403168
  time_this_iter_s: 25.867764949798584
  time_total_s: 7824.116273403168
  timers:
    learn_throughput: 8699.726
    learn_time_ms: 18597.368
    sample_throughput: 23683.003
    sample_time_ms: 6831.566
    update_time_ms: 25.738
  timestamp: 1602739754
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:29:15,410	WARNING util.py:136 -- The `process_trial` operation took 0.8607223033905029 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    305 |          7824.12 | 49346560 |  307.954 |              333.354 |              160.172 |            774.704 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2951.4088607396493
    time_step_min: 2786
  date: 2020-10-15_05-29-41
  done: false
  episode_len_mean: 774.733287377211
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.0154919924145
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 201
  episodes_total: 63829
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1168725363380421e-38
        cur_lr: 5.0e-05
        entropy: 0.06561924641331036
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008943538695651418
        total_loss: .inf
        vf_explained_var: 0.9990392327308655
        vf_loss: 0.3349000761906306
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55161290322581
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642111280506726
    mean_env_wait_ms: 1.192540848919576
    mean_inference_ms: 4.300888709150553
    mean_raw_obs_processing_ms: 0.37704990782709796
  time_since_restore: 7850.0247797966
  time_this_iter_s: 25.908506393432617
  time_total_s: 7850.0247797966
  timers:
    learn_throughput: 8712.699
    learn_time_ms: 18569.677
    sample_throughput: 23672.214
    sample_time_ms: 6834.68
    update_time_ms: 26.397
  timestamp: 1602739781
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:29:42,507	WARNING util.py:136 -- The `process_trial` operation took 0.8862721920013428 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    306 |          7850.02 | 49508352 |  308.015 |              333.354 |              160.172 |            774.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2951.037358343103
    time_step_min: 2786
  date: 2020-10-15_05-30-08
  done: false
  episode_len_mean: 774.7579392973742
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.0702251911304
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 64017
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.675308804507063e-38
        cur_lr: 5.0e-05
        entropy: 0.06916310886542003
        entropy_coeff: 0.0005000000000000001
        kl: 0.005571167256372671
        model: {}
        policy_loss: -0.009253364507458173
        total_loss: 0.2891332159439723
        vf_explained_var: 0.999138593673706
        vf_loss: 0.2984211767713229
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.073333333333338
    gpu_util_percent0: 0.33166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641967829979496
    mean_env_wait_ms: 1.192478673885467
    mean_inference_ms: 4.300808339799363
    mean_raw_obs_processing_ms: 0.377044309341831
  time_since_restore: 7875.8270354270935
  time_this_iter_s: 25.802255630493164
  time_total_s: 7875.8270354270935
  timers:
    learn_throughput: 8704.231
    learn_time_ms: 18587.743
    sample_throughput: 23703.126
    sample_time_ms: 6825.766
    update_time_ms: 26.69
  timestamp: 1602739808
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: af50e_00000
  
2020-10-15 05:30:09,582	WARNING util.py:136 -- The `process_trial` operation took 0.8769845962524414 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    307 |          7875.83 | 49670144 |   308.07 |              333.354 |              160.172 |            774.758 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2950.5919066681204
    time_step_min: 2786
  date: 2020-10-15_05-30-35
  done: false
  episode_len_mean: 774.7908721572778
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.1369948413982
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 64243
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.675308804507063e-38
        cur_lr: 5.0e-05
        entropy: 0.06647978102167447
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008433115440614833
        total_loss: .inf
        vf_explained_var: 0.9991247057914734
        vf_loss: 0.33446814119815826
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.519354838709674
    gpu_util_percent0: 0.36935483870967745
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641807317571434
    mean_env_wait_ms: 1.192403050777773
    mean_inference_ms: 4.300705419388029
    mean_raw_obs_processing_ms: 0.3770367947633129
  time_since_restore: 7901.577458381653
  time_this_iter_s: 25.750422954559326
  time_total_s: 7901.577458381653
  timers:
    learn_throughput: 8698.585
    learn_time_ms: 18599.808
    sample_throughput: 23782.84
    sample_time_ms: 6802.888
    update_time_ms: 25.932
  timestamp: 1602739835
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:30:36,491	WARNING util.py:136 -- The `process_trial` operation took 0.8528339862823486 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    308 |          7901.58 | 49831936 |  308.137 |              333.354 |              160.172 |            774.791 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2950.1915917594274
    time_step_min: 2786
  date: 2020-10-15_05-31-02
  done: false
  episode_len_mean: 774.8207276394384
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.19832723323515
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 64455
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.512963206760594e-38
        cur_lr: 5.0e-05
        entropy: 0.0669019731382529
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008752096985214545
        total_loss: .inf
        vf_explained_var: 0.9987070560455322
        vf_loss: 0.47429924458265305
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.580645161290324
    gpu_util_percent0: 0.34580645161290324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641607053380556
    mean_env_wait_ms: 1.1923325706610732
    mean_inference_ms: 4.300597463622775
    mean_raw_obs_processing_ms: 0.37702964596238314
  time_since_restore: 7927.54006934166
  time_this_iter_s: 25.962610960006714
  time_total_s: 7927.54006934166
  timers:
    learn_throughput: 8687.99
    learn_time_ms: 18622.49
    sample_throughput: 23826.984
    sample_time_ms: 6790.284
    update_time_ms: 25.954
  timestamp: 1602739862
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:31:03,706	WARNING util.py:136 -- The `process_trial` operation took 0.8594300746917725 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    309 |          7927.54 | 49993728 |  308.198 |              333.354 |              160.172 |            774.821 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2949.850507802824
    time_step_min: 2786
  date: 2020-10-15_05-31-29
  done: false
  episode_len_mean: 774.8454064424296
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.24813768779774
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 179
  episodes_total: 64634
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7694448101408925e-38
        cur_lr: 5.0e-05
        entropy: 0.07131886544326942
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010923025693045929
        total_loss: .inf
        vf_explained_var: 0.9986245632171631
        vf_loss: 0.4700316811601321
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77333333333333
    gpu_util_percent0: 0.33066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641479043904196
    mean_env_wait_ms: 1.1922729332571789
    mean_inference_ms: 4.300523587722325
    mean_raw_obs_processing_ms: 0.377024386421305
  time_since_restore: 7953.234658718109
  time_this_iter_s: 25.694589376449585
  time_total_s: 7953.234658718109
  timers:
    learn_throughput: 8687.007
    learn_time_ms: 18624.597
    sample_throughput: 23877.084
    sample_time_ms: 6776.037
    update_time_ms: 27.641
  timestamp: 1602739889
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:31:30,585	WARNING util.py:136 -- The `process_trial` operation took 0.8850390911102295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    310 |          7953.23 | 50155520 |  308.248 |              333.354 |              160.172 |            774.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2949.450533851756
    time_step_min: 2786
  date: 2020-10-15_05-31-56
  done: false
  episode_len_mean: 774.8770623246061
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.3058781916676
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 220
  episodes_total: 64854
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.654167215211338e-38
        cur_lr: 5.0e-05
        entropy: 0.07331203545133273
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011035427545721177
        total_loss: .inf
        vf_explained_var: 0.9983127117156982
        vf_loss: 0.6965688169002533
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.722580645161294
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641346708810013
    mean_env_wait_ms: 1.192199649802247
    mean_inference_ms: 4.300431804312664
    mean_raw_obs_processing_ms: 0.37701742344269956
  time_since_restore: 7979.24902844429
  time_this_iter_s: 26.01436972618103
  time_total_s: 7979.24902844429
  timers:
    learn_throughput: 8696.876
    learn_time_ms: 18603.462
    sample_throughput: 23844.453
    sample_time_ms: 6785.31
    update_time_ms: 27.607
  timestamp: 1602739916
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:31:57,827	WARNING util.py:136 -- The `process_trial` operation took 0.8936820030212402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    311 |          7979.25 | 50317312 |  308.306 |              333.354 |              160.172 |            774.877 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2949.030521557291
    time_step_min: 2786
  date: 2020-10-15_05-32-23
  done: false
  episode_len_mean: 774.9094778573404
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.3682403183001
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 65078
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.481250822817008e-38
        cur_lr: 5.0e-05
        entropy: 0.07181686038772266
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01075423953201001
        total_loss: .inf
        vf_explained_var: 0.9991280436515808
        vf_loss: 0.32804498573144275
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.35483870967742
    gpu_util_percent0: 0.2929032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641125966682467
    mean_env_wait_ms: 1.1921256425045013
    mean_inference_ms: 4.30031325003439
    mean_raw_obs_processing_ms: 0.37700986632355643
  time_since_restore: 8005.165948629379
  time_this_iter_s: 25.91692018508911
  time_total_s: 8005.165948629379
  timers:
    learn_throughput: 8687.125
    learn_time_ms: 18624.344
    sample_throughput: 23835.222
    sample_time_ms: 6787.938
    update_time_ms: 27.699
  timestamp: 1602739943
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:32:24,934	WARNING util.py:136 -- The `process_trial` operation took 0.8788225650787354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    312 |          8005.17 | 50479104 |  308.368 |              333.354 |              160.172 |            774.909 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2948.702602002484
    time_step_min: 2786
  date: 2020-10-15_05-32-50
  done: false
  episode_len_mean: 774.9330381085181
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.4151318427837
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 183
  episodes_total: 65261
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.272187623422551e-37
        cur_lr: 5.0e-05
        entropy: 0.0724186897277832
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0113393327628728
        total_loss: .inf
        vf_explained_var: 0.998590886592865
        vf_loss: 0.5018436734875044
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.85333333333334
    gpu_util_percent0: 0.345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146409632684612
    mean_env_wait_ms: 1.1920648823798883
    mean_inference_ms: 4.300238239310364
    mean_raw_obs_processing_ms: 0.3770042454003489
  time_since_restore: 8030.889844179153
  time_this_iter_s: 25.72389554977417
  time_total_s: 8030.889844179153
  timers:
    learn_throughput: 8681.665
    learn_time_ms: 18636.058
    sample_throughput: 23816.194
    sample_time_ms: 6793.361
    update_time_ms: 27.5
  timestamp: 1602739970
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:32:51,871	WARNING util.py:136 -- The `process_trial` operation took 0.9088983535766602 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    313 |          8030.89 | 50640896 |  308.415 |              333.354 |              160.172 |            774.933 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2948.3308623483017
    time_step_min: 2786
  date: 2020-10-15_05-33-17
  done: false
  episode_len_mean: 774.9608663774668
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.47219151865676
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 207
  episodes_total: 65468
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.908281435133826e-37
        cur_lr: 5.0e-05
        entropy: 0.06872118761142094
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010923818898542473
        total_loss: .inf
        vf_explained_var: 0.9987562298774719
        vf_loss: 0.4694838722546895
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.609677419354846
    gpu_util_percent0: 0.28935483870967743
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464081939708518
    mean_env_wait_ms: 1.191996485174182
    mean_inference_ms: 4.300150977541084
    mean_raw_obs_processing_ms: 0.37699795832961364
  time_since_restore: 8056.64670586586
  time_this_iter_s: 25.756861686706543
  time_total_s: 8056.64670586586
  timers:
    learn_throughput: 8680.551
    learn_time_ms: 18638.449
    sample_throughput: 23813.395
    sample_time_ms: 6794.159
    update_time_ms: 25.637
  timestamp: 1602739997
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:33:18,846	WARNING util.py:136 -- The `process_trial` operation took 0.9068121910095215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    314 |          8056.65 | 50802688 |  308.472 |              333.354 |              160.172 |            774.961 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2947.90286663011
    time_step_min: 2786
  date: 2020-10-15_05-33-44
  done: false
  episode_len_mean: 774.989968642494
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.5381124239009
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 65694
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.862422152700739e-37
        cur_lr: 5.0e-05
        entropy: 0.06995456106960773
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008188589073445959
        total_loss: .inf
        vf_explained_var: 0.9993896484375
        vf_loss: 0.24565583964188895
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.956666666666667
    gpu_util_percent0: 0.3073333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640656991194637
    mean_env_wait_ms: 1.191922376233204
    mean_inference_ms: 4.300043942445137
    mean_raw_obs_processing_ms: 0.3769909808387643
  time_since_restore: 8082.457594156265
  time_this_iter_s: 25.810888290405273
  time_total_s: 8082.457594156265
  timers:
    learn_throughput: 8680.929
    learn_time_ms: 18637.636
    sample_throughput: 23834.541
    sample_time_ms: 6788.132
    update_time_ms: 25.401
  timestamp: 1602740024
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:33:45,959	WARNING util.py:136 -- The `process_trial` operation took 0.9006848335266113 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    315 |          8082.46 | 50964480 |  308.538 |              333.354 |              160.172 |             774.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2947.5795732401853
    time_step_min: 2786
  date: 2020-10-15_05-34-11
  done: false
  episode_len_mean: 775.0128401657383
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.5772241209427
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 193
  episodes_total: 65887
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.293633229051108e-37
        cur_lr: 5.0e-05
        entropy: 0.0876172836869955
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012680647977200957
        total_loss: .inf
        vf_explained_var: 0.9971947073936462
        vf_loss: 0.9852501104275385
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.474193548387095
    gpu_util_percent0: 0.29225806451612907
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640475882470083
    mean_env_wait_ms: 1.1918579030877667
    mean_inference_ms: 4.299960100304695
    mean_raw_obs_processing_ms: 0.37698468270476787
  time_since_restore: 8108.375014781952
  time_this_iter_s: 25.917420625686646
  time_total_s: 8108.375014781952
  timers:
    learn_throughput: 8684.742
    learn_time_ms: 18629.455
    sample_throughput: 23773.052
    sample_time_ms: 6805.689
    update_time_ms: 24.273
  timestamp: 1602740051
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:34:13,086	WARNING util.py:136 -- The `process_trial` operation took 0.901252269744873 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    316 |          8108.38 | 51126272 |  308.577 |              333.354 |              160.172 |            775.013 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2947.4966839284093
    time_step_min: 2786
  date: 2020-10-15_05-34-38
  done: false
  episode_len_mean: 775.030355305369
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.57991390204916
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 66084
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.4404498435766625e-37
        cur_lr: 5.0e-05
        entropy: 0.10496878748138745
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01360684803997477
        total_loss: .inf
        vf_explained_var: 0.9951712489128113
        vf_loss: 2.0535956223805747
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.643333333333334
    gpu_util_percent0: 0.31833333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640346158521814
    mean_env_wait_ms: 1.191793483890122
    mean_inference_ms: 4.299882275968942
    mean_raw_obs_processing_ms: 0.3769791898315797
  time_since_restore: 8133.961318016052
  time_this_iter_s: 25.586303234100342
  time_total_s: 8133.961318016052
  timers:
    learn_throughput: 8693.587
    learn_time_ms: 18610.5
    sample_throughput: 23780.977
    sample_time_ms: 6803.421
    update_time_ms: 23.573
  timestamp: 1602740078
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:34:39,953	WARNING util.py:136 -- The `process_trial` operation took 0.9270381927490234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    317 |          8133.96 | 51288064 |   308.58 |              333.354 |              160.172 |             775.03 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2947.267734001298
    time_step_min: 2786
  date: 2020-10-15_05-35-05
  done: false
  episode_len_mean: 775.0587215176512
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.6179928459871
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 66313
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.660674765364993e-37
        cur_lr: 5.0e-05
        entropy: 0.07434148341417313
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011291735262299577
        total_loss: .inf
        vf_explained_var: 0.9972924590110779
        vf_loss: 1.146560808022817
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.158064516129034
    gpu_util_percent0: 0.34322580645161294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640177020194364
    mean_env_wait_ms: 1.1917178404146487
    mean_inference_ms: 4.299775107001117
    mean_raw_obs_processing_ms: 0.37697176282195205
  time_since_restore: 8159.615167856216
  time_this_iter_s: 25.653849840164185
  time_total_s: 8159.615167856216
  timers:
    learn_throughput: 8701.691
    learn_time_ms: 18593.168
    sample_throughput: 23752.588
    sample_time_ms: 6811.553
    update_time_ms: 23.594
  timestamp: 1602740105
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:35:06,808	WARNING util.py:136 -- The `process_trial` operation took 0.8995046615600586 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    318 |          8159.62 | 51449856 |  308.618 |              333.354 |              160.172 |            775.059 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2946.9347384573725
    time_step_min: 2786
  date: 2020-10-15_05-35-32
  done: false
  episode_len_mean: 775.0836528197495
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.67412886703835
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 200
  episodes_total: 66513
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4491012148047488e-36
        cur_lr: 5.0e-05
        entropy: 0.06456519663333893
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008079350440918157
        total_loss: .inf
        vf_explained_var: 0.9992897510528564
        vf_loss: 0.24184051776925722
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.487096774193553
    gpu_util_percent0: 0.30548387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640010154611097
    mean_env_wait_ms: 1.1916517847951291
    mean_inference_ms: 4.299686697590605
    mean_raw_obs_processing_ms: 0.37696571665191153
  time_since_restore: 8185.306946992874
  time_this_iter_s: 25.691779136657715
  time_total_s: 8185.306946992874
  timers:
    learn_throughput: 8710.036
    learn_time_ms: 18575.355
    sample_throughput: 23754.94
    sample_time_ms: 6810.878
    update_time_ms: 23.384
  timestamp: 1602740132
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:35:33,942	WARNING util.py:136 -- The `process_trial` operation took 0.9606592655181885 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    319 |          8185.31 | 51611648 |  308.674 |              333.354 |              160.172 |            775.084 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2946.5934082419476
    time_step_min: 2786
  date: 2020-10-15_05-35-59
  done: false
  episode_len_mean: 775.108364192441
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.72264753348975
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 66701
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1736518222071235e-36
        cur_lr: 5.0e-05
        entropy: 0.06815410405397415
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010088822692826701
        total_loss: .inf
        vf_explained_var: 0.9987596869468689
        vf_loss: 0.43375300119320553
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.193333333333335
    gpu_util_percent0: 0.3243333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463987649391291
    mean_env_wait_ms: 1.19158987906052
    mean_inference_ms: 4.2996134268665065
    mean_raw_obs_processing_ms: 0.3769604048983325
  time_since_restore: 8210.959682703018
  time_this_iter_s: 25.652735710144043
  time_total_s: 8210.959682703018
  timers:
    learn_throughput: 8709.786
    learn_time_ms: 18575.887
    sample_throughput: 23768.331
    sample_time_ms: 6807.041
    update_time_ms: 22.033
  timestamp: 1602740159
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:36:00,934	WARNING util.py:136 -- The `process_trial` operation took 0.9360828399658203 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    320 |          8210.96 | 51773440 |  308.723 |              333.354 |              160.172 |            775.108 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2946.230697924765
    time_step_min: 2786
  date: 2020-10-15_05-36-26
  done: false
  episode_len_mean: 775.1401547978364
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.78108808251034
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 225
  episodes_total: 66926
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2604777333106846e-36
        cur_lr: 5.0e-05
        entropy: 0.06305113590011995
        entropy_coeff: 0.0005000000000000001
        kl: 0.00408997378932933
        model: {}
        policy_loss: -0.010151193271061251
        total_loss: 0.3259549637635549
        vf_explained_var: 0.9991967678070068
        vf_loss: 0.33613767474889755
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.02258064516129
    gpu_util_percent0: 0.28032258064516136
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639737783777698
    mean_env_wait_ms: 1.1915170290867438
    mean_inference_ms: 4.299522846497233
    mean_raw_obs_processing_ms: 0.37695371767304636
  time_since_restore: 8236.596605300903
  time_this_iter_s: 25.636922597885132
  time_total_s: 8236.596605300903
  timers:
    learn_throughput: 8716.298
    learn_time_ms: 18562.009
    sample_throughput: 23884.72
    sample_time_ms: 6773.87
    update_time_ms: 21.913
  timestamp: 1602740186
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: af50e_00000
  
2020-10-15 05:36:27,778	WARNING util.py:136 -- The `process_trial` operation took 0.8961448669433594 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    321 |           8236.6 | 51935232 |  308.781 |              333.354 |              160.172 |             775.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2945.8595913379136
    time_step_min: 2786
  date: 2020-10-15_05-36-53
  done: false
  episode_len_mean: 775.1705863953887
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.8354324760584
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 213
  episodes_total: 67139
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6302388666553423e-36
        cur_lr: 5.0e-05
        entropy: 0.06697974850734074
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009466730853697905
        total_loss: .inf
        vf_explained_var: 0.9985641837120056
        vf_loss: 0.5667572642366091
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54193548387097
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639547525958005
    mean_env_wait_ms: 1.1914465921910096
    mean_inference_ms: 4.299421489542433
    mean_raw_obs_processing_ms: 0.376947009037282
  time_since_restore: 8262.398611545563
  time_this_iter_s: 25.802006244659424
  time_total_s: 8262.398611545563
  timers:
    learn_throughput: 8723.033
    learn_time_ms: 18547.678
    sample_throughput: 23893.14
    sample_time_ms: 6771.483
    update_time_ms: 24.478
  timestamp: 1602740213
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:36:54,871	WARNING util.py:136 -- The `process_trial` operation took 0.9220407009124756 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    322 |           8262.4 | 52097024 |  308.835 |              333.354 |              160.172 |            775.171 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2945.601073159131
    time_step_min: 2786
  date: 2020-10-15_05-37-20
  done: false
  episode_len_mean: 775.193018419489
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.87583049748804
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 67320
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4453582999830143e-36
        cur_lr: 5.0e-05
        entropy: 0.07009374226133029
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009104412850623097
        total_loss: .inf
        vf_explained_var: 0.997903048992157
        vf_loss: 0.7642063498497009
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.22333333333334
    gpu_util_percent0: 0.35533333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639401630566554
    mean_env_wait_ms: 1.19138725229741
    mean_inference_ms: 4.299351128842123
    mean_raw_obs_processing_ms: 0.3769420441799757
  time_since_restore: 8288.211614370346
  time_this_iter_s: 25.813002824783325
  time_total_s: 8288.211614370346
  timers:
    learn_throughput: 8719.324
    learn_time_ms: 18555.566
    sample_throughput: 23900.469
    sample_time_ms: 6769.407
    update_time_ms: 26.249
  timestamp: 1602740240
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:37:21,919	WARNING util.py:136 -- The `process_trial` operation took 0.9207181930541992 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    323 |          8288.21 | 52258816 |  308.876 |              333.354 |              160.172 |            775.193 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2945.262648710313
    time_step_min: 2786
  date: 2020-10-15_05-37-47
  done: false
  episode_len_mean: 775.2220939013015
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.9335840526706
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 219
  episodes_total: 67539
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.66803744997452e-36
        cur_lr: 5.0e-05
        entropy: 0.0655195073535045
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008672580297570676
        total_loss: .inf
        vf_explained_var: 0.9992004036903381
        vf_loss: 0.30461230377356213
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.825806451612905
    gpu_util_percent0: 0.26387096774193547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639293119845764
    mean_env_wait_ms: 1.1913157910626753
    mean_inference_ms: 4.299270622423673
    mean_raw_obs_processing_ms: 0.3769358865419011
  time_since_restore: 8313.996664524078
  time_this_iter_s: 25.7850501537323
  time_total_s: 8313.996664524078
  timers:
    learn_throughput: 8718.138
    learn_time_ms: 18558.09
    sample_throughput: 23911.762
    sample_time_ms: 6766.21
    update_time_ms: 27.601
  timestamp: 1602740267
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:37:48,971	WARNING util.py:136 -- The `process_trial` operation took 0.9440796375274658 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    324 |             8314 | 52420608 |  308.934 |              333.354 |              160.172 |            775.222 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2944.883239810986
    time_step_min: 2786
  date: 2020-10-15_05-38-14
  done: false
  episode_len_mean: 775.2530031581122
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 308.9921407934304
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 223
  episodes_total: 67762
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.502056174961782e-36
        cur_lr: 5.0e-05
        entropy: 0.06457627347360055
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008567832875996828
        total_loss: .inf
        vf_explained_var: 0.9989962577819824
        vf_loss: 0.38938330113887787
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.060000000000002
    gpu_util_percent0: 0.32199999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639103193918918
    mean_env_wait_ms: 1.1912425791188326
    mean_inference_ms: 4.299161293841948
    mean_raw_obs_processing_ms: 0.3769287035446889
  time_since_restore: 8339.678704738617
  time_this_iter_s: 25.682040214538574
  time_total_s: 8339.678704738617
  timers:
    learn_throughput: 8722.326
    learn_time_ms: 18549.18
    sample_throughput: 23929.792
    sample_time_ms: 6761.112
    update_time_ms: 27.478
  timestamp: 1602740294
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:38:15,969	WARNING util.py:136 -- The `process_trial` operation took 0.9098236560821533 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    325 |          8339.68 | 52582400 |  308.992 |              333.354 |              160.172 |            775.253 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2944.578916363529
    time_step_min: 2786
  date: 2020-10-15_05-38-41
  done: false
  episode_len_mean: 775.27726182241
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.03675511127324
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 67943
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.253084262442672e-36
        cur_lr: 5.0e-05
        entropy: 0.06407529550294082
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009425751108210534
        total_loss: .inf
        vf_explained_var: 0.9988136291503906
        vf_loss: 0.4161572977900505
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.070967741935487
    gpu_util_percent0: 0.3383870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638944666378353
    mean_env_wait_ms: 1.1911836023430735
    mean_inference_ms: 4.299092436788292
    mean_raw_obs_processing_ms: 0.37692372379163985
  time_since_restore: 8365.300410747528
  time_this_iter_s: 25.621706008911133
  time_total_s: 8365.300410747528
  timers:
    learn_throughput: 8732.421
    learn_time_ms: 18527.737
    sample_throughput: 23995.27
    sample_time_ms: 6742.662
    update_time_ms: 26.193
  timestamp: 1602740321
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:38:42,796	WARNING util.py:136 -- The `process_trial` operation took 0.885422945022583 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    326 |           8365.3 | 52744192 |  309.037 |              333.354 |              160.172 |            775.277 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2944.2671149413472
    time_step_min: 2786
  date: 2020-10-15_05-39-08
  done: false
  episode_len_mean: 775.2999046291542
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.07493910567763
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 68155
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2379626393664008e-35
        cur_lr: 5.0e-05
        entropy: 0.08740126652022202
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012916005100123584
        total_loss: .inf
        vf_explained_var: 0.9963285326957703
        vf_loss: 1.441277801990509
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73333333333333
    gpu_util_percent0: 0.39599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146388301644086
    mean_env_wait_ms: 1.1911147411724872
    mean_inference_ms: 4.299015577819681
    mean_raw_obs_processing_ms: 0.3769180243366319
  time_since_restore: 8390.969790697098
  time_this_iter_s: 25.669379949569702
  time_total_s: 8390.969790697098
  timers:
    learn_throughput: 8730.069
    learn_time_ms: 18532.728
    sample_throughput: 23990.689
    sample_time_ms: 6743.95
    update_time_ms: 26.251
  timestamp: 1602740348
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:39:09,727	WARNING util.py:136 -- The `process_trial` operation took 0.9463777542114258 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    327 |          8390.97 | 52905984 |  309.075 |              333.354 |              160.172 |              775.3 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2944.0834674705143
    time_step_min: 2786
  date: 2020-10-15_05-39-35
  done: false
  episode_len_mean: 775.3187042995028
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.10592839775325
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 225
  episodes_total: 68380
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8569439590496008e-35
        cur_lr: 5.0e-05
        entropy: 0.07644632769127686
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011192033882252872
        total_loss: .inf
        vf_explained_var: 0.9964158535003662
        vf_loss: 1.4968835711479187
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.754838709677422
    gpu_util_percent0: 0.31387096774193546
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463866524396066
    mean_env_wait_ms: 1.1910422737713768
    mean_inference_ms: 4.2989134378381495
    mean_raw_obs_processing_ms: 0.3769111856176038
  time_since_restore: 8416.727793931961
  time_this_iter_s: 25.75800323486328
  time_total_s: 8416.727793931961
  timers:
    learn_throughput: 8722.954
    learn_time_ms: 18547.844
    sample_throughput: 24006.908
    sample_time_ms: 6739.393
    update_time_ms: 26.332
  timestamp: 1602740375
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:39:36,696	WARNING util.py:136 -- The `process_trial` operation took 0.8808226585388184 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    328 |          8416.73 | 53067776 |  309.106 |              333.354 |              160.172 |            775.319 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.7829417773237
    time_step_min: 2786
  date: 2020-10-15_05-40-02
  done: false
  episode_len_mean: 775.3424575628536
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.1539216760733
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 192
  episodes_total: 68572
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7854159385744013e-35
        cur_lr: 5.0e-05
        entropy: 0.06083324241141478
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00916757810531029
        total_loss: .inf
        vf_explained_var: 0.9985684752464294
        vf_loss: 0.49961383392413455
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.203333333333333
    gpu_util_percent0: 0.35866666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463849441429647
    mean_env_wait_ms: 1.1909790837250727
    mean_inference_ms: 4.298837643316201
    mean_raw_obs_processing_ms: 0.37690561314422005
  time_since_restore: 8442.40038728714
  time_this_iter_s: 25.672593355178833
  time_total_s: 8442.40038728714
  timers:
    learn_throughput: 8724.072
    learn_time_ms: 18545.468
    sample_throughput: 24011.865
    sample_time_ms: 6738.002
    update_time_ms: 26.104
  timestamp: 1602740402
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:40:03,693	WARNING util.py:136 -- The `process_trial` operation took 0.9082639217376709 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    329 |           8442.4 | 53229568 |  309.154 |              333.354 |              160.172 |            775.342 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.430650136779
    time_step_min: 2786
  date: 2020-10-15_05-40-29
  done: false
  episode_len_mean: 775.3674199459035
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.2065114983707
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 194
  episodes_total: 68766
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1781239078616025e-35
        cur_lr: 5.0e-05
        entropy: 0.0598073648288846
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008921230354947815
        total_loss: .inf
        vf_explained_var: 0.9993404746055603
        vf_loss: 0.27940675740440685
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.967741935483875
    gpu_util_percent0: 0.2687096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638387388421706
    mean_env_wait_ms: 1.1909166255169654
    mean_inference_ms: 4.298771652059551
    mean_raw_obs_processing_ms: 0.37690100018489575
  time_since_restore: 8468.358088731766
  time_this_iter_s: 25.957701444625854
  time_total_s: 8468.358088731766
  timers:
    learn_throughput: 8714.244
    learn_time_ms: 18566.383
    sample_throughput: 23983.496
    sample_time_ms: 6745.972
    update_time_ms: 27.562
  timestamp: 1602740429
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:40:30,953	WARNING util.py:136 -- The `process_trial` operation took 0.8908901214599609 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    330 |          8468.36 | 53391360 |  309.207 |              333.354 |              160.172 |            775.367 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.0485367905103
    time_step_min: 2786
  date: 2020-10-15_05-40-56
  done: false
  episode_len_mean: 775.393
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.2567742643827
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 234
  episodes_total: 69000
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.267185861792404e-35
        cur_lr: 5.0e-05
        entropy: 0.0833861269056797
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012605215742951259
        total_loss: .inf
        vf_explained_var: 0.9975811839103699
        vf_loss: 1.0168883204460144
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32580645161291
    gpu_util_percent0: 0.31806451612903225
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8709677419354844
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638222526464165
    mean_env_wait_ms: 1.1908401026293935
    mean_inference_ms: 4.298670048168818
    mean_raw_obs_processing_ms: 0.3768937988829653
  time_since_restore: 8494.25796508789
  time_this_iter_s: 25.899876356124878
  time_total_s: 8494.25796508789
  timers:
    learn_throughput: 8705.534
    learn_time_ms: 18584.96
    sample_throughput: 23965.034
    sample_time_ms: 6751.169
    update_time_ms: 28.349
  timestamp: 1602740456
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:40:58,083	WARNING util.py:136 -- The `process_trial` operation took 0.9019536972045898 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    331 |          8494.26 | 53553152 |  309.257 |              333.354 |              160.172 |            775.393 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.0226148818647
    time_step_min: 2786
  date: 2020-10-15_05-41-24
  done: false
  episode_len_mean: 775.3951734104046
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.2483965376304
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 200
  episodes_total: 69200
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.400778792688605e-35
        cur_lr: 5.0e-05
        entropy: 0.12418625752131145
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.015126777444190035
        total_loss: .inf
        vf_explained_var: 0.9909959435462952
        vf_loss: 3.7282729943593345
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.667741935483875
    gpu_util_percent0: 0.3270967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638068052949543
    mean_env_wait_ms: 1.19077570937597
    mean_inference_ms: 4.298588366761843
    mean_raw_obs_processing_ms: 0.376888105909
  time_since_restore: 8520.217536449432
  time_this_iter_s: 25.959571361541748
  time_total_s: 8520.217536449432
  timers:
    learn_throughput: 8699.975
    learn_time_ms: 18596.835
    sample_throughput: 23910.382
    sample_time_ms: 6766.601
    update_time_ms: 25.721
  timestamp: 1602740484
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:41:25,473	WARNING util.py:136 -- The `process_trial` operation took 0.932819128036499 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    332 |          8520.22 | 53714944 |  309.248 |              333.354 |              160.172 |            775.395 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.315476791302
    time_step_min: 2786
  date: 2020-10-15_05-41-51
  done: false
  episode_len_mean: 775.3800204637489
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.2018922490019
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 191
  episodes_total: 69391
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.410116818903291e-34
        cur_lr: 5.0e-05
        entropy: 0.13744969417651495
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012374283959312985
        total_loss: .inf
        vf_explained_var: 0.987044632434845
        vf_loss: 5.837658683458964
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.870000000000008
    gpu_util_percent0: 0.35066666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637948031448117
    mean_env_wait_ms: 1.1907150056411013
    mean_inference_ms: 4.298520758514615
    mean_raw_obs_processing_ms: 0.3768834155583546
  time_since_restore: 8545.807913780212
  time_this_iter_s: 25.59037733078003
  time_total_s: 8545.807913780212
  timers:
    learn_throughput: 8710.846
    learn_time_ms: 18573.627
    sample_throughput: 23906.069
    sample_time_ms: 6767.821
    update_time_ms: 23.881
  timestamp: 1602740511
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:41:52,365	WARNING util.py:136 -- The `process_trial` operation took 0.9557859897613525 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    333 |          8545.81 | 53876736 |  309.202 |              333.354 |              160.172 |             775.38 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.458724957604
    time_step_min: 2786
  date: 2020-10-15_05-42-18
  done: false
  episode_len_mean: 775.3758330460761
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.18726040132424
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 233
  episodes_total: 69624
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.115175228354936e-34
        cur_lr: 5.0e-05
        entropy: 0.10932462972899278
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012981216054564962
        total_loss: .inf
        vf_explained_var: 0.9916715025901794
        vf_loss: 3.9155346155166626
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096777
    gpu_util_percent0: 0.30677419354838714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637809976044375
    mean_env_wait_ms: 1.1906418629313356
    mean_inference_ms: 4.29843202693607
    mean_raw_obs_processing_ms: 0.376876997145413
  time_since_restore: 8571.616829872131
  time_this_iter_s: 25.808916091918945
  time_total_s: 8571.616829872131
  timers:
    learn_throughput: 8711.845
    learn_time_ms: 18571.497
    sample_throughput: 23896.01
    sample_time_ms: 6770.67
    update_time_ms: 24.04
  timestamp: 1602740538
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:42:19,462	WARNING util.py:136 -- The `process_trial` operation took 0.9541716575622559 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    334 |          8571.62 | 54038528 |  309.187 |              333.354 |              160.172 |            775.376 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.397045337307
    time_step_min: 2786
  date: 2020-10-15_05-42-45
  done: false
  episode_len_mean: 775.3841472146642
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.20675681344426
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 69830
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.172762842532405e-34
        cur_lr: 5.0e-05
        entropy: 0.08542520739138126
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00977599861410757
        total_loss: .inf
        vf_explained_var: 0.9954659342765808
        vf_loss: 1.7820686399936676
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.07
    gpu_util_percent0: 0.33433333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637635007071215
    mean_env_wait_ms: 1.1905757948758584
    mean_inference_ms: 4.298343512634733
    mean_raw_obs_processing_ms: 0.3768709029994953
  time_since_restore: 8597.315500497818
  time_this_iter_s: 25.698670625686646
  time_total_s: 8597.315500497818
  timers:
    learn_throughput: 8709.855
    learn_time_ms: 18575.74
    sample_throughput: 23909.298
    sample_time_ms: 6766.907
    update_time_ms: 25.137
  timestamp: 1602740565
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:42:46,518	WARNING util.py:136 -- The `process_trial` operation took 0.9140419960021973 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    335 |          8597.32 | 54200320 |  309.207 |              333.354 |              160.172 |            775.384 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2943.1565693378498
    time_step_min: 2786
  date: 2020-10-15_05-43-12
  done: false
  episode_len_mean: 775.4005370584622
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.2410300892109
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 70011
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.759144263798607e-34
        cur_lr: 5.0e-05
        entropy: 0.07229431470235188
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00820038882860293
        total_loss: .inf
        vf_explained_var: 0.9972438216209412
        vf_loss: 0.9906981885433197
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09677419354839
    gpu_util_percent0: 0.3293548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637518404069036
    mean_env_wait_ms: 1.1905183964873642
    mean_inference_ms: 4.29827438113744
    mean_raw_obs_processing_ms: 0.37686627394778605
  time_since_restore: 8623.04098892212
  time_this_iter_s: 25.725488424301147
  time_total_s: 8623.04098892212
  timers:
    learn_throughput: 8696.854
    learn_time_ms: 18603.508
    sample_throughput: 23940.23
    sample_time_ms: 6758.164
    update_time_ms: 26.959
  timestamp: 1602740592
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:43:13,557	WARNING util.py:136 -- The `process_trial` operation took 0.9859092235565186 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    336 |          8623.04 | 54362112 |  309.241 |              333.354 |              160.172 |            775.401 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2942.8267967803977
    time_step_min: 2786
  date: 2020-10-15_05-43-39
  done: false
  episode_len_mean: 775.423096088956
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.29150467903526
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 70237
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.138716395697909e-34
        cur_lr: 5.0e-05
        entropy: 0.06810582739611466
        entropy_coeff: 0.0005000000000000001
        kl: 0.005718279823971291
        model: {}
        policy_loss: -0.008532464135593424
        total_loss: 0.5367181201775869
        vf_explained_var: 0.9985879063606262
        vf_loss: 0.545284646252791
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32903225806452
    gpu_util_percent0: 0.3090322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637404752476665
    mean_env_wait_ms: 1.1904479910641013
    mean_inference_ms: 4.2981957597927
    mean_raw_obs_processing_ms: 0.3768604094609612
  time_since_restore: 8649.059316158295
  time_this_iter_s: 26.018327236175537
  time_total_s: 8649.059316158295
  timers:
    learn_throughput: 8681.16
    learn_time_ms: 18637.141
    sample_throughput: 23943.825
    sample_time_ms: 6757.149
    update_time_ms: 28.698
  timestamp: 1602740619
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: af50e_00000
  
2020-10-15 05:43:40,893	WARNING util.py:136 -- The `process_trial` operation took 0.9928085803985596 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    337 |          8649.06 | 54523904 |  309.292 |              333.354 |              160.172 |            775.423 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2942.4518121911037
    time_step_min: 2786
  date: 2020-10-15_05-44-06
  done: false
  episode_len_mean: 775.4482783092515
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.347265627576
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 217
  episodes_total: 70454
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.138716395697909e-34
        cur_lr: 5.0e-05
        entropy: 0.06371995496253173
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0076373569706144435
        total_loss: .inf
        vf_explained_var: 0.9993067383766174
        vf_loss: 0.2527638090153535
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.819354838709682
    gpu_util_percent0: 0.3235483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637231911921686
    mean_env_wait_ms: 1.1903789426564382
    mean_inference_ms: 4.298101516321831
    mean_raw_obs_processing_ms: 0.3768540876534144
  time_since_restore: 8674.917558193207
  time_this_iter_s: 25.85824203491211
  time_total_s: 8674.917558193207
  timers:
    learn_throughput: 8677.801
    learn_time_ms: 18644.355
    sample_throughput: 23911.909
    sample_time_ms: 6766.168
    update_time_ms: 28.669
  timestamp: 1602740646
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:44:08,028	WARNING util.py:136 -- The `process_trial` operation took 0.9451313018798828 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    338 |          8674.92 | 54685696 |  309.347 |              333.354 |              160.172 |            775.448 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2942.1554403411105
    time_step_min: 2786
  date: 2020-10-15_05-44-33
  done: false
  episode_len_mean: 775.4713244142422
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.39403163081204
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 70635
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0708074593546863e-33
        cur_lr: 5.0e-05
        entropy: 0.0629182265450557
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00610736650802816
        total_loss: .inf
        vf_explained_var: 0.998879611492157
        vf_loss: 0.3710476333896319
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.990322580645163
    gpu_util_percent0: 0.3212903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637091435720173
    mean_env_wait_ms: 1.1903224710280238
    mean_inference_ms: 4.298039771143619
    mean_raw_obs_processing_ms: 0.37684956467945313
  time_since_restore: 8700.862893104553
  time_this_iter_s: 25.945334911346436
  time_total_s: 8700.862893104553
  timers:
    learn_throughput: 8668.147
    learn_time_ms: 18665.119
    sample_throughput: 23925.243
    sample_time_ms: 6762.397
    update_time_ms: 30.112
  timestamp: 1602740673
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:44:35,271	WARNING util.py:136 -- The `process_trial` operation took 0.9662525653839111 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    339 |          8700.86 | 54847488 |  309.394 |              333.354 |              160.172 |            775.471 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.7990311692347
    time_step_min: 2786
  date: 2020-10-15_05-45-00
  done: false
  episode_len_mean: 775.4990190404945
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.44934004614424
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 214
  episodes_total: 70849
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6062111890320298e-33
        cur_lr: 5.0e-05
        entropy: 0.06116232586403688
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009338843112345785
        total_loss: .inf
        vf_explained_var: 0.9994053840637207
        vf_loss: 0.22342306251327196
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.680645161290325
    gpu_util_percent0: 0.3412903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636981835332513
    mean_env_wait_ms: 1.1902551452621482
    mean_inference_ms: 4.297966089937697
    mean_raw_obs_processing_ms: 0.3768441881530086
  time_since_restore: 8726.517801523209
  time_this_iter_s: 25.654908418655396
  time_total_s: 8726.517801523209
  timers:
    learn_throughput: 8680.33
    learn_time_ms: 18638.923
    sample_throughput: 23947.275
    sample_time_ms: 6756.176
    update_time_ms: 30.231
  timestamp: 1602740700
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:45:02,378	WARNING util.py:136 -- The `process_trial` operation took 0.9463596343994141 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    340 |          8726.52 | 55009280 |  309.449 |              333.354 |              160.172 |            775.499 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.410268322212
    time_step_min: 2786
  date: 2020-10-15_05-45-27
  done: false
  episode_len_mean: 775.5237351567337
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.5059445544417
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 227
  episodes_total: 71076
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4093167835480447e-33
        cur_lr: 5.0e-05
        entropy: 0.06690925918519497
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00802004745734545
        total_loss: .inf
        vf_explained_var: 0.999157190322876
        vf_loss: 0.31709765642881393
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.053333333333335
    gpu_util_percent0: 0.3526666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636813968783174
    mean_env_wait_ms: 1.190183727383264
    mean_inference_ms: 4.297866735401915
    mean_raw_obs_processing_ms: 0.37683773691131417
  time_since_restore: 8752.121463298798
  time_this_iter_s: 25.60366177558899
  time_total_s: 8752.121463298798
  timers:
    learn_throughput: 8686.058
    learn_time_ms: 18626.632
    sample_throughput: 23976.442
    sample_time_ms: 6747.957
    update_time_ms: 29.183
  timestamp: 1602740727
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:45:29,362	WARNING util.py:136 -- The `process_trial` operation took 0.954582691192627 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    341 |          8752.12 | 55171072 |  309.506 |              333.354 |              160.172 |            775.524 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.112108064086
    time_step_min: 2786
  date: 2020-10-15_05-45-55
  done: false
  episode_len_mean: 775.5428226609972
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.5500157981106
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 183
  episodes_total: 71259
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.613975175322068e-33
        cur_lr: 5.0e-05
        entropy: 0.06845265192290147
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009319159706744054
        total_loss: .inf
        vf_explained_var: 0.9986874461174011
        vf_loss: 0.44200316816568375
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.425806451612903
    gpu_util_percent0: 0.29
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636677165084713
    mean_env_wait_ms: 1.1901263831450246
    mean_inference_ms: 4.297804018464419
    mean_raw_obs_processing_ms: 0.376832964229634
  time_since_restore: 8778.07511973381
  time_this_iter_s: 25.953656435012817
  time_total_s: 8778.07511973381
  timers:
    learn_throughput: 8680.072
    learn_time_ms: 18639.478
    sample_throughput: 24032.755
    sample_time_ms: 6732.145
    update_time_ms: 29.691
  timestamp: 1602740755
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:45:56,652	WARNING util.py:136 -- The `process_trial` operation took 0.9189140796661377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    342 |          8778.08 | 55332864 |   309.55 |              333.354 |              160.172 |            775.543 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.8508848070783
    time_step_min: 2786
  date: 2020-10-15_05-46-22
  done: false
  episode_len_mean: 775.5563033440604
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.5894406496755
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 211
  episodes_total: 71470
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.420962762983102e-33
        cur_lr: 5.0e-05
        entropy: 0.07808435335755348
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009909961527834335
        total_loss: .inf
        vf_explained_var: 0.9976899027824402
        vf_loss: 0.8852832068999609
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.129032258064516
    gpu_util_percent0: 0.3425806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636577037595297
    mean_env_wait_ms: 1.1900603019406448
    mean_inference_ms: 4.297734817526542
    mean_raw_obs_processing_ms: 0.3768280381399657
  time_since_restore: 8803.823784351349
  time_this_iter_s: 25.748664617538452
  time_total_s: 8803.823784351349
  timers:
    learn_throughput: 8673.566
    learn_time_ms: 18653.457
    sample_throughput: 24034.726
    sample_time_ms: 6731.593
    update_time_ms: 30.037
  timestamp: 1602740782
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:46:23,711	WARNING util.py:136 -- The `process_trial` operation took 0.9756443500518799 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    343 |          8803.82 | 55494656 |  309.589 |              333.354 |              160.172 |            775.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.5524200301456
    time_step_min: 2786
  date: 2020-10-15_05-46-49
  done: false
  episode_len_mean: 775.5767846681731
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.6389246046537
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 71694
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.13144414447465e-33
        cur_lr: 5.0e-05
        entropy: 0.06962184111277263
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0074249956718025105
        total_loss: .inf
        vf_explained_var: 0.9986796975135803
        vf_loss: 0.5325941195090612
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.787096774193547
    gpu_util_percent0: 0.3016129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636426399065477
    mean_env_wait_ms: 1.1899902135222635
    mean_inference_ms: 4.297642560120992
    mean_raw_obs_processing_ms: 0.37682185278367625
  time_since_restore: 8829.603699684143
  time_this_iter_s: 25.77991533279419
  time_total_s: 8829.603699684143
  timers:
    learn_throughput: 8673.565
    learn_time_ms: 18653.461
    sample_throughput: 24047.362
    sample_time_ms: 6728.056
    update_time_ms: 28.46
  timestamp: 1602740809
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:46:50,801	WARNING util.py:136 -- The `process_trial` operation took 0.9785449504852295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    344 |           8829.6 | 55656448 |  309.639 |              333.354 |              160.172 |            775.577 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.261371228148
    time_step_min: 2786
  date: 2020-10-15_05-47-16
  done: false
  episode_len_mean: 775.5974544442898
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.6858169959434
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 71890
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2197166216711976e-32
        cur_lr: 5.0e-05
        entropy: 0.06765530010064442
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008772620174568146
        total_loss: .inf
        vf_explained_var: 0.9988725781440735
        vf_loss: 0.3829319203893344
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.967741935483872
    gpu_util_percent0: 0.2803225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636274668070365
    mean_env_wait_ms: 1.1899288718973546
    mean_inference_ms: 4.297574306026762
    mean_raw_obs_processing_ms: 0.376816710180726
  time_since_restore: 8855.435140132904
  time_this_iter_s: 25.831440448760986
  time_total_s: 8855.435140132904
  timers:
    learn_throughput: 8677.682
    learn_time_ms: 18644.61
    sample_throughput: 23997.139
    sample_time_ms: 6742.137
    update_time_ms: 27.391
  timestamp: 1602740836
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:47:17,979	WARNING util.py:136 -- The `process_trial` operation took 0.9633069038391113 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    345 |          8855.44 | 55818240 |  309.686 |              333.354 |              160.172 |            775.597 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2939.9715316815877
    time_step_min: 2786
  date: 2020-10-15_05-47-43
  done: false
  episode_len_mean: 775.616199869602
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.7303607467574
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 72087
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8295749325067964e-32
        cur_lr: 5.0e-05
        entropy: 0.06696347643931706
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009344614397074716
        total_loss: .inf
        vf_explained_var: 0.9986292719841003
        vf_loss: 0.5791442319750786
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.41
    gpu_util_percent0: 0.3183333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463618163284989
    mean_env_wait_ms: 1.189867611022333
    mean_inference_ms: 4.297510716213728
    mean_raw_obs_processing_ms: 0.37681225574749605
  time_since_restore: 8881.327589511871
  time_this_iter_s: 25.892449378967285
  time_total_s: 8881.327589511871
  timers:
    learn_throughput: 8681.387
    learn_time_ms: 18636.654
    sample_throughput: 23921.008
    sample_time_ms: 6763.595
    update_time_ms: 27.628
  timestamp: 1602740863
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:47:45,240	WARNING util.py:136 -- The `process_trial` operation took 0.9332911968231201 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    346 |          8881.33 | 55980032 |   309.73 |              333.354 |              160.172 |            775.616 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2939.6157550263597
    time_step_min: 2786
  date: 2020-10-15_05-48-11
  done: false
  episode_len_mean: 775.6383952648974
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.78496390939836
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 72311
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7443623987601947e-32
        cur_lr: 5.0e-05
        entropy: 0.07875335402786732
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011035439684443796
        total_loss: .inf
        vf_explained_var: 0.9991661906242371
        vf_loss: 0.3214402024944623
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.41290322580645
    gpu_util_percent0: 0.34322580645161294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463605184226847
    mean_env_wait_ms: 1.1897978321991787
    mean_inference_ms: 4.297427328178364
    mean_raw_obs_processing_ms: 0.37680647179535
  time_since_restore: 8907.412872076035
  time_this_iter_s: 26.085282564163208
  time_total_s: 8907.412872076035
  timers:
    learn_throughput: 8683.073
    learn_time_ms: 18633.035
    sample_throughput: 23886.619
    sample_time_ms: 6773.332
    update_time_ms: 27.519
  timestamp: 1602740891
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:48:12,725	WARNING util.py:136 -- The `process_trial` operation took 0.9795007705688477 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    347 |          8907.41 | 56141824 |  309.785 |              333.354 |              160.172 |            775.638 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2939.5128180363154
    time_step_min: 2786
  date: 2020-10-15_05-48-38
  done: false
  episode_len_mean: 775.6306158471
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.7789432146555
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 207
  episodes_total: 72518
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1165435981402925e-32
        cur_lr: 5.0e-05
        entropy: 0.12266518610219161
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01640952512389049
        total_loss: .inf
        vf_explained_var: 0.9911894202232361
        vf_loss: 3.432590822378794
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.451612903225808
    gpu_util_percent0: 0.31516129032258056
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635880982678134
    mean_env_wait_ms: 1.189733267090813
    mean_inference_ms: 4.297347393540573
    mean_raw_obs_processing_ms: 0.37680057735492817
  time_since_restore: 8933.233102321625
  time_this_iter_s: 25.82023024559021
  time_total_s: 8933.233102321625
  timers:
    learn_throughput: 8696.116
    learn_time_ms: 18605.087
    sample_throughput: 23833.923
    sample_time_ms: 6788.308
    update_time_ms: 27.236
  timestamp: 1602740918
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:48:39,832	WARNING util.py:136 -- The `process_trial` operation took 0.9529738426208496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    348 |          8933.23 | 56303616 |  309.779 |              333.354 |              160.172 |            775.631 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2939.8878078987204
    time_step_min: 2786
  date: 2020-10-15_05-49-05
  done: false
  episode_len_mean: 775.6002448014083
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.7211454683259
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 194
  episodes_total: 72712
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.174815397210439e-32
        cur_lr: 5.0e-05
        entropy: 0.15746093293031058
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014156988821923733
        total_loss: .inf
        vf_explained_var: 0.9863012433052063
        vf_loss: 5.968090295791626
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.158064516129027
    gpu_util_percent0: 0.35096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463577364287905
    mean_env_wait_ms: 1.1896741418472327
    mean_inference_ms: 4.2972844195683475
    mean_raw_obs_processing_ms: 0.37679624746181245
  time_since_restore: 8959.27252626419
  time_this_iter_s: 26.039423942565918
  time_total_s: 8959.27252626419
  timers:
    learn_throughput: 8692.246
    learn_time_ms: 18613.372
    sample_throughput: 23803.536
    sample_time_ms: 6796.973
    update_time_ms: 26.553
  timestamp: 1602740945
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:49:07,251	WARNING util.py:136 -- The `process_trial` operation took 1.00246000289917 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    349 |          8959.27 | 56465408 |  309.721 |              333.354 |              160.172 |              775.6 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.410430154724
    time_step_min: 2786
  date: 2020-10-15_05-49-33
  done: false
  episode_len_mean: 775.5641159213665
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.64549977055094
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 234
  episodes_total: 72946
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.262223095815658e-32
        cur_lr: 5.0e-05
        entropy: 0.1549971861143907
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012515773123595864
        total_loss: .inf
        vf_explained_var: 0.9858129620552063
        vf_loss: 6.985186139742534
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.325806451612905
    gpu_util_percent0: 0.30516129032258066
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635650983591939
    mean_env_wait_ms: 1.1896039905677573
    mean_inference_ms: 4.297202002939917
    mean_raw_obs_processing_ms: 0.3767905625746973
  time_since_restore: 8985.192142009735
  time_this_iter_s: 25.919615745544434
  time_total_s: 8985.192142009735
  timers:
    learn_throughput: 8685.114
    learn_time_ms: 18628.657
    sample_throughput: 23764.398
    sample_time_ms: 6808.167
    update_time_ms: 25.11
  timestamp: 1602740973
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:49:34,569	WARNING util.py:136 -- The `process_trial` operation took 0.9403092861175537 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    350 |          8985.19 | 56627200 |  309.645 |              333.354 |              160.172 |            775.564 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.8286828067294
    time_step_min: 2786
  date: 2020-10-15_05-50-00
  done: false
  episode_len_mean: 775.533163823272
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.58321444431164
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 73152
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3893334643723486e-31
        cur_lr: 5.0e-05
        entropy: 0.14827747891346613
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01380440645152703
        total_loss: .inf
        vf_explained_var: 0.9873831868171692
        vf_loss: 5.382535894711812
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.125806451612906
    gpu_util_percent0: 0.3348387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463548395006985
    mean_env_wait_ms: 1.1895416984668017
    mean_inference_ms: 4.297124391994603
    mean_raw_obs_processing_ms: 0.3767850284683611
  time_since_restore: 9010.857857465744
  time_this_iter_s: 25.66571545600891
  time_total_s: 9010.857857465744
  timers:
    learn_throughput: 8691.087
    learn_time_ms: 18615.853
    sample_throughput: 23708.541
    sample_time_ms: 6824.207
    update_time_ms: 26.204
  timestamp: 1602741000
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:50:01,576	WARNING util.py:136 -- The `process_trial` operation took 0.999032735824585 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    351 |          9010.86 | 56788992 |  309.583 |              333.354 |              160.172 |            775.533 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.225923500846
    time_step_min: 2786
  date: 2020-10-15_05-50-27
  done: false
  episode_len_mean: 775.5058350374915
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.52825804052776
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 198
  episodes_total: 73350
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0840001965585234e-31
        cur_lr: 5.0e-05
        entropy: 0.15110726157824197
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011709111897895733
        total_loss: .inf
        vf_explained_var: 0.9882664680480957
        vf_loss: 4.971982638041179
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.148387096774197
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463537909054418
    mean_env_wait_ms: 1.1894830167763581
    mean_inference_ms: 4.2970591325819045
    mean_raw_obs_processing_ms: 0.37678063232501685
  time_since_restore: 9036.801217794418
  time_this_iter_s: 25.943360328674316
  time_total_s: 9036.801217794418
  timers:
    learn_throughput: 8697.449
    learn_time_ms: 18602.236
    sample_throughput: 23671.502
    sample_time_ms: 6834.885
    update_time_ms: 26.373
  timestamp: 1602741027
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:50:28,837	WARNING util.py:136 -- The `process_trial` operation took 0.9793758392333984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    352 |           9036.8 | 56950784 |  309.528 |              333.354 |              160.172 |            775.506 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.654550646596
    time_step_min: 2786
  date: 2020-10-15_05-50-54
  done: false
  episode_len_mean: 775.4704203530803
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.46467117458246
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 231
  episodes_total: 73581
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1260002948377844e-31
        cur_lr: 5.0e-05
        entropy: 0.1418029603858789
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013484111560198167
        total_loss: .inf
        vf_explained_var: 0.9881167411804199
        vf_loss: 5.675227006276448
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870967
    gpu_util_percent0: 0.32290322580645164
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463525505674369
    mean_env_wait_ms: 1.1894162608717536
    mean_inference_ms: 4.296976944804229
    mean_raw_obs_processing_ms: 0.3767749977324559
  time_since_restore: 9062.551988601685
  time_this_iter_s: 25.750770807266235
  time_total_s: 9062.551988601685
  timers:
    learn_throughput: 8697.433
    learn_time_ms: 18602.27
    sample_throughput: 23701.171
    sample_time_ms: 6826.33
    update_time_ms: 26.857
  timestamp: 1602741054
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:50:55,933	WARNING util.py:136 -- The `process_trial` operation took 0.9601597785949707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    353 |          9062.55 | 57112576 |  309.465 |              333.354 |              160.172 |             775.47 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.9386797569973
    time_step_min: 2786
  date: 2020-10-15_05-51-21
  done: false
  episode_len_mean: 775.4455858835009
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.4246878839076
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 205
  episodes_total: 73786
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.689000442256678e-31
        cur_lr: 5.0e-05
        entropy: 0.13215691720445952
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01301488840059998
        total_loss: .inf
        vf_explained_var: 0.9906929135322571
        vf_loss: 3.7758508920669556
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96129032258065
    gpu_util_percent0: 0.33580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635090762861208
    mean_env_wait_ms: 1.1893563170423969
    mean_inference_ms: 4.29690140892895
    mean_raw_obs_processing_ms: 0.3767697532200014
  time_since_restore: 9088.490325689316
  time_this_iter_s: 25.938337087631226
  time_total_s: 9088.490325689316
  timers:
    learn_throughput: 8691.432
    learn_time_ms: 18615.113
    sample_throughput: 23693.494
    sample_time_ms: 6828.541
    update_time_ms: 29.113
  timestamp: 1602741081
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:51:23,216	WARNING util.py:136 -- The `process_trial` operation took 0.9939892292022705 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    354 |          9088.49 | 57274368 |  309.425 |              333.354 |              160.172 |            775.446 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2942.1544068919816
    time_step_min: 2786
  date: 2020-10-15_05-51-48
  done: false
  episode_len_mean: 775.4284497790032
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.39677870305155
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 73983
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.033500663385015e-31
        cur_lr: 5.0e-05
        entropy: 0.1233693261941274
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011715491367795039
        total_loss: .inf
        vf_explained_var: 0.9876088500022888
        vf_loss: 5.224152723948161
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92666666666667
    gpu_util_percent0: 0.397
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634995672978698
    mean_env_wait_ms: 1.1892997269814576
    mean_inference_ms: 4.2968388498149634
    mean_raw_obs_processing_ms: 0.37676560625560396
  time_since_restore: 9114.076119661331
  time_this_iter_s: 25.58579397201538
  time_total_s: 9114.076119661331
  timers:
    learn_throughput: 8695.804
    learn_time_ms: 18605.755
    sample_throughput: 23721.549
    sample_time_ms: 6820.465
    update_time_ms: 29.484
  timestamp: 1602741108
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:51:50,134	WARNING util.py:136 -- The `process_trial` operation took 0.990527868270874 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    355 |          9114.08 | 57436160 |  309.397 |              333.354 |              160.172 |            775.428 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2942.160525286845
    time_step_min: 2786
  date: 2020-10-15_05-52-16
  done: false
  episode_len_mean: 775.4290199566102
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.4045353073932
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 228
  episodes_total: 74211
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0550250995077523e-30
        cur_lr: 5.0e-05
        entropy: 0.0933923280487458
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01097795378882438
        total_loss: .inf
        vf_explained_var: 0.9949862360954285
        vf_loss: 2.151122788588206
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.532258064516125
    gpu_util_percent0: 0.357741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634866309338238
    mean_env_wait_ms: 1.1892349014916181
    mean_inference_ms: 4.296756971962619
    mean_raw_obs_processing_ms: 0.37676012822391336
  time_since_restore: 9139.971286296844
  time_this_iter_s: 25.895166635513306
  time_total_s: 9139.971286296844
  timers:
    learn_throughput: 8690.077
    learn_time_ms: 18618.018
    sample_throughput: 23785.306
    sample_time_ms: 6802.183
    update_time_ms: 26.97
  timestamp: 1602741136
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:52:17,409	WARNING util.py:136 -- The `process_trial` operation took 1.0250537395477295 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    356 |          9139.97 | 57597952 |  309.405 |              333.354 |              160.172 |            775.429 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.9619216651204
    time_step_min: 2786
  date: 2020-10-15_05-52-43
  done: false
  episode_len_mean: 775.4362561311564
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.43326023793657
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 204
  episodes_total: 74415
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5825376492616282e-30
        cur_lr: 5.0e-05
        entropy: 0.07376469671726227
        entropy_coeff: 0.0005000000000000001
        kl: 0.003924395015928894
        model: {}
        policy_loss: -0.011843312958565852
        total_loss: 0.9991165896256765
        vf_explained_var: 0.9972758293151855
        vf_loss: 1.0109968036413193
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43548387096774
    gpu_util_percent0: 0.32838709677419353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634706034598283
    mean_env_wait_ms: 1.1891764240043907
    mean_inference_ms: 4.296683521052842
    mean_raw_obs_processing_ms: 0.3767548361918906
  time_since_restore: 9166.077453374863
  time_this_iter_s: 26.10616707801819
  time_total_s: 9166.077453374863
  timers:
    learn_throughput: 8687.809
    learn_time_ms: 18622.877
    sample_throughput: 23828.364
    sample_time_ms: 6789.891
    update_time_ms: 33.724
  timestamp: 1602741163
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: af50e_00000
  
2020-10-15 05:52:44,878	WARNING util.py:136 -- The `process_trial` operation took 1.015918493270874 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    357 |          9166.08 | 57759744 |  309.433 |              333.354 |              160.172 |            775.436 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.72071032901
    time_step_min: 2786
  date: 2020-10-15_05-53-10
  done: false
  episode_len_mean: 775.4483035965629
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.4690877189702
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 74599
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.912688246308141e-31
        cur_lr: 5.0e-05
        entropy: 0.06626446979741256
        entropy_coeff: 0.0005000000000000001
        kl: 0.004155970605400701
        model: {}
        policy_loss: -0.00930598294326046
        total_loss: 0.7011605103810629
        vf_explained_var: 0.9980237483978271
        vf_loss: 0.710499624411265
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.887096774193555
    gpu_util_percent0: 0.33129032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634602146189732
    mean_env_wait_ms: 1.1891240626844923
    mean_inference_ms: 4.2966241902833575
    mean_raw_obs_processing_ms: 0.37675102941387084
  time_since_restore: 9191.893365859985
  time_this_iter_s: 25.81591248512268
  time_total_s: 9191.893365859985
  timers:
    learn_throughput: 8679.308
    learn_time_ms: 18641.117
    sample_throughput: 23903.0
    sample_time_ms: 6768.69
    update_time_ms: 34.079
  timestamp: 1602741190
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: af50e_00000
  
2020-10-15 05:53:12,094	WARNING util.py:136 -- The `process_trial` operation took 1.018512487411499 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    358 |          9191.89 | 57921536 |  309.469 |              333.354 |              160.172 |            775.448 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.395702116848
    time_step_min: 2786
  date: 2020-10-15_05-53-38
  done: false
  episode_len_mean: 775.4647234139235
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.5142868482748
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 74823
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.956344123154071e-31
        cur_lr: 5.0e-05
        entropy: 0.06396869476884604
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008459122787826345
        total_loss: .inf
        vf_explained_var: 0.9977390170097351
        vf_loss: 0.8988497108221054
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.125806451612902
    gpu_util_percent0: 0.29451612903225816
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463451701123093
    mean_env_wait_ms: 1.189060015583164
    mean_inference_ms: 4.296551075152219
    mean_raw_obs_processing_ms: 0.3767460349263657
  time_since_restore: 9217.945642709732
  time_this_iter_s: 26.052276849746704
  time_total_s: 9217.945642709732
  timers:
    learn_throughput: 8681.167
    learn_time_ms: 18637.126
    sample_throughput: 23887.213
    sample_time_ms: 6773.164
    update_time_ms: 34.769
  timestamp: 1602741218
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:53:39,650	WARNING util.py:136 -- The `process_trial` operation took 1.023122787475586 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    359 |          9217.95 | 58083328 |  309.514 |              333.354 |              160.172 |            775.465 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2941.0844167100017
    time_step_min: 2786
  date: 2020-10-15_05-54-05
  done: false
  episode_len_mean: 775.4833753115047
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.56348193888647
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 216
  episodes_total: 75039
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.934516184731107e-31
        cur_lr: 5.0e-05
        entropy: 0.058396111552913986
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007640307439335932
        total_loss: .inf
        vf_explained_var: 0.9984314441680908
        vf_loss: 0.5839222396413485
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95483870967742
    gpu_util_percent0: 0.33806451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463434128903888
    mean_env_wait_ms: 1.1889985054776768
    mean_inference_ms: 4.29646826967568
    mean_raw_obs_processing_ms: 0.3767403371265609
  time_since_restore: 9243.668224334717
  time_this_iter_s: 25.72258162498474
  time_total_s: 9243.668224334717
  timers:
    learn_throughput: 8687.799
    learn_time_ms: 18622.899
    sample_throughput: 23910.339
    sample_time_ms: 6766.612
    update_time_ms: 34.208
  timestamp: 1602741245
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:54:06,755	WARNING util.py:136 -- The `process_trial` operation took 1.036027431488037 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    360 |          9243.67 | 58245120 |  309.563 |              333.354 |              160.172 |            775.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.808813748703
    time_step_min: 2786
  date: 2020-10-15_05-54-32
  done: false
  episode_len_mean: 775.4998803509704
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.6040953271076
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 75220
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.901774277096662e-31
        cur_lr: 5.0e-05
        entropy: 0.05961633659899235
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005929469703308617
        total_loss: .inf
        vf_explained_var: 0.9988275170326233
        vf_loss: 0.4073222403724988
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.661290322580644
    gpu_util_percent0: 0.3229032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634227038786987
    mean_env_wait_ms: 1.1889471003854313
    mean_inference_ms: 4.296410852834879
    mean_raw_obs_processing_ms: 0.3767363384553054
  time_since_restore: 9269.56031370163
  time_this_iter_s: 25.892089366912842
  time_total_s: 9269.56031370163
  timers:
    learn_throughput: 8673.983
    learn_time_ms: 18652.562
    sample_throughput: 23934.343
    sample_time_ms: 6759.826
    update_time_ms: 33.092
  timestamp: 1602741272
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:54:34,079	WARNING util.py:136 -- The `process_trial` operation took 1.021937608718872 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    361 |          9269.56 | 58406912 |  309.604 |              333.354 |              160.172 |              775.5 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.478487493037
    time_step_min: 2786
  date: 2020-10-15_05-54-59
  done: false
  episode_len_mean: 775.5173913043478
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.65627443576784
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 220
  episodes_total: 75440
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.335266141564499e-30
        cur_lr: 5.0e-05
        entropy: 0.05957935284823179
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009169366327114403
        total_loss: .inf
        vf_explained_var: 0.9991142153739929
        vf_loss: 0.3251520295937856
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.161290322580644
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463412786697392
    mean_env_wait_ms: 1.1888847093775783
    mean_inference_ms: 4.296346534121773
    mean_raw_obs_processing_ms: 0.37673189741356616
  time_since_restore: 9295.35785317421
  time_this_iter_s: 25.797539472579956
  time_total_s: 9295.35785317421
  timers:
    learn_throughput: 8682.469
    learn_time_ms: 18634.33
    sample_throughput: 23951.269
    sample_time_ms: 6755.049
    update_time_ms: 32.411
  timestamp: 1602741299
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:55:01,225	WARNING util.py:136 -- The `process_trial` operation took 1.0069947242736816 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    362 |          9295.36 | 58568704 |  309.656 |              333.354 |              160.172 |            775.517 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2940.1229965617563
    time_step_min: 2786
  date: 2020-10-15_05-55-27
  done: false
  episode_len_mean: 775.5377336047158
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.7069928755449
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 75662
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0028992123467484e-30
        cur_lr: 5.0e-05
        entropy: 0.05476818916698297
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008762787736486644
        total_loss: .inf
        vf_explained_var: 0.99853515625
        vf_loss: 0.5999425748984019
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.05483870967742
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463397844192943
    mean_env_wait_ms: 1.1888216913093883
    mean_inference_ms: 4.296257977601298
    mean_raw_obs_processing_ms: 0.37672616982715373
  time_since_restore: 9321.567025661469
  time_this_iter_s: 26.20917248725891
  time_total_s: 9321.567025661469
  timers:
    learn_throughput: 8670.771
    learn_time_ms: 18659.472
    sample_throughput: 23892.417
    sample_time_ms: 6771.688
    update_time_ms: 32.419
  timestamp: 1602741327
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:55:28,844	WARNING util.py:136 -- The `process_trial` operation took 1.0387582778930664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    363 |          9321.57 | 58730496 |  309.707 |              333.354 |              160.172 |            775.538 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2939.823529411765
    time_step_min: 2786
  date: 2020-10-15_05-55-54
  done: false
  episode_len_mean: 775.556068297185
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.75180375180355
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 183
  episodes_total: 75845
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0043488185201226e-30
        cur_lr: 5.0e-05
        entropy: 0.05921625656386217
        entropy_coeff: 0.0005000000000000001
        kl: 0.005649667698889971
        model: {}
        policy_loss: -0.009182563924696296
        total_loss: 0.21247325837612152
        vf_explained_var: 0.9993146061897278
        vf_loss: 0.22168542941411337
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.758064516129036
    gpu_util_percent0: 0.27096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633857069692552
    mean_env_wait_ms: 1.1887700483447214
    mean_inference_ms: 4.2962015122144175
    mean_raw_obs_processing_ms: 0.37672187822663933
  time_since_restore: 9347.507927894592
  time_this_iter_s: 25.94090223312378
  time_total_s: 9347.507927894592
  timers:
    learn_throughput: 8682.354
    learn_time_ms: 18634.578
    sample_throughput: 23806.837
    sample_time_ms: 6796.031
    update_time_ms: 30.276
  timestamp: 1602741354
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: af50e_00000
  
2020-10-15 05:55:56,282	WARNING util.py:136 -- The `process_trial` operation took 1.0082006454467773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    364 |          9347.51 | 58892288 |  309.752 |              333.354 |              160.172 |            775.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2939.4998881770225
    time_step_min: 2786
  date: 2020-10-15_05-56-22
  done: false
  episode_len_mean: 775.563276576162
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.7991438944037
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 210
  episodes_total: 76055
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0043488185201226e-30
        cur_lr: 5.0e-05
        entropy: 0.0649469131603837
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008583155534627926
        total_loss: .inf
        vf_explained_var: 0.998798668384552
        vf_loss: 0.42992227524518967
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.506451612903227
    gpu_util_percent0: 0.31935483870967746
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463376701733163
    mean_env_wait_ms: 1.1887108139233244
    mean_inference_ms: 4.296140865355479
    mean_raw_obs_processing_ms: 0.37671804471999004
  time_since_restore: 9373.538632392883
  time_this_iter_s: 26.030704498291016
  time_total_s: 9373.538632392883
  timers:
    learn_throughput: 8671.314
    learn_time_ms: 18658.303
    sample_throughput: 23745.147
    sample_time_ms: 6813.687
    update_time_ms: 31.633
  timestamp: 1602741382
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:56:23,790	WARNING util.py:136 -- The `process_trial` operation took 1.0294008255004883 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    365 |          9373.54 | 59054080 |  309.799 |              333.354 |              160.172 |            775.563 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2939.133700570529
    time_step_min: 2786
  date: 2020-10-15_05-56-49
  done: false
  episode_len_mean: 775.5758386094616
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.85520905702566
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 232
  episodes_total: 76287
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5065232277801844e-30
        cur_lr: 5.0e-05
        entropy: 0.05913041904568672
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008389532633979494
        total_loss: .inf
        vf_explained_var: 0.9994041323661804
        vf_loss: 0.21943617860476175
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.322580645161292
    gpu_util_percent0: 0.3693548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146336089411046
    mean_env_wait_ms: 1.1886441381243005
    mean_inference_ms: 4.296051401709691
    mean_raw_obs_processing_ms: 0.3767119794165738
  time_since_restore: 9399.39041852951
  time_this_iter_s: 25.851786136627197
  time_total_s: 9399.39041852951
  timers:
    learn_throughput: 8677.936
    learn_time_ms: 18644.064
    sample_throughput: 23714.207
    sample_time_ms: 6822.577
    update_time_ms: 32.794
  timestamp: 1602741409
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:56:51,024	WARNING util.py:136 -- The `process_trial` operation took 1.0338332653045654 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    366 |          9399.39 | 59215872 |  309.855 |              333.354 |              160.172 |            775.576 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2938.843244233361
    time_step_min: 2786
  date: 2020-10-15_05-57-16
  done: false
  episode_len_mean: 775.5858276777425
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.8988024690035
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 186
  episodes_total: 76473
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.759784841670276e-30
        cur_lr: 5.0e-05
        entropy: 0.059065885531405606
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009379089307913091
        total_loss: .inf
        vf_explained_var: 0.9990429878234863
        vf_loss: 0.3191121195753415
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.470967741935485
    gpu_util_percent0: 0.2867741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633483550919685
    mean_env_wait_ms: 1.1885922978854924
    mean_inference_ms: 4.2959976609239465
    mean_raw_obs_processing_ms: 0.3767078981720549
  time_since_restore: 9425.221423625946
  time_this_iter_s: 25.831005096435547
  time_total_s: 9425.221423625946
  timers:
    learn_throughput: 8688.974
    learn_time_ms: 18620.38
    sample_throughput: 23729.648
    sample_time_ms: 6818.137
    update_time_ms: 24.798
  timestamp: 1602741436
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:57:18,274	WARNING util.py:136 -- The `process_trial` operation took 1.010204553604126 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    367 |          9425.22 | 59377664 |  309.899 |              333.354 |              160.172 |            775.586 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2938.523578996816
    time_step_min: 2786
  date: 2020-10-15_05-57-44
  done: false
  episode_len_mean: 775.6054409348184
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.94625642955
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 205
  episodes_total: 76678
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0139677262505414e-29
        cur_lr: 5.0e-05
        entropy: 0.06067312601953745
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008025268810645988
        total_loss: .inf
        vf_explained_var: 0.9990128874778748
        vf_loss: 0.3607192610700925
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.57741935483871
    gpu_util_percent0: 0.3054838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463340061298235
    mean_env_wait_ms: 1.1885348123856794
    mean_inference_ms: 4.295940433676944
    mean_raw_obs_processing_ms: 0.37670408215924095
  time_since_restore: 9450.987761974335
  time_this_iter_s: 25.766338348388672
  time_total_s: 9450.987761974335
  timers:
    learn_throughput: 8687.666
    learn_time_ms: 18623.183
    sample_throughput: 23724.632
    sample_time_ms: 6819.579
    update_time_ms: 24.74
  timestamp: 1602741464
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:57:45,601	WARNING util.py:136 -- The `process_trial` operation took 1.0324079990386963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    368 |          9450.99 | 59539456 |  309.946 |              333.354 |              160.172 |            775.605 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2938.214043167714
    time_step_min: 2786
  date: 2020-10-15_05-58-11
  done: false
  episode_len_mean: 775.6220141733307
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 309.99170956689954
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 227
  episodes_total: 76905
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5209515893758119e-29
        cur_lr: 5.0e-05
        entropy: 0.07180590679248174
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01212132772585998
        total_loss: .inf
        vf_explained_var: 0.9985406398773193
        vf_loss: 0.5697307139635086
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.564516129032263
    gpu_util_percent0: 0.3196774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633275667324971
    mean_env_wait_ms: 1.1884706536262024
    mean_inference_ms: 4.295859362661495
    mean_raw_obs_processing_ms: 0.37669864333996433
  time_since_restore: 9477.148653268814
  time_this_iter_s: 26.16089129447937
  time_total_s: 9477.148653268814
  timers:
    learn_throughput: 8684.659
    learn_time_ms: 18629.631
    sample_throughput: 23740.441
    sample_time_ms: 6815.038
    update_time_ms: 32.209
  timestamp: 1602741491
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:58:13,156	WARNING util.py:136 -- The `process_trial` operation took 1.0399904251098633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    369 |          9477.15 | 59701248 |  309.992 |              333.354 |              160.172 |            775.622 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2938.0149749552306
    time_step_min: 2786
  date: 2020-10-15_05-58-38
  done: false
  episode_len_mean: 775.6358943764267
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.0161343147178
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 199
  episodes_total: 77104
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2814273840637176e-29
        cur_lr: 5.0e-05
        entropy: 0.0909043587744236
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013915671365490804
        total_loss: .inf
        vf_explained_var: 0.9951252937316895
        vf_loss: 1.7326024572054546
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.245161290322585
    gpu_util_percent0: 0.3448387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633123393241454
    mean_env_wait_ms: 1.1884144613860341
    mean_inference_ms: 4.2957969413372705
    mean_raw_obs_processing_ms: 0.3766940525551337
  time_since_restore: 9502.942809343338
  time_this_iter_s: 25.794156074523926
  time_total_s: 9502.942809343338
  timers:
    learn_throughput: 8682.723
    learn_time_ms: 18633.785
    sample_throughput: 23730.001
    sample_time_ms: 6818.036
    update_time_ms: 32.522
  timestamp: 1602741518
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:58:40,342	WARNING util.py:136 -- The `process_trial` operation took 1.0242998600006104 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    370 |          9502.94 | 59863040 |  310.016 |              333.354 |              160.172 |            775.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2938.029577373633
    time_step_min: 2786
  date: 2020-10-15_05-59-06
  done: false
  episode_len_mean: 775.6439835957411
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.01498601158335
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 193
  episodes_total: 77297
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.422141076095577e-29
        cur_lr: 5.0e-05
        entropy: 0.09619641614456971
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047948716674000025
        model: {}
        policy_loss: -0.015213505326149365
        total_loss: 2.648840010166168
        vf_explained_var: 0.9934597611427307
        vf_loss: 2.6641016801198325
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05806451612904
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633032899218293
    mean_env_wait_ms: 1.188360371896034
    mean_inference_ms: 4.295741689554253
    mean_raw_obs_processing_ms: 0.3766902521606356
  time_since_restore: 9528.7370698452
  time_this_iter_s: 25.794260501861572
  time_total_s: 9528.7370698452
  timers:
    learn_throughput: 8694.529
    learn_time_ms: 18608.484
    sample_throughput: 23709.731
    sample_time_ms: 6823.865
    update_time_ms: 32.513
  timestamp: 1602741546
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: af50e_00000
  
2020-10-15 05:59:07,499	WARNING util.py:136 -- The `process_trial` operation took 0.995903730392456 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    371 |          9528.74 | 60024832 |  310.015 |              333.354 |              160.172 |            775.644 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2937.785123326923
    time_step_min: 2786
  date: 2020-10-15_05-59-33
  done: false
  episode_len_mean: 775.6684812755583
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.0598775327936
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 77519
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7110705380477886e-29
        cur_lr: 5.0e-05
        entropy: 0.06537425642212231
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010378354675291726
        total_loss: .inf
        vf_explained_var: 0.9986610412597656
        vf_loss: 0.5337206025918325
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17096774193549
    gpu_util_percent0: 0.2725806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632932682771982
    mean_env_wait_ms: 1.1882985368834826
    mean_inference_ms: 4.295670725483895
    mean_raw_obs_processing_ms: 0.3766857137603989
  time_since_restore: 9554.495356321335
  time_this_iter_s: 25.758286476135254
  time_total_s: 9554.495356321335
  timers:
    learn_throughput: 8696.838
    learn_time_ms: 18603.544
    sample_throughput: 23701.744
    sample_time_ms: 6826.164
    update_time_ms: 32.651
  timestamp: 1602741573
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 05:59:34,657	WARNING util.py:136 -- The `process_trial` operation took 1.034726619720459 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    372 |           9554.5 | 60186624 |   310.06 |              333.354 |              160.172 |            775.668 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2937.4688232113895
    time_step_min: 2786
  date: 2020-10-15_06-00-00
  done: false
  episode_len_mean: 775.6898800946892
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.1071446764889
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 209
  episodes_total: 77728
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5666058070716825e-29
        cur_lr: 5.0e-05
        entropy: 0.06226194308449825
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007403660138758521
        total_loss: .inf
        vf_explained_var: 0.9992663264274597
        vf_loss: 0.2655385695397854
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.900000000000002
    gpu_util_percent0: 0.32838709677419353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632780149433622
    mean_env_wait_ms: 1.1882398652933757
    mean_inference_ms: 4.29560070658088
    mean_raw_obs_processing_ms: 0.37668085406285284
  time_since_restore: 9580.214423894882
  time_this_iter_s: 25.719067573547363
  time_total_s: 9580.214423894882
  timers:
    learn_throughput: 8707.03
    learn_time_ms: 18581.767
    sample_throughput: 23759.191
    sample_time_ms: 6809.66
    update_time_ms: 32.078
  timestamp: 1602741600
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:00:01,945	WARNING util.py:136 -- The `process_trial` operation took 1.038254976272583 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    373 |          9580.21 | 60348416 |  310.107 |              333.354 |              160.172 |             775.69 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2937.1839347630666
    time_step_min: 2786
  date: 2020-10-15_06-00-27
  done: false
  episode_len_mean: 775.7043459287402
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.1502710646871
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 77912
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.849908710607525e-29
        cur_lr: 5.0e-05
        entropy: 0.06750498277445634
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008653140937288603
        total_loss: .inf
        vf_explained_var: 0.9992833137512207
        vf_loss: 0.23667478933930397
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.632258064516126
    gpu_util_percent0: 0.2883870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463267082613453
    mean_env_wait_ms: 1.1881883751134048
    mean_inference_ms: 4.295548787663272
    mean_raw_obs_processing_ms: 0.3766770163081005
  time_since_restore: 9605.992376565933
  time_this_iter_s: 25.777952671051025
  time_total_s: 9605.992376565933
  timers:
    learn_throughput: 8701.33
    learn_time_ms: 18593.938
    sample_throughput: 23827.045
    sample_time_ms: 6790.267
    update_time_ms: 31.956
  timestamp: 1602741627
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:00:29,357	WARNING util.py:136 -- The `process_trial` operation took 1.0966999530792236 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    374 |          9605.99 | 60510208 |   310.15 |              333.354 |              160.172 |            775.704 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2936.867844794468
    time_step_min: 2786
  date: 2020-10-15_06-00-55
  done: false
  episode_len_mean: 775.7237879485998
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.1971500444467
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 220
  episodes_total: 78132
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.774863065911287e-29
        cur_lr: 5.0e-05
        entropy: 0.0689313259596626
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007638928606562938
        total_loss: .inf
        vf_explained_var: 0.9983363151550293
        vf_loss: 0.6259235913554827
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.696774193548386
    gpu_util_percent0: 0.29258064516129034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463258156607458
    mean_env_wait_ms: 1.1881275653816656
    mean_inference_ms: 4.295486628658028
    mean_raw_obs_processing_ms: 0.37667287423667395
  time_since_restore: 9631.80143046379
  time_this_iter_s: 25.809053897857666
  time_total_s: 9631.80143046379
  timers:
    learn_throughput: 8705.581
    learn_time_ms: 18584.859
    sample_throughput: 23888.726
    sample_time_ms: 6772.734
    update_time_ms: 30.151
  timestamp: 1602741655
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:00:56,605	WARNING util.py:136 -- The `process_trial` operation took 1.0140368938446045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    375 |           9631.8 | 60672000 |  310.197 |              333.354 |              160.172 |            775.724 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2936.5406185356387
    time_step_min: 2786
  date: 2020-10-15_06-01-22
  done: false
  episode_len_mean: 775.7447802338047
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.2463802866067
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 78356
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.662294598866932e-29
        cur_lr: 5.0e-05
        entropy: 0.06595819567640622
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00929112630395442
        total_loss: .inf
        vf_explained_var: 0.9986114501953125
        vf_loss: 0.5218171154459318
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.723333333333336
    gpu_util_percent0: 0.3893333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463242508118605
    mean_env_wait_ms: 1.1880647014040995
    mean_inference_ms: 4.295405807565298
    mean_raw_obs_processing_ms: 0.3766674293723672
  time_since_restore: 9657.409292697906
  time_this_iter_s: 25.6078622341156
  time_total_s: 9657.409292697906
  timers:
    learn_throughput: 8708.331
    learn_time_ms: 18578.991
    sample_throughput: 23922.472
    sample_time_ms: 6763.181
    update_time_ms: 29.274
  timestamp: 1602741682
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:01:23,653	WARNING util.py:136 -- The `process_trial` operation took 1.0737414360046387 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    376 |          9657.41 | 60833792 |  310.246 |              333.354 |              160.172 |            775.745 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2936.2621795851805
    time_step_min: 2786
  date: 2020-10-15_06-01-49
  done: false
  episode_len_mean: 775.7596582371966
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.28877513773205
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 178
  episodes_total: 78534
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2993441898300395e-28
        cur_lr: 5.0e-05
        entropy: 0.06834282291432221
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00821577274473384
        total_loss: .inf
        vf_explained_var: 0.9989859461784363
        vf_loss: 0.32662223279476166
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.874193548387094
    gpu_util_percent0: 0.30451612903225805
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463232093758719
    mean_env_wait_ms: 1.1880149378034122
    mean_inference_ms: 4.295353133643898
    mean_raw_obs_processing_ms: 0.376663729752242
  time_since_restore: 9683.227703809738
  time_this_iter_s: 25.818411111831665
  time_total_s: 9683.227703809738
  timers:
    learn_throughput: 8710.378
    learn_time_ms: 18574.625
    sample_throughput: 23887.693
    sample_time_ms: 6773.028
    update_time_ms: 30.238
  timestamp: 1602741709
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:01:50,858	WARNING util.py:136 -- The `process_trial` operation took 1.0270111560821533 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    377 |          9683.23 | 60995584 |  310.289 |              333.354 |              160.172 |             775.76 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2935.924462854021
    time_step_min: 2786
  date: 2020-10-15_06-02-16
  done: false
  episode_len_mean: 775.7778398628484
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.3404051307408
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 211
  episodes_total: 78745
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9490162847450595e-28
        cur_lr: 5.0e-05
        entropy: 0.07283337228000164
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00821674921705077
        total_loss: .inf
        vf_explained_var: 0.9989208579063416
        vf_loss: 0.3799966226021449
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16774193548387
    gpu_util_percent0: 0.30838709677419357
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463222948342239
    mean_env_wait_ms: 1.187956342429973
    mean_inference_ms: 4.295297710408574
    mean_raw_obs_processing_ms: 0.37665996216599423
  time_since_restore: 9708.97524523735
  time_this_iter_s: 25.747541427612305
  time_total_s: 9708.97524523735
  timers:
    learn_throughput: 8720.374
    learn_time_ms: 18553.333
    sample_throughput: 23857.989
    sample_time_ms: 6781.46
    update_time_ms: 30.197
  timestamp: 1602741736
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:02:18,048	WARNING util.py:136 -- The `process_trial` operation took 1.0702743530273438 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    378 |          9708.98 | 61157376 |   310.34 |              333.354 |              160.172 |            775.778 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2935.612289370328
    time_step_min: 2786
  date: 2020-10-15_06-02-43
  done: false
  episode_len_mean: 775.7962948893279
  episode_reward_max: 333.3535353535355
  episode_reward_mean: 310.3820391270245
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 227
  episodes_total: 78972
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9235244271175893e-28
        cur_lr: 5.0e-05
        entropy: 0.08968403562903404
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012380655338347424
        total_loss: .inf
        vf_explained_var: 0.9975985884666443
        vf_loss: 0.9231323252121607
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89677419354839
    gpu_util_percent0: 0.3009677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632096176722
    mean_env_wait_ms: 1.1878927470088807
    mean_inference_ms: 4.295217440885844
    mean_raw_obs_processing_ms: 0.37665431287288165
  time_since_restore: 9734.884345531464
  time_this_iter_s: 25.90910029411316
  time_total_s: 9734.884345531464
  timers:
    learn_throughput: 8732.893
    learn_time_ms: 18526.736
    sample_throughput: 23866.436
    sample_time_ms: 6779.06
    update_time_ms: 22.589
  timestamp: 1602741763
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:02:45,358	WARNING util.py:136 -- The `process_trial` operation took 1.0334014892578125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    379 |          9734.88 | 61319168 |  310.382 |              333.354 |              160.172 |            775.796 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2935.5121652194794
    time_step_min: 2786
  date: 2020-10-15_06-03-11
  done: false
  episode_len_mean: 775.8142645999924
  episode_reward_max: 334.2626262626258
  episode_reward_mean: 310.3929234360505
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 189
  episodes_total: 79161
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.385286640676383e-28
        cur_lr: 5.0e-05
        entropy: 0.09641382532815139
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013051199940188477
        total_loss: .inf
        vf_explained_var: 0.9946422576904297
        vf_loss: 1.9796209931373596
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76666666666667
    gpu_util_percent0: 0.4056666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631978532990428
    mean_env_wait_ms: 1.1878407694691981
    mean_inference_ms: 4.2951666432483915
    mean_raw_obs_processing_ms: 0.37665069242914484
  time_since_restore: 9760.599134683609
  time_this_iter_s: 25.714789152145386
  time_total_s: 9760.599134683609
  timers:
    learn_throughput: 8731.621
    learn_time_ms: 18529.435
    sample_throughput: 23907.846
    sample_time_ms: 6767.318
    update_time_ms: 22.57
  timestamp: 1602741791
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:03:12,630	WARNING util.py:136 -- The `process_trial` operation took 1.0828790664672852 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    380 |           9760.6 | 61480960 |  310.393 |              334.263 |              160.172 |            775.814 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2935.305561858659
    time_step_min: 2786
  date: 2020-10-15_06-03-38
  done: false
  episode_len_mean: 775.8306118334846
  episode_reward_max: 334.2626262626258
  episode_reward_mean: 310.42862744469164
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 207
  episodes_total: 79368
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.577929961014574e-28
        cur_lr: 5.0e-05
        entropy: 0.08474686555564404
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01176129700616002
        total_loss: .inf
        vf_explained_var: 0.9972020983695984
        vf_loss: 1.0295434792836506
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.400000000000002
    gpu_util_percent0: 0.32225806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631892110759634
    mean_env_wait_ms: 1.1877839889209023
    mean_inference_ms: 4.295114427519717
    mean_raw_obs_processing_ms: 0.37664710207006563
  time_since_restore: 9786.76641201973
  time_this_iter_s: 26.167277336120605
  time_total_s: 9786.76641201973
  timers:
    learn_throughput: 8716.932
    learn_time_ms: 18560.658
    sample_throughput: 23865.473
    sample_time_ms: 6779.334
    update_time_ms: 24.666
  timestamp: 1602741818
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:03:40,367	WARNING util.py:136 -- The `process_trial` operation took 1.102644681930542 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    381 |          9786.77 | 61642752 |  310.429 |              334.263 |              160.172 |            775.831 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2934.9924825262733
    time_step_min: 2786
  date: 2020-10-15_06-04-06
  done: false
  episode_len_mean: 775.843686392763
  episode_reward_max: 334.2626262626258
  episode_reward_mean: 310.47603627682764
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 222
  episodes_total: 79590
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.866894941521862e-28
        cur_lr: 5.0e-05
        entropy: 0.07563599074880283
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009319153672549874
        total_loss: .inf
        vf_explained_var: 0.9985313415527344
        vf_loss: 0.539523700873057
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.484375
    gpu_util_percent0: 0.31843750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463178028471088
    mean_env_wait_ms: 1.1877225256829227
    mean_inference_ms: 4.295041734740706
    mean_raw_obs_processing_ms: 0.37664245240104105
  time_since_restore: 9812.576295852661
  time_this_iter_s: 25.80988383293152
  time_total_s: 9812.576295852661
  timers:
    learn_throughput: 8712.461
    learn_time_ms: 18570.183
    sample_throughput: 23865.256
    sample_time_ms: 6779.395
    update_time_ms: 25.307
  timestamp: 1602741846
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:04:07,801	WARNING util.py:136 -- The `process_trial` operation took 1.0497980117797852 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    382 |          9812.58 | 61804544 |  310.476 |              334.263 |              160.172 |            775.844 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2934.661345739078
    time_step_min: 2786
  date: 2020-10-15_06-04-33
  done: false
  episode_len_mean: 775.8544303797469
  episode_reward_max: 334.2626262626258
  episode_reward_mean: 310.5256386398131
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 200
  episodes_total: 79790
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4800342412282795e-27
        cur_lr: 5.0e-05
        entropy: 0.07128243024150531
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00809858053495797
        total_loss: .inf
        vf_explained_var: 0.9992573261260986
        vf_loss: 0.232312290618817
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.661290322580644
    gpu_util_percent0: 0.3412903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463163269200376
    mean_env_wait_ms: 1.1876671152265126
    mean_inference_ms: 4.294982641735144
    mean_raw_obs_processing_ms: 0.3766380632947966
  time_since_restore: 9838.586237430573
  time_this_iter_s: 26.009941577911377
  time_total_s: 9838.586237430573
  timers:
    learn_throughput: 8702.855
    learn_time_ms: 18590.681
    sample_throughput: 23845.147
    sample_time_ms: 6785.112
    update_time_ms: 27.666
  timestamp: 1602741873
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:04:35,238	WARNING util.py:136 -- The `process_trial` operation took 1.052422046661377 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    383 |          9838.59 | 61966336 |  310.526 |              334.263 |              160.172 |            775.854 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2934.3386453186567
    time_step_min: 2786
  date: 2020-10-15_06-05-00
  done: false
  episode_len_mean: 775.8677160038507
  episode_reward_max: 334.2626262626258
  episode_reward_mean: 310.5720738963514
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 197
  episodes_total: 79987
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.220051361842419e-27
        cur_lr: 5.0e-05
        entropy: 0.07300003928442796
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009634743008064106
        total_loss: .inf
        vf_explained_var: 0.9991209506988525
        vf_loss: 0.2917647510766983
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080645161290324
    gpu_util_percent0: 0.3616129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631570695546026
    mean_env_wait_ms: 1.1876130857643885
    mean_inference_ms: 4.2949342639589965
    mean_raw_obs_processing_ms: 0.3766347454441081
  time_since_restore: 9864.322716236115
  time_this_iter_s: 25.736478805541992
  time_total_s: 9864.322716236115
  timers:
    learn_throughput: 8708.482
    learn_time_ms: 18578.667
    sample_throughput: 23852.377
    sample_time_ms: 6783.056
    update_time_ms: 28.17
  timestamp: 1602741900
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:05:02,446	WARNING util.py:136 -- The `process_trial` operation took 1.0862483978271484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    384 |          9864.32 | 62128128 |  310.572 |              334.263 |              160.172 |            775.868 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2933.980678558064
    time_step_min: 2778
  date: 2020-10-15_06-05-28
  done: false
  episode_len_mean: 775.8809155737296
  episode_reward_max: 334.2626262626259
  episode_reward_mean: 310.62619273571477
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 225
  episodes_total: 80212
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.330077042763629e-27
        cur_lr: 5.0e-05
        entropy: 0.07141490963598092
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010429196391972559
        total_loss: .inf
        vf_explained_var: 0.9988296627998352
        vf_loss: 0.4205821454524994
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61935483870968
    gpu_util_percent0: 0.36419354838709667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631453278348192
    mean_env_wait_ms: 1.187551529993856
    mean_inference_ms: 4.2948653724767665
    mean_raw_obs_processing_ms: 0.37663019609794707
  time_since_restore: 9890.235343694687
  time_this_iter_s: 25.912627458572388
  time_total_s: 9890.235343694687
  timers:
    learn_throughput: 8713.411
    learn_time_ms: 18568.159
    sample_throughput: 23771.001
    sample_time_ms: 6806.276
    update_time_ms: 29.911
  timestamp: 1602741928
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:05:29,860	WARNING util.py:136 -- The `process_trial` operation took 1.1287293434143066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    385 |          9890.24 | 62289920 |  310.626 |              334.263 |              160.172 |            775.881 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2933.667388275107
    time_step_min: 2778
  date: 2020-10-15_06-05-56
  done: false
  episode_len_mean: 775.8888681638439
  episode_reward_max: 334.2626262626259
  episode_reward_mean: 310.6703391446357
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 80418
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.995115564145444e-27
        cur_lr: 5.0e-05
        entropy: 0.07179508668680985
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01002485137723852
        total_loss: .inf
        vf_explained_var: 0.9982280731201172
        vf_loss: 0.5966778496901194
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.378124999999997
    gpu_util_percent0: 0.26625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631308084740968
    mean_env_wait_ms: 1.1874943170455772
    mean_inference_ms: 4.294802898841481
    mean_raw_obs_processing_ms: 0.3766256824321951
  time_since_restore: 9916.404338359833
  time_this_iter_s: 26.168994665145874
  time_total_s: 9916.404338359833
  timers:
    learn_throughput: 8699.629
    learn_time_ms: 18597.574
    sample_throughput: 23720.697
    sample_time_ms: 6820.71
    update_time_ms: 32.036
  timestamp: 1602741956
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:05:57,538	WARNING util.py:136 -- The `process_trial` operation took 1.0889244079589844 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    386 |           9916.4 | 62451712 |   310.67 |              334.263 |              160.172 |            775.889 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2933.372064445658
    time_step_min: 2778
  date: 2020-10-15_06-06-23
  done: false
  episode_len_mean: 775.8958638314766
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 310.7160239719476
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 188
  episodes_total: 80606
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.492673346218166e-27
        cur_lr: 5.0e-05
        entropy: 0.0689255241304636
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009292534998773286
        total_loss: .inf
        vf_explained_var: 0.9989256858825684
        vf_loss: 0.3648132458329201
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.158064516129034
    gpu_util_percent0: 0.3535483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631212868254123
    mean_env_wait_ms: 1.1874417854151975
    mean_inference_ms: 4.294752747400219
    mean_raw_obs_processing_ms: 0.376622123400966
  time_since_restore: 9942.237270832062
  time_this_iter_s: 25.832932472229004
  time_total_s: 9942.237270832062
  timers:
    learn_throughput: 8696.989
    learn_time_ms: 18603.22
    sample_throughput: 23739.091
    sample_time_ms: 6815.425
    update_time_ms: 31.323
  timestamp: 1602741983
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:06:24,960	WARNING util.py:136 -- The `process_trial` operation took 1.0477347373962402 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    387 |          9942.24 | 62613504 |  310.716 |              334.263 |              160.172 |            775.896 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2932.988352518876
    time_step_min: 2778
  date: 2020-10-15_06-06-51
  done: false
  episode_len_mean: 775.9065963974664
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 310.77409261858463
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 80832
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.123901001932725e-26
        cur_lr: 5.0e-05
        entropy: 0.06502497879167397
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006483186725745327
        total_loss: .inf
        vf_explained_var: 0.9990598559379578
        vf_loss: 0.3332155955334504
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.383870967741938
    gpu_util_percent0: 0.31548387096774183
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463113543515153
    mean_env_wait_ms: 1.1873812457732373
    mean_inference_ms: 4.2946914850828
    mean_raw_obs_processing_ms: 0.3766183347959483
  time_since_restore: 9968.310722827911
  time_this_iter_s: 26.07345199584961
  time_total_s: 9968.310722827911
  timers:
    learn_throughput: 8678.216
    learn_time_ms: 18643.464
    sample_throughput: 23769.44
    sample_time_ms: 6806.723
    update_time_ms: 32.171
  timestamp: 1602742011
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:06:52,503	WARNING util.py:136 -- The `process_trial` operation took 1.0682377815246582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    388 |          9968.31 | 62775296 |  310.774 |              334.263 |              160.172 |            775.907 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2932.621014036517
    time_step_min: 2778
  date: 2020-10-15_06-07-18
  done: false
  episode_len_mean: 775.9189339255969
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 310.8297821823638
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 213
  episodes_total: 81045
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6858515028990872e-26
        cur_lr: 5.0e-05
        entropy: 0.06213368599613508
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006504881098711242
        total_loss: .inf
        vf_explained_var: 0.9994677901268005
        vf_loss: 0.17523453881343207
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.1
    gpu_util_percent0: 0.32
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630974795495122
    mean_env_wait_ms: 1.187321783085585
    mean_inference_ms: 4.294622856481569
    mean_raw_obs_processing_ms: 0.3766131626433525
  time_since_restore: 9994.34979057312
  time_this_iter_s: 26.03906774520874
  time_total_s: 9994.34979057312
  timers:
    learn_throughput: 8671.324
    learn_time_ms: 18658.281
    sample_throughput: 23769.966
    sample_time_ms: 6806.573
    update_time_ms: 31.477
  timestamp: 1602742038
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:07:20,129	WARNING util.py:136 -- The `process_trial` operation took 1.1375172138214111 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    389 |          9994.35 | 62937088 |   310.83 |              334.263 |              160.172 |            775.919 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2932.328656771571
    time_step_min: 2778
  date: 2020-10-15_06-07-46
  done: false
  episode_len_mean: 775.9276841444347
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 310.87419481480543
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 182
  episodes_total: 81227
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5287772543486313e-26
        cur_lr: 5.0e-05
        entropy: 0.06471944351991017
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009427895604555184
        total_loss: .inf
        vf_explained_var: 0.9992051124572754
        vf_loss: 0.2631804707149665
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.854838709677423
    gpu_util_percent0: 0.31000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463087172556031
    mean_env_wait_ms: 1.1872716644289771
    mean_inference_ms: 4.294574760720677
    mean_raw_obs_processing_ms: 0.3766097195338704
  time_since_restore: 10020.452796936035
  time_this_iter_s: 26.10300636291504
  time_total_s: 10020.452796936035
  timers:
    learn_throughput: 8661.625
    learn_time_ms: 18679.173
    sample_throughput: 23719.097
    sample_time_ms: 6821.17
    update_time_ms: 33.004
  timestamp: 1602742066
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:07:47,850	WARNING util.py:136 -- The `process_trial` operation took 1.0931544303894043 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    390 |          10020.5 | 63098880 |  310.874 |              334.263 |              160.172 |            775.928 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2931.972384649403
    time_step_min: 2778
  date: 2020-10-15_06-08-13
  done: false
  episode_len_mean: 775.9382290106328
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 310.9280446584548
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 219
  episodes_total: 81446
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7931658815229456e-26
        cur_lr: 5.0e-05
        entropy: 0.06580102816224098
        entropy_coeff: 0.0005000000000000001
        kl: 0.005192325566895306
        model: {}
        policy_loss: -0.007332787732593715
        total_loss: 0.2238591561714808
        vf_explained_var: 0.9993712902069092
        vf_loss: 0.23122484361131987
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612907
    gpu_util_percent0: 0.2954838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630802323745828
    mean_env_wait_ms: 1.1872114822288604
    mean_inference_ms: 4.294520963206403
    mean_raw_obs_processing_ms: 0.37660621520359044
  time_since_restore: 10046.26544380188
  time_this_iter_s: 25.812646865844727
  time_total_s: 10046.26544380188
  timers:
    learn_throughput: 8668.63
    learn_time_ms: 18664.079
    sample_throughput: 23795.405
    sample_time_ms: 6799.296
    update_time_ms: 33.028
  timestamp: 1602742093
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: af50e_00000
  
2020-10-15 06:08:15,263	WARNING util.py:136 -- The `process_trial` operation took 1.1247549057006836 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    391 |          10046.3 | 63260672 |  310.928 |              334.263 |              160.172 |            775.938 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2931.6008845004167
    time_step_min: 2778
  date: 2020-10-15_06-08-41
  done: false
  episode_len_mean: 775.9511203624342
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 310.9817360577735
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 224
  episodes_total: 81670
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7931658815229456e-26
        cur_lr: 5.0e-05
        entropy: 0.06722974839309852
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008086384147948896
        total_loss: .inf
        vf_explained_var: 0.9988283514976501
        vf_loss: 0.42162665476401645
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.046875
    gpu_util_percent0: 0.2975
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630645683545254
    mean_env_wait_ms: 1.1871505990873872
    mean_inference_ms: 4.2944476544569925
    mean_raw_obs_processing_ms: 0.3766011026263153
  time_since_restore: 10072.365596294403
  time_this_iter_s: 26.100152492523193
  time_total_s: 10072.365596294403
  timers:
    learn_throughput: 8656.51
    learn_time_ms: 18690.211
    sample_throughput: 23821.479
    sample_time_ms: 6791.854
    update_time_ms: 33.173
  timestamp: 1602742121
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:08:42,911	WARNING util.py:136 -- The `process_trial` operation took 1.091557264328003 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    392 |          10072.4 | 63422464 |  310.982 |              334.263 |              160.172 |            775.951 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2931.302778422912
    time_step_min: 2778
  date: 2020-10-15_06-09-09
  done: false
  episode_len_mean: 775.9613321767603
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.0248545984454
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 181
  episodes_total: 81851
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.689748822284418e-26
        cur_lr: 5.0e-05
        entropy: 0.0685452651232481
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009015453096556788
        total_loss: .inf
        vf_explained_var: 0.9989383220672607
        vf_loss: 0.3486739322543144
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870967
    gpu_util_percent0: 0.3058064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630556939208383
    mean_env_wait_ms: 1.187101121825439
    mean_inference_ms: 4.294399974761388
    mean_raw_obs_processing_ms: 0.37659768391845067
  time_since_restore: 10098.491855859756
  time_this_iter_s: 26.126259565353394
  time_total_s: 10098.491855859756
  timers:
    learn_throughput: 8654.748
    learn_time_ms: 18694.017
    sample_throughput: 23830.885
    sample_time_ms: 6789.173
    update_time_ms: 32.784
  timestamp: 1602742149
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:09:10,535	WARNING util.py:136 -- The `process_trial` operation took 1.1057767868041992 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    393 |          10098.5 | 63584256 |  311.025 |              334.263 |              160.172 |            775.961 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2931.015410687507
    time_step_min: 2778
  date: 2020-10-15_06-09-36
  done: false
  episode_len_mean: 775.9714243934538
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.06857985556036
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 212
  episodes_total: 82063
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.534623233426627e-26
        cur_lr: 5.0e-05
        entropy: 0.07270629331469536
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008722060235110499
        total_loss: .inf
        vf_explained_var: 0.9977371096611023
        vf_loss: 0.8046610802412033
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.480645161290326
    gpu_util_percent0: 0.3003225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630468232264418
    mean_env_wait_ms: 1.1870430421920728
    mean_inference_ms: 4.294348841477122
    mean_raw_obs_processing_ms: 0.37659427730511535
  time_since_restore: 10124.140830278397
  time_this_iter_s: 25.648974418640137
  time_total_s: 10124.140830278397
  timers:
    learn_throughput: 8655.666
    learn_time_ms: 18692.034
    sample_throughput: 23857.592
    sample_time_ms: 6781.573
    update_time_ms: 32.476
  timestamp: 1602742176
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:09:37,656	WARNING util.py:136 -- The `process_trial` operation took 1.0822348594665527 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    394 |          10124.1 | 63746048 |  311.069 |              334.263 |              160.172 |            775.971 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2930.704328267477
    time_step_min: 2778
  date: 2020-10-15_06-10-03
  done: false
  episode_len_mean: 775.9818086812813
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.11982104130766
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 82292
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.280193485013994e-25
        cur_lr: 5.0e-05
        entropy: 0.06576552738746007
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033429366691658893
        model: {}
        policy_loss: -0.00859380631785219
        total_loss: 0.4768439307808876
        vf_explained_var: 0.9986841678619385
        vf_loss: 0.48547061284383136
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.306451612903228
    gpu_util_percent0: 0.2932258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630334824744728
    mean_env_wait_ms: 1.1869803085423354
    mean_inference_ms: 4.294274849407527
    mean_raw_obs_processing_ms: 0.3765893687162694
  time_since_restore: 10150.123655319214
  time_this_iter_s: 25.98282504081726
  time_total_s: 10150.123655319214
  timers:
    learn_throughput: 8642.793
    learn_time_ms: 18719.874
    sample_throughput: 23963.173
    sample_time_ms: 6751.693
    update_time_ms: 31.223
  timestamp: 1602742203
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: af50e_00000
  
2020-10-15 06:10:05,125	WARNING util.py:136 -- The `process_trial` operation took 1.1144747734069824 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    395 |          10150.1 | 63907840 |   311.12 |              334.263 |              160.172 |            775.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2930.4186712277256
    time_step_min: 2778
  date: 2020-10-15_06-10-31
  done: false
  episode_len_mean: 775.9900823239855
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.1633286269009
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 187
  episodes_total: 82479
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.40096742506997e-26
        cur_lr: 5.0e-05
        entropy: 0.06215081208695968
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008699667853458474
        total_loss: .inf
        vf_explained_var: 0.9990477561950684
        vf_loss: 0.2935050080219905
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.13548387096774
    gpu_util_percent0: 0.3087096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630238079557967
    mean_env_wait_ms: 1.1869298272456792
    mean_inference_ms: 4.294230842747964
    mean_raw_obs_processing_ms: 0.37658590996810853
  time_since_restore: 10176.013539075851
  time_this_iter_s: 25.889883756637573
  time_total_s: 10176.013539075851
  timers:
    learn_throughput: 8647.648
    learn_time_ms: 18709.365
    sample_throughput: 23992.17
    sample_time_ms: 6743.533
    update_time_ms: 29.256
  timestamp: 1602742231
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:10:32,616	WARNING util.py:136 -- The `process_trial` operation took 1.1044635772705078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    396 |            10176 | 64069632 |  311.163 |              334.263 |              160.172 |             775.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2930.104533959319
    time_step_min: 2778
  date: 2020-10-15_06-10-58
  done: false
  episode_len_mean: 776.0005563282336
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.211237366102
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 82685
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.601451137604955e-26
        cur_lr: 5.0e-05
        entropy: 0.06802057971556981
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006777553217640768
        total_loss: .inf
        vf_explained_var: 0.9988800883293152
        vf_loss: 0.37740888943274814
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.348387096774193
    gpu_util_percent0: 0.26806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463016333565518
    mean_env_wait_ms: 1.18687431226527
    mean_inference_ms: 4.294186180407617
    mean_raw_obs_processing_ms: 0.3765826707642075
  time_since_restore: 10202.010666847229
  time_this_iter_s: 25.997127771377563
  time_total_s: 10202.010666847229
  timers:
    learn_throughput: 8641.731
    learn_time_ms: 18722.174
    sample_throughput: 23973.973
    sample_time_ms: 6748.652
    update_time_ms: 28.754
  timestamp: 1602742258
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:11:00,102	WARNING util.py:136 -- The `process_trial` operation took 1.1029834747314453 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    397 |            10202 | 64231424 |  311.211 |              334.263 |              160.172 |            776.001 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2929.773744735794
    time_step_min: 2778
  date: 2020-10-15_06-11-25
  done: false
  episode_len_mean: 776.0134719525286
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.262139077994
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 228
  episodes_total: 82913
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4402176706407434e-25
        cur_lr: 5.0e-05
        entropy: 0.06748180525998275
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009586542759886166
        total_loss: .inf
        vf_explained_var: 0.9992491602897644
        vf_loss: 0.28263966739177704
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95806451612904
    gpu_util_percent0: 0.3009677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630042380487837
    mean_env_wait_ms: 1.1868119867100182
    mean_inference_ms: 4.29411252067133
    mean_raw_obs_processing_ms: 0.37657806191520976
  time_since_restore: 10227.768594741821
  time_this_iter_s: 25.757927894592285
  time_total_s: 10227.768594741821
  timers:
    learn_throughput: 8653.184
    learn_time_ms: 18697.396
    sample_throughput: 23990.71
    sample_time_ms: 6743.944
    update_time_ms: 27.749
  timestamp: 1602742285
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:11:27,351	WARNING util.py:136 -- The `process_trial` operation took 1.1041388511657715 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    398 |          10227.8 | 64393216 |  311.262 |              334.263 |              160.172 |            776.013 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2929.5009931741847
    time_step_min: 2778
  date: 2020-10-15_06-11-53
  done: false
  episode_len_mean: 776.0223080532794
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.30469077301524
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 83109
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1603265059611145e-25
        cur_lr: 5.0e-05
        entropy: 0.06652824518581231
        entropy_coeff: 0.0005000000000000001
        kl: 0.003955842150996129
        model: {}
        policy_loss: -0.008292879453316951
        total_loss: 0.2721024230122566
        vf_explained_var: 0.9991104006767273
        vf_loss: 0.28042855858802795
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73870967741936
    gpu_util_percent0: 0.26354838709677425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629920264480484
    mean_env_wait_ms: 1.1867587618863389
    mean_inference_ms: 4.294063228092878
    mean_raw_obs_processing_ms: 0.37657430539182535
  time_since_restore: 10253.552402973175
  time_this_iter_s: 25.78380823135376
  time_total_s: 10253.552402973175
  timers:
    learn_throughput: 8663.507
    learn_time_ms: 18675.116
    sample_throughput: 23996.335
    sample_time_ms: 6742.363
    update_time_ms: 27.164
  timestamp: 1602742313
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: af50e_00000
  
2020-10-15 06:11:54,733	WARNING util.py:136 -- The `process_trial` operation took 1.13820481300354 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    399 |          10253.6 | 64555008 |  311.305 |              334.263 |              160.172 |            776.022 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2929.2041603113025
    time_step_min: 2778
  date: 2020-10-15_06-12-20
  done: false
  episode_len_mean: 776.0338515095133
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.3491338570263
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 196
  episodes_total: 83305
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0801632529805572e-25
        cur_lr: 5.0e-05
        entropy: 0.07102679647505283
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007360601176818212
        total_loss: .inf
        vf_explained_var: 0.9988569617271423
        vf_loss: 0.3758435348669688
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.63870967741936
    gpu_util_percent0: 0.32483870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629855060016755
    mean_env_wait_ms: 1.1867057577682163
    mean_inference_ms: 4.294021640407081
    mean_raw_obs_processing_ms: 0.37657137327326196
  time_since_restore: 10279.533683538437
  time_this_iter_s: 25.98128056526184
  time_total_s: 10279.533683538437
  timers:
    learn_throughput: 8669.636
    learn_time_ms: 18661.913
    sample_throughput: 24021.476
    sample_time_ms: 6735.306
    update_time_ms: 25.475
  timestamp: 1602742340
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:12:22,265	WARNING util.py:136 -- The `process_trial` operation took 1.1531693935394287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    400 |          10279.5 | 64716800 |  311.349 |              334.263 |              160.172 |            776.034 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.8938089135354
    time_step_min: 2778
  date: 2020-10-15_06-12-48
  done: false
  episode_len_mean: 776.0525660517401
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.4006363178065
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 228
  episodes_total: 83533
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6202448794708362e-25
        cur_lr: 5.0e-05
        entropy: 0.07020596352716287
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009225638132193126
        total_loss: .inf
        vf_explained_var: 0.9993231892585754
        vf_loss: 0.24697339286406836
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.796774193548384
    gpu_util_percent0: 0.3548387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462975430744998
    mean_env_wait_ms: 1.1866444194474512
    mean_inference_ms: 4.293958176854206
    mean_raw_obs_processing_ms: 0.3765672044729653
  time_since_restore: 10305.527806282043
  time_this_iter_s: 25.994122743606567
  time_total_s: 10305.527806282043
  timers:
    learn_throughput: 8666.219
    learn_time_ms: 18669.272
    sample_throughput: 24009.452
    sample_time_ms: 6738.679
    update_time_ms: 31.402
  timestamp: 1602742368
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:12:49,768	WARNING util.py:136 -- The `process_trial` operation took 1.115452766418457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    401 |          10305.5 | 64878592 |  311.401 |              334.263 |              160.172 |            776.053 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.602045571314
    time_step_min: 2778
  date: 2020-10-15_06-13-15
  done: false
  episode_len_mean: 776.0672359228519
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.44553012057617
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 202
  episodes_total: 83735
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.430367319206254e-25
        cur_lr: 5.0e-05
        entropy: 0.06802641414105892
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008497007171778629
        total_loss: .inf
        vf_explained_var: 0.9988676905632019
        vf_loss: 0.3760253886381785
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.016129032258064
    gpu_util_percent0: 0.30612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629619473646072
    mean_env_wait_ms: 1.1865893041811648
    mean_inference_ms: 4.293900164486807
    mean_raw_obs_processing_ms: 0.376563010962291
  time_since_restore: 10331.508172273636
  time_this_iter_s: 25.980365991592407
  time_total_s: 10331.508172273636
  timers:
    learn_throughput: 8671.941
    learn_time_ms: 18656.954
    sample_throughput: 24011.943
    sample_time_ms: 6737.98
    update_time_ms: 30.504
  timestamp: 1602742395
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:13:17,310	WARNING util.py:136 -- The `process_trial` operation took 1.173140287399292 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    402 |          10331.5 | 65040384 |  311.446 |              334.263 |              160.172 |            776.067 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.335284579985
    time_step_min: 2778
  date: 2020-10-15_06-13-43
  done: false
  episode_len_mean: 776.081434699714
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.48740743950447
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 185
  episodes_total: 83920
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.645550978809381e-25
        cur_lr: 5.0e-05
        entropy: 0.0726179163902998
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006559905397201267
        total_loss: .inf
        vf_explained_var: 0.9992443919181824
        vf_loss: 0.25800136600931484
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91935483870968
    gpu_util_percent0: 0.3083870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629528272195078
    mean_env_wait_ms: 1.1865391226883906
    mean_inference_ms: 4.293858849682105
    mean_raw_obs_processing_ms: 0.3765599665911985
  time_since_restore: 10357.629730701447
  time_this_iter_s: 26.12155842781067
  time_total_s: 10357.629730701447
  timers:
    learn_throughput: 8673.495
    learn_time_ms: 18653.612
    sample_throughput: 23976.594
    sample_time_ms: 6747.914
    update_time_ms: 30.605
  timestamp: 1602742423
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:13:45,039	WARNING util.py:136 -- The `process_trial` operation took 1.1293776035308838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    403 |          10357.6 | 65202176 |  311.487 |              334.263 |              160.172 |            776.081 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.1411773098553
    time_step_min: 2778
  date: 2020-10-15_06-14-10
  done: false
  episode_len_mean: 776.0867271149984
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.50056135395215
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 84149
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.468326468214072e-25
        cur_lr: 5.0e-05
        entropy: 0.13391422977050146
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.016895428624896642
        total_loss: .inf
        vf_explained_var: 0.9931471347808838
        vf_loss: 2.6035930514335632
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89354838709677
    gpu_util_percent0: 0.38032258064516133
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629456480343983
    mean_env_wait_ms: 1.1864777461404168
    mean_inference_ms: 4.293799305073885
    mean_raw_obs_processing_ms: 0.3765562809122333
  time_since_restore: 10383.290750265121
  time_this_iter_s: 25.661019563674927
  time_total_s: 10383.290750265121
  timers:
    learn_throughput: 8672.9
    learn_time_ms: 18654.89
    sample_throughput: 23950.869
    sample_time_ms: 6755.162
    update_time_ms: 30.858
  timestamp: 1602742450
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:14:12,290	WARNING util.py:136 -- The `process_trial` operation took 1.1067183017730713 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    404 |          10383.3 | 65363968 |  311.501 |              334.263 |              160.172 |            776.087 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.4936612786546
    time_step_min: 2778
  date: 2020-10-15_06-14-38
  done: false
  episode_len_mean: 776.0828424109524
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.4489756212032
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 216
  episodes_total: 84365
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.202489702321109e-25
        cur_lr: 5.0e-05
        entropy: 0.14646070450544357
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014915673799502352
        total_loss: .inf
        vf_explained_var: 0.9883748888969421
        vf_loss: 5.227720697720845
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.519354838709674
    gpu_util_percent0: 0.3183870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629315344971278
    mean_env_wait_ms: 1.1864199562708286
    mean_inference_ms: 4.29373894305373
    mean_raw_obs_processing_ms: 0.3765517319078626
  time_since_restore: 10409.200808525085
  time_this_iter_s: 25.91005825996399
  time_total_s: 10409.200808525085
  timers:
    learn_throughput: 8679.764
    learn_time_ms: 18640.139
    sample_throughput: 23901.405
    sample_time_ms: 6769.142
    update_time_ms: 32.862
  timestamp: 1602742478
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:14:39,769	WARNING util.py:136 -- The `process_trial` operation took 1.0795085430145264 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    405 |          10409.2 | 65525760 |  311.449 |              334.263 |              160.172 |            776.083 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.70895213301
    time_step_min: 2778
  date: 2020-10-15_06-15-05
  done: false
  episode_len_mean: 776.0800856328433
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.4176908116253
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 182
  episodes_total: 84547
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2303734553481665e-24
        cur_lr: 5.0e-05
        entropy: 0.13308406869570413
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012901757872896269
        total_loss: .inf
        vf_explained_var: 0.9897269606590271
        vf_loss: 4.331202944119771
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.567741935483873
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629222661364388
    mean_env_wait_ms: 1.1863714394724187
    mean_inference_ms: 4.2936961809914065
    mean_raw_obs_processing_ms: 0.3765484639046298
  time_since_restore: 10435.165745735168
  time_this_iter_s: 25.964937210083008
  time_total_s: 10435.165745735168
  timers:
    learn_throughput: 8681.58
    learn_time_ms: 18636.238
    sample_throughput: 23870.427
    sample_time_ms: 6777.927
    update_time_ms: 34.826
  timestamp: 1602742505
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:15:07,381	WARNING util.py:136 -- The `process_trial` operation took 1.167222023010254 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    406 |          10435.2 | 65687552 |  311.418 |              334.263 |              160.172 |             776.08 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.7233304218503
    time_step_min: 2778
  date: 2020-10-15_06-15-33
  done: false
  episode_len_mean: 776.0872540229343
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.4154734547588
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 217
  episodes_total: 84764
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.84556018302225e-24
        cur_lr: 5.0e-05
        entropy: 0.09908587920169036
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057444106399392085
        model: {}
        policy_loss: -0.01546236627958327
        total_loss: 2.5864118734995523
        vf_explained_var: 0.9937698841094971
        vf_loss: 2.6019238432248435
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.27096774193548
    gpu_util_percent0: 0.2967741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462914243189046
    mean_env_wait_ms: 1.1863139862374228
    mean_inference_ms: 4.293643221054124
    mean_raw_obs_processing_ms: 0.3765453313762161
  time_since_restore: 10461.139938116074
  time_this_iter_s: 25.97419238090515
  time_total_s: 10461.139938116074
  timers:
    learn_throughput: 8684.586
    learn_time_ms: 18629.788
    sample_throughput: 23863.722
    sample_time_ms: 6779.831
    update_time_ms: 35.173
  timestamp: 1602742533
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: af50e_00000
  
2020-10-15 06:15:34,968	WARNING util.py:136 -- The `process_trial` operation took 1.1292345523834229 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    407 |          10461.1 | 65849344 |  311.415 |              334.263 |              160.172 |            776.087 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.5342444789753
    time_step_min: 2778
  date: 2020-10-15_06-16-00
  done: false
  episode_len_mean: 776.091375455936
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.4500796885192
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 226
  episodes_total: 84990
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.84556018302225e-24
        cur_lr: 5.0e-05
        entropy: 0.07937600960334142
        entropy_coeff: 0.0005000000000000001
        kl: 0.005448285218638678
        model: {}
        policy_loss: -0.01135979825630784
        total_loss: 0.8933796534935633
        vf_explained_var: 0.9974620938301086
        vf_loss: 0.9047791461149851
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.783870967741937
    gpu_util_percent0: 0.3403225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629005210986593
    mean_env_wait_ms: 1.1862543352170871
    mean_inference_ms: 4.2935768006869806
    mean_raw_obs_processing_ms: 0.3765407130799986
  time_since_restore: 10486.961789369583
  time_this_iter_s: 25.82185125350952
  time_total_s: 10486.961789369583
  timers:
    learn_throughput: 8682.298
    learn_time_ms: 18634.698
    sample_throughput: 23835.845
    sample_time_ms: 6787.76
    update_time_ms: 35.194
  timestamp: 1602742560
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: af50e_00000
  
2020-10-15 06:16:02,431	WARNING util.py:136 -- The `process_trial` operation took 1.1522178649902344 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    408 |            10487 | 66011136 |   311.45 |              334.263 |              160.172 |            776.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2928.297588422549
    time_step_min: 2778
  date: 2020-10-15_06-16-28
  done: false
  episode_len_mean: 776.0958637126789
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.487742772375
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 183
  episodes_total: 85173
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.84556018302225e-24
        cur_lr: 5.0e-05
        entropy: 0.07113996396462123
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045846112770959735
        model: {}
        policy_loss: -0.008376381932369744
        total_loss: 0.43720601747433346
        vf_explained_var: 0.9985694289207458
        vf_loss: 0.44561796883742016
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.529032258064518
    gpu_util_percent0: 0.3112903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628923924122947
    mean_env_wait_ms: 1.1862067338623323
    mean_inference_ms: 4.293536466324347
    mean_raw_obs_processing_ms: 0.3765375749123681
  time_since_restore: 10512.707403182983
  time_this_iter_s: 25.74561381340027
  time_total_s: 10512.707403182983
  timers:
    learn_throughput: 8684.516
    learn_time_ms: 18629.938
    sample_throughput: 23815.768
    sample_time_ms: 6793.482
    update_time_ms: 37.051
  timestamp: 1602742588
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: af50e_00000
  
2020-10-15 06:16:29,793	WARNING util.py:136 -- The `process_trial` operation took 1.114691972732544 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    409 |          10512.7 | 66172928 |  311.488 |              334.263 |              160.172 |            776.096 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2927.9962153168353
    time_step_min: 2778
  date: 2020-10-15_06-16-55
  done: false
  episode_len_mean: 776.1002272035229
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.5340244550768
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 213
  episodes_total: 85386
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.22780091511125e-25
        cur_lr: 5.0e-05
        entropy: 0.07244717516005039
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00887951000186149
        total_loss: .inf
        vf_explained_var: 0.9985768795013428
        vf_loss: 0.4948466221491496
    num_steps_sampled: 66334720
    num_steps_trained: 66334720
  iterations_since_restore: 410
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596774193548384
    gpu_util_percent0: 0.38096774193548394
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628848482315857
    mean_env_wait_ms: 1.1861502568142535
    mean_inference_ms: 4.293489920386832
    mean_raw_obs_processing_ms: 0.3765345658335994
  time_since_restore: 10538.485679149628
  time_this_iter_s: 25.778275966644287
  time_total_s: 10538.485679149628
  timers:
    learn_throughput: 8690.327
    learn_time_ms: 18617.482
    sample_throughput: 23812.132
    sample_time_ms: 6794.52
    update_time_ms: 37.316
  timestamp: 1602742615
  timesteps_since_restore: 0
  timesteps_total: 66334720
  training_iteration: 410
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:16:57,212	WARNING util.py:136 -- The `process_trial` operation took 1.149336338043213 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    410 |          10538.5 | 66334720 |  311.534 |              334.263 |              160.172 |              776.1 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2927.665716222217
    time_step_min: 2778
  date: 2020-10-15_06-17-23
  done: false
  episode_len_mean: 776.1053774995328
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.58468586066203
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 230
  episodes_total: 85616
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3841701372666874e-24
        cur_lr: 5.0e-05
        entropy: 0.07009231671690941
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007661203465734919
        total_loss: .inf
        vf_explained_var: 0.9994335174560547
        vf_loss: 0.19616367047031721
    num_steps_sampled: 66496512
    num_steps_trained: 66496512
  iterations_since_restore: 411
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.74516129032258
    gpu_util_percent0: 0.344516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628715683710297
    mean_env_wait_ms: 1.1860896663123246
    mean_inference_ms: 4.293419727866738
    mean_raw_obs_processing_ms: 0.37652992852291256
  time_since_restore: 10564.289430856705
  time_this_iter_s: 25.803751707077026
  time_total_s: 10564.289430856705
  timers:
    learn_throughput: 8695.642
    learn_time_ms: 18606.102
    sample_throughput: 23813.341
    sample_time_ms: 6794.175
    update_time_ms: 29.977
  timestamp: 1602742643
  timesteps_since_restore: 0
  timesteps_total: 66496512
  training_iteration: 411
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:17:24,668	WARNING util.py:136 -- The `process_trial` operation took 1.1532273292541504 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    411 |          10564.3 | 66496512 |  311.585 |              334.263 |              160.172 |            776.105 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2927.4287413419156
    time_step_min: 2778
  date: 2020-10-15_06-17-50
  done: false
  episode_len_mean: 776.1127855477855
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.62320406865837
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 184
  episodes_total: 85800
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.076255205900031e-24
        cur_lr: 5.0e-05
        entropy: 0.07124839909374714
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044236211106181145
        model: {}
        policy_loss: -0.0077506388479378074
        total_loss: 0.491159588098526
        vf_explained_var: 0.9984919428825378
        vf_loss: 0.4989458570877711
    num_steps_sampled: 66658304
    num_steps_trained: 66658304
  iterations_since_restore: 412
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.203225806451616
    gpu_util_percent0: 0.3629032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628629400581344
    mean_env_wait_ms: 1.1860420786274217
    mean_inference_ms: 4.293378278269764
    mean_raw_obs_processing_ms: 0.3765267830861391
  time_since_restore: 10590.15294122696
  time_this_iter_s: 25.863510370254517
  time_total_s: 10590.15294122696
  timers:
    learn_throughput: 8701.687
    learn_time_ms: 18593.177
    sample_throughput: 23784.522
    sample_time_ms: 6802.407
    update_time_ms: 31.082
  timestamp: 1602742670
  timesteps_since_restore: 0
  timesteps_total: 66658304
  training_iteration: 412
  trial_id: af50e_00000
  
2020-10-15 06:17:52,114	WARNING util.py:136 -- The `process_trial` operation took 1.1847515106201172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    412 |          10590.2 | 66658304 |  311.623 |              334.263 |              160.172 |            776.113 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2927.1571239123355
    time_step_min: 2778
  date: 2020-10-15_06-18-18
  done: false
  episode_len_mean: 776.1230379275864
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.66499917670745
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 206
  episodes_total: 86006
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0381276029500155e-24
        cur_lr: 5.0e-05
        entropy: 0.07515060280760129
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010139093200753754
        total_loss: .inf
        vf_explained_var: 0.9986240267753601
        vf_loss: 0.4770349711179733
    num_steps_sampled: 66820096
    num_steps_trained: 66820096
  iterations_since_restore: 413
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.638709677419357
    gpu_util_percent0: 0.3693548387096775
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628558592261684
    mean_env_wait_ms: 1.185988557167308
    mean_inference_ms: 4.29333882002478
    mean_raw_obs_processing_ms: 0.3765241690533539
  time_since_restore: 10616.062361001968
  time_this_iter_s: 25.909419775009155
  time_total_s: 10616.062361001968
  timers:
    learn_throughput: 8706.367
    learn_time_ms: 18583.182
    sample_throughput: 23821.556
    sample_time_ms: 6791.832
    update_time_ms: 29.812
  timestamp: 1602742698
  timesteps_since_restore: 0
  timesteps_total: 66820096
  training_iteration: 413
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:18:19,718	WARNING util.py:136 -- The `process_trial` operation took 1.1692025661468506 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    413 |          10616.1 | 66820096 |  311.665 |              334.263 |              160.172 |            776.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2926.8466928868934
    time_step_min: 2778
  date: 2020-10-15_06-18-45
  done: false
  episode_len_mean: 776.1322664811272
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.70942333405344
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 86235
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5571914044250231e-24
        cur_lr: 5.0e-05
        entropy: 0.07326317516465981
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010612096210631231
        total_loss: .inf
        vf_explained_var: 0.998750627040863
        vf_loss: 0.47708792984485626
    num_steps_sampled: 66981888
    num_steps_trained: 66981888
  iterations_since_restore: 414
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.019354838709685
    gpu_util_percent0: 0.317741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628439142538835
    mean_env_wait_ms: 1.1859283596141026
    mean_inference_ms: 4.293273709758272
    mean_raw_obs_processing_ms: 0.37651991629299164
  time_since_restore: 10641.950907468796
  time_this_iter_s: 25.888546466827393
  time_total_s: 10641.950907468796
  timers:
    learn_throughput: 8704.959
    learn_time_ms: 18586.188
    sample_throughput: 23759.56
    sample_time_ms: 6809.554
    update_time_ms: 30.403
  timestamp: 1602742725
  timesteps_since_restore: 0
  timesteps_total: 66981888
  training_iteration: 414
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:18:47,196	WARNING util.py:136 -- The `process_trial` operation took 1.1907634735107422 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    414 |            10642 | 66981888 |  311.709 |              334.263 |              160.172 |            776.132 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2926.6030954089783
    time_step_min: 2778
  date: 2020-10-15_06-19-13
  done: false
  episode_len_mean: 776.1425926782987
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.74498853018525
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 193
  episodes_total: 86428
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.335787106637535e-24
        cur_lr: 5.0e-05
        entropy: 0.07105517449478309
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01078164552745875
        total_loss: .inf
        vf_explained_var: 0.9987449645996094
        vf_loss: 0.40738962839047116
    num_steps_sampled: 67143680
    num_steps_trained: 67143680
  iterations_since_restore: 415
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683870967741935
    gpu_util_percent0: 0.2899999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628333792175185
    mean_env_wait_ms: 1.1858783051802666
    mean_inference_ms: 4.293226361467403
    mean_raw_obs_processing_ms: 0.37651647456051546
  time_since_restore: 10667.975968122482
  time_this_iter_s: 26.025060653686523
  time_total_s: 10667.975968122482
  timers:
    learn_throughput: 8698.021
    learn_time_ms: 18601.014
    sample_throughput: 23769.645
    sample_time_ms: 6806.664
    update_time_ms: 28.387
  timestamp: 1602742753
  timesteps_since_restore: 0
  timesteps_total: 67143680
  training_iteration: 415
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:19:14,901	WARNING util.py:136 -- The `process_trial` operation took 1.1739604473114014 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    415 |            10668 | 67143680 |  311.745 |              334.263 |              160.172 |            776.143 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2926.3382726600757
    time_step_min: 2778
  date: 2020-10-15_06-19-41
  done: false
  episode_len_mean: 776.1531295454021
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.7903315705013
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 198
  episodes_total: 86626
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.503680659956302e-24
        cur_lr: 5.0e-05
        entropy: 0.06904085353016853
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007482546905521303
        total_loss: .inf
        vf_explained_var: 0.9995027184486389
        vf_loss: 0.15875447044769922
    num_steps_sampled: 67305472
    num_steps_trained: 67305472
  iterations_since_restore: 416
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.137500000000003
    gpu_util_percent0: 0.33062499999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628276963670783
    mean_env_wait_ms: 1.1858267057880578
    mean_inference_ms: 4.293189175071047
    mean_raw_obs_processing_ms: 0.37651395399756404
  time_since_restore: 10694.082925796509
  time_this_iter_s: 26.10695767402649
  time_total_s: 10694.082925796509
  timers:
    learn_throughput: 8688.702
    learn_time_ms: 18620.964
    sample_throughput: 23823.384
    sample_time_ms: 6791.31
    update_time_ms: 26.459
  timestamp: 1602742781
  timesteps_since_restore: 0
  timesteps_total: 67305472
  training_iteration: 416
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:19:42,679	WARNING util.py:136 -- The `process_trial` operation took 1.1879212856292725 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    416 |          10694.1 | 67305472 |   311.79 |              334.263 |              160.172 |            776.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2925.9913031458423
    time_step_min: 2778
  date: 2020-10-15_06-20-08
  done: false
  episode_len_mean: 776.163145472339
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.84059755926637
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 229
  episodes_total: 86855
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.255520989934453e-24
        cur_lr: 5.0e-05
        entropy: 0.0669871202359597
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009142802756590148
        total_loss: .inf
        vf_explained_var: 0.9992097020149231
        vf_loss: 0.3116980915268262
    num_steps_sampled: 67467264
    num_steps_trained: 67467264
  iterations_since_restore: 417
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.638709677419353
    gpu_util_percent0: 0.2948387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628174412243955
    mean_env_wait_ms: 1.1857673836753213
    mean_inference_ms: 4.2931292773257805
    mean_raw_obs_processing_ms: 0.37650995065849435
  time_since_restore: 10720.103790998459
  time_this_iter_s: 26.020865201950073
  time_total_s: 10720.103790998459
  timers:
    learn_throughput: 8687.568
    learn_time_ms: 18623.394
    sample_throughput: 23852.843
    sample_time_ms: 6782.923
    update_time_ms: 34.921
  timestamp: 1602742808
  timesteps_since_restore: 0
  timesteps_total: 67467264
  training_iteration: 417
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:20:10,248	WARNING util.py:136 -- The `process_trial` operation took 1.1427953243255615 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    417 |          10720.1 | 67467264 |  311.841 |              334.263 |              160.172 |            776.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2925.7198381869584
    time_step_min: 2778
  date: 2020-10-15_06-20-36
  done: false
  episode_len_mean: 776.1739684800588
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.88301550702744
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 201
  episodes_total: 87056
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.883281484901678e-24
        cur_lr: 5.0e-05
        entropy: 0.06693484323720138
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010377654820331372
        total_loss: .inf
        vf_explained_var: 0.9993676543235779
        vf_loss: 0.20365390926599503
    num_steps_sampled: 67629056
    num_steps_trained: 67629056
  iterations_since_restore: 418
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.98125
    gpu_util_percent0: 0.28125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462806045177339
    mean_env_wait_ms: 1.1857152958982515
    mean_inference_ms: 4.293079581954447
    mean_raw_obs_processing_ms: 0.3765064890549755
  time_since_restore: 10746.510206222534
  time_this_iter_s: 26.406415224075317
  time_total_s: 10746.510206222534
  timers:
    learn_throughput: 8675.845
    learn_time_ms: 18648.558
    sample_throughput: 23780.803
    sample_time_ms: 6803.471
    update_time_ms: 35.673
  timestamp: 1602742836
  timesteps_since_restore: 0
  timesteps_total: 67629056
  training_iteration: 418
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:20:38,330	WARNING util.py:136 -- The `process_trial` operation took 1.1913728713989258 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    418 |          10746.5 | 67629056 |  311.883 |              334.263 |              160.172 |            776.174 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2925.4433194202365
    time_step_min: 2778
  date: 2020-10-15_06-21-04
  done: false
  episode_len_mean: 776.1823839541547
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.92386269572495
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 194
  episodes_total: 87250
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1824922227352517e-23
        cur_lr: 5.0e-05
        entropy: 0.07039990524450938
        entropy_coeff: 0.0005000000000000001
        kl: 0.004210664890706539
        model: {}
        policy_loss: -0.009358073817566037
        total_loss: 0.2944495951135953
        vf_explained_var: 0.9990795254707336
        vf_loss: 0.30384287734826404
    num_steps_sampled: 67790848
    num_steps_trained: 67790848
  iterations_since_restore: 419
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.919354838709676
    gpu_util_percent0: 0.2874193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627992192219283
    mean_env_wait_ms: 1.1856648216906367
    mean_inference_ms: 4.293040222218534
    mean_raw_obs_processing_ms: 0.3765036998398231
  time_since_restore: 10772.452397108078
  time_this_iter_s: 25.942190885543823
  time_total_s: 10772.452397108078
  timers:
    learn_throughput: 8670.014
    learn_time_ms: 18661.1
    sample_throughput: 23765.922
    sample_time_ms: 6807.731
    update_time_ms: 35.65
  timestamp: 1602742864
  timesteps_since_restore: 0
  timesteps_total: 67790848
  training_iteration: 419
  trial_id: af50e_00000
  
2020-10-15 06:21:05,830	WARNING util.py:136 -- The `process_trial` operation took 1.145672082901001 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    419 |          10772.5 | 67790848 |  311.924 |              334.263 |              160.172 |            776.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2925.1347499885633
    time_step_min: 2778
  date: 2020-10-15_06-21-31
  done: false
  episode_len_mean: 776.1912938110154
  episode_reward_max: 334.26262626262593
  episode_reward_mean: 311.97144932948197
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 228
  episodes_total: 87478
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.912461113676258e-24
        cur_lr: 5.0e-05
        entropy: 0.0705221425741911
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008309526684267135
        total_loss: .inf
        vf_explained_var: 0.9990583062171936
        vf_loss: 0.35238905251026154
    num_steps_sampled: 67952640
    num_steps_trained: 67952640
  iterations_since_restore: 420
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.664516129032258
    gpu_util_percent0: 0.36258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627904377690437
    mean_env_wait_ms: 1.1856057497491619
    mean_inference_ms: 4.292982489853618
    mean_raw_obs_processing_ms: 0.3765000329152722
  time_since_restore: 10798.559077262878
  time_this_iter_s: 26.106680154800415
  time_total_s: 10798.559077262878
  timers:
    learn_throughput: 8657.639
    learn_time_ms: 18687.775
    sample_throughput: 23752.433
    sample_time_ms: 6811.597
    update_time_ms: 35.421
  timestamp: 1602742891
  timesteps_since_restore: 0
  timesteps_total: 67952640
  training_iteration: 420
  trial_id: af50e_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 06:21:33,657	WARNING util.py:136 -- The `process_trial` operation took 1.1944854259490967 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | RUNNING  | 172.17.0.4:20935 |    420 |          10798.6 | 67952640 |  311.971 |              334.263 |              160.172 |            776.191 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_af50e_00000:
  custom_metrics:
    time_step_max: 3927
    time_step_mean: 2924.8430985497657
    time_step_min: 2778
  date: 2020-10-15_06-21-59
  done: true
  episode_len_mean: 776.2003124893081
  episode_reward_max: 334.262626262626
  episode_reward_mean: 312.01601406904575
  episode_reward_min: 160.17171717171726
  episodes_this_iter: 205
  episodes_total: 87683
  experiment_id: 8ef870dc48fb488e883a17980c489677
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.868691670514389e-24
        cur_lr: 5.0e-05
        entropy: 0.06981087910632293
        entropy_coeff: 0.0005000000000000001
        kl: 0.005998127084846298
        model: {}
        policy_loss: -0.008017157689513018
        total_loss: 0.26981477811932564
        vf_explained_var: 0.9991735816001892
        vf_loss: 0.27786683042844135
    num_steps_sampled: 68114432
    num_steps_trained: 68114432
  iterations_since_restore: 421
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.043750000000003
    gpu_util_percent0: 0.304375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 20935
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627783109116724
    mean_env_wait_ms: 1.1855527719823031
    mean_inference_ms: 4.2929325741008295
    mean_raw_obs_processing_ms: 0.376496316366583
  time_since_restore: 10824.671195745468
  time_this_iter_s: 26.11211848258972
  time_total_s: 10824.671195745468
  timers:
    learn_throughput: 8647.519
    learn_time_ms: 18709.644
    sample_throughput: 23763.318
    sample_time_ms: 6808.477
    update_time_ms: 35.681
  timestamp: 1602742919
  timesteps_since_restore: 0
  timesteps_total: 68114432
  training_iteration: 421
  trial_id: af50e_00000
  
2020-10-15 06:22:01,552	WARNING util.py:136 -- The `process_trial` operation took 1.376793384552002 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | TERMINATED |       |    421 |          10824.7 | 68114432 |  312.016 |              334.263 |              160.172 |              776.2 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/556.74 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_af50e_00000 | TERMINATED |       |    421 |          10824.7 | 68114432 |  312.016 |              334.263 |              160.172 |              776.2 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


