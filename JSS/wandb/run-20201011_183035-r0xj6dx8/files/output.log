2020-10-11 18:30:39,242	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_dd71d_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=46598)[0m 2020-10-11 18:30:41,917	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=46577)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46577)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46603)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46603)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46585)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46585)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46544)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46544)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46553)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46553)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46608)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46608)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46580)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46580)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46556)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46556)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46559)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46559)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46574)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46574)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46589)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46589)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46575)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46575)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46571)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46571)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46582)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46582)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46567)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46567)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46560)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46560)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46550)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46550)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46554)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46554)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46552)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46552)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46519)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46519)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46549)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46549)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46478)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46478)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46467)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46467)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46476)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46476)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46477)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46477)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46486)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46486)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46531)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46531)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46461)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46461)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46482)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46482)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46470)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46470)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46496)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46496)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46489)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46489)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46536)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46536)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46539)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46539)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46469)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46469)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46499)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46499)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46481)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46481)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46480)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46480)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46503)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46503)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46492)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46492)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46541)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46541)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46464)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46464)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46538)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46538)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46463)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46463)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46502)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46502)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46465)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46465)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46546)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46546)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46475)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46475)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46595)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46595)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46540)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46540)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46468)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46468)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46521)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46521)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46472)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46472)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46537)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46537)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46490)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46490)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46462)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46462)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46524)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46524)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46548)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46548)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46466)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46466)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46570)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46570)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46562)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46562)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46493)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46493)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46586)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46586)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46473)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46473)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46569)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46569)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46551)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46551)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46545)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46545)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46579)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46579)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46592)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46592)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46591)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46591)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46515)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46515)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46474)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46474)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46484)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46484)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46534)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46534)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46533)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46533)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46535)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46535)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46568)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46568)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46542)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46542)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=46547)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=46547)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_18-31-19
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.000000000000001e-05
        entropy: 1.1842431681496757
        entropy_coeff: 0.00010000000000000002
        kl: 0.0042609808733686805
        model: {}
        policy_loss: -0.007733698848044567
        total_loss: 500.26104954310824
        vf_explained_var: 0.5808696746826172
        vf_loss: 500.2680402483259
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.06923076923077
    gpu_util_percent0: 0.3138461538461538
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.576923076923076
    vram_util_percent0: 0.08363608076628735
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16985851145335354
    mean_env_wait_ms: 1.1633194107522047
    mean_inference_ms: 5.5326712532667655
    mean_raw_obs_processing_ms: 0.44486212487499005
  time_since_restore: 31.83664107322693
  time_this_iter_s: 31.83664107322693
  time_total_s: 31.83664107322693
  timers:
    learn_throughput: 7071.184
    learn_time_ms: 22880.467
    sample_throughput: 18240.787
    sample_time_ms: 8869.793
    update_time_ms: 49.235
  timestamp: 1602441079
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      1 |          31.8366 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4064
    time_step_mean: 3620.996527777778
    time_step_min: 3298
  date: 2020-10-11_18-31-49
  done: false
  episode_len_mean: 889.7120253164557
  episode_reward_max: 270.2626262626262
  episode_reward_mean: 217.53199718706026
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.151713797024318
        entropy_coeff: 0.00010000000000000002
        kl: 0.00851665813076709
        model: {}
        policy_loss: -0.005689780643608954
        total_loss: 117.15264565604073
        vf_explained_var: 0.8269878029823303
        vf_loss: 117.15759876796177
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.135135135135137
    gpu_util_percent0: 0.37891891891891893
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.75945945945946
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16539375943892085
    mean_env_wait_ms: 1.1610591665845946
    mean_inference_ms: 5.363907847292412
    mean_raw_obs_processing_ms: 0.43555079985769646
  time_since_restore: 62.10499715805054
  time_this_iter_s: 30.26835608482361
  time_total_s: 62.10499715805054
  timers:
    learn_throughput: 7129.193
    learn_time_ms: 22694.292
    sample_throughput: 19551.142
    sample_time_ms: 8275.322
    update_time_ms: 41.872
  timestamp: 1602441109
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      2 |           62.105 | 323584 |  217.532 |              270.263 |              145.717 |            889.712 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3613.2421524663678
    time_step_min: 3298
  date: 2020-10-11_18-32-19
  done: false
  episode_len_mean: 885.2004219409283
  episode_reward_max: 270.2626262626262
  episode_reward_mean: 218.47001662191516
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.1454135264669145
        entropy_coeff: 0.00010000000000000002
        kl: 0.008895854465663433
        model: {}
        policy_loss: -0.008713115804961749
        total_loss: 54.55594744001116
        vf_explained_var: 0.908130943775177
        vf_loss: 54.56388691493443
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06388888888889
    gpu_util_percent0: 0.3644444444444444
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775000000000001
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16243468484036022
    mean_env_wait_ms: 1.161177079572124
    mean_inference_ms: 5.213689704292592
    mean_raw_obs_processing_ms: 0.4278302893824263
  time_since_restore: 92.20360684394836
  time_this_iter_s: 30.098609685897827
  time_total_s: 92.20360684394836
  timers:
    learn_throughput: 7116.215
    learn_time_ms: 22735.683
    sample_throughput: 20436.486
    sample_time_ms: 7916.821
    update_time_ms: 37.694
  timestamp: 1602441139
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      3 |          92.2036 | 485376 |   218.47 |              270.263 |              119.051 |              885.2 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3605.3062913907283
    time_step_min: 3278
  date: 2020-10-11_18-32-49
  done: false
  episode_len_mean: 881.381329113924
  episode_reward_max: 270.2626262626262
  episode_reward_mean: 220.21779503899737
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.1218737874712263
        entropy_coeff: 0.00010000000000000002
        kl: 0.00891786950108196
        model: {}
        policy_loss: -0.014840671381015065
        total_loss: 27.36101872580392
        vf_explained_var: 0.9493417739868164
        vf_loss: 27.375080108642578
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.863888888888887
    gpu_util_percent0: 0.3936111111111111
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775000000000001
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1603676495002737
    mean_env_wait_ms: 1.1621135244205114
    mean_inference_ms: 5.100311055160574
    mean_raw_obs_processing_ms: 0.4217913890552066
  time_since_restore: 121.79301929473877
  time_this_iter_s: 29.589412450790405
  time_total_s: 121.79301929473877
  timers:
    learn_throughput: 7140.874
    learn_time_ms: 22657.172
    sample_throughput: 20994.973
    sample_time_ms: 7706.226
    update_time_ms: 38.938
  timestamp: 1602441169
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      4 |          121.793 | 647168 |  220.218 |              270.263 |              119.051 |            881.381 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3596.8438320209975
    time_step_min: 3278
  date: 2020-10-11_18-33-19
  done: false
  episode_len_mean: 877.9151898734177
  episode_reward_max: 270.2626262626262
  episode_reward_mean: 221.22330903976453
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.09405916929245
        entropy_coeff: 0.00010000000000000002
        kl: 0.00880110835922616
        model: {}
        policy_loss: -0.016141751830998276
        total_loss: 25.345387595040457
        vf_explained_var: 0.9580163359642029
        vf_loss: 25.360758781433105
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.929729729729733
    gpu_util_percent0: 0.3524324324324325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770270270270271
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15886141344937615
    mean_env_wait_ms: 1.1635826829402032
    mean_inference_ms: 5.014398060589454
    mean_raw_obs_processing_ms: 0.4169941519737568
  time_since_restore: 151.75279808044434
  time_this_iter_s: 29.959778785705566
  time_total_s: 151.75279808044434
  timers:
    learn_throughput: 7130.889
    learn_time_ms: 22688.896
    sample_throughput: 21358.26
    sample_time_ms: 7575.149
    update_time_ms: 39.719
  timestamp: 1602441199
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      5 |          151.753 | 808960 |  221.223 |              270.263 |              119.051 |            877.915 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3586.307619943556
    time_step_min: 3233
  date: 2020-10-11_18-33-49
  done: false
  episode_len_mean: 870.3648029330889
  episode_reward_max: 278.8989898989895
  episode_reward_mean: 223.5423529520686
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 301
  episodes_total: 1091
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.0783298781939916
        entropy_coeff: 0.00010000000000000002
        kl: 0.007864019467628427
        model: {}
        policy_loss: -0.009657187133728127
        total_loss: 30.331582614353724
        vf_explained_var: 0.965345561504364
        vf_loss: 30.340561321803502
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.836111111111112
    gpu_util_percent0: 0.39833333333333343
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666675
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1569599475693246
    mean_env_wait_ms: 1.1671028462810127
    mean_inference_ms: 4.902845250015302
    mean_raw_obs_processing_ms: 0.4111343261163129
  time_since_restore: 181.4237608909607
  time_this_iter_s: 29.670962810516357
  time_total_s: 181.4237608909607
  timers:
    learn_throughput: 7142.183
    learn_time_ms: 22653.018
    sample_throughput: 21609.532
    sample_time_ms: 7487.066
    update_time_ms: 49.404
  timestamp: 1602441229
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      6 |          181.424 | 970752 |  223.542 |              278.899 |              119.051 |            870.365 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3575.2257281553398
    time_step_min: 3202
  date: 2020-10-11_18-34-19
  done: false
  episode_len_mean: 866.0403481012659
  episode_reward_max: 280.86868686868644
  episode_reward_mean: 225.19418073136413
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 173
  episodes_total: 1264
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.0696193405560084
        entropy_coeff: 0.00010000000000000002
        kl: 0.008170564492632235
        model: {}
        policy_loss: -0.020031401413559382
        total_loss: 18.162574768066406
        vf_explained_var: 0.9688040018081665
        vf_loss: 18.18189593723842
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.455555555555556
    gpu_util_percent0: 0.3783333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7888888888888896
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15617396406043674
    mean_env_wait_ms: 1.1687931499325936
    mean_inference_ms: 4.856115423220315
    mean_raw_obs_processing_ms: 0.4087556585648302
  time_since_restore: 211.43817353248596
  time_this_iter_s: 30.01441264152527
  time_total_s: 211.43817353248596
  timers:
    learn_throughput: 7131.871
    learn_time_ms: 22685.772
    sample_throughput: 21787.792
    sample_time_ms: 7425.81
    update_time_ms: 45.729
  timestamp: 1602441259
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      7 |          211.438 | 1132544 |  225.194 |              280.869 |              119.051 |             866.04 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3564.942611190818
    time_step_min: 3202
  date: 2020-10-11_18-34-48
  done: false
  episode_len_mean: 862.5963431786216
  episode_reward_max: 280.86868686868644
  episode_reward_mean: 226.83105314750867
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.0511813248906816
        entropy_coeff: 0.00010000000000000002
        kl: 0.007943998057661312
        model: {}
        policy_loss: -0.014018751173612795
        total_loss: 16.737896306174143
        vf_explained_var: 0.9693654179573059
        vf_loss: 16.75122574397496
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.072222222222223
    gpu_util_percent0: 0.36472222222222217
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775000000000001
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1555723358927754
    mean_env_wait_ms: 1.1702147029078287
    mean_inference_ms: 4.820065342502843
    mean_raw_obs_processing_ms: 0.4069376125674209
  time_since_restore: 241.0429220199585
  time_this_iter_s: 29.604748487472534
  time_total_s: 241.0429220199585
  timers:
    learn_throughput: 7139.057
    learn_time_ms: 22662.937
    sample_throughput: 21936.348
    sample_time_ms: 7375.521
    update_time_ms: 43.122
  timestamp: 1602441288
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      8 |          241.043 | 1294336 |  226.831 |              280.869 |              119.051 |            862.596 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3557.6623711340208
    time_step_min: 3202
  date: 2020-10-11_18-35-18
  done: false
  episode_len_mean: 859.3550632911392
  episode_reward_max: 280.86868686868644
  episode_reward_mean: 227.9976665388056
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 1.0239795914718084
        entropy_coeff: 0.00010000000000000002
        kl: 0.007866386024813567
        model: {}
        policy_loss: -0.010834371903911233
        total_loss: 17.428128378731863
        vf_explained_var: 0.968439519405365
        vf_loss: 17.438278334481375
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.244444444444444
    gpu_util_percent0: 0.38611111111111107
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775000000000001
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1550503309437625
    mean_env_wait_ms: 1.171572229432544
    mean_inference_ms: 4.788561083389448
    mean_raw_obs_processing_ms: 0.40526885064636653
  time_since_restore: 270.7625253200531
  time_this_iter_s: 29.719603300094604
  time_total_s: 270.7625253200531
  timers:
    learn_throughput: 7138.101
    learn_time_ms: 22665.972
    sample_throughput: 22080.82
    sample_time_ms: 7327.264
    update_time_ms: 42.61
  timestamp: 1602441318
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |      9 |          270.763 | 1456128 |  227.998 |              280.869 |              119.051 |            859.355 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3546.3253348214284
    time_step_min: 3202
  date: 2020-10-11_18-35-48
  done: false
  episode_len_mean: 854.2010989010989
  episode_reward_max: 280.86868686868644
  episode_reward_mean: 229.62509712509697
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 240
  episodes_total: 1820
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.9795162635190147
        entropy_coeff: 0.00010000000000000002
        kl: 0.007149326082851205
        model: {}
        policy_loss: -0.014177071695615138
        total_loss: 18.199185916355678
        vf_explained_var: 0.9768562316894531
        vf_loss: 18.21274539402553
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92777777777778
    gpu_util_percent0: 0.34777777777777774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666675
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15440129819244133
    mean_env_wait_ms: 1.1738803219078024
    mean_inference_ms: 4.748244863299096
    mean_raw_obs_processing_ms: 0.403161871201213
  time_since_restore: 300.5468728542328
  time_this_iter_s: 29.784347534179688
  time_total_s: 300.5468728542328
  timers:
    learn_throughput: 7136.221
    learn_time_ms: 22671.944
    sample_throughput: 22190.731
    sample_time_ms: 7290.972
    update_time_ms: 42.157
  timestamp: 1602441348
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     10 |          300.547 | 1617920 |  229.625 |              280.869 |              119.051 |            854.201 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3535.551826258638
    time_step_min: 3202
  date: 2020-10-11_18-36-18
  done: false
  episode_len_mean: 849.9970788704966
  episode_reward_max: 280.86868686868644
  episode_reward_mean: 231.09839387054564
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 234
  episodes_total: 2054
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.9773590096405574
        entropy_coeff: 0.00010000000000000002
        kl: 0.0073229073812919
        model: {}
        policy_loss: -0.008997091823922736
        total_loss: 14.330252306801933
        vf_explained_var: 0.9771823883056641
        vf_loss: 14.338614736284528
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65555555555555
    gpu_util_percent0: 0.3499999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7694444444444457
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.153848338476082
    mean_env_wait_ms: 1.17577293051
    mean_inference_ms: 4.715639952934618
    mean_raw_obs_processing_ms: 0.40150029425521305
  time_since_restore: 330.277884721756
  time_this_iter_s: 29.731011867523193
  time_total_s: 330.277884721756
  timers:
    learn_throughput: 7144.027
    learn_time_ms: 22647.172
    sample_throughput: 22774.054
    sample_time_ms: 7104.225
    update_time_ms: 40.925
  timestamp: 1602441378
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     11 |          330.278 | 1779712 |  231.098 |              280.869 |              119.051 |            849.997 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3529.147893772894
    time_step_min: 3202
  date: 2020-10-11_18-36-48
  done: false
  episode_len_mean: 847.5397830018084
  episode_reward_max: 283.2929292929295
  episode_reward_mean: 232.1294408826053
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.9571453673498971
        entropy_coeff: 0.00010000000000000002
        kl: 0.006742163228669337
        model: {}
        policy_loss: -0.015433644925776337
        total_loss: 11.31096989767892
        vf_explained_var: 0.9790560603141785
        vf_loss: 11.325825486864362
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.73333333333333
    gpu_util_percent0: 0.36000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7861111111111114
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15353355215799755
    mean_env_wait_ms: 1.17696948546007
    mean_inference_ms: 4.696550363072858
    mean_raw_obs_processing_ms: 0.4005120761284293
  time_since_restore: 360.0126452445984
  time_this_iter_s: 29.734760522842407
  time_total_s: 360.0126452445984
  timers:
    learn_throughput: 7138.37
    learn_time_ms: 22665.117
    sample_throughput: 23006.059
    sample_time_ms: 7032.582
    update_time_ms: 39.808
  timestamp: 1602441408
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     12 |          360.013 | 1941504 |  232.129 |              283.293 |              119.051 |             847.54 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3522.580879214682
    time_step_min: 3198
  date: 2020-10-11_18-37-17
  done: false
  episode_len_mean: 845.4744833403627
  episode_reward_max: 283.2929292929295
  episode_reward_mean: 233.15246944348576
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 159
  episodes_total: 2371
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.9293903623308454
        entropy_coeff: 0.00010000000000000002
        kl: 0.007966022206736463
        model: {}
        policy_loss: -0.009410955652128905
        total_loss: 11.383836950574603
        vf_explained_var: 0.9782673716545105
        vf_loss: 11.392544269561768
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.141666666666666
    gpu_util_percent0: 0.36694444444444435
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775000000000001
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15324565543516097
    mean_env_wait_ms: 1.1781258528154868
    mean_inference_ms: 4.678972205288673
    mean_raw_obs_processing_ms: 0.399584770286857
  time_since_restore: 389.6255421638489
  time_this_iter_s: 29.61289691925049
  time_total_s: 389.6255421638489
  timers:
    learn_throughput: 7145.961
    learn_time_ms: 22641.042
    sample_throughput: 23086.565
    sample_time_ms: 7008.058
    update_time_ms: 39.463
  timestamp: 1602441437
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     13 |          389.626 | 2103296 |  233.152 |              283.293 |              119.051 |            845.474 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3513.388013830196
    time_step_min: 3198
  date: 2020-10-11_18-37-47
  done: false
  episode_len_mean: 842.4215127328013
  episode_reward_max: 283.2929292929295
  episode_reward_mean: 234.591605910876
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 260
  episodes_total: 2631
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.8930229927812304
        entropy_coeff: 0.00010000000000000002
        kl: 0.00682640092314354
        model: {}
        policy_loss: -0.012827662052586675
        total_loss: 15.9546080998012
        vf_explained_var: 0.97901451587677
        vf_loss: 15.966842651367188
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.350000000000005
    gpu_util_percent0: 0.3844444444444445
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666675
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15283053401350982
    mean_env_wait_ms: 1.1799559453737085
    mean_inference_ms: 4.653426514432764
    mean_raw_obs_processing_ms: 0.39827334711581985
  time_since_restore: 419.3126015663147
  time_this_iter_s: 29.68705940246582
  time_total_s: 419.3126015663147
  timers:
    learn_throughput: 7140.389
    learn_time_ms: 22658.71
    sample_throughput: 23107.503
    sample_time_ms: 7001.709
    update_time_ms: 37.367
  timestamp: 1602441467
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     14 |          419.313 | 2265088 |  234.592 |              283.293 |              119.051 |            842.422 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3506.116122159091
    time_step_min: 3150
  date: 2020-10-11_18-38-17
  done: false
  episode_len_mean: 840.3727144866385
  episode_reward_max: 288.7474747474744
  episode_reward_mean: 235.46363423262147
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 213
  episodes_total: 2844
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.8917857834271022
        entropy_coeff: 0.00010000000000000002
        kl: 0.006300544632332665
        model: {}
        policy_loss: -0.013950613348112841
        total_loss: 11.79027795791626
        vf_explained_var: 0.9809303283691406
        vf_loss: 11.803687981196813
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.755555555555556
    gpu_util_percent0: 0.3825
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769444444444445
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1525283140673585
    mean_env_wait_ms: 1.1812484029594683
    mean_inference_ms: 4.635153447586527
    mean_raw_obs_processing_ms: 0.39731362715280755
  time_since_restore: 449.2859194278717
  time_this_iter_s: 29.973317861557007
  time_total_s: 449.2859194278717
  timers:
    learn_throughput: 7141.34
    learn_time_ms: 22655.692
    sample_throughput: 23089.227
    sample_time_ms: 7007.251
    update_time_ms: 35.541
  timestamp: 1602441497
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     15 |          449.286 | 2426880 |  235.464 |              288.747 |              119.051 |            840.373 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3502.22999327505
    time_step_min: 3150
  date: 2020-10-11_18-38-47
  done: false
  episode_len_mean: 839.0116588940706
  episode_reward_max: 288.7474747474744
  episode_reward_mean: 236.00344551443808
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 3002
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.8795053575720105
        entropy_coeff: 0.00010000000000000002
        kl: 0.005646086896636656
        model: {}
        policy_loss: -0.014436870987992734
        total_loss: 11.753721032823835
        vf_explained_var: 0.9787305593490601
        vf_loss: 11.76768125806536
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.669444444444444
    gpu_util_percent0: 0.32305555555555554
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333337
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1523269547853077
    mean_env_wait_ms: 1.1821737702035142
    mean_inference_ms: 4.622786605504147
    mean_raw_obs_processing_ms: 0.39667997333352983
  time_since_restore: 478.9017629623413
  time_this_iter_s: 29.615843534469604
  time_total_s: 478.9017629623413
  timers:
    learn_throughput: 7138.059
    learn_time_ms: 22666.106
    sample_throughput: 23129.838
    sample_time_ms: 6994.947
    update_time_ms: 29.836
  timestamp: 1602441527
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     16 |          478.902 | 2588672 |  236.003 |              288.747 |              119.051 |            839.012 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3497.9253588516744
    time_step_min: 3150
  date: 2020-10-11_18-39-17
  done: false
  episode_len_mean: 837.8662662029718
  episode_reward_max: 288.7474747474744
  episode_reward_mean: 236.7427611556602
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 161
  episodes_total: 3163
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.8609165975025722
        entropy_coeff: 0.00010000000000000002
        kl: 0.006552721406998379
        model: {}
        policy_loss: -0.01209145415175174
        total_loss: 9.55405058179583
        vf_explained_var: 0.9822942018508911
        vf_loss: 9.565572670527867
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.23611111111111
    gpu_util_percent0: 0.355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7777777777777786
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15213893590787836
    mean_env_wait_ms: 1.1830759678675395
    mean_inference_ms: 4.611061693758517
    mean_raw_obs_processing_ms: 0.39606790291589306
  time_since_restore: 508.6449730396271
  time_this_iter_s: 29.743210077285767
  time_total_s: 508.6449730396271
  timers:
    learn_throughput: 7143.067
    learn_time_ms: 22650.215
    sample_throughput: 23177.434
    sample_time_ms: 6980.583
    update_time_ms: 31.837
  timestamp: 1602441557
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     17 |          508.645 | 2750464 |  236.743 |              288.747 |              119.051 |            837.866 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3490.9441030143403
    time_step_min: 3150
  date: 2020-10-11_18-39-46
  done: false
  episode_len_mean: 835.4507982583455
  episode_reward_max: 288.7474747474744
  episode_reward_mean: 237.7662986908269
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 282
  episodes_total: 3445
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.8376523000853402
        entropy_coeff: 0.00010000000000000002
        kl: 0.006769107588167701
        model: {}
        policy_loss: -0.012537456525024027
        total_loss: 12.214472770690918
        vf_explained_var: 0.983778178691864
        vf_loss: 12.226417064666748
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.619444444444447
    gpu_util_percent0: 0.38222222222222224
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666675
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1518333521474748
    mean_env_wait_ms: 1.184563989828704
    mean_inference_ms: 4.592314452218395
    mean_raw_obs_processing_ms: 0.3951049329201395
  time_since_restore: 538.1932604312897
  time_this_iter_s: 29.548287391662598
  time_total_s: 538.1932604312897
  timers:
    learn_throughput: 7141.732
    learn_time_ms: 22654.45
    sample_throughput: 23212.09
    sample_time_ms: 6970.161
    update_time_ms: 31.783
  timestamp: 1602441586
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     18 |          538.193 | 2912256 |  237.766 |              288.747 |              119.051 |            835.451 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3486.4900166389352
    time_step_min: 3150
  date: 2020-10-11_18-40-17
  done: false
  episode_len_mean: 834.044028618602
  episode_reward_max: 288.7474747474744
  episode_reward_mean: 238.55097202070223
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 189
  episodes_total: 3634
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.8302127761500222
        entropy_coeff: 0.00010000000000000002
        kl: 0.0064518076833337545
        model: {}
        policy_loss: -0.014030464292903031
        total_loss: 8.691656589508057
        vf_explained_var: 0.9844393134117126
        vf_loss: 8.70512478692191
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.21666666666667
    gpu_util_percent0: 0.3425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333337
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1516536939825404
    mean_env_wait_ms: 1.1854878988943116
    mean_inference_ms: 4.581188448405747
    mean_raw_obs_processing_ms: 0.3945444092478014
  time_since_restore: 568.1642317771912
  time_this_iter_s: 29.97097134590149
  time_total_s: 568.1642317771912
  timers:
    learn_throughput: 7135.697
    learn_time_ms: 22673.608
    sample_throughput: 23195.24
    sample_time_ms: 6975.224
    update_time_ms: 31.631
  timestamp: 1602441617
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     19 |          568.164 | 3074048 |  238.551 |              288.747 |              119.051 |            834.044 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3482.2085547290117
    time_step_min: 3150
  date: 2020-10-11_18-40-46
  done: false
  episode_len_mean: 832.6194620253165
  episode_reward_max: 288.7474747474744
  episode_reward_mean: 239.15551080424487
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 158
  episodes_total: 3792
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.8268815449305943
        entropy_coeff: 0.00010000000000000002
        kl: 0.0062077513762882775
        model: {}
        policy_loss: -0.004932760394045285
        total_loss: 8.870643070765905
        vf_explained_var: 0.9820408225059509
        vf_loss: 8.875037738255092
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.51111111111111
    gpu_util_percent0: 0.3186111111111111
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.788888888888889
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15151057209724875
    mean_env_wait_ms: 1.1862277957938883
    mean_inference_ms: 4.572392689173073
    mean_raw_obs_processing_ms: 0.39408531766523053
  time_since_restore: 597.6368017196655
  time_this_iter_s: 29.472569942474365
  time_total_s: 597.6368017196655
  timers:
    learn_throughput: 7141.164
    learn_time_ms: 22656.251
    sample_throughput: 23239.314
    sample_time_ms: 6961.995
    update_time_ms: 29.724
  timestamp: 1602441646
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | RUNNING  | 172.17.0.4:46598 |     20 |          597.637 | 3235840 |  239.156 |              288.747 |              119.051 |            832.619 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_dd71d_00000:
  custom_metrics:
    time_step_max: 4270
    time_step_mean: 3477.8800303720577
    time_step_min: 3150
  date: 2020-10-11_18-41-16
  done: true
  episode_len_mean: 830.8569992460417
  episode_reward_max: 288.7474747474744
  episode_reward_mean: 239.85461298077522
  episode_reward_min: 119.05050505050518
  episodes_this_iter: 187
  episodes_total: 3979
  experiment_id: 270358e82b9e4bda8c769319d84383e1
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.000000000000001e-05
        entropy: 0.802539587020874
        entropy_coeff: 0.00010000000000000002
        kl: 0.0070011974312365055
        model: {}
        policy_loss: -0.012771061322252666
        total_loss: 9.86598995753697
        vf_explained_var: 0.9832652807235718
        vf_loss: 9.87814119883946
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.59166666666667
    gpu_util_percent0: 0.37416666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7722222222222235
    vram_util_percent0: 0.09732699245654311
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 46598
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15135397466108594
    mean_env_wait_ms: 1.1871109773413706
    mean_inference_ms: 4.562628958760348
    mean_raw_obs_processing_ms: 0.3935629088721746
  time_since_restore: 627.0245785713196
  time_this_iter_s: 29.387776851654053
  time_total_s: 627.0245785713196
  timers:
    learn_throughput: 7146.755
    learn_time_ms: 22638.526
    sample_throughput: 23304.054
    sample_time_ms: 6942.655
    update_time_ms: 30.0
  timestamp: 1602441676
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: dd71d_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | TERMINATED |       |     21 |          627.025 | 3397632 |  239.855 |              288.747 |              119.051 |            830.857 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_dd71d_00000 | TERMINATED |       |     21 |          627.025 | 3397632 |  239.855 |              288.747 |              119.051 |            830.857 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


