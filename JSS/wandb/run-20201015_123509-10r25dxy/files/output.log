2020-10-15 12:35:13,672	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_e00ea_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=75217)[0m 2020-10-15 12:35:16,411	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=75175)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75175)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75220)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75220)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75214)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75214)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75215)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75215)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75190)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75190)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75198)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75198)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75199)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75199)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75187)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75187)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75211)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75211)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75166)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75166)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75206)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75206)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75216)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75216)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75131)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75131)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75229)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75229)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75205)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75205)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75167)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75167)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75178)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75178)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75098)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75098)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75126)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75126)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75101)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75101)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75164)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75164)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75226)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75226)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75171)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75171)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75176)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75176)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75189)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75189)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75130)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75130)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75219)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75219)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75152)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75152)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75117)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75117)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75222)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75222)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75118)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75118)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75121)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75121)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75102)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75102)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75172)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75172)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75201)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75201)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75128)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75128)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75170)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75170)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75155)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75155)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75115)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75115)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75097)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75097)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75168)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75168)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75196)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75196)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75093)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75093)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75224)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75224)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75096)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75096)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75114)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75114)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75188)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75188)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75160)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75160)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75113)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75113)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75111)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75111)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75156)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75156)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75095)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75095)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75104)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75104)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75116)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75116)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75209)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75209)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75174)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75174)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75169)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75169)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75197)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75197)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75173)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75173)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75180)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75180)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75106)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75106)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75182)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75182)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75103)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75103)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75094)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75094)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75110)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75110)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75100)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75100)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75202)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75202)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75122)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75122)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75207)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75207)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75123)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75123)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75144)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75144)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75108)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75108)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75149)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75149)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75099)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75099)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75185)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75185)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75107)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75107)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75157)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75157)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75150)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75150)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=75191)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=75191)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3924
    time_step_mean: 3604.0258620689656
    time_step_min: 3359
  date: 2020-10-15_12-35-49
  done: false
  episode_len_mean: 901.8924050632911
  episode_reward_max: 263.14141414141426
  episode_reward_mean: 220.7612837233088
  episode_reward_min: 158.89898989898953
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1608085731665294
        entropy_coeff: 0.0005000000000000001
        kl: 0.004805326461791992
        model: {}
        policy_loss: -0.009253671441304808
        total_loss: 406.1570561726888
        vf_explained_var: 0.5482549071311951
        vf_loss: 406.1659342447917
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.856250000000003
    gpu_util_percent0: 0.299375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5625
    vram_util_percent0: 0.08698036241390619
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16412172320449236
    mean_env_wait_ms: 1.1492228412304297
    mean_inference_ms: 5.34653900360833
    mean_raw_obs_processing_ms: 0.43106299044357926
  time_since_restore: 27.266273736953735
  time_this_iter_s: 27.266273736953735
  time_total_s: 27.266273736953735
  timers:
    learn_throughput: 8658.566
    learn_time_ms: 18685.773
    sample_throughput: 19020.41
    sample_time_ms: 8506.231
    update_time_ms: 44.531
  timestamp: 1602765349
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      1 |          27.2663 | 161792 |  220.761 |              263.141 |              158.899 |            901.892 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3600.970802919708
    time_step_min: 3333
  date: 2020-10-15_12-36-15
  done: false
  episode_len_mean: 899.1962025316456
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 221.7322273366575
  episode_reward_min: 149.65656565656565
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1335585514704387
        entropy_coeff: 0.0005000000000000001
        kl: 0.008734231581911445
        model: {}
        policy_loss: -0.010306129270854095
        total_loss: 93.4173018137614
        vf_explained_var: 0.8161735534667969
        vf_loss: 93.42730140686035
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.12666666666667
    gpu_util_percent0: 0.32933333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7466666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16103066223969056
    mean_env_wait_ms: 1.1505032885323394
    mean_inference_ms: 5.215641963707779
    mean_raw_obs_processing_ms: 0.4242766587493151
  time_since_restore: 53.376423597335815
  time_this_iter_s: 26.11014986038208
  time_total_s: 53.376423597335815
  timers:
    learn_throughput: 8732.653
    learn_time_ms: 18527.245
    sample_throughput: 20028.19
    sample_time_ms: 8078.214
    update_time_ms: 43.285
  timestamp: 1602765375
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      2 |          53.3764 | 323584 |  221.732 |              273.293 |              149.657 |            899.196 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3595.8611111111113
    time_step_min: 3295
  date: 2020-10-15_12-36-41
  done: false
  episode_len_mean: 892.9345991561181
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 222.41484464902166
  episode_reward_min: 149.65656565656565
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1224475900332134
        entropy_coeff: 0.0005000000000000001
        kl: 0.009928210948904356
        model: {}
        policy_loss: -0.013527336258751651
        total_loss: 46.36474482218424
        vf_explained_var: 0.8946500420570374
        vf_loss: 46.37784067789713
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.196666666666662
    gpu_util_percent0: 0.309
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15894169181983941
    mean_env_wait_ms: 1.1534221866353769
    mean_inference_ms: 5.0917944872267356
    mean_raw_obs_processing_ms: 0.41848606648569586
  time_since_restore: 79.12175941467285
  time_this_iter_s: 25.745335817337036
  time_total_s: 79.12175941467285
  timers:
    learn_throughput: 8738.3
    learn_time_ms: 18515.272
    sample_throughput: 20808.529
    sample_time_ms: 7775.273
    update_time_ms: 40.827
  timestamp: 1602765401
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      3 |          79.1218 | 485376 |  222.415 |              273.293 |              149.657 |            892.935 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3588.886440677966
    time_step_min: 3295
  date: 2020-10-15_12-37-06
  done: false
  episode_len_mean: 889.3892405063291
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 223.09178813450944
  episode_reward_min: 149.65656565656565
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1078311800956726
        entropy_coeff: 0.0005000000000000001
        kl: 0.008517272498769065
        model: {}
        policy_loss: -0.011693460956280433
        total_loss: 35.83105627695719
        vf_explained_var: 0.9196924567222595
        vf_loss: 35.84245204925537
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.323333333333334
    gpu_util_percent0: 0.3693333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15736939536086536
    mean_env_wait_ms: 1.1562636942248414
    mean_inference_ms: 4.995749719297608
    mean_raw_obs_processing_ms: 0.41397574683222393
  time_since_restore: 104.76685523986816
  time_this_iter_s: 25.645095825195312
  time_total_s: 104.76685523986816
  timers:
    learn_throughput: 8736.545
    learn_time_ms: 18518.991
    sample_throughput: 21385.934
    sample_time_ms: 7565.346
    update_time_ms: 40.927
  timestamp: 1602765426
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      4 |          104.767 | 647168 |  223.092 |              273.293 |              149.657 |            889.389 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3577.756684491979
    time_step_min: 3290
  date: 2020-10-15_12-37-32
  done: false
  episode_len_mean: 881.9746835443038
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 224.92679964198936
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0736220180988312
        entropy_coeff: 0.0005000000000000001
        kl: 0.007788454958548148
        model: {}
        policy_loss: -0.013847309475143751
        total_loss: 26.951021671295166
        vf_explained_var: 0.9353466629981995
        vf_loss: 26.964626948038738
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.97586206896552
    gpu_util_percent0: 0.39827586206896554
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.768965517241379
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15615200467985954
    mean_env_wait_ms: 1.1594675103644967
    mean_inference_ms: 4.920719863884257
    mean_raw_obs_processing_ms: 0.4102826552477741
  time_since_restore: 130.2019453048706
  time_this_iter_s: 25.43509006500244
  time_total_s: 130.2019453048706
  timers:
    learn_throughput: 8742.843
    learn_time_ms: 18505.651
    sample_throughput: 21768.414
    sample_time_ms: 7432.42
    update_time_ms: 38.87
  timestamp: 1602765452
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      5 |          130.202 | 808960 |  224.927 |              273.293 |              148.141 |            881.975 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3563.447592067989
    time_step_min: 3284
  date: 2020-10-15_12-37-58
  done: false
  episode_len_mean: 868.5930971843778
  episode_reward_max: 273.2929292929292
  episode_reward_mean: 227.7086120056146
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 311
  episodes_total: 1101
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0425472756226857
        entropy_coeff: 0.0005000000000000001
        kl: 0.008018907392397523
        model: {}
        policy_loss: -0.0111910367947227
        total_loss: 29.674411137898762
        vf_explained_var: 0.9557506442070007
        vf_loss: 29.685321807861328
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.993333333333332
    gpu_util_percent0: 0.32166666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15451493926032356
    mean_env_wait_ms: 1.16582688538227
    mean_inference_ms: 4.8187989153356945
    mean_raw_obs_processing_ms: 0.40565298396953003
  time_since_restore: 155.89351439476013
  time_this_iter_s: 25.691569089889526
  time_total_s: 155.89351439476013
  timers:
    learn_throughput: 8748.078
    learn_time_ms: 18494.576
    sample_throughput: 21901.806
    sample_time_ms: 7387.153
    update_time_ms: 38.784
  timestamp: 1602765478
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      6 |          155.894 | 970752 |  227.709 |              273.293 |              148.141 |            868.593 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3552.6792144026185
    time_step_min: 3193
  date: 2020-10-15_12-38-23
  done: false
  episode_len_mean: 863.0371835443038
  episode_reward_max: 282.8383838383837
  episode_reward_mean: 229.1686964582533
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 1264
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.037640631198883
        entropy_coeff: 0.0005000000000000001
        kl: 0.007880023564212024
        model: {}
        policy_loss: -0.013542136293835938
        total_loss: 18.04273223876953
        vf_explained_var: 0.9574272632598877
        vf_loss: 18.056005160013836
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.9551724137931
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275853
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15391138024454012
    mean_env_wait_ms: 1.1684601791846205
    mean_inference_ms: 4.780157435606865
    mean_raw_obs_processing_ms: 0.40390125292989965
  time_since_restore: 181.46268796920776
  time_this_iter_s: 25.569173574447632
  time_total_s: 181.46268796920776
  timers:
    learn_throughput: 8747.068
    learn_time_ms: 18496.712
    sample_throughput: 22073.329
    sample_time_ms: 7329.751
    update_time_ms: 36.786
  timestamp: 1602765503
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      7 |          181.463 | 1132544 |  229.169 |              282.838 |              148.141 |            863.037 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3546.1608695652176
    time_step_min: 3193
  date: 2020-10-15_12-38-49
  done: false
  episode_len_mean: 858.1448663853727
  episode_reward_max: 282.8383838383837
  episode_reward_mean: 230.0799982951881
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0278969705104828
        entropy_coeff: 0.0005000000000000001
        kl: 0.007529285775187115
        model: {}
        policy_loss: -0.011286740329524036
        total_loss: 17.775658925374348
        vf_explained_var: 0.9589188098907471
        vf_loss: 17.78670644760132
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.479310344827585
    gpu_util_percent0: 0.3472413793103449
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1534045284477034
    mean_env_wait_ms: 1.1709579701178312
    mean_inference_ms: 4.747519042386493
    mean_raw_obs_processing_ms: 0.40233302422576983
  time_since_restore: 206.90838980674744
  time_this_iter_s: 25.445701837539673
  time_total_s: 206.90838980674744
  timers:
    learn_throughput: 8748.864
    learn_time_ms: 18492.914
    sample_throughput: 22239.478
    sample_time_ms: 7274.991
    update_time_ms: 37.068
  timestamp: 1602765529
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      8 |          206.908 | 1294336 |   230.08 |              282.838 |              148.141 |            858.145 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3541.243823146944
    time_step_min: 3193
  date: 2020-10-15_12-39-15
  done: false
  episode_len_mean: 853.8582278481013
  episode_reward_max: 282.8383838383837
  episode_reward_mean: 230.84768571793876
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9909720321496328
        entropy_coeff: 0.0005000000000000001
        kl: 0.007027940048525731
        model: {}
        policy_loss: -0.012216423288919032
        total_loss: 17.90696128209432
        vf_explained_var: 0.9601621627807617
        vf_loss: 17.918970108032227
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.00666666666667
    gpu_util_percent0: 0.4076666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15296049323157812
    mean_env_wait_ms: 1.1733270788740608
    mean_inference_ms: 4.718938186034314
    mean_raw_obs_processing_ms: 0.4008800672719854
  time_since_restore: 232.58840584754944
  time_this_iter_s: 25.680016040802002
  time_total_s: 232.58840584754944
  timers:
    learn_throughput: 8746.384
    learn_time_ms: 18498.16
    sample_throughput: 22315.062
    sample_time_ms: 7250.35
    update_time_ms: 37.013
  timestamp: 1602765555
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |      9 |          232.588 | 1456128 |  230.848 |              282.838 |              148.141 |            853.858 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3526.0804101457097
    time_step_min: 3193
  date: 2020-10-15_12-39-40
  done: false
  episode_len_mean: 845.9282321899736
  episode_reward_max: 284.2020202020208
  episode_reward_mean: 232.8910743317075
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 315
  episodes_total: 1895
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9543150613705317
        entropy_coeff: 0.0005000000000000001
        kl: 0.006918751479436954
        model: {}
        policy_loss: -0.0114073078148067
        total_loss: 22.750616709391277
        vf_explained_var: 0.9664721488952637
        vf_loss: 22.761809190114338
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.896551724137932
    gpu_util_percent0: 0.30655172413793097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1522491930454205
    mean_env_wait_ms: 1.177795713934783
    mean_inference_ms: 4.672772365749702
    mean_raw_obs_processing_ms: 0.39864230406176
  time_since_restore: 258.21523213386536
  time_this_iter_s: 25.626826286315918
  time_total_s: 258.21523213386536
  timers:
    learn_throughput: 8756.024
    learn_time_ms: 18477.794
    sample_throughput: 22317.51
    sample_time_ms: 7249.554
    update_time_ms: 37.092
  timestamp: 1602765580
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     10 |          258.215 | 1617920 |  232.891 |              284.202 |              148.141 |            845.928 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3519.0531809145127
    time_step_min: 3193
  date: 2020-10-15_12-40-06
  done: false
  episode_len_mean: 842.6231742940604
  episode_reward_max: 284.2020202020208
  episode_reward_mean: 233.80722512368075
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 2054
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9550424267848333
        entropy_coeff: 0.0005000000000000001
        kl: 0.006833299140756329
        model: {}
        policy_loss: -0.010028554146022847
        total_loss: 14.67298936843872
        vf_explained_var: 0.9665574431419373
        vf_loss: 14.682812054951986
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.083333333333332
    gpu_util_percent0: 0.3543333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15195712363195307
    mean_env_wait_ms: 1.179736193151178
    mean_inference_ms: 4.653615128522581
    mean_raw_obs_processing_ms: 0.39771190773981446
  time_since_restore: 283.8142831325531
  time_this_iter_s: 25.599050998687744
  time_total_s: 283.8142831325531
  timers:
    learn_throughput: 8762.944
    learn_time_ms: 18463.201
    sample_throughput: 22799.9
    sample_time_ms: 7096.171
    update_time_ms: 36.265
  timestamp: 1602765606
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     11 |          283.814 | 1779712 |  233.807 |              284.202 |              148.141 |            842.623 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3512.883410138249
    time_step_min: 3193
  date: 2020-10-15_12-40-32
  done: false
  episode_len_mean: 839.2165461121157
  episode_reward_max: 284.2020202020208
  episode_reward_mean: 234.68375892742978
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9333042154709498
        entropy_coeff: 0.0005000000000000001
        kl: 0.006979815157440801
        model: {}
        policy_loss: -0.012605925294337794
        total_loss: 12.56671412785848
        vf_explained_var: 0.9697521328926086
        vf_loss: 12.57908852895101
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.758620689655167
    gpu_util_percent0: 0.4258620689655173
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1516915484977673
    mean_env_wait_ms: 1.1815891413807313
    mean_inference_ms: 4.636212102683476
    mean_raw_obs_processing_ms: 0.39683169897363785
  time_since_restore: 309.3607301712036
  time_this_iter_s: 25.546447038650513
  time_total_s: 309.3607301712036
  timers:
    learn_throughput: 8756.581
    learn_time_ms: 18476.617
    sample_throughput: 23028.003
    sample_time_ms: 7025.881
    update_time_ms: 34.416
  timestamp: 1602765632
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     12 |          309.361 | 1941504 |  234.684 |              284.202 |              148.141 |            839.217 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3505.884792626728
    time_step_min: 3193
  date: 2020-10-15_12-40-57
  done: false
  episode_len_mean: 834.7291066282421
  episode_reward_max: 288.1414141414142
  episode_reward_mean: 235.89819146591478
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 217
  episodes_total: 2429
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8839850127696991
        entropy_coeff: 0.0005000000000000001
        kl: 0.006719793581093351
        model: {}
        policy_loss: -0.011859170898484686
        total_loss: 17.87238534291585
        vf_explained_var: 0.9688119888305664
        vf_loss: 17.884014129638672
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.726666666666674
    gpu_util_percent0: 0.29900000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1513700893078314
    mean_env_wait_ms: 1.1842938790594162
    mean_inference_ms: 4.615063716266721
    mean_raw_obs_processing_ms: 0.39575553531391316
  time_since_restore: 335.089257478714
  time_this_iter_s: 25.728527307510376
  time_total_s: 335.089257478714
  timers:
    learn_throughput: 8748.576
    learn_time_ms: 18493.525
    sample_throughput: 23087.263
    sample_time_ms: 7007.847
    update_time_ms: 32.975
  timestamp: 1602765657
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     13 |          335.089 | 2103296 |  235.898 |              288.141 |              148.141 |            834.729 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3497.321482602118
    time_step_min: 3193
  date: 2020-10-15_12-41-23
  done: false
  episode_len_mean: 830.2896500372301
  episode_reward_max: 288.1414141414142
  episode_reward_mean: 237.30001429033445
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 257
  episodes_total: 2686
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8755106230576833
        entropy_coeff: 0.0005000000000000001
        kl: 0.006404241585793595
        model: {}
        policy_loss: -0.011880003226300081
        total_loss: 13.644367694854736
        vf_explained_var: 0.973446786403656
        vf_loss: 13.656045198440552
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.73793103448276
    gpu_util_percent0: 0.3320689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1510273140587327
    mean_env_wait_ms: 1.1868691889758363
    mean_inference_ms: 4.592735536507646
    mean_raw_obs_processing_ms: 0.3946327979122016
  time_since_restore: 360.64304423332214
  time_this_iter_s: 25.553786754608154
  time_total_s: 360.64304423332214
  timers:
    learn_throughput: 8747.849
    learn_time_ms: 18495.062
    sample_throughput: 23089.331
    sample_time_ms: 7007.219
    update_time_ms: 30.845
  timestamp: 1602765683
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     14 |          360.643 | 2265088 |    237.3 |              288.141 |              148.141 |             830.29 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3490.0749464668093
    time_step_min: 3174
  date: 2020-10-15_12-41-49
  done: false
  episode_len_mean: 827.7123769338959
  episode_reward_max: 288.1414141414142
  episode_reward_mean: 238.29095810424926
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8695435126622518
        entropy_coeff: 0.0005000000000000001
        kl: 0.006383650974991421
        model: {}
        policy_loss: -0.01124639364813144
        total_loss: 11.072142362594604
        vf_explained_var: 0.9715961813926697
        vf_loss: 11.08318535486857
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09666666666667
    gpu_util_percent0: 0.3883333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1508430509169017
    mean_env_wait_ms: 1.1883897594904205
    mean_inference_ms: 4.580687117964246
    mean_raw_obs_processing_ms: 0.3940234344587521
  time_since_restore: 386.2784163951874
  time_this_iter_s: 25.635372161865234
  time_total_s: 386.2784163951874
  timers:
    learn_throughput: 8747.169
    learn_time_ms: 18496.498
    sample_throughput: 23034.058
    sample_time_ms: 7024.034
    update_time_ms: 32.298
  timestamp: 1602765709
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     15 |          386.278 | 2426880 |  238.291 |              288.141 |              148.141 |            827.712 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3484.8203572632287
    time_step_min: 3141
  date: 2020-10-15_12-42-14
  done: false
  episode_len_mean: 825.2964440013293
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 239.07910611599544
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 165
  episodes_total: 3009
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8375518967707952
        entropy_coeff: 0.0005000000000000001
        kl: 0.007002773229032755
        model: {}
        policy_loss: -0.009874908602796495
        total_loss: 12.741058508555094
        vf_explained_var: 0.9726340174674988
        vf_loss: 12.750651995340982
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.727586206896554
    gpu_util_percent0: 0.3779310344827586
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15067033562858875
    mean_env_wait_ms: 1.1899979887799794
    mean_inference_ms: 4.569157562852901
    mean_raw_obs_processing_ms: 0.3934267903615507
  time_since_restore: 411.5833327770233
  time_this_iter_s: 25.304916381835938
  time_total_s: 411.5833327770233
  timers:
    learn_throughput: 8750.201
    learn_time_ms: 18490.09
    sample_throughput: 23140.421
    sample_time_ms: 6991.748
    update_time_ms: 31.939
  timestamp: 1602765734
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     16 |          411.583 | 2588672 |  239.079 |              290.717 |              148.141 |            825.296 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3474.40231990232
    time_step_min: 3141
  date: 2020-10-15_12-42-40
  done: false
  episode_len_mean: 820.6868595539481
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 240.6259795057263
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 309
  episodes_total: 3318
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8005305876334509
        entropy_coeff: 0.0005000000000000001
        kl: 0.005828887418222924
        model: {}
        policy_loss: -0.010037912928964943
        total_loss: 13.088919321695963
        vf_explained_var: 0.9782760739326477
        vf_loss: 13.098775068918863
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.51034482758621
    gpu_util_percent0: 0.36896551724137927
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15037159585049859
    mean_env_wait_ms: 1.1927265106114946
    mean_inference_ms: 4.54946672241447
    mean_raw_obs_processing_ms: 0.39242432321178805
  time_since_restore: 437.18624567985535
  time_this_iter_s: 25.60291290283203
  time_total_s: 437.18624567985535
  timers:
    learn_throughput: 8743.37
    learn_time_ms: 18504.536
    sample_throughput: 23185.722
    sample_time_ms: 6978.088
    update_time_ms: 33.499
  timestamp: 1602765760
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     17 |          437.186 | 2750464 |  240.626 |              290.717 |              148.141 |            820.687 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3469.9321490972625
    time_step_min: 3141
  date: 2020-10-15_12-43-06
  done: false
  episode_len_mean: 818.5382623705409
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 241.24904394927412
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8076162983973821
        entropy_coeff: 0.0005000000000000001
        kl: 0.005866569311668475
        model: {}
        policy_loss: -0.010886732585883388
        total_loss: 10.134718100229898
        vf_explained_var: 0.9754086136817932
        vf_loss: 10.145421981811523
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.163333333333334
    gpu_util_percent0: 0.3033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15023623120172072
    mean_env_wait_ms: 1.1940000237599968
    mean_inference_ms: 4.54054824975666
    mean_raw_obs_processing_ms: 0.3919617373763048
  time_since_restore: 462.8540699481964
  time_this_iter_s: 25.667824268341064
  time_total_s: 462.8540699481964
  timers:
    learn_throughput: 8738.62
    learn_time_ms: 18514.594
    sample_throughput: 23149.082
    sample_time_ms: 6989.132
    update_time_ms: 33.25
  timestamp: 1602765786
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     18 |          462.854 | 2912256 |  241.249 |              290.717 |              148.141 |            818.538 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3465.5599443671767
    time_step_min: 3141
  date: 2020-10-15_12-43-31
  done: false
  episode_len_mean: 816.4492713775089
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 241.9722437462333
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 161
  episodes_total: 3637
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7810584157705307
        entropy_coeff: 0.0005000000000000001
        kl: 0.005782775154026846
        model: {}
        policy_loss: -0.010712247264261046
        total_loss: 10.520259141921997
        vf_explained_var: 0.9756563305854797
        vf_loss: 10.530783812204996
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.77241379310345
    gpu_util_percent0: 0.30896551724137933
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15010750282525157
    mean_env_wait_ms: 1.1953065707443629
    mean_inference_ms: 4.5320251819686135
    mean_raw_obs_processing_ms: 0.39150796570264035
  time_since_restore: 488.0490171909332
  time_this_iter_s: 25.194947242736816
  time_total_s: 488.0490171909332
  timers:
    learn_throughput: 8753.258
    learn_time_ms: 18483.632
    sample_throughput: 23208.294
    sample_time_ms: 6971.301
    update_time_ms: 32.036
  timestamp: 1602765811
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     19 |          488.049 | 3074048 |  241.972 |              290.717 |              148.141 |            816.449 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3457.474923234391
    time_step_min: 3141
  date: 2020-10-15_12-43-57
  done: false
  episode_len_mean: 812.6521518987341
  episode_reward_max: 290.7171717171717
  episode_reward_mean: 243.1544175936581
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 313
  episodes_total: 3950
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7413140634695689
        entropy_coeff: 0.0005000000000000001
        kl: 0.005068241152912378
        model: {}
        policy_loss: -0.011031584620165328
        total_loss: 13.177871386210123
        vf_explained_var: 0.9785845279693604
        vf_loss: 13.188766797383627
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.160000000000007
    gpu_util_percent0: 0.3406666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14987780376757143
    mean_env_wait_ms: 1.1977204173190472
    mean_inference_ms: 4.5169635376514705
    mean_raw_obs_processing_ms: 0.390735710921718
  time_since_restore: 513.730217218399
  time_this_iter_s: 25.68120002746582
  time_total_s: 513.730217218399
  timers:
    learn_throughput: 8749.606
    learn_time_ms: 18491.348
    sample_throughput: 23236.206
    sample_time_ms: 6962.927
    update_time_ms: 36.835
  timestamp: 1602765837
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     20 |           513.73 | 3235840 |  243.154 |              290.717 |              148.141 |            812.652 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3453.5418101328087
    time_step_min: 3099
  date: 2020-10-15_12-44-23
  done: false
  episode_len_mean: 811.0791139240506
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 243.73076677190602
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 4108
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7540231595436732
        entropy_coeff: 0.0005000000000000001
        kl: 0.006251458660699427
        model: {}
        policy_loss: -0.010166963950420419
        total_loss: 9.666405280431112
        vf_explained_var: 0.9757277369499207
        vf_loss: 9.676324129104614
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.69655172413793
    gpu_util_percent0: 0.3793103448275862
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275853
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14977269940244906
    mean_env_wait_ms: 1.1988207312361205
    mean_inference_ms: 4.510068990785691
    mean_raw_obs_processing_ms: 0.3903725568029846
  time_since_restore: 539.2199733257294
  time_this_iter_s: 25.489756107330322
  time_total_s: 539.2199733257294
  timers:
    learn_throughput: 8750.342
    learn_time_ms: 18489.792
    sample_throughput: 23272.506
    sample_time_ms: 6952.066
    update_time_ms: 37.056
  timestamp: 1602765863
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     21 |           539.22 | 3397632 |  243.731 |              297.081 |              148.141 |            811.079 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3449.165326395459
    time_step_min: 3099
  date: 2020-10-15_12-44-48
  done: false
  episode_len_mean: 809.4182669789227
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 244.39348993447354
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 4270
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7344886114199957
        entropy_coeff: 0.0005000000000000001
        kl: 0.00542777714629968
        model: {}
        policy_loss: -0.010750282885661969
        total_loss: 10.668164094289144
        vf_explained_var: 0.9743292331695557
        vf_loss: 10.678739309310913
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.83448275862069
    gpu_util_percent0: 0.3055172413793103
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496707600154484
    mean_env_wait_ms: 1.1999514411431618
    mean_inference_ms: 4.50334993278272
    mean_raw_obs_processing_ms: 0.3900128191888894
  time_since_restore: 564.7299513816833
  time_this_iter_s: 25.50997805595398
  time_total_s: 564.7299513816833
  timers:
    learn_throughput: 8747.134
    learn_time_ms: 18496.573
    sample_throughput: 23310.282
    sample_time_ms: 6940.8
    update_time_ms: 39.018
  timestamp: 1602765888
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     22 |           564.73 | 3559424 |  244.393 |              297.081 |              148.141 |            809.418 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3441.851321585903
    time_step_min: 3099
  date: 2020-10-15_12-45-14
  done: false
  episode_len_mean: 806.577258838935
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 245.50656499521628
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 312
  episodes_total: 4582
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6938638339440028
        entropy_coeff: 0.0005000000000000001
        kl: 0.005188658911113937
        model: {}
        policy_loss: -0.00690779621557643
        total_loss: 12.988922595977783
        vf_explained_var: 0.9786821007728577
        vf_loss: 12.995658238728842
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.893103448275863
    gpu_util_percent0: 0.3120689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7586206896551717
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14948701445573478
    mean_env_wait_ms: 1.2019894890658886
    mean_inference_ms: 4.491367669541816
    mean_raw_obs_processing_ms: 0.38937627377911666
  time_since_restore: 590.0131430625916
  time_this_iter_s: 25.283191680908203
  time_total_s: 590.0131430625916
  timers:
    learn_throughput: 8758.685
    learn_time_ms: 18472.18
    sample_throughput: 23386.229
    sample_time_ms: 6918.26
    update_time_ms: 40.75
  timestamp: 1602765914
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     23 |          590.013 | 3721216 |  245.507 |              297.081 |              148.141 |            806.577 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3438.1868880374627
    time_step_min: 3099
  date: 2020-10-15_12-45-39
  done: false
  episode_len_mean: 805.4289029535865
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 246.11207006776633
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 4740
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7138400276501974
        entropy_coeff: 0.0005000000000000001
        kl: 0.005280776919486622
        model: {}
        policy_loss: -0.011575660440333499
        total_loss: 8.737238804499308
        vf_explained_var: 0.9775153994560242
        vf_loss: 8.748643398284912
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.217241379310348
    gpu_util_percent0: 0.35551724137931034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14940139445456302
    mean_env_wait_ms: 1.2029335842817481
    mean_inference_ms: 4.485774904183164
    mean_raw_obs_processing_ms: 0.38907605759039166
  time_since_restore: 615.4238128662109
  time_this_iter_s: 25.410669803619385
  time_total_s: 615.4238128662109
  timers:
    learn_throughput: 8760.823
    learn_time_ms: 18467.671
    sample_throughput: 23422.838
    sample_time_ms: 6907.446
    update_time_ms: 41.143
  timestamp: 1602765939
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     24 |          615.424 | 3883008 |  246.112 |              297.081 |              148.141 |            805.429 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3434.383333333333
    time_step_min: 3099
  date: 2020-10-15_12-46-05
  done: false
  episode_len_mean: 804.2396980824153
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 246.67522223458576
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 4902
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6957149008909861
        entropy_coeff: 0.0005000000000000001
        kl: 0.005473119051506122
        model: {}
        policy_loss: -0.010031304535611222
        total_loss: 9.043355305989584
        vf_explained_var: 0.9785754680633545
        vf_loss: 9.053186972935995
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333337
    gpu_util_percent0: 0.29566666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1493174061854554
    mean_env_wait_ms: 1.2038968629188995
    mean_inference_ms: 4.4802793203017615
    mean_raw_obs_processing_ms: 0.3887770700958244
  time_since_restore: 640.9884045124054
  time_this_iter_s: 25.564591646194458
  time_total_s: 640.9884045124054
  timers:
    learn_throughput: 8756.68
    learn_time_ms: 18476.408
    sample_throughput: 23478.562
    sample_time_ms: 6891.052
    update_time_ms: 40.692
  timestamp: 1602765965
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     25 |          640.988 | 4044800 |  246.675 |              297.081 |              148.141 |             804.24 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3427.224671307038
    time_step_min: 3099
  date: 2020-10-15_12-46-31
  done: false
  episode_len_mean: 802.1108553893364
  episode_reward_max: 297.08080808080797
  episode_reward_mean: 247.71807836710022
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 312
  episodes_total: 5214
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6606731961170832
        entropy_coeff: 0.0005000000000000001
        kl: 0.005305661625849704
        model: {}
        policy_loss: -0.009964583994587883
        total_loss: 10.540854771931967
        vf_explained_var: 0.9823284149169922
        vf_loss: 10.55061904589335
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.331034482758625
    gpu_util_percent0: 0.4210344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7655172413793094
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14916611846331942
    mean_env_wait_ms: 1.2056261909924098
    mean_inference_ms: 4.470433738391399
    mean_raw_obs_processing_ms: 0.3882432260355998
  time_since_restore: 666.5746607780457
  time_this_iter_s: 25.58625626564026
  time_total_s: 666.5746607780457
  timers:
    learn_throughput: 8741.086
    learn_time_ms: 18509.371
    sample_throughput: 23495.541
    sample_time_ms: 6886.073
    update_time_ms: 39.854
  timestamp: 1602765991
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     26 |          666.575 | 4206592 |  247.718 |              297.081 |              148.141 |            802.111 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3423.242589118199
    time_step_min: 3085
  date: 2020-10-15_12-46-56
  done: false
  episode_len_mean: 801.129746835443
  episode_reward_max: 299.20202020202026
  episode_reward_mean: 248.3269158449725
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 5372
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6744110981623331
        entropy_coeff: 0.0005000000000000001
        kl: 0.005423576609852414
        model: {}
        policy_loss: -0.014040446258150041
        total_loss: 6.8246026039123535
        vf_explained_var: 0.9812327027320862
        vf_loss: 6.838438073794047
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39
    gpu_util_percent0: 0.30166666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14909544481885129
    mean_env_wait_ms: 1.2064329038348323
    mean_inference_ms: 4.465794327890715
    mean_raw_obs_processing_ms: 0.3879922692682553
  time_since_restore: 692.1698696613312
  time_this_iter_s: 25.595208883285522
  time_total_s: 692.1698696613312
  timers:
    learn_throughput: 8738.658
    learn_time_ms: 18514.513
    sample_throughput: 23516.352
    sample_time_ms: 6879.979
    update_time_ms: 38.888
  timestamp: 1602766016
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     27 |           692.17 | 4368384 |  248.327 |              299.202 |              148.141 |             801.13 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3419.3061521659993
    time_step_min: 3085
  date: 2020-10-15_12-47-22
  done: false
  episode_len_mean: 800.1177745664739
  episode_reward_max: 299.20202020202026
  episode_reward_mean: 248.8839916506102
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 5536
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6572760492563248
        entropy_coeff: 0.0005000000000000001
        kl: 0.004983084547954301
        model: {}
        policy_loss: -0.00991726026404649
        total_loss: 8.127788146336874
        vf_explained_var: 0.9796605706214905
        vf_loss: 8.13753585020701
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.882758620689657
    gpu_util_percent0: 0.3589655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14902489837642838
    mean_env_wait_ms: 1.2072554904936523
    mean_inference_ms: 4.461141624254657
    mean_raw_obs_processing_ms: 0.38773470608367616
  time_since_restore: 717.5400624275208
  time_this_iter_s: 25.370192766189575
  time_total_s: 717.5400624275208
  timers:
    learn_throughput: 8750.46
    learn_time_ms: 18489.541
    sample_throughput: 23558.471
    sample_time_ms: 6867.678
    update_time_ms: 45.241
  timestamp: 1602766042
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     28 |           717.54 | 4530176 |  248.884 |              299.202 |              148.141 |            800.118 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3412.38266712612
    time_step_min: 3077
  date: 2020-10-15_12-47-48
  done: false
  episode_len_mean: 798.2724940130004
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 249.90936736506362
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 310
  episodes_total: 5846
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6118175039688746
        entropy_coeff: 0.0005000000000000001
        kl: 0.005293370961832504
        model: {}
        policy_loss: -0.009223727480275556
        total_loss: 11.607327302296957
        vf_explained_var: 0.9801087379455566
        vf_loss: 11.616592248280844
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.734482758620697
    gpu_util_percent0: 0.2996551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488999688041893
    mean_env_wait_ms: 1.2087281908960292
    mean_inference_ms: 4.452952879362087
    mean_raw_obs_processing_ms: 0.3872844447138094
  time_since_restore: 743.2227468490601
  time_this_iter_s: 25.682684421539307
  time_total_s: 743.2227468490601
  timers:
    learn_throughput: 8728.676
    learn_time_ms: 18535.687
    sample_throughput: 23557.796
    sample_time_ms: 6867.875
    update_time_ms: 47.048
  timestamp: 1602766068
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     29 |          743.223 | 4691968 |  249.909 |              300.414 |              148.141 |            798.272 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3408.7141898691716
    time_step_min: 3077
  date: 2020-10-15_12-48-14
  done: false
  episode_len_mean: 797.3502664890074
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 250.42060175371304
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 6004
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6205476025740305
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642907655177017
        model: {}
        policy_loss: -0.010487376295107728
        total_loss: 7.41852347056071
        vf_explained_var: 0.9801657199859619
        vf_loss: 7.429039080937703
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50344827586207
    gpu_util_percent0: 0.3203448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14884005227073777
    mean_env_wait_ms: 1.2094197494520282
    mean_inference_ms: 4.449030330592927
    mean_raw_obs_processing_ms: 0.387065924124788
  time_since_restore: 768.8340706825256
  time_this_iter_s: 25.611323833465576
  time_total_s: 768.8340706825256
  timers:
    learn_throughput: 8714.604
    learn_time_ms: 18565.616
    sample_throughput: 23670.664
    sample_time_ms: 6835.127
    update_time_ms: 42.158
  timestamp: 1602766094
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     30 |          768.834 | 4853760 |  250.421 |              300.414 |              148.141 |             797.35 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3404.9504240052183
    time_step_min: 3077
  date: 2020-10-15_12-48-39
  done: false
  episode_len_mean: 796.4544865565274
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 251.00721991538325
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 6174
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.609478568037351
        entropy_coeff: 0.0005000000000000001
        kl: 0.005292494820120434
        model: {}
        policy_loss: -0.010638982135181626
        total_loss: 7.736140251159668
        vf_explained_var: 0.9807620048522949
        vf_loss: 7.746819416681926
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28666666666667
    gpu_util_percent0: 0.3626666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14877943366605076
    mean_env_wait_ms: 1.2101642731062463
    mean_inference_ms: 4.445018490360872
    mean_raw_obs_processing_ms: 0.3868385562594002
  time_since_restore: 794.5156807899475
  time_this_iter_s: 25.681610107421875
  time_total_s: 794.5156807899475
  timers:
    learn_throughput: 8706.71
    learn_time_ms: 18582.45
    sample_throughput: 23692.448
    sample_time_ms: 6828.843
    update_time_ms: 41.806
  timestamp: 1602766119
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     31 |          794.516 | 5015552 |  251.007 |              300.414 |              148.141 |            796.454 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3398.488657551274
    time_step_min: 3077
  date: 2020-10-15_12-49-05
  done: false
  episode_len_mean: 794.938561284347
  episode_reward_max: 300.41414141414185
  episode_reward_mean: 251.91502552539913
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 304
  episodes_total: 6478
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5693186322848002
        entropy_coeff: 0.0005000000000000001
        kl: 0.005235559811505179
        model: {}
        policy_loss: -0.009933298317870745
        total_loss: 12.006545066833496
        vf_explained_var: 0.9790152907371521
        vf_loss: 12.016501188278198
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.521428571428572
    gpu_util_percent0: 0.3560714285714286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7749999999999995
    vram_util_percent0: 0.1043784847490981
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14867244164840313
    mean_env_wait_ms: 1.2113864732498263
    mean_inference_ms: 4.43809025614415
    mean_raw_obs_processing_ms: 0.3864465327939608
  time_since_restore: 819.7227339744568
  time_this_iter_s: 25.207053184509277
  time_total_s: 819.7227339744568
  timers:
    learn_throughput: 8721.453
    learn_time_ms: 18551.038
    sample_throughput: 23684.943
    sample_time_ms: 6831.006
    update_time_ms: 39.388
  timestamp: 1602766145
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     32 |          819.723 | 5177344 |  251.915 |              300.414 |              148.141 |            794.939 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3395.281922960267
    time_step_min: 3071
  date: 2020-10-15_12-49-30
  done: false
  episode_len_mean: 794.1461723930079
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 252.3927201490493
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 6636
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.582343578338623
        entropy_coeff: 0.0005000000000000001
        kl: 0.005434371608619888
        model: {}
        policy_loss: -0.009906434977892786
        total_loss: 6.680933952331543
        vf_explained_var: 0.9813895225524902
        vf_loss: 6.69085967540741
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.243333333333336
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486208510976241
    mean_env_wait_ms: 1.2119814688329795
    mean_inference_ms: 4.434728677868809
    mean_raw_obs_processing_ms: 0.38625511494908155
  time_since_restore: 845.1591203212738
  time_this_iter_s: 25.436386346817017
  time_total_s: 845.1591203212738
  timers:
    learn_throughput: 8720.7
    learn_time_ms: 18552.639
    sample_throughput: 23641.772
    sample_time_ms: 6843.48
    update_time_ms: 39.461
  timestamp: 1602766170
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     33 |          845.159 | 5339136 |  252.393 |              304.505 |              148.141 |            794.146 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3391.4129889298893
    time_step_min: 3071
  date: 2020-10-15_12-49-56
  done: false
  episode_len_mean: 793.2998386386973
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 252.95733779040222
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 181
  episodes_total: 6817
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5738237996896108
        entropy_coeff: 0.0005000000000000001
        kl: 0.005473659529040257
        model: {}
        policy_loss: -0.009085840865736827
        total_loss: 8.724187850952148
        vf_explained_var: 0.9792914390563965
        vf_loss: 8.73328693707784
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.317241379310342
    gpu_util_percent0: 0.30103448275862066
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14856492912624664
    mean_env_wait_ms: 1.212666986280034
    mean_inference_ms: 4.43104509986011
    mean_raw_obs_processing_ms: 0.38604164467771895
  time_since_restore: 870.713413476944
  time_this_iter_s: 25.554293155670166
  time_total_s: 870.713413476944
  timers:
    learn_throughput: 8711.713
    learn_time_ms: 18571.777
    sample_throughput: 23664.63
    sample_time_ms: 6836.87
    update_time_ms: 39.853
  timestamp: 1602766196
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     34 |          870.713 | 5500928 |  252.957 |              304.505 |              148.141 |              793.3 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3385.9376061120543
    time_step_min: 3071
  date: 2020-10-15_12-50-21
  done: false
  episode_len_mean: 791.9855133614627
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 253.80582193240426
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 293
  episodes_total: 7110
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5296166688203812
        entropy_coeff: 0.0005000000000000001
        kl: 0.005162264181611438
        model: {}
        policy_loss: -0.011187633450996751
        total_loss: 9.327556371688843
        vf_explained_var: 0.9825034737586975
        vf_loss: 9.33875060081482
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.975862068965515
    gpu_util_percent0: 0.3662068965517242
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.76551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14847745790063513
    mean_env_wait_ms: 1.2136824514747016
    mean_inference_ms: 4.4253262851662845
    mean_raw_obs_processing_ms: 0.38571441704644954
  time_since_restore: 895.802018404007
  time_this_iter_s: 25.08860492706299
  time_total_s: 895.802018404007
  timers:
    learn_throughput: 8727.692
    learn_time_ms: 18537.776
    sample_throughput: 23708.434
    sample_time_ms: 6824.238
    update_time_ms: 37.781
  timestamp: 1602766221
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     35 |          895.802 | 5662720 |  253.806 |              304.505 |              148.141 |            791.986 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3383.206338223083
    time_step_min: 3071
  date: 2020-10-15_12-50-47
  done: false
  episode_len_mean: 791.3719042377545
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 254.2109009745224
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 7268
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5531562368075053
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056630732336392
        model: {}
        policy_loss: -0.009172938026798269
        total_loss: 7.078703244527181
        vf_explained_var: 0.9806365370750427
        vf_loss: 7.087869644165039
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.12068965517241
    gpu_util_percent0: 0.3658620689655172
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148432935295442
    mean_env_wait_ms: 1.2141982628761931
    mean_inference_ms: 4.422413701193594
    mean_raw_obs_processing_ms: 0.3855452659116106
  time_since_restore: 921.0868089199066
  time_this_iter_s: 25.284790515899658
  time_total_s: 921.0868089199066
  timers:
    learn_throughput: 8741.601
    learn_time_ms: 18508.28
    sample_throughput: 23716.367
    sample_time_ms: 6821.955
    update_time_ms: 38.529
  timestamp: 1602766247
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     36 |          921.087 | 5824512 |  254.211 |              304.505 |              148.141 |            791.372 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3379.571044133477
    time_step_min: 3071
  date: 2020-10-15_12-51-13
  done: false
  episode_len_mean: 790.5432164838105
  episode_reward_max: 304.50505050505086
  episode_reward_mean: 254.79522817146585
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 206
  episodes_total: 7474
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5319242378075918
        entropy_coeff: 0.0005000000000000001
        kl: 0.005267381357649962
        model: {}
        policy_loss: -0.009007454413222149
        total_loss: 8.483049949010214
        vf_explained_var: 0.981417179107666
        vf_loss: 8.49205986658732
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.633333333333336
    gpu_util_percent0: 0.348
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14837856816158704
    mean_env_wait_ms: 1.2148746898758154
    mean_inference_ms: 4.418806460691645
    mean_raw_obs_processing_ms: 0.3853336249074798
  time_since_restore: 946.8735601902008
  time_this_iter_s: 25.78675127029419
  time_total_s: 946.8735601902008
  timers:
    learn_throughput: 8747.208
    learn_time_ms: 18496.416
    sample_throughput: 23636.04
    sample_time_ms: 6845.14
    update_time_ms: 37.884
  timestamp: 1602766273
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     37 |          946.874 | 5986304 |  254.795 |              304.505 |              148.141 |            790.543 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3374.3127272727274
    time_step_min: 3071
  date: 2020-10-15_12-51-38
  done: false
  episode_len_mean: 789.5503745802118
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 255.5712733117797
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 268
  episodes_total: 7742
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49097402890523273
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049834261105085416
        model: {}
        policy_loss: -0.0085185122055312
        total_loss: 6.79603111743927
        vf_explained_var: 0.986015796661377
        vf_loss: 6.804545998573303
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.620689655172416
    gpu_util_percent0: 0.3510344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7689655172413787
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14830719560258948
    mean_env_wait_ms: 1.2156650053730749
    mean_inference_ms: 4.41424092292184
    mean_raw_obs_processing_ms: 0.38506433905993853
  time_since_restore: 972.2933912277222
  time_this_iter_s: 25.419831037521362
  time_total_s: 972.2933912277222
  timers:
    learn_throughput: 8743.099
    learn_time_ms: 18505.11
    sample_throughput: 23623.66
    sample_time_ms: 6848.727
    update_time_ms: 29.954
  timestamp: 1602766298
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     38 |          972.293 | 6148096 |  255.571 |              306.323 |              148.141 |             789.55 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3371.3611606006616
    time_step_min: 3071
  date: 2020-10-15_12-52-04
  done: false
  episode_len_mean: 789.0391139240506
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 255.9956143715638
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 7900
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5179962168137232
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053879035403952
        model: {}
        policy_loss: -0.012518745080645507
        total_loss: 5.8687297105789185
        vf_explained_var: 0.9830215573310852
        vf_loss: 5.881372769673665
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.610000000000003
    gpu_util_percent0: 0.303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826802136671305
    mean_env_wait_ms: 1.2161099982869188
    mean_inference_ms: 4.411696616117266
    mean_raw_obs_processing_ms: 0.38491214618011554
  time_since_restore: 998.16375041008
  time_this_iter_s: 25.870359182357788
  time_total_s: 998.16375041008
  timers:
    learn_throughput: 8734.709
    learn_time_ms: 18522.884
    sample_throughput: 23618.077
    sample_time_ms: 6850.346
    update_time_ms: 28.504
  timestamp: 1602766324
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     39 |          998.164 | 6309888 |  255.996 |              306.323 |              148.141 |            789.039 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3367.3386099431114
    time_step_min: 3071
  date: 2020-10-15_12-52-30
  done: false
  episode_len_mean: 788.2563976377953
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 256.63211594289356
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 228
  episodes_total: 8128
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.49680029104153317
        entropy_coeff: 0.0005000000000000001
        kl: 0.00554760933543245
        model: {}
        policy_loss: -0.010252086280767495
        total_loss: 7.7728356917699175
        vf_explained_var: 0.9832698702812195
        vf_loss: 7.783197442690532
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.124137931034483
    gpu_util_percent0: 0.3348275862068965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7758620689655173
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14821653080887226
    mean_env_wait_ms: 1.2167560278314733
    mean_inference_ms: 4.40825066790291
    mean_raw_obs_processing_ms: 0.3847086966311891
  time_since_restore: 1023.6573171615601
  time_this_iter_s: 25.493566751480103
  time_total_s: 1023.6573171615601
  timers:
    learn_throughput: 8744.146
    learn_time_ms: 18502.894
    sample_throughput: 23599.331
    sample_time_ms: 6855.787
    update_time_ms: 29.046
  timestamp: 1602766350
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     40 |          1023.66 | 6471680 |  256.632 |              306.323 |              148.141 |            788.256 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3362.974915986558
    time_step_min: 3071
  date: 2020-10-15_12-52-56
  done: false
  episode_len_mean: 787.4352758538333
  episode_reward_max: 306.3232323232325
  episode_reward_mean: 257.2809767124312
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 8374
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4574356699983279
        entropy_coeff: 0.0005000000000000001
        kl: 0.004904644874234994
        model: {}
        policy_loss: -0.008429468813119456
        total_loss: 5.771370013554891
        vf_explained_var: 0.9868906140327454
        vf_loss: 5.779905597368876
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08
    gpu_util_percent0: 0.3556666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481567827316176
    mean_env_wait_ms: 1.2173821274063608
    mean_inference_ms: 4.404527964139654
    mean_raw_obs_processing_ms: 0.3844798648283589
  time_since_restore: 1049.5110170841217
  time_this_iter_s: 25.853699922561646
  time_total_s: 1049.5110170841217
  timers:
    learn_throughput: 8732.113
    learn_time_ms: 18528.391
    sample_throughput: 23603.867
    sample_time_ms: 6854.47
    update_time_ms: 29.193
  timestamp: 1602766376
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     41 |          1049.51 | 6633472 |  257.281 |              306.323 |              148.141 |            787.435 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3360.0226148409893
    time_step_min: 3036
  date: 2020-10-15_12-53-22
  done: false
  episode_len_mean: 786.9049460853258
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 257.7013773458921
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 8532
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.48722415914138156
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051285685040056705
        model: {}
        policy_loss: -0.009884546627290547
        total_loss: 5.357740481694539
        vf_explained_var: 0.9837141036987305
        vf_loss: 5.367804487546285
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.23793103448276
    gpu_util_percent0: 0.33724137931034487
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7862068965517235
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14812184418546373
    mean_env_wait_ms: 1.217770171478199
    mean_inference_ms: 4.4022796036873
    mean_raw_obs_processing_ms: 0.38434251308733247
  time_since_restore: 1074.8351211547852
  time_this_iter_s: 25.324104070663452
  time_total_s: 1074.8351211547852
  timers:
    learn_throughput: 8722.105
    learn_time_ms: 18549.651
    sample_throughput: 23641.092
    sample_time_ms: 6843.677
    update_time_ms: 29.346
  timestamp: 1602766402
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     42 |          1074.84 | 6795264 |  257.701 |              306.626 |              148.141 |            786.905 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3355.575002856164
    time_step_min: 3036
  date: 2020-10-15_12-53-48
  done: false
  episode_len_mean: 786.059806708357
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 258.3516920196853
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 263
  episodes_total: 8795
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.46071107933918637
        entropy_coeff: 0.0005000000000000001
        kl: 0.004932920370871822
        model: {}
        policy_loss: -0.007133805193006992
        total_loss: 9.12200411160787
        vf_explained_var: 0.9816290736198425
        vf_loss: 9.129306475321451
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56
    gpu_util_percent0: 0.4133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806949036468608
    mean_env_wait_ms: 1.2184232371084613
    mean_inference_ms: 4.398719960383519
    mean_raw_obs_processing_ms: 0.3841312669440828
  time_since_restore: 1100.4729871749878
  time_this_iter_s: 25.637866020202637
  time_total_s: 1100.4729871749878
  timers:
    learn_throughput: 8709.434
    learn_time_ms: 18576.637
    sample_throughput: 23664.085
    sample_time_ms: 6837.027
    update_time_ms: 27.816
  timestamp: 1602766428
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     43 |          1100.47 | 6957056 |  258.352 |              306.626 |              148.141 |             786.06 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3352.261490406069
    time_step_min: 3036
  date: 2020-10-15_12-54-13
  done: false
  episode_len_mean: 785.4203864090606
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 258.858865133682
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 211
  episodes_total: 9006
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.43047382185856503
        entropy_coeff: 0.0005000000000000001
        kl: 0.005120153616492947
        model: {}
        policy_loss: -0.01005569346170887
        total_loss: 5.529540300369263
        vf_explained_var: 0.9862681031227112
        vf_loss: 5.539779146512349
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.855172413793102
    gpu_util_percent0: 0.3727586206896552
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480232595745818
    mean_env_wait_ms: 1.21889022297388
    mean_inference_ms: 4.395911418911921
    mean_raw_obs_processing_ms: 0.3839560117505281
  time_since_restore: 1125.993898153305
  time_this_iter_s: 25.52091097831726
  time_total_s: 1125.993898153305
  timers:
    learn_throughput: 8716.231
    learn_time_ms: 18562.152
    sample_throughput: 23633.698
    sample_time_ms: 6845.818
    update_time_ms: 29.418
  timestamp: 1602766453
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     44 |          1125.99 | 7118848 |  258.859 |              306.626 |              148.141 |             785.42 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3349.7587151940365
    time_step_min: 3036
  date: 2020-10-15_12-54-39
  done: false
  episode_len_mean: 784.9489305979921
  episode_reward_max: 306.6262626262632
  episode_reward_mean: 259.2045167960708
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 9164
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.4585626423358917
        entropy_coeff: 0.0005000000000000001
        kl: 0.005443395425875981
        model: {}
        policy_loss: -0.010675911258052414
        total_loss: 5.313385645548503
        vf_explained_var: 0.9842596650123596
        vf_loss: 5.324256658554077
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793333333333333
    gpu_util_percent0: 0.43433333333333324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7799999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479921921674081
    mean_env_wait_ms: 1.219236171520259
    mean_inference_ms: 4.393904836606129
    mean_raw_obs_processing_ms: 0.3838336470903474
  time_since_restore: 1151.5682694911957
  time_this_iter_s: 25.574371337890625
  time_total_s: 1151.5682694911957
  timers:
    learn_throughput: 8700.037
    learn_time_ms: 18596.703
    sample_throughput: 23624.385
    sample_time_ms: 6848.517
    update_time_ms: 38.0
  timestamp: 1602766479
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     45 |          1151.57 | 7280640 |  259.205 |              306.626 |              148.141 |            784.949 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3345.5995323626316
    time_step_min: 3024
  date: 2020-10-15_12-55-05
  done: false
  episode_len_mean: 784.1214686276585
  episode_reward_max: 308.44444444444434
  episode_reward_mean: 259.84969897899754
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 287
  episodes_total: 9451
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.4285256241758664
        entropy_coeff: 0.0005000000000000001
        kl: 0.005139084688077371
        model: {}
        policy_loss: -0.010109816081239842
        total_loss: 8.411476532618204
        vf_explained_var: 0.9839906096458435
        vf_loss: 8.421768506368002
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.820689655172416
    gpu_util_percent0: 0.3110344827586207
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7793103448275867
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479401437257867
    mean_env_wait_ms: 1.2198699982834942
    mean_inference_ms: 4.39044204487913
    mean_raw_obs_processing_ms: 0.38363060159293116
  time_since_restore: 1177.1411015987396
  time_this_iter_s: 25.572832107543945
  time_total_s: 1177.1411015987396
  timers:
    learn_throughput: 8690.557
    learn_time_ms: 18616.989
    sample_throughput: 23595.421
    sample_time_ms: 6856.924
    update_time_ms: 36.532
  timestamp: 1602766505
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     46 |          1177.14 | 7442432 |   259.85 |              308.444 |              148.141 |            784.121 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3343.2197790746145
    time_step_min: 3024
  date: 2020-10-15_12-55-31
  done: false
  episode_len_mean: 783.603755965968
  episode_reward_max: 308.44444444444434
  episode_reward_mean: 260.20273077318114
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 187
  episodes_total: 9638
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.4119056686758995
        entropy_coeff: 0.0005000000000000001
        kl: 0.004877317541589339
        model: {}
        policy_loss: -0.00912873458582908
        total_loss: 5.750106652577718
        vf_explained_var: 0.9852020144462585
        vf_loss: 5.759410897890727
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29310344827586
    gpu_util_percent0: 0.3696551724137932
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14790389409424468
    mean_env_wait_ms: 1.2202390283883766
    mean_inference_ms: 4.388196429156283
    mean_raw_obs_processing_ms: 0.38349099659200503
  time_since_restore: 1202.6844487190247
  time_this_iter_s: 25.543347120285034
  time_total_s: 1202.6844487190247
  timers:
    learn_throughput: 8690.68
    learn_time_ms: 18616.725
    sample_throughput: 23654.68
    sample_time_ms: 6839.746
    update_time_ms: 36.071
  timestamp: 1602766531
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     47 |          1202.68 | 7604224 |  260.203 |              308.444 |              148.141 |            783.604 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3341.1566536805412
    time_step_min: 3024
  date: 2020-10-15_12-55-56
  done: false
  episode_len_mean: 783.1785422621479
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 260.51501746744714
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 158
  episodes_total: 9796
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.43464770168066025
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052989632822573185
        model: {}
        policy_loss: -0.011595062193615982
        total_loss: 6.020051558812459
        vf_explained_var: 0.982515275478363
        vf_loss: 6.03184727827708
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.172413793103452
    gpu_util_percent0: 0.3462068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14787603009394773
    mean_env_wait_ms: 1.2205519236128086
    mean_inference_ms: 4.386390619193796
    mean_raw_obs_processing_ms: 0.3833806112478566
  time_since_restore: 1228.199054479599
  time_this_iter_s: 25.51460576057434
  time_total_s: 1228.199054479599
  timers:
    learn_throughput: 8680.415
    learn_time_ms: 18638.74
    sample_throughput: 23713.113
    sample_time_ms: 6822.891
    update_time_ms: 37.791
  timestamp: 1602766556
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     48 |           1228.2 | 7766016 |  260.515 |              311.778 |              148.141 |            783.179 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3337.640946604355
    time_step_min: 3024
  date: 2020-10-15_12-56-23
  done: false
  episode_len_mean: 782.4769779186058
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 261.04423680312397
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 303
  episodes_total: 10099
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.39842069894075394
        entropy_coeff: 0.0005000000000000001
        kl: 0.004988226729134719
        model: {}
        policy_loss: -0.008534401912280979
        total_loss: 9.333666324615479
        vf_explained_var: 0.983616054058075
        vf_loss: 9.342384417851767
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.08
    gpu_util_percent0: 0.3483333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782457181357508
    mean_env_wait_ms: 1.2211423979943181
    mean_inference_ms: 4.383060566416595
    mean_raw_obs_processing_ms: 0.38318045254333805
  time_since_restore: 1254.076593875885
  time_this_iter_s: 25.87753939628601
  time_total_s: 1254.076593875885
  timers:
    learn_throughput: 8688.48
    learn_time_ms: 18621.44
    sample_throughput: 23658.701
    sample_time_ms: 6838.583
    update_time_ms: 38.634
  timestamp: 1602766583
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     49 |          1254.08 | 7927808 |  261.044 |              311.778 |              148.141 |            782.477 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3335.3886390301136
    time_step_min: 3024
  date: 2020-10-15_12-56-48
  done: false
  episode_len_mean: 782.1044790652386
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 261.363980604487
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 10270
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3967706337571144
        entropy_coeff: 0.0005000000000000001
        kl: 0.005579448809536795
        model: {}
        policy_loss: -0.008805945467126245
        total_loss: 5.692355473836263
        vf_explained_var: 0.984670102596283
        vf_loss: 5.701350967089335
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.66
    gpu_util_percent0: 0.3263333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477967872380537
    mean_env_wait_ms: 1.2214479916403542
    mean_inference_ms: 4.38125751406215
    mean_raw_obs_processing_ms: 0.38306911181063297
  time_since_restore: 1279.4620187282562
  time_this_iter_s: 25.385424852371216
  time_total_s: 1279.4620187282562
  timers:
    learn_throughput: 8687.82
    learn_time_ms: 18622.854
    sample_throughput: 23700.389
    sample_time_ms: 6826.555
    update_time_ms: 38.641
  timestamp: 1602766608
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     50 |          1279.46 | 8089600 |  261.364 |              311.778 |              148.141 |            782.104 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3333.4857060352297
    time_step_min: 3024
  date: 2020-10-15_12-57-14
  done: false
  episode_len_mean: 781.7468123861566
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 261.63259573009367
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 161
  episodes_total: 10431
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.4086659401655197
        entropy_coeff: 0.0005000000000000001
        kl: 0.00538623674462239
        model: {}
        policy_loss: -0.009682931665641567
        total_loss: 5.9207688967386884
        vf_explained_var: 0.9842954277992249
        vf_loss: 5.930647770563762
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.8
    gpu_util_percent0: 0.36827586206896556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.796551724137931
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777086264627398
    mean_env_wait_ms: 1.221733356244061
    mean_inference_ms: 4.37958469953319
    mean_raw_obs_processing_ms: 0.3829669113246883
  time_since_restore: 1305.0540421009064
  time_this_iter_s: 25.592023372650146
  time_total_s: 1305.0540421009064
  timers:
    learn_throughput: 8702.898
    learn_time_ms: 18590.59
    sample_throughput: 23683.679
    sample_time_ms: 6831.371
    update_time_ms: 38.803
  timestamp: 1602766634
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     51 |          1305.05 | 8251392 |  261.633 |              311.778 |              148.141 |            781.747 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3329.986912218379
    time_step_min: 3024
  date: 2020-10-15_12-57-40
  done: false
  episode_len_mean: 781.1074587950461
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 262.160636065469
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 308
  episodes_total: 10739
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.37412210553884506
        entropy_coeff: 0.0005000000000000001
        kl: 0.005711305304430425
        model: {}
        policy_loss: -0.00832333512759457
        total_loss: 7.988898714383443
        vf_explained_var: 0.9857848286628723
        vf_loss: 7.997400124867757
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.024137931034485
    gpu_util_percent0: 0.3603448275862068
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.772413793103448
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14772336679266274
    mean_env_wait_ms: 1.222270474981076
    mean_inference_ms: 4.376538883302908
    mean_raw_obs_processing_ms: 0.3827807255533505
  time_since_restore: 1330.506403207779
  time_this_iter_s: 25.45236110687256
  time_total_s: 1330.506403207779
  timers:
    learn_throughput: 8702.207
    learn_time_ms: 18592.066
    sample_throughput: 23656.866
    sample_time_ms: 6839.114
    update_time_ms: 40.231
  timestamp: 1602766660
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     52 |          1330.51 | 8413184 |  262.161 |              311.778 |              148.141 |            781.107 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3328.094290976059
    time_step_min: 3024
  date: 2020-10-15_12-58-06
  done: false
  episode_len_mean: 780.8067327095946
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 262.4718400293524
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 10902
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.38097796340783435
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049072811380028725
        model: {}
        policy_loss: -0.009647895698435605
        total_loss: 4.56990373134613
        vf_explained_var: 0.9864748120307922
        vf_loss: 4.579734484354655
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.443333333333335
    gpu_util_percent0: 0.3456666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769959045862213
    mean_env_wait_ms: 1.222534844337727
    mean_inference_ms: 4.374988629816615
    mean_raw_obs_processing_ms: 0.38268644054383844
  time_since_restore: 1356.179714679718
  time_this_iter_s: 25.673311471939087
  time_total_s: 1356.179714679718
  timers:
    learn_throughput: 8703.061
    learn_time_ms: 18590.24
    sample_throughput: 23648.201
    sample_time_ms: 6841.62
    update_time_ms: 42.203
  timestamp: 1602766686
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     53 |          1356.18 | 8574976 |  262.472 |              311.778 |              148.141 |            780.807 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3326.0176854707056
    time_step_min: 3024
  date: 2020-10-15_12-58-31
  done: false
  episode_len_mean: 780.5256595590893
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 262.76309170490595
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 166
  episodes_total: 11068
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.38549116005500156
        entropy_coeff: 0.0005000000000000001
        kl: 0.005399355975290139
        model: {}
        policy_loss: -0.00958065803812739
        total_loss: 6.490892767906189
        vf_explained_var: 0.9827330708503723
        vf_loss: 6.500661730766296
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.32758620689655
    gpu_util_percent0: 0.3337931034482758
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476754614344038
    mean_env_wait_ms: 1.2227997698914324
    mean_inference_ms: 4.373429918834136
    mean_raw_obs_processing_ms: 0.38259147699219753
  time_since_restore: 1381.4384219646454
  time_this_iter_s: 25.258707284927368
  time_total_s: 1381.4384219646454
  timers:
    learn_throughput: 8711.456
    learn_time_ms: 18572.326
    sample_throughput: 23672.457
    sample_time_ms: 6834.609
    update_time_ms: 40.222
  timestamp: 1602766711
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     54 |          1381.44 | 8736768 |  262.763 |              311.778 |              148.141 |            780.526 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3322.6696373422747
    time_step_min: 3024
  date: 2020-10-15_12-58-57
  done: false
  episode_len_mean: 780.0428131868132
  episode_reward_max: 311.7777777777778
  episode_reward_mean: 263.27888999889007
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 307
  episodes_total: 11375
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.352639839053154
        entropy_coeff: 0.0005000000000000001
        kl: 0.004577385921341677
        model: {}
        policy_loss: -0.0089119218828273
        total_loss: 7.640864094098409
        vf_explained_var: 0.9857537150382996
        vf_loss: 7.649948636690776
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.436666666666667
    gpu_util_percent0: 0.295
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14763278113434897
    mean_env_wait_ms: 1.2232827849917465
    mean_inference_ms: 4.370674333127737
    mean_raw_obs_processing_ms: 0.3824251557456441
  time_since_restore: 1407.1599278450012
  time_this_iter_s: 25.721505880355835
  time_total_s: 1407.1599278450012
  timers:
    learn_throughput: 8699.409
    learn_time_ms: 18598.044
    sample_throughput: 23681.302
    sample_time_ms: 6832.057
    update_time_ms: 31.931
  timestamp: 1602766737
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     55 |          1407.16 | 8898560 |  263.279 |              311.778 |              148.141 |            780.043 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3320.749216846502
    time_step_min: 3024
  date: 2020-10-15_12-59-23
  done: false
  episode_len_mean: 779.7830761227675
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 263.55831069495025
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 11534
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.36082518845796585
        entropy_coeff: 0.0005000000000000001
        kl: 0.005253409151919186
        model: {}
        policy_loss: -0.007194112098053059
        total_loss: 5.2075207233428955
        vf_explained_var: 0.9848111271858215
        vf_loss: 5.214893341064453
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.537931034482757
    gpu_util_percent0: 0.36482758620689654
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14761129715285912
    mean_env_wait_ms: 1.2235134252542381
    mean_inference_ms: 4.369286000759616
    mean_raw_obs_processing_ms: 0.3823412300045523
  time_since_restore: 1432.4394056797028
  time_this_iter_s: 25.279477834701538
  time_total_s: 1432.4394056797028
  timers:
    learn_throughput: 8709.537
    learn_time_ms: 18576.418
    sample_throughput: 23716.28
    sample_time_ms: 6821.981
    update_time_ms: 33.876
  timestamp: 1602766763
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     56 |          1432.44 | 9060352 |  263.558 |              315.414 |              148.141 |            779.783 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3318.733390484355
    time_step_min: 3024
  date: 2020-10-15_12-59-49
  done: false
  episode_len_mean: 779.5961390620996
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 263.86238916024524
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 11707
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.36391862233479816
        entropy_coeff: 0.0005000000000000001
        kl: 0.005209354994197686
        model: {}
        policy_loss: -0.009775459048493454
        total_loss: 5.793869376182556
        vf_explained_var: 0.9854834079742432
        vf_loss: 5.80382486184438
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.576666666666668
    gpu_util_percent0: 0.3876666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14758870555394546
    mean_env_wait_ms: 1.223761645219537
    mean_inference_ms: 4.367819310906015
    mean_raw_obs_processing_ms: 0.3822508676036684
  time_since_restore: 1458.0712344646454
  time_this_iter_s: 25.631828784942627
  time_total_s: 1458.0712344646454
  timers:
    learn_throughput: 8710.395
    learn_time_ms: 18574.588
    sample_throughput: 23690.837
    sample_time_ms: 6829.307
    update_time_ms: 36.252
  timestamp: 1602766789
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     57 |          1458.07 | 9222144 |  263.862 |              315.414 |              148.141 |            779.596 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3315.3643961554535
    time_step_min: 3024
  date: 2020-10-15_13-00-14
  done: false
  episode_len_mean: 779.2465228616641
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 264.3666085355934
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 300
  episodes_total: 12007
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.33325885236263275
        entropy_coeff: 0.0005000000000000001
        kl: 0.004892570781521499
        model: {}
        policy_loss: -0.009160817047813907
        total_loss: 6.927696585655212
        vf_explained_var: 0.9869993329048157
        vf_loss: 6.9370220104853315
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.165517241379312
    gpu_util_percent0: 0.3737931034482758
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.782758620689655
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475502671838777
    mean_env_wait_ms: 1.224173409809411
    mean_inference_ms: 4.365355272043559
    mean_raw_obs_processing_ms: 0.3821028197966451
  time_since_restore: 1483.50257563591
  time_this_iter_s: 25.43134117126465
  time_total_s: 1483.50257563591
  timers:
    learn_throughput: 8718.155
    learn_time_ms: 18558.055
    sample_throughput: 23658.701
    sample_time_ms: 6838.583
    update_time_ms: 34.969
  timestamp: 1602766814
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     58 |           1483.5 | 9383936 |  264.367 |              315.414 |              148.141 |            779.247 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3313.7609699769055
    time_step_min: 3024
  date: 2020-10-15_13-00-40
  done: false
  episode_len_mean: 779.0914844649022
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 264.61368659469935
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 12166
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.35047547519207
        entropy_coeff: 0.0005000000000000001
        kl: 0.005249437099943559
        model: {}
        policy_loss: -0.010204963085319227
        total_loss: 5.445613503456116
        vf_explained_var: 0.9846398234367371
        vf_loss: 5.455992619196574
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.679310344827588
    gpu_util_percent0: 0.4179310344827586
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753052826527796
    mean_env_wait_ms: 1.224375526224797
    mean_inference_ms: 4.364086897637769
    mean_raw_obs_processing_ms: 0.3820256815677228
  time_since_restore: 1509.147379398346
  time_this_iter_s: 25.644803762435913
  time_total_s: 1509.147379398346
  timers:
    learn_throughput: 8716.792
    learn_time_ms: 18560.957
    sample_throughput: 23751.504
    sample_time_ms: 6811.863
    update_time_ms: 35.312
  timestamp: 1602766840
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     59 |          1509.15 | 9545728 |  264.614 |              315.414 |              148.141 |            779.091 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3312.001382226197
    time_step_min: 3024
  date: 2020-10-15_13-01-06
  done: false
  episode_len_mean: 778.980228506604
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 264.8945872303786
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 12341
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3581472461422284
        entropy_coeff: 0.0005000000000000001
        kl: 0.005312831800741454
        model: {}
        policy_loss: -0.009652167519864937
        total_loss: 5.827342351277669
        vf_explained_var: 0.9856369495391846
        vf_loss: 5.837172667185466
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.94666666666667
    gpu_util_percent0: 0.4250000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750992369946983
    mean_env_wait_ms: 1.2245985157143047
    mean_inference_ms: 4.362744245973674
    mean_raw_obs_processing_ms: 0.38194213444800945
  time_since_restore: 1534.723444700241
  time_this_iter_s: 25.57606530189514
  time_total_s: 1534.723444700241
  timers:
    learn_throughput: 8712.384
    learn_time_ms: 18570.348
    sample_throughput: 23714.912
    sample_time_ms: 6822.374
    update_time_ms: 33.772
  timestamp: 1602766866
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     60 |          1534.72 | 9707520 |  264.895 |              315.414 |              148.141 |             778.98 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3308.945379485551
    time_step_min: 3024
  date: 2020-10-15_13-01-32
  done: false
  episode_len_mean: 778.8282164899509
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 265.3624454706905
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 297
  episodes_total: 12638
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.32700370997190475
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520384400772552
        model: {}
        policy_loss: -0.008895810936034346
        total_loss: 6.448522210121155
        vf_explained_var: 0.9879592061042786
        vf_loss: 6.4575806856155396
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.126666666666665
    gpu_util_percent0: 0.3246666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747482618315894
    mean_env_wait_ms: 1.2249472402531156
    mean_inference_ms: 4.360482939251004
    mean_raw_obs_processing_ms: 0.38180723010278106
  time_since_restore: 1560.2470831871033
  time_this_iter_s: 25.523638486862183
  time_total_s: 1560.2470831871033
  timers:
    learn_throughput: 8712.867
    learn_time_ms: 18569.318
    sample_throughput: 23736.655
    sample_time_ms: 6816.125
    update_time_ms: 33.528
  timestamp: 1602766892
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     61 |          1560.25 | 9869312 |  265.362 |              315.414 |              148.141 |            778.828 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3307.315067419254
    time_step_min: 3024
  date: 2020-10-15_13-01-58
  done: false
  episode_len_mean: 778.7503516174402
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 265.59788224485845
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 160
  episodes_total: 12798
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3362369438012441
        entropy_coeff: 0.0005000000000000001
        kl: 0.00530867965426296
        model: {}
        policy_loss: -0.01086225847636039
        total_loss: 3.9687401254971824
        vf_explained_var: 0.9888959527015686
        vf_loss: 3.9797699650128684
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.513793103448275
    gpu_util_percent0: 0.36620689655172417
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7827586206896546
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474566760473338
    mean_env_wait_ms: 1.2251230170248264
    mean_inference_ms: 4.359319790294328
    mean_raw_obs_processing_ms: 0.38173605658898124
  time_since_restore: 1585.706372976303
  time_this_iter_s: 25.45928978919983
  time_total_s: 1585.706372976303
  timers:
    learn_throughput: 8718.347
    learn_time_ms: 18557.646
    sample_throughput: 23689.029
    sample_time_ms: 6829.828
    update_time_ms: 32.455
  timestamp: 1602766918
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     62 |          1585.71 | 10031104 |  265.598 |              315.414 |              148.141 |             778.75 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3305.4487744529497
    time_step_min: 3024
  date: 2020-10-15_13-02-23
  done: false
  episode_len_mean: 778.6287475915221
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 265.87828574765
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 177
  episodes_total: 12975
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.34269552926222485
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053061171201989055
        model: {}
        policy_loss: -0.010365672062713807
        total_loss: 4.789053201675415
        vf_explained_var: 0.9874215722084045
        vf_loss: 4.79958959420522
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.55517241379311
    gpu_util_percent0: 0.3758620689655172
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.779310344827586
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743722268042134
    mean_env_wait_ms: 1.2253181145026244
    mean_inference_ms: 4.358077861012141
    mean_raw_obs_processing_ms: 0.38165968008810186
  time_since_restore: 1610.9906125068665
  time_this_iter_s: 25.284239530563354
  time_total_s: 1610.9906125068665
  timers:
    learn_throughput: 8732.532
    learn_time_ms: 18527.502
    sample_throughput: 23714.103
    sample_time_ms: 6822.607
    update_time_ms: 29.986
  timestamp: 1602766943
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     63 |          1610.99 | 10192896 |  265.878 |              315.414 |              148.141 |            778.629 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3302.5839256010886
    time_step_min: 3024
  date: 2020-10-15_13-02-49
  done: false
  episode_len_mean: 778.4318661441062
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 266.31922404631183
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 293
  episodes_total: 13268
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3090958495934804
        entropy_coeff: 0.0005000000000000001
        kl: 0.004946761842196186
        model: {}
        policy_loss: -0.009172378049697727
        total_loss: 7.741289019584656
        vf_explained_var: 0.9857645630836487
        vf_loss: 7.750615358352661
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.217241379310348
    gpu_util_percent0: 0.3613793103448276
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775862068965517
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474050332337446
    mean_env_wait_ms: 1.2256159833315248
    mean_inference_ms: 4.356006539652954
    mean_raw_obs_processing_ms: 0.3815341877966914
  time_since_restore: 1636.335007429123
  time_this_iter_s: 25.34439492225647
  time_total_s: 1636.335007429123
  timers:
    learn_throughput: 8733.015
    learn_time_ms: 18526.477
    sample_throughput: 23681.911
    sample_time_ms: 6831.881
    update_time_ms: 29.354
  timestamp: 1602766969
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     64 |          1636.34 | 10354688 |  266.319 |              315.414 |              148.141 |            778.432 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3301.0759635494474
    time_step_min: 3024
  date: 2020-10-15_13-03-15
  done: false
  episode_len_mean: 778.3418466120626
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 266.5508999150102
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 13430
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.31657233089208603
        entropy_coeff: 0.0005000000000000001
        kl: 0.004489539540372789
        model: {}
        policy_loss: -0.009044326672058864
        total_loss: 5.1394115289052325
        vf_explained_var: 0.9857456684112549
        vf_loss: 5.148613969484965
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.876666666666665
    gpu_util_percent0: 0.35566666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7933333333333326
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738813632225226
    mean_env_wait_ms: 1.2257714629961303
    mean_inference_ms: 4.3549371415944815
    mean_raw_obs_processing_ms: 0.3814682554434059
  time_since_restore: 1661.7536072731018
  time_this_iter_s: 25.418599843978882
  time_total_s: 1661.7536072731018
  timers:
    learn_throughput: 8749.476
    learn_time_ms: 18491.622
    sample_throughput: 23671.423
    sample_time_ms: 6834.908
    update_time_ms: 30.244
  timestamp: 1602766995
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     65 |          1661.75 | 10516480 |  266.551 |              315.414 |              148.141 |            778.342 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3299.356784660767
    time_step_min: 2995
  date: 2020-10-15_13-03-40
  done: false
  episode_len_mean: 778.2442287898839
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 266.81794566752666
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 13602
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.32824891805648804
        entropy_coeff: 0.0005000000000000001
        kl: 0.005106824372584621
        model: {}
        policy_loss: -0.011087989172665402
        total_loss: 4.441884398460388
        vf_explained_var: 0.988169252872467
        vf_loss: 4.453136285146077
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.827586206896548
    gpu_util_percent0: 0.323448275862069
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786206896551724
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737016662209135
    mean_env_wait_ms: 1.2259344620628763
    mean_inference_ms: 4.353800587598137
    mean_raw_obs_processing_ms: 0.3813986966493129
  time_since_restore: 1687.2487461566925
  time_this_iter_s: 25.4951388835907
  time_total_s: 1687.2487461566925
  timers:
    learn_throughput: 8750.89
    learn_time_ms: 18488.635
    sample_throughput: 23601.484
    sample_time_ms: 6855.162
    update_time_ms: 29.888
  timestamp: 1602767020
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     66 |          1687.25 | 10678272 |  266.818 |              315.414 |              148.141 |            778.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3296.671333718245
    time_step_min: 2995
  date: 2020-10-15_13-04-06
  done: false
  episode_len_mean: 778.0613037847172
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 267.2281586915348
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 296
  episodes_total: 13898
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.29735277593135834
        entropy_coeff: 0.0005000000000000001
        kl: 0.004554468595112364
        model: {}
        policy_loss: -0.008563800603345348
        total_loss: 6.99308701356252
        vf_explained_var: 0.987199604511261
        vf_loss: 7.001799424489339
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.663333333333334
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734060824413683
    mean_env_wait_ms: 1.2261968742589058
    mean_inference_ms: 4.351927492342077
    mean_raw_obs_processing_ms: 0.3812834732443637
  time_since_restore: 1712.9855782985687
  time_this_iter_s: 25.73683214187622
  time_total_s: 1712.9855782985687
  timers:
    learn_throughput: 8747.368
    learn_time_ms: 18496.078
    sample_throughput: 23616.21
    sample_time_ms: 6850.887
    update_time_ms: 36.041
  timestamp: 1602767046
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     67 |          1712.99 | 10840064 |  267.228 |              315.414 |              148.141 |            778.061 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3295.099928673324
    time_step_min: 2995
  date: 2020-10-15_13-04-32
  done: false
  episode_len_mean: 777.9766747262125
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 267.46460623874935
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 14062
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.2976345916589101
        entropy_coeff: 0.0005000000000000001
        kl: 0.004579706466756761
        model: {}
        policy_loss: -0.011283939481169606
        total_loss: 4.580750783284505
        vf_explained_var: 0.9872176647186279
        vf_loss: 4.592183470726013
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.168965517241382
    gpu_util_percent0: 0.3596551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.793103448275862
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14732429320753776
    mean_env_wait_ms: 1.2263355388079495
    mean_inference_ms: 4.35091736756671
    mean_raw_obs_processing_ms: 0.38122089100868367
  time_since_restore: 1738.6446266174316
  time_this_iter_s: 25.659048318862915
  time_total_s: 1738.6446266174316
  timers:
    learn_throughput: 8737.379
    learn_time_ms: 18517.225
    sample_throughput: 23614.607
    sample_time_ms: 6851.353
    update_time_ms: 37.355
  timestamp: 1602767072
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     68 |          1738.64 | 11001856 |  267.465 |              315.414 |              148.141 |            777.977 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3293.4242125290675
    time_step_min: 2995
  date: 2020-10-15_13-04-58
  done: false
  episode_len_mean: 777.8486615611607
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 267.69856791763635
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 14233
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.31406886130571365
        entropy_coeff: 0.0005000000000000001
        kl: 0.004998420171129207
        model: {}
        policy_loss: -0.008851290365176586
        total_loss: 5.732178886731465
        vf_explained_var: 0.9851000905036926
        vf_loss: 5.741187214851379
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62333333333334
    gpu_util_percent0: 0.317
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473071914641967
    mean_env_wait_ms: 1.2264742267887025
    mean_inference_ms: 4.349878174147558
    mean_raw_obs_processing_ms: 0.3811558552188044
  time_since_restore: 1763.970831155777
  time_this_iter_s: 25.326204538345337
  time_total_s: 1763.970831155777
  timers:
    learn_throughput: 8757.136
    learn_time_ms: 18475.447
    sample_throughput: 23582.927
    sample_time_ms: 6860.556
    update_time_ms: 36.314
  timestamp: 1602767098
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     69 |          1763.97 | 11163648 |  267.699 |              315.414 |              148.141 |            777.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3290.6533655505696
    time_step_min: 2995
  date: 2020-10-15_13-05-24
  done: false
  episode_len_mean: 777.6519584222482
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.11284664640493
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 294
  episodes_total: 14527
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2854648754000664
        entropy_coeff: 0.0005000000000000001
        kl: 0.004360687259274225
        model: {}
        policy_loss: -0.01067612434659774
        total_loss: 6.5105700095494585
        vf_explained_var: 0.9877765774726868
        vf_loss: 6.52138872941335
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.646666666666665
    gpu_util_percent0: 0.367
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.796666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728039661081158
    mean_env_wait_ms: 1.2267026396347918
    mean_inference_ms: 4.348184028503439
    mean_raw_obs_processing_ms: 0.3810504775957221
  time_since_restore: 1789.904976606369
  time_this_iter_s: 25.93414545059204
  time_total_s: 1789.904976606369
  timers:
    learn_throughput: 8752.539
    learn_time_ms: 18485.151
    sample_throughput: 23527.401
    sample_time_ms: 6876.748
    update_time_ms: 36.948
  timestamp: 1602767124
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     70 |           1789.9 | 11325440 |  268.113 |              315.414 |              148.141 |            777.652 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3289.0803985803987
    time_step_min: 2995
  date: 2020-10-15_13-05-50
  done: false
  episode_len_mean: 777.5599564448074
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.33892827829135
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 14694
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.28460531185070675
        entropy_coeff: 0.0005000000000000001
        kl: 0.004675061209127307
        model: {}
        policy_loss: -0.006905456558646013
        total_loss: 4.242927551269531
        vf_explained_var: 0.988189160823822
        vf_loss: 4.249975363413493
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.782758620689652
    gpu_util_percent0: 0.3424137931034482
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472644506006036
    mean_env_wait_ms: 1.2268241529260524
    mean_inference_ms: 4.34723860210583
    mean_raw_obs_processing_ms: 0.38098965371327476
  time_since_restore: 1815.3011572360992
  time_this_iter_s: 25.396180629730225
  time_total_s: 1815.3011572360992
  timers:
    learn_throughput: 8763.976
    learn_time_ms: 18461.027
    sample_throughput: 23495.834
    sample_time_ms: 6885.987
    update_time_ms: 37.96
  timestamp: 1602767150
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     71 |           1815.3 | 11487232 |  268.339 |              315.414 |              148.141 |             777.56 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3287.544258534611
    time_step_min: 2995
  date: 2020-10-15_13-06-16
  done: false
  episode_len_mean: 777.483113562971
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.57426797577506
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 14864
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.30236978580554325
        entropy_coeff: 0.0005000000000000001
        kl: 0.004458291650128861
        model: {}
        policy_loss: -0.008076763845262272
        total_loss: 4.782263120015462
        vf_explained_var: 0.9872530102729797
        vf_loss: 4.790490984916687
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.129999999999995
    gpu_util_percent0: 0.2926666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724911249990194
    mean_env_wait_ms: 1.2269428669511333
    mean_inference_ms: 4.346298821053355
    mean_raw_obs_processing_ms: 0.38092823969067013
  time_since_restore: 1841.051542520523
  time_this_iter_s: 25.750385284423828
  time_total_s: 1841.051542520523
  timers:
    learn_throughput: 8747.146
    learn_time_ms: 18496.547
    sample_throughput: 23523.539
    sample_time_ms: 6877.877
    update_time_ms: 38.442
  timestamp: 1602767176
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     72 |          1841.05 | 11649024 |  268.574 |              315.414 |              148.141 |            777.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3284.9156578686247
    time_step_min: 2990
  date: 2020-10-15_13-06-42
  done: false
  episode_len_mean: 777.3542450029686
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 268.9728933906651
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 295
  episodes_total: 15159
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2736733357111613
        entropy_coeff: 0.0005000000000000001
        kl: 0.004125134243319432
        model: {}
        policy_loss: -0.008963234004719803
        total_loss: 5.995750387509664
        vf_explained_var: 0.9887940883636475
        vf_loss: 6.004850506782532
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15333333333334
    gpu_util_percent0: 0.3283333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722333740647878
    mean_env_wait_ms: 1.2271386436502574
    mean_inference_ms: 4.3447138405575085
    mean_raw_obs_processing_ms: 0.38082833196366067
  time_since_restore: 1866.8030128479004
  time_this_iter_s: 25.75147032737732
  time_total_s: 1866.8030128479004
  timers:
    learn_throughput: 8730.079
    learn_time_ms: 18532.708
    sample_throughput: 23519.312
    sample_time_ms: 6879.113
    update_time_ms: 38.823
  timestamp: 1602767202
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     73 |           1866.8 | 11810816 |  268.973 |              315.414 |              148.141 |            777.354 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3283.4514524993456
    time_step_min: 2990
  date: 2020-10-15_13-07-08
  done: false
  episode_len_mean: 777.288855539606
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 269.20196088511375
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 15326
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2707260126868884
        entropy_coeff: 0.0005000000000000001
        kl: 0.004260215442627668
        model: {}
        policy_loss: -0.009617043571779504
        total_loss: 3.375151733557383
        vf_explained_var: 0.9905970692634583
        vf_loss: 3.3849041064580283
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73448275862069
    gpu_util_percent0: 0.3662068965517241
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720894862506587
    mean_env_wait_ms: 1.2272449489818003
    mean_inference_ms: 4.343852084061988
    mean_raw_obs_processing_ms: 0.38077130748215526
  time_since_restore: 1892.5701401233673
  time_this_iter_s: 25.76712727546692
  time_total_s: 1892.5701401233673
  timers:
    learn_throughput: 8716.233
    learn_time_ms: 18562.147
    sample_throughput: 23490.476
    sample_time_ms: 6887.557
    update_time_ms: 41.845
  timestamp: 1602767228
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     74 |          1892.57 | 11972608 |  269.202 |              315.414 |              148.141 |            777.289 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3281.9438834951457
    time_step_min: 2990
  date: 2020-10-15_13-07-35
  done: false
  episode_len_mean: 777.2535502194681
  episode_reward_max: 315.41414141414157
  episode_reward_mean: 269.4389460053674
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 166
  episodes_total: 15492
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.29079828908046085
        entropy_coeff: 0.0005000000000000001
        kl: 0.004924953216686845
        model: {}
        policy_loss: -0.01033506359817693
        total_loss: 4.022189676761627
        vf_explained_var: 0.988994300365448
        vf_loss: 4.032670080661774
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.613333333333337
    gpu_util_percent0: 0.34166666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719513738927284
    mean_env_wait_ms: 1.2273442740379312
    mean_inference_ms: 4.342994301930349
    mean_raw_obs_processing_ms: 0.3807148513673987
  time_since_restore: 1918.658400297165
  time_this_iter_s: 26.088260173797607
  time_total_s: 1918.658400297165
  timers:
    learn_throughput: 8706.552
    learn_time_ms: 18582.787
    sample_throughput: 23341.895
    sample_time_ms: 6931.399
    update_time_ms: 42.305
  timestamp: 1602767255
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     75 |          1918.66 | 12134400 |  269.439 |              315.414 |              148.141 |            777.254 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3279.5305331384634
    time_step_min: 2976
  date: 2020-10-15_13-08-01
  done: false
  episode_len_mean: 777.2401926611319
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 269.81499448506236
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 287
  episodes_total: 15779
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2678173507253329
        entropy_coeff: 0.0005000000000000001
        kl: 0.004497473322165509
        model: {}
        policy_loss: -0.007366475348438446
        total_loss: 6.172531088193257
        vf_explained_var: 0.9887259006500244
        vf_loss: 6.180031458536784
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.343333333333337
    gpu_util_percent0: 0.39233333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471731705025684
    mean_env_wait_ms: 1.2275135551405474
    mean_inference_ms: 4.341612720102889
    mean_raw_obs_processing_ms: 0.38062570770817084
  time_since_restore: 1944.183937072754
  time_this_iter_s: 25.52553677558899
  time_total_s: 1944.183937072754
  timers:
    learn_throughput: 8704.094
    learn_time_ms: 18588.034
    sample_throughput: 23335.957
    sample_time_ms: 6933.163
    update_time_ms: 41.307
  timestamp: 1602767281
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     76 |          1944.18 | 12296192 |  269.815 |              315.717 |              148.141 |             777.24 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3277.913546117115
    time_step_min: 2976
  date: 2020-10-15_13-08-27
  done: false
  episode_len_mean: 777.2010903622008
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.0572956029781
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 15958
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.25022794554630917
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041656779940240085
        model: {}
        policy_loss: -0.009161299113960316
        total_loss: 3.767794907093048
        vf_explained_var: 0.9899099469184875
        vf_loss: 3.777081330617269
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.276666666666667
    gpu_util_percent0: 0.3120000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14715730818007283
    mean_env_wait_ms: 1.2276000113104677
    mean_inference_ms: 4.340720267775488
    mean_raw_obs_processing_ms: 0.38056452236826865
  time_since_restore: 1970.022028207779
  time_this_iter_s: 25.838091135025024
  time_total_s: 1970.022028207779
  timers:
    learn_throughput: 8700.361
    learn_time_ms: 18596.01
    sample_throughput: 23305.34
    sample_time_ms: 6942.272
    update_time_ms: 34.184
  timestamp: 1602767307
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     77 |          1970.02 | 12457984 |  270.057 |              315.717 |              148.141 |            777.201 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3276.4429104477613
    time_step_min: 2976
  date: 2020-10-15_13-08-53
  done: false
  episode_len_mean: 777.1418558491503
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.2705863999128
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 16122
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.2755880306164424
        entropy_coeff: 0.0005000000000000001
        kl: 0.00465339922811836
        model: {}
        policy_loss: -0.009429567047239592
        total_loss: 4.504302501678467
        vf_explained_var: 0.9873778820037842
        vf_loss: 4.513869802157084
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.231034482758623
    gpu_util_percent0: 0.33206896551724135
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714442850439377
    mean_env_wait_ms: 1.2276822458544852
    mean_inference_ms: 4.3399265267153035
    mean_raw_obs_processing_ms: 0.38051154467331005
  time_since_restore: 1995.576556444168
  time_this_iter_s: 25.55452823638916
  time_total_s: 1995.576556444168
  timers:
    learn_throughput: 8703.065
    learn_time_ms: 18590.233
    sample_throughput: 23322.363
    sample_time_ms: 6937.204
    update_time_ms: 33.689
  timestamp: 1602767333
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     78 |          1995.58 | 12619776 |  270.271 |              315.717 |              148.141 |            777.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3274.2086797066013
    time_step_min: 2976
  date: 2020-10-15_13-09-18
  done: false
  episode_len_mean: 777.0912693573954
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.6076691805263
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 280
  episodes_total: 16402
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.2577415630221367
        entropy_coeff: 0.0005000000000000001
        kl: 0.004504388198256493
        model: {}
        policy_loss: -0.008300847936576853
        total_loss: 6.0371244351069135
        vf_explained_var: 0.9887755513191223
        vf_loss: 6.0455542008082075
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.243333333333332
    gpu_util_percent0: 0.36700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712454933666427
    mean_env_wait_ms: 1.2278220472967154
    mean_inference_ms: 4.338673135329996
    mean_raw_obs_processing_ms: 0.38042962811329334
  time_since_restore: 2021.0493109226227
  time_this_iter_s: 25.47275447845459
  time_total_s: 2021.0493109226227
  timers:
    learn_throughput: 8693.164
    learn_time_ms: 18611.406
    sample_throughput: 23343.465
    sample_time_ms: 6930.933
    update_time_ms: 32.609
  timestamp: 1602767358
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     79 |          2021.05 | 12781568 |  270.608 |              315.717 |              148.141 |            777.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3272.632946579647
    time_step_min: 2976
  date: 2020-10-15_13-09-44
  done: false
  episode_len_mean: 777.0795660036166
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 270.8373152866825
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 16590
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.23359899843732515
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044753586407750845
        model: {}
        policy_loss: -0.008296948430749277
        total_loss: 3.4977787931760154
        vf_explained_var: 0.9911777377128601
        vf_loss: 3.506192664305369
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.440000000000005
    gpu_util_percent0: 0.412
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147109347778341
    mean_env_wait_ms: 1.227898543431269
    mean_inference_ms: 4.337805573628999
    mean_raw_obs_processing_ms: 0.38036871765217223
  time_since_restore: 2046.7483088970184
  time_this_iter_s: 25.698997974395752
  time_total_s: 2046.7483088970184
  timers:
    learn_throughput: 8706.741
    learn_time_ms: 18582.384
    sample_throughput: 23303.853
    sample_time_ms: 6942.715
    update_time_ms: 33.848
  timestamp: 1602767384
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     80 |          2046.75 | 12943360 |  270.837 |              315.717 |              148.141 |             777.08 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3271.2847824786068
    time_step_min: 2976
  date: 2020-10-15_13-10-10
  done: false
  episode_len_mean: 777.0659583358205
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 271.064508874334
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 16753
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.26309418429931003
        entropy_coeff: 0.0005000000000000001
        kl: 0.005505056430896123
        model: {}
        policy_loss: -0.011706334364134818
        total_loss: 3.52339369058609
        vf_explained_var: 0.9898431897163391
        vf_loss: 3.535231610139211
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.85666666666667
    gpu_util_percent0: 0.373
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470972687985088
    mean_env_wait_ms: 1.2279638914721793
    mean_inference_ms: 4.337062953154771
    mean_raw_obs_processing_ms: 0.3803183821922215
  time_since_restore: 2072.338574886322
  time_this_iter_s: 25.59026598930359
  time_total_s: 2072.338574886322
  timers:
    learn_throughput: 8694.808
    learn_time_ms: 18607.887
    sample_throughput: 23325.765
    sample_time_ms: 6936.193
    update_time_ms: 33.374
  timestamp: 1602767410
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     81 |          2072.34 | 13105152 |  271.065 |              315.717 |              148.141 |            777.066 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3269.123225540437
    time_step_min: 2976
  date: 2020-10-15_13-10-37
  done: false
  episode_len_mean: 777.0140431282684
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 271.3739065251493
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 266
  episodes_total: 17019
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.25557995587587357
        entropy_coeff: 0.0005000000000000001
        kl: 0.004823980115664502
        model: {}
        policy_loss: -0.006437495481804945
        total_loss: 5.8934242725372314
        vf_explained_var: 0.9887437224388123
        vf_loss: 5.899989485740662
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.134482758620685
    gpu_util_percent0: 0.40344827586206894
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470801307832831
    mean_env_wait_ms: 1.2280786329763593
    mean_inference_ms: 4.3359632355427715
    mean_raw_obs_processing_ms: 0.38024580787780815
  time_since_restore: 2098.0129318237305
  time_this_iter_s: 25.674356937408447
  time_total_s: 2098.0129318237305
  timers:
    learn_throughput: 8698.191
    learn_time_ms: 18600.649
    sample_throughput: 23330.748
    sample_time_ms: 6934.711
    update_time_ms: 34.244
  timestamp: 1602767437
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     82 |          2098.01 | 13266944 |  271.374 |              315.717 |              148.141 |            777.014 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3267.5665308498255
    time_step_min: 2976
  date: 2020-10-15_13-11-03
  done: false
  episode_len_mean: 777.0182905585879
  episode_reward_max: 315.7171717171722
  episode_reward_mean: 271.61703376817775
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 203
  episodes_total: 17222
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.22632338851690292
        entropy_coeff: 0.0005000000000000001
        kl: 0.004703229797693591
        model: {}
        policy_loss: -0.00881632497961012
        total_loss: 3.778484364350637
        vf_explained_var: 0.9909164309501648
        vf_loss: 3.7874138951301575
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38666666666667
    gpu_util_percent0: 0.30700000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706442111473209
    mean_env_wait_ms: 1.2281444238404542
    mean_inference_ms: 4.335086770399273
    mean_raw_obs_processing_ms: 0.38018304880680853
  time_since_restore: 2123.8264620304108
  time_this_iter_s: 25.813530206680298
  time_total_s: 2123.8264620304108
  timers:
    learn_throughput: 8701.211
    learn_time_ms: 18594.193
    sample_throughput: 23268.901
    sample_time_ms: 6953.143
    update_time_ms: 35.323
  timestamp: 1602767463
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     83 |          2123.83 | 13428736 |  271.617 |              315.717 |              148.141 |            777.018 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3266.211970937608
    time_step_min: 2971
  date: 2020-10-15_13-11-28
  done: false
  episode_len_mean: 776.981649792913
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 271.8259673355739
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 17384
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.24729155004024506
        entropy_coeff: 0.0005000000000000001
        kl: 0.004852148044543962
        model: {}
        policy_loss: -0.008908574345696252
        total_loss: 3.4848879178365073
        vf_explained_var: 0.9898207187652588
        vf_loss: 3.493920107682546
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.120689655172416
    gpu_util_percent0: 0.3255172413793104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470531837455057
    mean_env_wait_ms: 1.2281976698895267
    mean_inference_ms: 4.3344035922848825
    mean_raw_obs_processing_ms: 0.38013603417371605
  time_since_restore: 2149.0321271419525
  time_this_iter_s: 25.205665111541748
  time_total_s: 2149.0321271419525
  timers:
    learn_throughput: 8720.385
    learn_time_ms: 18553.309
    sample_throughput: 23313.353
    sample_time_ms: 6939.886
    update_time_ms: 32.879
  timestamp: 1602767488
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     84 |          2149.03 | 13590528 |  271.826 |              316.475 |              148.141 |            776.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3264.239397384878
    time_step_min: 2971
  date: 2020-10-15_13-11-54
  done: false
  episode_len_mean: 776.9624546279492
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.1311899622359
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 248
  episodes_total: 17632
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.2447493076324463
        entropy_coeff: 0.0005000000000000001
        kl: 0.004481222596950829
        model: {}
        policy_loss: -0.008443929565449556
        total_loss: 4.945951541264852
        vf_explained_var: 0.9899372458457947
        vf_loss: 4.9545178810755415
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58
    gpu_util_percent0: 0.3409999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470389239693123
    mean_env_wait_ms: 1.2282872407022076
    mean_inference_ms: 4.333443011576932
    mean_raw_obs_processing_ms: 0.38007067561234353
  time_since_restore: 2174.2744505405426
  time_this_iter_s: 25.242323398590088
  time_total_s: 2174.2744505405426
  timers:
    learn_throughput: 8740.354
    learn_time_ms: 18510.92
    sample_throughput: 23447.149
    sample_time_ms: 6900.285
    update_time_ms: 30.863
  timestamp: 1602767514
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     85 |          2174.27 | 13752320 |  272.131 |              316.475 |              148.141 |            776.962 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3262.4823152930608
    time_step_min: 2971
  date: 2020-10-15_13-12-20
  done: false
  episode_len_mean: 776.9564803405399
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.39875737321694
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 222
  episodes_total: 17854
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.21237351497014365
        entropy_coeff: 0.0005000000000000001
        kl: 0.004442582760627071
        model: {}
        policy_loss: -0.006997300889148998
        total_loss: 4.398995478947957
        vf_explained_var: 0.9899685382843018
        vf_loss: 4.406099021434784
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79655172413793
    gpu_util_percent0: 0.33620689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470224376940788
    mean_env_wait_ms: 1.2283419968245903
    mean_inference_ms: 4.332547158246978
    mean_raw_obs_processing_ms: 0.38000666989272935
  time_since_restore: 2199.617164850235
  time_this_iter_s: 25.342714309692383
  time_total_s: 2199.617164850235
  timers:
    learn_throughput: 8741.625
    learn_time_ms: 18508.23
    sample_throughput: 23503.382
    sample_time_ms: 6883.775
    update_time_ms: 30.272
  timestamp: 1602767540
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     86 |          2199.62 | 13914112 |  272.399 |              316.475 |              148.141 |            776.956 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3261.2060541984306
    time_step_min: 2971
  date: 2020-10-15_13-12-46
  done: false
  episode_len_mean: 776.9353244878698
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.5907848820746
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 18013
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.2334639442463716
        entropy_coeff: 0.0005000000000000001
        kl: 0.004541010401832561
        model: {}
        policy_loss: -0.008917031387682073
        total_loss: 3.4354718724886575
        vf_explained_var: 0.9900593757629395
        vf_loss: 3.4445056915283203
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116666666666664
    gpu_util_percent0: 0.3713333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470122806703285
    mean_env_wait_ms: 1.228383183259724
    mean_inference_ms: 4.3319288581519
    mean_raw_obs_processing_ms: 0.3799634597100784
  time_since_restore: 2225.4640412330627
  time_this_iter_s: 25.84687638282776
  time_total_s: 2225.4640412330627
  timers:
    learn_throughput: 8742.6
    learn_time_ms: 18506.166
    sample_throughput: 23521.249
    sample_time_ms: 6878.546
    update_time_ms: 30.53
  timestamp: 1602767566
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     87 |          2225.46 | 14075904 |  272.591 |              316.475 |              148.141 |            776.935 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3259.330548991592
    time_step_min: 2971
  date: 2020-10-15_13-13-12
  done: false
  episode_len_mean: 776.9176489939141
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 272.87457003280247
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 226
  episodes_total: 18239
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.24373838305473328
        entropy_coeff: 0.0005000000000000001
        kl: 0.006135160801932216
        model: {}
        policy_loss: -0.007553313926716025
        total_loss: 4.122738420963287
        vf_explained_var: 0.9909818172454834
        vf_loss: 4.1304135123888654
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.093103448275862
    gpu_util_percent0: 0.30034482758620684
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699884755904202
    mean_env_wait_ms: 1.228447512685596
    mean_inference_ms: 4.331096298493526
    mean_raw_obs_processing_ms: 0.37990551557321617
  time_since_restore: 2250.8072690963745
  time_this_iter_s: 25.343227863311768
  time_total_s: 2250.8072690963745
  timers:
    learn_throughput: 8751.138
    learn_time_ms: 18488.109
    sample_throughput: 23532.975
    sample_time_ms: 6875.119
    update_time_ms: 29.433
  timestamp: 1602767592
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     88 |          2250.81 | 14237696 |  272.875 |              316.475 |              148.141 |            776.918 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3257.418532776663
    time_step_min: 2971
  date: 2020-10-15_13-13-37
  done: false
  episode_len_mean: 776.8952123343252
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.16792212085704
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 18485
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.2055367132027944
        entropy_coeff: 0.0005000000000000001
        kl: 0.004180201096460223
        model: {}
        policy_loss: -0.010965465280605713
        total_loss: 4.429649392763774
        vf_explained_var: 0.9903015494346619
        vf_loss: 4.4407176574071245
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.255172413793108
    gpu_util_percent0: 0.3786206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698309219903632
    mean_env_wait_ms: 1.2284953051587986
    mean_inference_ms: 4.330165687164251
    mean_raw_obs_processing_ms: 0.37983943500729384
  time_since_restore: 2276.1914660930634
  time_this_iter_s: 25.384196996688843
  time_total_s: 2276.1914660930634
  timers:
    learn_throughput: 8753.854
    learn_time_ms: 18482.373
    sample_throughput: 23548.2
    sample_time_ms: 6870.674
    update_time_ms: 30.548
  timestamp: 1602767617
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     89 |          2276.19 | 14399488 |  273.168 |              316.475 |              148.141 |            776.895 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3256.142027738953
    time_step_min: 2971
  date: 2020-10-15_13-14-03
  done: false
  episode_len_mean: 776.8852177644283
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.35431877236215
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 159
  episodes_total: 18644
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21800185119112334
        entropy_coeff: 0.0005000000000000001
        kl: 0.003973691374994814
        model: {}
        policy_loss: -0.008119370116522381
        total_loss: 3.5141924818356833
        vf_explained_var: 0.9897346496582031
        vf_loss: 3.522420803705851
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16
    gpu_util_percent0: 0.2856666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697345143297336
    mean_env_wait_ms: 1.2285249356458343
    mean_inference_ms: 4.329587869044992
    mean_raw_obs_processing_ms: 0.37979778103102585
  time_since_restore: 2301.762736558914
  time_this_iter_s: 25.57127046585083
  time_total_s: 2301.762736558914
  timers:
    learn_throughput: 8744.233
    learn_time_ms: 18502.71
    sample_throughput: 23658.413
    sample_time_ms: 6838.667
    update_time_ms: 27.818
  timestamp: 1602767643
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     90 |          2301.76 | 14561280 |  273.354 |              316.475 |              148.141 |            776.885 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3254.4844281462583
    time_step_min: 2971
  date: 2020-10-15_13-14-29
  done: false
  episode_len_mean: 776.8501962032029
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.6017846296244
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 214
  episodes_total: 18858
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.23492631067832312
        entropy_coeff: 0.0005000000000000001
        kl: 0.004685264585229258
        model: {}
        policy_loss: -0.010369188152253628
        total_loss: 4.419384280840556
        vf_explained_var: 0.9900035262107849
        vf_loss: 4.429870883623759
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.589655172413792
    gpu_util_percent0: 0.3396551724137931
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469620646896978
    mean_env_wait_ms: 1.2285740963963363
    mean_inference_ms: 4.328854623326407
    mean_raw_obs_processing_ms: 0.3797446865460311
  time_since_restore: 2327.296405315399
  time_this_iter_s: 25.533668756484985
  time_total_s: 2327.296405315399
  timers:
    learn_throughput: 8749.194
    learn_time_ms: 18492.217
    sample_throughput: 23643.233
    sample_time_ms: 6843.057
    update_time_ms: 27.737
  timestamp: 1602767669
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     91 |           2327.3 | 14723072 |  273.602 |              316.475 |              148.141 |             776.85 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3252.4802348746985
    time_step_min: 2971
  date: 2020-10-15_13-14-55
  done: false
  episode_len_mean: 776.803881565181
  episode_reward_max: 316.47474747474723
  episode_reward_mean: 273.9075183726785
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 258
  episodes_total: 19116
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.20404858887195587
        entropy_coeff: 0.0005000000000000001
        kl: 0.005306126511034866
        model: {}
        policy_loss: -0.010063391081833592
        total_loss: 3.767662982145945
        vf_explained_var: 0.9920833110809326
        vf_loss: 3.7778284152348838
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.013333333333332
    gpu_util_percent0: 0.3373333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694602262401962
    mean_env_wait_ms: 1.2286069579157461
    mean_inference_ms: 4.3279327830232015
    mean_raw_obs_processing_ms: 0.3796788214333268
  time_since_restore: 2352.864337205887
  time_this_iter_s: 25.56793189048767
  time_total_s: 2352.864337205887
  timers:
    learn_throughput: 8755.008
    learn_time_ms: 18479.937
    sample_throughput: 23642.061
    sample_time_ms: 6843.397
    update_time_ms: 28.118
  timestamp: 1602767695
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     92 |          2352.86 | 14884864 |  273.908 |              316.475 |              148.141 |            776.804 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3251.193251533742
    time_step_min: 2971
  date: 2020-10-15_13-15-21
  done: false
  episode_len_mean: 776.7681054160614
  episode_reward_max: 317.98989898989885
  episode_reward_mean: 274.0990659866984
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 160
  episodes_total: 19276
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.20951137940088907
        entropy_coeff: 0.0005000000000000001
        kl: 0.004479876991050939
        model: {}
        policy_loss: -0.007850972489298632
        total_loss: 4.130880792935689
        vf_explained_var: 0.9880260825157166
        vf_loss: 4.138836522897084
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996551724137927
    gpu_util_percent0: 0.39137931034482765
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693675042503376
    mean_env_wait_ms: 1.2286290392782049
    mean_inference_ms: 4.3273827887902385
    mean_raw_obs_processing_ms: 0.37963814332881
  time_since_restore: 2378.143862724304
  time_this_iter_s: 25.27952551841736
  time_total_s: 2378.143862724304
  timers:
    learn_throughput: 8771.174
    learn_time_ms: 18445.877
    sample_throughput: 23714.027
    sample_time_ms: 6822.629
    update_time_ms: 27.935
  timestamp: 1602767721
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     93 |          2378.14 | 15046656 |  274.099 |               317.99 |              148.141 |            776.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3249.776742989452
    time_step_min: 2971
  date: 2020-10-15_13-15-47
  done: false
  episode_len_mean: 776.7269600041074
  episode_reward_max: 317.98989898989885
  episode_reward_mean: 274.32230193291963
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 201
  episodes_total: 19477
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.22809051846464476
        entropy_coeff: 0.0005000000000000001
        kl: 0.004330610565375537
        model: {}
        policy_loss: -0.010108465377318984
        total_loss: 4.2880149483680725
        vf_explained_var: 0.990037739276886
        vf_loss: 4.298237403233846
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.790000000000003
    gpu_util_percent0: 0.36199999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469265260009878
    mean_env_wait_ms: 1.228664126182054
    mean_inference_ms: 4.326729001677748
    mean_raw_obs_processing_ms: 0.3795898457307838
  time_since_restore: 2403.725620031357
  time_this_iter_s: 25.581757307052612
  time_total_s: 2403.725620031357
  timers:
    learn_throughput: 8762.887
    learn_time_ms: 18463.322
    sample_throughput: 23685.639
    sample_time_ms: 6830.806
    update_time_ms: 29.631
  timestamp: 1602767747
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     94 |          2403.73 | 15208448 |  274.322 |               317.99 |              148.141 |            776.727 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3247.7777101096226
    time_step_min: 2961
  date: 2020-10-15_13-16-13
  done: false
  episode_len_mean: 776.694773625038
  episode_reward_max: 317.98989898989925
  episode_reward_mean: 274.6242384341747
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 269
  episodes_total: 19746
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.19512154906988144
        entropy_coeff: 0.0005000000000000001
        kl: 0.004368953096369903
        model: {}
        policy_loss: -0.01058005481869865
        total_loss: 4.657853643099467
        vf_explained_var: 0.990476131439209
        vf_loss: 4.668531219164531
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.168965517241382
    gpu_util_percent0: 0.3548275862068966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691069358637696
    mean_env_wait_ms: 1.2286862126763105
    mean_inference_ms: 4.325829149759993
    mean_raw_obs_processing_ms: 0.3795245218656024
  time_since_restore: 2429.340170145035
  time_this_iter_s: 25.61455011367798
  time_total_s: 2429.340170145035
  timers:
    learn_throughput: 8742.769
    learn_time_ms: 18505.807
    sample_throughput: 23716.341
    sample_time_ms: 6821.963
    update_time_ms: 32.428
  timestamp: 1602767773
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     95 |          2429.34 | 15370240 |  274.624 |               317.99 |              148.141 |            776.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3246.5545655894493
    time_step_min: 2961
  date: 2020-10-15_13-16-38
  done: false
  episode_len_mean: 776.6632509543902
  episode_reward_max: 317.98989898989925
  episode_reward_mean: 274.81723656090753
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 19908
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.19664488981167474
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046378555707633495
        model: {}
        policy_loss: -0.009079183825330498
        total_loss: 2.6752074162165322
        vf_explained_var: 0.9920947551727295
        vf_loss: 2.6843850016593933
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78666666666667
    gpu_util_percent0: 0.3386666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690197864413854
    mean_env_wait_ms: 1.2287003791324362
    mean_inference_ms: 4.325305270895478
    mean_raw_obs_processing_ms: 0.3794851555838515
  time_since_restore: 2454.6652550697327
  time_this_iter_s: 25.325084924697876
  time_total_s: 2454.6652550697327
  timers:
    learn_throughput: 8750.05
    learn_time_ms: 18490.408
    sample_throughput: 23700.483
    sample_time_ms: 6826.528
    update_time_ms: 32.526
  timestamp: 1602767798
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     96 |          2454.67 | 15532032 |  274.817 |               317.99 |              148.141 |            776.663 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3244.990030407258
    time_step_min: 2961
  date: 2020-10-15_13-17-04
  done: false
  episode_len_mean: 776.6024971397304
  episode_reward_max: 317.98989898989925
  episode_reward_mean: 275.05291837943685
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 195
  episodes_total: 20103
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.2139376476407051
        entropy_coeff: 0.0005000000000000001
        kl: 0.005073503435899814
        model: {}
        policy_loss: -0.0073867844249662085
        total_loss: 3.4668161869049072
        vf_explained_var: 0.9913925528526306
        vf_loss: 3.4743099411328635
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.70689655172414
    gpu_util_percent0: 0.3072413793103448
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468925025726707
    mean_env_wait_ms: 1.2287224978409923
    mean_inference_ms: 4.324704509916442
    mean_raw_obs_processing_ms: 0.379439856638702
  time_since_restore: 2479.9354927539825
  time_this_iter_s: 25.270237684249878
  time_total_s: 2479.9354927539825
  timers:
    learn_throughput: 8768.168
    learn_time_ms: 18452.201
    sample_throughput: 23741.941
    sample_time_ms: 6814.607
    update_time_ms: 30.896
  timestamp: 1602767824
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     97 |          2479.94 | 15693824 |  275.053 |               317.99 |              148.141 |            776.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3243.023258100998
    time_step_min: 2961
  date: 2020-10-15_13-17-30
  done: false
  episode_len_mean: 776.5060601599686
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 275.3638445399082
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 276
  episodes_total: 20379
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.191166240721941
        entropy_coeff: 0.0005000000000000001
        kl: 0.004569540731608868
        model: {}
        policy_loss: -0.008908689846672738
        total_loss: 4.590634385744731
        vf_explained_var: 0.9906993508338928
        vf_loss: 4.59963842233022
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.21379310344828
    gpu_util_percent0: 0.3531034482758621
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687720094113452
    mean_env_wait_ms: 1.2287356248428123
    mean_inference_ms: 4.323832691188717
    mean_raw_obs_processing_ms: 0.37937572728899943
  time_since_restore: 2505.125547170639
  time_this_iter_s: 25.190054416656494
  time_total_s: 2505.125547170639
  timers:
    learn_throughput: 8777.884
    learn_time_ms: 18431.776
    sample_throughput: 23723.118
    sample_time_ms: 6820.014
    update_time_ms: 30.207
  timestamp: 1602767850
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     98 |          2505.13 | 15855616 |  275.364 |              319.808 |              148.141 |            776.506 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3241.7598790125867
    time_step_min: 2952
  date: 2020-10-15_13-17-55
  done: false
  episode_len_mean: 776.4785296981499
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 275.5524450935844
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 161
  episodes_total: 20540
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.18675213803847632
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038226922430718937
        model: {}
        policy_loss: -0.007531334820669144
        total_loss: 3.2655821442604065
        vf_explained_var: 0.990511417388916
        vf_loss: 3.2732067704200745
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.503333333333337
    gpu_util_percent0: 0.3623333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468690368383175
    mean_env_wait_ms: 1.2287435910591602
    mean_inference_ms: 4.323342927151467
    mean_raw_obs_processing_ms: 0.3793380631471465
  time_since_restore: 2530.448502063751
  time_this_iter_s: 25.322954893112183
  time_total_s: 2530.448502063751
  timers:
    learn_throughput: 8785.64
    learn_time_ms: 18415.504
    sample_throughput: 23703.617
    sample_time_ms: 6825.625
    update_time_ms: 31.445
  timestamp: 1602767875
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |     99 |          2530.45 | 16017408 |  275.552 |              319.808 |              148.141 |            776.479 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3240.313593734893
    time_step_min: 2952
  date: 2020-10-15_13-18-22
  done: false
  episode_len_mean: 776.4451949054419
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 275.77515944859647
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 20728
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.20801711827516556
        entropy_coeff: 0.0005000000000000001
        kl: 0.005674606072716415
        model: {}
        policy_loss: -0.008645180948709216
        total_loss: 3.3662743965784707
        vf_explained_var: 0.9913938641548157
        vf_loss: 3.3750235637029014
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.58275862068966
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685941420670312
    mean_env_wait_ms: 1.2287546336500097
    mean_inference_ms: 4.322785526855456
    mean_raw_obs_processing_ms: 0.3792955801019332
  time_since_restore: 2556.126496553421
  time_this_iter_s: 25.6779944896698
  time_total_s: 2556.126496553421
  timers:
    learn_throughput: 8780.87
    learn_time_ms: 18425.508
    sample_throughput: 23709.392
    sample_time_ms: 6823.962
    update_time_ms: 33.31
  timestamp: 1602767902
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    100 |          2556.13 | 16179200 |  275.775 |              319.808 |              148.141 |            776.445 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3238.0968188105116
    time_step_min: 2952
  date: 2020-10-15_13-18-48
  done: false
  episode_len_mean: 776.4024465705174
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 276.1068748314216
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 281
  episodes_total: 21009
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.18861758460601172
        entropy_coeff: 0.0005000000000000001
        kl: 0.004452064128903051
        model: {}
        policy_loss: -0.009123142333313202
        total_loss: 3.4913241465886435
        vf_explained_var: 0.9929396510124207
        vf_loss: 3.500541607538859
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.74
    gpu_util_percent0: 0.35566666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14684593128380216
    mean_env_wait_ms: 1.2287606784895753
    mean_inference_ms: 4.321962327807155
    mean_raw_obs_processing_ms: 0.37923410464166313
  time_since_restore: 2581.794452190399
  time_this_iter_s: 25.66795563697815
  time_total_s: 2581.794452190399
  timers:
    learn_throughput: 8773.474
    learn_time_ms: 18441.042
    sample_throughput: 23720.439
    sample_time_ms: 6820.784
    update_time_ms: 33.047
  timestamp: 1602767928
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    101 |          2581.79 | 16340992 |  276.107 |              319.808 |              148.141 |            776.402 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3236.8575485092288
    time_step_min: 2952
  date: 2020-10-15_13-19-13
  done: false
  episode_len_mean: 776.3793689778953
  episode_reward_max: 319.80808080808055
  episode_reward_mean: 276.29205191915383
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 163
  episodes_total: 21172
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.18021325394511223
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042955120249340934
        model: {}
        policy_loss: -0.006359915253900302
        total_loss: 3.6041420300801597
        vf_explained_var: 0.9898276925086975
        vf_loss: 3.6105920871098838
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.926666666666666
    gpu_util_percent0: 0.37000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683787636744178
    mean_env_wait_ms: 1.228760033819787
    mean_inference_ms: 4.321487652103918
    mean_raw_obs_processing_ms: 0.3791973487990441
  time_since_restore: 2607.211180448532
  time_this_iter_s: 25.416728258132935
  time_total_s: 2607.211180448532
  timers:
    learn_throughput: 8780.313
    learn_time_ms: 18426.678
    sample_throughput: 23745.264
    sample_time_ms: 6813.653
    update_time_ms: 31.127
  timestamp: 1602767953
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    102 |          2607.21 | 16502784 |  276.292 |              319.808 |              148.141 |            776.379 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3235.524117867868
    time_step_min: 2952
  date: 2020-10-15_13-19-39
  done: false
  episode_len_mean: 776.3679872623396
  episode_reward_max: 319.9595959595957
  episode_reward_mean: 276.50217828751136
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 182
  episodes_total: 21354
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.20342449098825455
        entropy_coeff: 0.0005000000000000001
        kl: 0.004762681084685028
        model: {}
        policy_loss: -0.010126531822606921
        total_loss: 3.2652917305628457
        vf_explained_var: 0.9915415644645691
        vf_loss: 3.2755199670791626
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.00344827586207
    gpu_util_percent0: 0.33827586206896554
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468288830583388
    mean_env_wait_ms: 1.2287623037084972
    mean_inference_ms: 4.320964239951015
    mean_raw_obs_processing_ms: 0.3791558366240272
  time_since_restore: 2632.727113008499
  time_this_iter_s: 25.51593255996704
  time_total_s: 2632.727113008499
  timers:
    learn_throughput: 8770.829
    learn_time_ms: 18446.603
    sample_throughput: 23726.308
    sample_time_ms: 6819.097
    update_time_ms: 29.899
  timestamp: 1602767979
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    103 |          2632.73 | 16664576 |  276.502 |               319.96 |              148.141 |            776.368 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3233.4344805224882
    time_step_min: 2949
  date: 2020-10-15_13-20-06
  done: false
  episode_len_mean: 776.3580509454024
  episode_reward_max: 319.9595959595957
  episode_reward_mean: 276.812282129697
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 277
  episodes_total: 21631
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.18621143326163292
        entropy_coeff: 0.0005000000000000001
        kl: 0.004067339294124395
        model: {}
        policy_loss: -0.00802563641142721
        total_loss: 4.23588103055954
        vf_explained_var: 0.991602897644043
        vf_loss: 4.243999779224396
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03000000000001
    gpu_util_percent0: 0.29533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681718892698867
    mean_env_wait_ms: 1.2287578468341547
    mean_inference_ms: 4.320220430092102
    mean_raw_obs_processing_ms: 0.3791028543804112
  time_since_restore: 2658.3809473514557
  time_this_iter_s: 25.653834342956543
  time_total_s: 2658.3809473514557
  timers:
    learn_throughput: 8755.687
    learn_time_ms: 18478.503
    sample_throughput: 23782.409
    sample_time_ms: 6803.011
    update_time_ms: 29.62
  timestamp: 1602768006
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    104 |          2658.38 | 16826368 |  276.812 |               319.96 |              148.141 |            776.358 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3232.1961676316514
    time_step_min: 2939
  date: 2020-10-15_13-20-31
  done: false
  episode_len_mean: 776.3557145477894
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 276.9994009995387
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 21804
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.1707239386936029
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034220329641054073
        model: {}
        policy_loss: -0.008444460826770714
        total_loss: 3.379118025302887
        vf_explained_var: 0.9907882809638977
        vf_loss: 3.387647807598114
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.555172413793105
    gpu_util_percent0: 0.35275862068965513
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680824365968442
    mean_env_wait_ms: 1.2287491043746115
    mean_inference_ms: 4.3197306418291745
    mean_raw_obs_processing_ms: 0.3790631735462793
  time_since_restore: 2683.772440433502
  time_this_iter_s: 25.39149308204651
  time_total_s: 2683.772440433502
  timers:
    learn_throughput: 8764.987
    learn_time_ms: 18458.897
    sample_throughput: 23788.727
    sample_time_ms: 6801.205
    update_time_ms: 27.584
  timestamp: 1602768031
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    105 |          2683.77 | 16988160 |  276.999 |              321.323 |              148.141 |            776.356 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3230.8406490724283
    time_step_min: 2939
  date: 2020-10-15_13-20-57
  done: false
  episode_len_mean: 776.3512124107184
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.20532333020395
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 177
  episodes_total: 21981
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.19544888163606325
        entropy_coeff: 0.0005000000000000001
        kl: 0.004837796402474244
        model: {}
        policy_loss: -0.007723270636536957
        total_loss: 3.552420735359192
        vf_explained_var: 0.9902071952819824
        vf_loss: 3.560241719086965
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.024137931034478
    gpu_util_percent0: 0.3155172413793104
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680008311098075
    mean_env_wait_ms: 1.2287442196027993
    mean_inference_ms: 4.319249139292571
    mean_raw_obs_processing_ms: 0.37902521546846824
  time_since_restore: 2709.002603530884
  time_this_iter_s: 25.230163097381592
  time_total_s: 2709.002603530884
  timers:
    learn_throughput: 8762.167
    learn_time_ms: 18464.838
    sample_throughput: 23816.845
    sample_time_ms: 6793.175
    update_time_ms: 27.592
  timestamp: 1602768057
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    106 |             2709 | 17149952 |  277.205 |              321.323 |              148.141 |            776.351 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3228.768596902017
    time_step_min: 2939
  date: 2020-10-15_13-21-23
  done: false
  episode_len_mean: 776.344584269663
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.50906140052217
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 269
  episodes_total: 22250
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.18585548425714174
        entropy_coeff: 0.0005000000000000001
        kl: 0.004263442708179355
        model: {}
        policy_loss: -0.008482252967951354
        total_loss: 3.678061227003733
        vf_explained_var: 0.9925315380096436
        vf_loss: 3.6866363088289895
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39
    gpu_util_percent0: 0.3046666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678970749994077
    mean_env_wait_ms: 1.2287280488926922
    mean_inference_ms: 4.318565044675587
    mean_raw_obs_processing_ms: 0.37897351046964806
  time_since_restore: 2734.416863679886
  time_this_iter_s: 25.414260149002075
  time_total_s: 2734.416863679886
  timers:
    learn_throughput: 8761.748
    learn_time_ms: 18465.723
    sample_throughput: 23811.61
    sample_time_ms: 6794.669
    update_time_ms: 36.947
  timestamp: 1602768083
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    107 |          2734.42 | 17311744 |  277.509 |              321.323 |              148.141 |            776.345 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3227.293069572207
    time_step_min: 2939
  date: 2020-10-15_13-21-49
  done: false
  episode_len_mean: 776.3161436976288
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.7186763336702
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 186
  episodes_total: 22436
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.1633200210829576
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042159569954189164
        model: {}
        policy_loss: -0.008422416833733829
        total_loss: 2.697547455628713
        vf_explained_var: 0.9925214648246765
        vf_loss: 2.706051508585612
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793103448275865
    gpu_util_percent0: 0.3372413793103448
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678023201727433
    mean_env_wait_ms: 1.2287117561587637
    mean_inference_ms: 4.318064316301984
    mean_raw_obs_processing_ms: 0.37893358569873165
  time_since_restore: 2759.794024705887
  time_this_iter_s: 25.377161026000977
  time_total_s: 2759.794024705887
  timers:
    learn_throughput: 8751.3
    learn_time_ms: 18487.767
    sample_throughput: 23828.011
    sample_time_ms: 6789.992
    update_time_ms: 37.439
  timestamp: 1602768109
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    108 |          2759.79 | 17473536 |  277.719 |              321.323 |              148.141 |            776.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3226.0286727232437
    time_step_min: 2939
  date: 2020-10-15_13-22-15
  done: false
  episode_len_mean: 776.3090635643828
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 277.90879646198806
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 22607
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.18519586200515428
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039576644776389
        model: {}
        policy_loss: -0.009751538705510635
        total_loss: 3.283907334009806
        vf_explained_var: 0.9909891486167908
        vf_loss: 3.2937514980634055
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83
    gpu_util_percent0: 0.2889999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467728041134488
    mean_env_wait_ms: 1.2286993895639764
    mean_inference_ms: 4.317618791596362
    mean_raw_obs_processing_ms: 0.3788985752004653
  time_since_restore: 2785.270636320114
  time_this_iter_s: 25.476611614227295
  time_total_s: 2785.270636320114
  timers:
    learn_throughput: 8740.706
    learn_time_ms: 18510.176
    sample_throughput: 23842.916
    sample_time_ms: 6785.747
    update_time_ms: 35.258
  timestamp: 1602768135
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    109 |          2785.27 | 17635328 |  277.909 |              321.323 |              148.141 |            776.309 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3224.0629872974155
    time_step_min: 2939
  date: 2020-10-15_13-22-40
  done: false
  episode_len_mean: 776.2803427771948
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.1983581883897
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 265
  episodes_total: 22872
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.1804860420525074
        entropy_coeff: 0.0005000000000000001
        kl: 0.004425623725789289
        model: {}
        policy_loss: -0.007380277859435107
        total_loss: 3.9703678091367087
        vf_explained_var: 0.9919913411140442
        vf_loss: 3.9778383572896323
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.9551724137931
    gpu_util_percent0: 0.3517241379310345
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676223056062176
    mean_env_wait_ms: 1.2286773869985383
    mean_inference_ms: 4.316976604139337
    mean_raw_obs_processing_ms: 0.3788483725923899
  time_since_restore: 2810.4785170555115
  time_this_iter_s: 25.20788073539734
  time_total_s: 2810.4785170555115
  timers:
    learn_throughput: 8762.816
    learn_time_ms: 18463.471
    sample_throughput: 23846.787
    sample_time_ms: 6784.646
    update_time_ms: 34.877
  timestamp: 1602768160
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    110 |          2810.48 | 17797120 |  278.198 |              321.323 |              148.141 |             776.28 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3222.5806913923393
    time_step_min: 2939
  date: 2020-10-15_13-23-06
  done: false
  episode_len_mean: 776.2584532685972
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.4153867441539
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 196
  episodes_total: 23068
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.15374662106235823
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034466406796127558
        model: {}
        policy_loss: -0.009092114923987538
        total_loss: 2.9404216210047402
        vf_explained_var: 0.9921563267707825
        vf_loss: 2.949590583642324
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.424137931034487
    gpu_util_percent0: 0.4306896551724138
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675355326677986
    mean_env_wait_ms: 1.2286525473440943
    mean_inference_ms: 4.316480037131051
    mean_raw_obs_processing_ms: 0.3788083968110128
  time_since_restore: 2835.601416826248
  time_this_iter_s: 25.122899770736694
  time_total_s: 2835.601416826248
  timers:
    learn_throughput: 8785.945
    learn_time_ms: 18414.866
    sample_throughput: 23871.21
    sample_time_ms: 6777.704
    update_time_ms: 34.297
  timestamp: 1602768186
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    111 |           2835.6 | 17958912 |  278.415 |              321.323 |              148.141 |            776.258 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3221.350737259636
    time_step_min: 2939
  date: 2020-10-15_13-23-32
  done: false
  episode_len_mean: 776.2295145463935
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.6003936768269
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 23236
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.1732874003549417
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037704111309722066
        model: {}
        policy_loss: -0.0089009863731917
        total_loss: 2.27765546242396
        vf_explained_var: 0.9933651089668274
        vf_loss: 2.2866430481274924
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.396551724137936
    gpu_util_percent0: 0.42172413793103464
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674639250441288
    mean_env_wait_ms: 1.2286316321108564
    mean_inference_ms: 4.316064674015462
    mean_raw_obs_processing_ms: 0.37877514054865785
  time_since_restore: 2860.9909374713898
  time_this_iter_s: 25.3895206451416
  time_total_s: 2860.9909374713898
  timers:
    learn_throughput: 8784.54
    learn_time_ms: 18417.812
    sample_throughput: 23870.46
    sample_time_ms: 6777.917
    update_time_ms: 35.156
  timestamp: 1602768212
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    112 |          2860.99 | 18120704 |    278.6 |              321.323 |              148.141 |             776.23 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3219.3320257536348
    time_step_min: 2939
  date: 2020-10-15_13-23-58
  done: false
  episode_len_mean: 776.1922536709939
  episode_reward_max: 321.32323232323233
  episode_reward_mean: 278.89368251572984
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 23495
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.17279337719082832
        entropy_coeff: 0.0005000000000000001
        kl: 0.004146678683658441
        model: {}
        policy_loss: -0.008295049832668155
        total_loss: 2.999313751856486
        vf_explained_var: 0.9935783743858337
        vf_loss: 3.007695118586222
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.503333333333334
    gpu_util_percent0: 0.3076666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673725661848
    mean_env_wait_ms: 1.2286059475673756
    mean_inference_ms: 4.315473727158327
    mean_raw_obs_processing_ms: 0.3787282354499575
  time_since_restore: 2886.3223025798798
  time_this_iter_s: 25.33136510848999
  time_total_s: 2886.3223025798798
  timers:
    learn_throughput: 8790.944
    learn_time_ms: 18404.395
    sample_throughput: 23892.788
    sample_time_ms: 6771.583
    update_time_ms: 34.874
  timestamp: 1602768238
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    113 |          2886.32 | 18282496 |  278.894 |              321.323 |              148.141 |            776.192 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3217.8046326823905
    time_step_min: 2939
  date: 2020-10-15_13-24-24
  done: false
  episode_len_mean: 776.1815611814346
  episode_reward_max: 322.08080808080814
  episode_reward_mean: 279.116097685718
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 205
  episodes_total: 23700
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.14530424028635025
        entropy_coeff: 0.0005000000000000001
        kl: 0.003431735715518395
        model: {}
        policy_loss: -0.010111487830727128
        total_loss: 2.6148765087127686
        vf_explained_var: 0.9932143092155457
        vf_loss: 2.625060558319092
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.553333333333335
    gpu_util_percent0: 0.3473333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672824838758844
    mean_env_wait_ms: 1.2285757206041266
    mean_inference_ms: 4.31497526189701
    mean_raw_obs_processing_ms: 0.37868761741251206
  time_since_restore: 2911.953302383423
  time_this_iter_s: 25.63099980354309
  time_total_s: 2911.953302383423
  timers:
    learn_throughput: 8801.565
    learn_time_ms: 18382.185
    sample_throughput: 23824.919
    sample_time_ms: 6790.873
    update_time_ms: 33.278
  timestamp: 1602768264
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    114 |          2911.95 | 18444288 |  279.116 |              322.081 |              148.141 |            776.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3216.641130047855
    time_step_min: 2939
  date: 2020-10-15_13-24-50
  done: false
  episode_len_mean: 776.1713878645659
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.2906474229388
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 164
  episodes_total: 23864
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.16208608696858087
        entropy_coeff: 0.0005000000000000001
        kl: 0.003911696413221459
        model: {}
        policy_loss: -0.009447523450944573
        total_loss: 2.3124568263689675
        vf_explained_var: 0.9932478070259094
        vf_loss: 2.321985363960266
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.613793103448277
    gpu_util_percent0: 0.3368965517241379
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672196439776639
    mean_env_wait_ms: 1.2285510893274683
    mean_inference_ms: 4.314594060677872
    mean_raw_obs_processing_ms: 0.3786565850956914
  time_since_restore: 2937.503413915634
  time_this_iter_s: 25.550111532211304
  time_total_s: 2937.503413915634
  timers:
    learn_throughput: 8796.098
    learn_time_ms: 18393.611
    sample_throughput: 23815.521
    sample_time_ms: 6793.553
    update_time_ms: 34.577
  timestamp: 1602768290
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    115 |           2937.5 | 18606080 |  279.291 |              322.838 |              148.141 |            776.171 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3214.8228989240165
    time_step_min: 2939
  date: 2020-10-15_13-25-16
  done: false
  episode_len_mean: 776.1555592419027
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.56592550143756
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 249
  episodes_total: 24113
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.17199470102787018
        entropy_coeff: 0.0005000000000000001
        kl: 0.005043217524265249
        model: {}
        policy_loss: -0.007935532092233188
        total_loss: 3.053463876247406
        vf_explained_var: 0.9933443069458008
        vf_loss: 3.0614852905273438
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17
    gpu_util_percent0: 0.32933333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671292581490128
    mean_env_wait_ms: 1.2285195657231534
    mean_inference_ms: 4.3140474468643815
    mean_raw_obs_processing_ms: 0.3786115046413062
  time_since_restore: 2963.172000169754
  time_this_iter_s: 25.668586254119873
  time_total_s: 2963.172000169754
  timers:
    learn_throughput: 8778.479
    learn_time_ms: 18430.528
    sample_throughput: 23798.327
    sample_time_ms: 6798.461
    update_time_ms: 35.215
  timestamp: 1602768316
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    116 |          2963.17 | 18767872 |  279.566 |              322.838 |              148.141 |            776.156 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3213.3534521799993
    time_step_min: 2939
  date: 2020-10-15_13-25-42
  done: false
  episode_len_mean: 776.1464386996013
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.79271818924946
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 218
  episodes_total: 24331
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.14029322812954584
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033506578571784
        model: {}
        policy_loss: -0.0079016112285899
        total_loss: 2.3935567339261374
        vf_explained_var: 0.9941191077232361
        vf_loss: 2.4015283981959024
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.273333333333333
    gpu_util_percent0: 0.292
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670390581353313
    mean_env_wait_ms: 1.2284817936995327
    mean_inference_ms: 4.313542763620203
    mean_raw_obs_processing_ms: 0.3785706576789935
  time_since_restore: 2988.5770666599274
  time_this_iter_s: 25.40506649017334
  time_total_s: 2988.5770666599274
  timers:
    learn_throughput: 8770.04
    learn_time_ms: 18448.263
    sample_throughput: 23827.629
    sample_time_ms: 6790.101
    update_time_ms: 26.033
  timestamp: 1602768342
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    117 |          2988.58 | 18929664 |  279.793 |              322.838 |              148.141 |            776.146 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3212.34428039753
    time_step_min: 2939
  date: 2020-10-15_13-26-07
  done: false
  episode_len_mean: 776.1532682807333
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 279.95457906546795
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 24493
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.15088662753502527
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034930842424121997
        model: {}
        policy_loss: -0.010404171235402751
        total_loss: 2.4042382637659707
        vf_explained_var: 0.9930008053779602
        vf_loss: 2.4147178332010903
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.24827586206897
    gpu_util_percent0: 0.3479310344827585
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669804262481495
    mean_env_wait_ms: 1.2284543377068589
    mean_inference_ms: 4.313190273690699
    mean_raw_obs_processing_ms: 0.3785410083444944
  time_since_restore: 3013.7480552196503
  time_this_iter_s: 25.1709885597229
  time_total_s: 3013.7480552196503
  timers:
    learn_throughput: 8781.762
    learn_time_ms: 18423.637
    sample_throughput: 23818.906
    sample_time_ms: 6792.587
    update_time_ms: 26.535
  timestamp: 1602768367
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    118 |          3013.75 | 19091456 |  279.955 |              322.838 |              148.141 |            776.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3210.7152406417113
    time_step_min: 2939
  date: 2020-10-15_13-26-33
  done: false
  episode_len_mean: 776.140055002831
  episode_reward_max: 322.83838383838406
  episode_reward_mean: 280.2083465080311
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 233
  episodes_total: 24726
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.15919426083564758
        entropy_coeff: 0.0005000000000000001
        kl: 0.004037204567187776
        model: {}
        policy_loss: -0.008802295196801424
        total_loss: 2.924891233444214
        vf_explained_var: 0.9933714270591736
        vf_loss: 2.9337732195854187
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.824137931034482
    gpu_util_percent0: 0.3241379310344828
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668958848105002
    mean_env_wait_ms: 1.2284157363359096
    mean_inference_ms: 4.312683581593167
    mean_raw_obs_processing_ms: 0.37849933140194847
  time_since_restore: 3038.99609041214
  time_this_iter_s: 25.248035192489624
  time_total_s: 3038.99609041214
  timers:
    learn_throughput: 8791.58
    learn_time_ms: 18403.063
    sample_throughput: 23828.047
    sample_time_ms: 6789.981
    update_time_ms: 26.535
  timestamp: 1602768393
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    119 |             3039 | 19253248 |  280.208 |              322.838 |              148.141 |             776.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3209.0445443236085
    time_step_min: 2929
  date: 2020-10-15_13-26-59
  done: false
  episode_len_mean: 776.1294819919074
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 280.45546041724094
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 235
  episodes_total: 24961
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.13122796515623728
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033048008917830884
        model: {}
        policy_loss: -0.007630673043119411
        total_loss: 2.5804986357688904
        vf_explained_var: 0.993885338306427
        vf_loss: 2.588194986184438
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.648275862068967
    gpu_util_percent0: 0.3575862068965517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668105839303605
    mean_env_wait_ms: 1.2283741113232731
    mean_inference_ms: 4.312190593733187
    mean_raw_obs_processing_ms: 0.37845824897245756
  time_since_restore: 3064.2016730308533
  time_this_iter_s: 25.20558261871338
  time_total_s: 3064.2016730308533
  timers:
    learn_throughput: 8792.144
    learn_time_ms: 18401.882
    sample_throughput: 23820.627
    sample_time_ms: 6792.097
    update_time_ms: 24.943
  timestamp: 1602768419
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    120 |           3064.2 | 19415040 |  280.455 |              322.838 |              148.141 |            776.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3207.867549140784
    time_step_min: 2929
  date: 2020-10-15_13-27-25
  done: false
  episode_len_mean: 776.123233690244
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 280.63060369245943
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 162
  episodes_total: 25123
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.13777459785342216
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037779376919691763
        model: {}
        policy_loss: -0.009241481297067367
        total_loss: 1.8713032007217407
        vf_explained_var: 0.9943371415138245
        vf_loss: 1.8806135455767314
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.51
    gpu_util_percent0: 0.305
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466752917142214
    mean_env_wait_ms: 1.228341799021022
    mean_inference_ms: 4.311844938332643
    mean_raw_obs_processing_ms: 0.37842942695702975
  time_since_restore: 3089.8468317985535
  time_this_iter_s: 25.645158767700195
  time_total_s: 3089.8468317985535
  timers:
    learn_throughput: 8770.698
    learn_time_ms: 18446.879
    sample_throughput: 23793.674
    sample_time_ms: 6799.79
    update_time_ms: 23.889
  timestamp: 1602768445
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    121 |          3089.85 | 19576832 |  280.631 |              322.838 |              148.141 |            776.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3206.2786451128413
    time_step_min: 2929
  date: 2020-10-15_13-27-51
  done: false
  episode_len_mean: 776.0978179378922
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 280.8678857389745
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 220
  episodes_total: 25343
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.1535676084458828
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037683748135653636
        model: {}
        policy_loss: -0.00816029119111287
        total_loss: 2.491507132848104
        vf_explained_var: 0.9939917922019958
        vf_loss: 2.499744196732839
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.959999999999997
    gpu_util_percent0: 0.33266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666736185283796
    mean_env_wait_ms: 1.2282962325765767
    mean_inference_ms: 4.311385796835662
    mean_raw_obs_processing_ms: 0.37839028575009886
  time_since_restore: 3115.5101635456085
  time_this_iter_s: 25.663331747055054
  time_total_s: 3115.5101635456085
  timers:
    learn_throughput: 8759.55
    learn_time_ms: 18470.354
    sample_throughput: 23785.244
    sample_time_ms: 6802.2
    update_time_ms: 24.504
  timestamp: 1602768471
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    122 |          3115.51 | 19738624 |  280.868 |              322.838 |              148.141 |            776.098 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3204.54029827377
    time_step_min: 2929
  date: 2020-10-15_13-28-17
  done: false
  episode_len_mean: 776.0791746453555
  episode_reward_max: 322.8383838383841
  episode_reward_mean: 281.1281733667916
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 25589
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.12881876900792122
        entropy_coeff: 0.0005000000000000001
        kl: 0.003385108410536001
        model: {}
        policy_loss: -0.0073744756446103565
        total_loss: 2.510595420996348
        vf_explained_var: 0.9941862225532532
        vf_loss: 2.5180342396100364
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.896666666666672
    gpu_util_percent0: 0.378
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466594086815143
    mean_env_wait_ms: 1.2282515068274198
    mean_inference_ms: 4.3109044940239025
    mean_raw_obs_processing_ms: 0.3783503297371938
  time_since_restore: 3141.1251635551453
  time_this_iter_s: 25.615000009536743
  time_total_s: 3141.1251635551453
  timers:
    learn_throughput: 8749.054
    learn_time_ms: 18492.514
    sample_throughput: 23769.319
    sample_time_ms: 6806.758
    update_time_ms: 25.454
  timestamp: 1602768497
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    123 |          3141.13 | 19900416 |  281.128 |              322.838 |              148.141 |            776.079 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3203.3868860109674
    time_step_min: 2913
  date: 2020-10-15_13-28-43
  done: false
  episode_len_mean: 776.0806057076296
  episode_reward_max: 325.2626262626264
  episode_reward_mean: 281.30228316949353
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 166
  episodes_total: 25755
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.13009627535939217
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038847767476302883
        model: {}
        policy_loss: -0.008719050629830841
        total_loss: 1.7567763328552246
        vf_explained_var: 0.99471515417099
        vf_loss: 1.7655604481697083
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.783333333333335
    gpu_util_percent0: 0.32599999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665358809784573
    mean_env_wait_ms: 1.2282146527868574
    mean_inference_ms: 4.3105642577247565
    mean_raw_obs_processing_ms: 0.37832117338292953
  time_since_restore: 3166.8027522563934
  time_this_iter_s: 25.67758870124817
  time_total_s: 3166.8027522563934
  timers:
    learn_throughput: 8744.603
    learn_time_ms: 18501.925
    sample_throughput: 23790.824
    sample_time_ms: 6800.605
    update_time_ms: 27.012
  timestamp: 1602768523
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    124 |           3166.8 | 20062208 |  281.302 |              325.263 |              148.141 |            776.081 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3201.9593301435407
    time_step_min: 2913
  date: 2020-10-15_13-29-09
  done: false
  episode_len_mean: 776.0910316665382
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 281.5123198235534
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 203
  episodes_total: 25958
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.14802184825142226
        entropy_coeff: 0.0005000000000000001
        kl: 0.004089848545845598
        model: {}
        policy_loss: -0.008584606655252477
        total_loss: 2.518684128920237
        vf_explained_var: 0.9938375353813171
        vf_loss: 2.5273427963256836
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.930000000000003
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466467404905623
    mean_env_wait_ms: 1.2281708380749576
    mean_inference_ms: 4.310172249184209
    mean_raw_obs_processing_ms: 0.3782870358736329
  time_since_restore: 3192.152811765671
  time_this_iter_s: 25.350059509277344
  time_total_s: 3192.152811765671
  timers:
    learn_throughput: 8757.7
    learn_time_ms: 18474.256
    sample_throughput: 23764.921
    sample_time_ms: 6808.018
    update_time_ms: 25.909
  timestamp: 1602768549
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    125 |          3192.15 | 20224000 |  281.512 |              325.566 |              148.141 |            776.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3200.2166488386306
    time_step_min: 2913
  date: 2020-10-15_13-29-35
  done: false
  episode_len_mean: 776.1191166374247
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 281.7804534782567
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 260
  episodes_total: 26218
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.13204827283819517
        entropy_coeff: 0.0005000000000000001
        kl: 0.028698720193157595
        model: {}
        policy_loss: -0.007668172408254274
        total_loss: 2.3563475807507834
        vf_explained_var: 0.9947864413261414
        vf_loss: 2.3640817999839783
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.09310344827587
    gpu_util_percent0: 0.3486206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663893900758682
    mean_env_wait_ms: 1.228116282819878
    mean_inference_ms: 4.309681796310695
    mean_raw_obs_processing_ms: 0.3782464777852673
  time_since_restore: 3217.415008544922
  time_this_iter_s: 25.2621967792511
  time_total_s: 3217.415008544922
  timers:
    learn_throughput: 8773.293
    learn_time_ms: 18441.422
    sample_throughput: 23791.42
    sample_time_ms: 6800.435
    update_time_ms: 25.067
  timestamp: 1602768575
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    126 |          3217.42 | 20385792 |   281.78 |              325.566 |              148.141 |            776.119 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3199.1153962951716
    time_step_min: 2913
  date: 2020-10-15_13-30-01
  done: false
  episode_len_mean: 776.1385204274994
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 281.9498961417404
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 26386
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.0657581468206415e-21
        cur_lr: 5.0e-05
        entropy: 0.12528735275069872
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036473724176175892
        model: {}
        policy_loss: -0.008693015120419053
        total_loss: 1.7045717636744182
        vf_explained_var: 0.99503093957901
        vf_loss: 1.7133275071779888
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.070000000000007
    gpu_util_percent0: 0.33100000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663294538073823
    mean_env_wait_ms: 1.228074395407355
    mean_inference_ms: 4.309351779413995
    mean_raw_obs_processing_ms: 0.3782173365471738
  time_since_restore: 3243.169011592865
  time_this_iter_s: 25.754003047943115
  time_total_s: 3243.169011592865
  timers:
    learn_throughput: 8772.914
    learn_time_ms: 18442.219
    sample_throughput: 23709.99
    sample_time_ms: 6823.79
    update_time_ms: 33.577
  timestamp: 1602768601
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    127 |          3243.17 | 20547584 |   281.95 |              325.566 |              148.141 |            776.139 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3197.8714382632293
    time_step_min: 2913
  date: 2020-10-15_13-30-28
  done: false
  episode_len_mean: 776.1434108527131
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 282.1367695925159
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 26574
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0328790734103207e-21
        cur_lr: 5.0e-05
        entropy: 0.14094755674401918
        entropy_coeff: 0.0005000000000000001
        kl: 0.003903547185473144
        model: {}
        policy_loss: -0.009575619517515102
        total_loss: 2.0196957190831504
        vf_explained_var: 0.9947615265846252
        vf_loss: 2.029341826836268
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490000000000006
    gpu_util_percent0: 0.36533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662654210269624
    mean_env_wait_ms: 1.2280332946861896
    mean_inference_ms: 4.3090033159433805
    mean_raw_obs_processing_ms: 0.37818683173570605
  time_since_restore: 3268.8113372325897
  time_this_iter_s: 25.64232563972473
  time_total_s: 3268.8113372325897
  timers:
    learn_throughput: 8750.471
    learn_time_ms: 18489.519
    sample_throughput: 23715.963
    sample_time_ms: 6822.072
    update_time_ms: 33.573
  timestamp: 1602768628
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    128 |          3268.81 | 20709376 |  282.137 |              325.566 |              148.141 |            776.143 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3196.0485730274204
    time_step_min: 2913
  date: 2020-10-15_13-30-54
  done: false
  episode_len_mean: 776.1438149513912
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 282.41856378061544
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 273
  episodes_total: 26847
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0164395367051604e-21
        cur_lr: 5.0e-05
        entropy: 0.13146775464216867
        entropy_coeff: 0.0005000000000000001
        kl: 0.004833194233166675
        model: {}
        policy_loss: -0.007567190941093334
        total_loss: 2.4045091470082602
        vf_explained_var: 0.9947856068611145
        vf_loss: 2.4121420780817666
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.420689655172417
    gpu_util_percent0: 0.31206896551724145
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661880519988257
    mean_env_wait_ms: 1.2279646681568903
    mean_inference_ms: 4.308506310179026
    mean_raw_obs_processing_ms: 0.3781449795615421
  time_since_restore: 3294.3058354854584
  time_this_iter_s: 25.494498252868652
  time_total_s: 3294.3058354854584
  timers:
    learn_throughput: 8742.075
    learn_time_ms: 18507.276
    sample_throughput: 23696.63
    sample_time_ms: 6827.637
    update_time_ms: 33.386
  timestamp: 1602768654
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    129 |          3294.31 | 20871168 |  282.419 |              325.566 |              148.141 |            776.144 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3194.8207666073545
    time_step_min: 2913
  date: 2020-10-15_13-31-20
  done: false
  episode_len_mean: 776.1440521134059
  episode_reward_max: 325.5656565656567
  episode_reward_mean: 282.59960923918294
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 27018
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.082197683525802e-22
        cur_lr: 5.0e-05
        entropy: 0.11730944241086642
        entropy_coeff: 0.0005000000000000001
        kl: 0.004376026995790501
        model: {}
        policy_loss: -0.009059199013184601
        total_loss: 1.881285935640335
        vf_explained_var: 0.9945165514945984
        vf_loss: 1.8904037078221638
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.73333333333334
    gpu_util_percent0: 0.39733333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661314081298116
    mean_env_wait_ms: 1.2279208982873608
    mean_inference_ms: 4.308191852828328
    mean_raw_obs_processing_ms: 0.3781169054654193
  time_since_restore: 3319.953765630722
  time_this_iter_s: 25.647930145263672
  time_total_s: 3319.953765630722
  timers:
    learn_throughput: 8721.113
    learn_time_ms: 18551.761
    sample_throughput: 23708.442
    sample_time_ms: 6824.236
    update_time_ms: 35.499
  timestamp: 1602768680
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    130 |          3319.95 | 21032960 |    282.6 |              325.566 |              148.141 |            776.144 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3193.5971276008104
    time_step_min: 2907
  date: 2020-10-15_13-31-46
  done: false
  episode_len_mean: 776.1513402213479
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 282.7825008922925
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 27197
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.541098841762901e-22
        cur_lr: 5.0e-05
        entropy: 0.13284215020636717
        entropy_coeff: 0.0005000000000000001
        kl: 0.004299151323114832
        model: {}
        policy_loss: -0.008989826989515374
        total_loss: 1.886713832616806
        vf_explained_var: 0.9947434067726135
        vf_loss: 1.8957700928052266
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.293333333333333
    gpu_util_percent0: 0.32166666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660705267830793
    mean_env_wait_ms: 1.2278754069761908
    mean_inference_ms: 4.307869146773212
    mean_raw_obs_processing_ms: 0.3780891800934034
  time_since_restore: 3345.276374101639
  time_this_iter_s: 25.322608470916748
  time_total_s: 3345.276374101639
  timers:
    learn_throughput: 8740.38
    learn_time_ms: 18510.865
    sample_throughput: 23684.142
    sample_time_ms: 6831.238
    update_time_ms: 36.992
  timestamp: 1602768706
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    131 |          3345.28 | 21194752 |  282.783 |              326.172 |              148.141 |            776.151 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3191.7305673034857
    time_step_min: 2907
  date: 2020-10-15_13-32-12
  done: false
  episode_len_mean: 776.1509646887514
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 283.06908730552715
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 273
  episodes_total: 27470
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2705494208814505e-22
        cur_lr: 5.0e-05
        entropy: 0.1297167713443438
        entropy_coeff: 0.0005000000000000001
        kl: 0.003745588765013963
        model: {}
        policy_loss: -0.007870971912173749
        total_loss: 2.4279608527819314
        vf_explained_var: 0.994748055934906
        vf_loss: 2.435896654923757
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.241379310344833
    gpu_util_percent0: 0.33448275862068966
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659981934015362
    mean_env_wait_ms: 1.227805282948123
    mean_inference_ms: 4.3074054931295995
    mean_raw_obs_processing_ms: 0.37804794734209124
  time_since_restore: 3370.6685626506805
  time_this_iter_s: 25.392188549041748
  time_total_s: 3370.6685626506805
  timers:
    learn_throughput: 8752.345
    learn_time_ms: 18485.559
    sample_throughput: 23696.83
    sample_time_ms: 6827.58
    update_time_ms: 37.288
  timestamp: 1602768732
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    132 |          3370.67 | 21356544 |  283.069 |              326.172 |              148.141 |            776.151 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3190.45034048102
    time_step_min: 2907
  date: 2020-10-15_13-32-38
  done: false
  episode_len_mean: 776.1473417721519
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 283.261524832411
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 180
  episodes_total: 27650
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.352747104407252e-23
        cur_lr: 5.0e-05
        entropy: 0.10594903180996577
        entropy_coeff: 0.0005000000000000001
        kl: 0.003527582564856857
        model: {}
        policy_loss: -0.006154920420764635
        total_loss: 1.3211583296457927
        vf_explained_var: 0.9961581826210022
        vf_loss: 1.3273661732673645
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.863333333333333
    gpu_util_percent0: 0.3813333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465941465482604
    mean_env_wait_ms: 1.227756105888139
    mean_inference_ms: 4.3070869189762435
    mean_raw_obs_processing_ms: 0.37802016355827994
  time_since_restore: 3396.0554060935974
  time_this_iter_s: 25.38684344291687
  time_total_s: 3396.0554060935974
  timers:
    learn_throughput: 8765.064
    learn_time_ms: 18458.736
    sample_throughput: 23697.627
    sample_time_ms: 6827.35
    update_time_ms: 38.154
  timestamp: 1602768758
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    133 |          3396.06 | 21518336 |  283.262 |              326.172 |              148.141 |            776.147 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3189.2884089272857
    time_step_min: 2907
  date: 2020-10-15_13-33-04
  done: false
  episode_len_mean: 776.1417942635325
  episode_reward_max: 326.17171717171743
  episode_reward_mean: 283.4410625556842
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 27822
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.176373552203626e-23
        cur_lr: 5.0e-05
        entropy: 0.11555669270455837
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031900168784583607
        model: {}
        policy_loss: -0.007046100625302643
        total_loss: 1.592195341984431
        vf_explained_var: 0.995314359664917
        vf_loss: 1.5992992719014485
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.043333333333337
    gpu_util_percent0: 0.3516666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465888971743882
    mean_env_wait_ms: 1.2277083350341944
    mean_inference_ms: 4.306797745962928
    mean_raw_obs_processing_ms: 0.3779944955326896
  time_since_restore: 3421.5842311382294
  time_this_iter_s: 25.528825044631958
  time_total_s: 3421.5842311382294
  timers:
    learn_throughput: 8774.826
    learn_time_ms: 18438.2
    sample_throughput: 23680.583
    sample_time_ms: 6832.264
    update_time_ms: 37.346
  timestamp: 1602768784
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    134 |          3421.58 | 21680128 |  283.441 |              326.172 |              148.141 |            776.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3187.4292538590425
    time_step_min: 2892
  date: 2020-10-15_13-33-30
  done: false
  episode_len_mean: 776.1285729541166
  episode_reward_max: 328.4444444444443
  episode_reward_mean: 283.72969289952175
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 271
  episodes_total: 28093
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.588186776101813e-23
        cur_lr: 5.0e-05
        entropy: 0.12740024427572885
        entropy_coeff: 0.0005000000000000001
        kl: 0.00971499465716382
        model: {}
        policy_loss: -0.008060082114146402
        total_loss: 2.152137597401937
        vf_explained_var: 0.9952964782714844
        vf_loss: 2.1602613031864166
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.144827586206898
    gpu_util_percent0: 0.30655172413793097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658145095110459
    mean_env_wait_ms: 1.2276371575640157
    mean_inference_ms: 4.306355566595848
    mean_raw_obs_processing_ms: 0.377954792732413
  time_since_restore: 3447.0501832962036
  time_this_iter_s: 25.465952157974243
  time_total_s: 3447.0501832962036
  timers:
    learn_throughput: 8770.893
    learn_time_ms: 18446.467
    sample_throughput: 23676.534
    sample_time_ms: 6833.433
    update_time_ms: 38.372
  timestamp: 1602768810
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    135 |          3447.05 | 21841920 |   283.73 |              328.444 |              148.141 |            776.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3186.065049575071
    time_step_min: 2892
  date: 2020-10-15_13-33-56
  done: false
  episode_len_mean: 776.1097871437663
  episode_reward_max: 328.4444444444443
  episode_reward_mean: 283.9290122067861
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 189
  episodes_total: 28282
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.588186776101813e-23
        cur_lr: 5.0e-05
        entropy: 0.11030422834058602
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043239609415953355
        model: {}
        policy_loss: -0.008025283052120358
        total_loss: 1.1861658692359924
        vf_explained_var: 0.9964837431907654
        vf_loss: 1.1942463219165802
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.803333333333338
    gpu_util_percent0: 0.3273333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465759983874813
    mean_env_wait_ms: 1.2275805383755702
    mean_inference_ms: 4.306039076322623
    mean_raw_obs_processing_ms: 0.3779267396762718
  time_since_restore: 3472.6085772514343
  time_this_iter_s: 25.558393955230713
  time_total_s: 3472.6085772514343
  timers:
    learn_throughput: 8762.103
    learn_time_ms: 18464.973
    sample_throughput: 23645.01
    sample_time_ms: 6842.543
    update_time_ms: 39.235
  timestamp: 1602768836
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    136 |          3472.61 | 22003712 |  283.929 |              328.444 |              148.141 |             776.11 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3184.8920098556846
    time_step_min: 2892
  date: 2020-10-15_13-34-22
  done: false
  episode_len_mean: 776.0864262617742
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.10418397385934
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 28452
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.940933880509065e-24
        cur_lr: 5.0e-05
        entropy: 0.11905373570819695
        entropy_coeff: 0.0005000000000000001
        kl: 0.004403135972097516
        model: {}
        policy_loss: -0.00856983138267727
        total_loss: 1.6307201584180195
        vf_explained_var: 0.9949879050254822
        vf_loss: 1.6393495698769887
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89666666666667
    gpu_util_percent0: 0.3693333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465711899814456
    mean_env_wait_ms: 1.2275305983719516
    mean_inference_ms: 4.305767253686115
    mean_raw_obs_processing_ms: 0.37790212720291255
  time_since_restore: 3498.1565811634064
  time_this_iter_s: 25.548003911972046
  time_total_s: 3498.1565811634064
  timers:
    learn_throughput: 8768.127
    learn_time_ms: 18452.287
    sample_throughput: 23651.846
    sample_time_ms: 6840.566
    update_time_ms: 32.606
  timestamp: 1602768862
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    137 |          3498.16 | 22165504 |  284.104 |              329.354 |              148.141 |            776.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3183.162893717953
    time_step_min: 2892
  date: 2020-10-15_13-34-48
  done: false
  episode_len_mean: 776.0514436975375
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.3688552129917
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 28711
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.970466940254533e-24
        cur_lr: 5.0e-05
        entropy: 0.1257868049045404
        entropy_coeff: 0.0005000000000000001
        kl: 0.004212674160953611
        model: {}
        policy_loss: -0.006877902759394298
        total_loss: 2.365839660167694
        vf_explained_var: 0.9947009086608887
        vf_loss: 2.372780521710714
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98
    gpu_util_percent0: 0.3296666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656466679392247
    mean_env_wait_ms: 1.2274595322566042
    mean_inference_ms: 4.305373790660473
    mean_raw_obs_processing_ms: 0.3778661032609911
  time_since_restore: 3523.841600418091
  time_this_iter_s: 25.68501925468445
  time_total_s: 3523.841600418091
  timers:
    learn_throughput: 8771.16
    learn_time_ms: 18445.906
    sample_throughput: 23622.54
    sample_time_ms: 6849.052
    update_time_ms: 33.74
  timestamp: 1602768888
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    138 |          3523.84 | 22327296 |  284.369 |              329.354 |              148.141 |            776.051 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3181.7505541701303
    time_step_min: 2892
  date: 2020-10-15_13-35-14
  done: false
  episode_len_mean: 776.0364529293768
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.58066659540003
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 203
  episodes_total: 28914
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9852334701272663e-24
        cur_lr: 5.0e-05
        entropy: 0.10629856400191784
        entropy_coeff: 0.0005000000000000001
        kl: 0.004265572448881964
        model: {}
        policy_loss: -0.009370756107576502
        total_loss: 1.5155181686083476
        vf_explained_var: 0.9956105351448059
        vf_loss: 1.5249420702457428
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.060000000000006
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655849658439482
    mean_env_wait_ms: 1.2273959095013516
    mean_inference_ms: 4.305044510224545
    mean_raw_obs_processing_ms: 0.37783647553636346
  time_since_restore: 3549.4619495868683
  time_this_iter_s: 25.620349168777466
  time_total_s: 3549.4619495868683
  timers:
    learn_throughput: 8769.746
    learn_time_ms: 18448.882
    sample_throughput: 23601.277
    sample_time_ms: 6855.222
    update_time_ms: 35.899
  timestamp: 1602768914
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    139 |          3549.46 | 22489088 |  284.581 |              329.354 |              148.141 |            776.036 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3180.577926997245
    time_step_min: 2892
  date: 2020-10-15_13-35-40
  done: false
  episode_len_mean: 776.0161268138368
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 284.7554813661685
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 29082
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.926167350636332e-25
        cur_lr: 5.0e-05
        entropy: 0.10871118493378162
        entropy_coeff: 0.0005000000000000001
        kl: 0.004006147869707395
        model: {}
        policy_loss: -0.00800386757813006
        total_loss: 1.3685668210188549
        vf_explained_var: 0.9955697655677795
        vf_loss: 1.3766250709692638
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.653333333333332
    gpu_util_percent0: 0.30533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655397955500488
    mean_env_wait_ms: 1.227344982031871
    mean_inference_ms: 4.304789335193043
    mean_raw_obs_processing_ms: 0.37781295711606944
  time_since_restore: 3574.929210662842
  time_this_iter_s: 25.46726107597351
  time_total_s: 3574.929210662842
  timers:
    learn_throughput: 8781.438
    learn_time_ms: 18424.317
    sample_throughput: 23579.863
    sample_time_ms: 6861.448
    update_time_ms: 35.32
  timestamp: 1602768940
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    140 |          3574.93 | 22650880 |  284.755 |              329.354 |              148.141 |            776.016 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3178.8409176253713
    time_step_min: 2890
  date: 2020-10-15_13-36-07
  done: false
  episode_len_mean: 775.9697971706153
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 285.01108580263184
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 253
  episodes_total: 29335
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.963083675318166e-25
        cur_lr: 5.0e-05
        entropy: 0.12080007170637448
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037232624599710107
        model: {}
        policy_loss: -0.008237912495739389
        total_loss: 2.016016165415446
        vf_explained_var: 0.9952734112739563
        vf_loss: 2.0243144730726876
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43448275862069
    gpu_util_percent0: 0.3486206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654764467197548
    mean_env_wait_ms: 1.2272711246435988
    mean_inference_ms: 4.304426909718503
    mean_raw_obs_processing_ms: 0.37777875147433615
  time_since_restore: 3600.590278148651
  time_this_iter_s: 25.661067485809326
  time_total_s: 3600.590278148651
  timers:
    learn_throughput: 8767.552
    learn_time_ms: 18453.499
    sample_throughput: 23564.489
    sample_time_ms: 6865.924
    update_time_ms: 34.331
  timestamp: 1602768967
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    141 |          3600.59 | 22812672 |  285.011 |              329.354 |              148.141 |             775.97 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3177.4481917093176
    time_step_min: 2890
  date: 2020-10-15_13-36-33
  done: false
  episode_len_mean: 775.938331358944
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 285.2140614129107
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 210
  episodes_total: 29545
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.481541837659083e-25
        cur_lr: 5.0e-05
        entropy: 0.09764574219783147
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032265939516946673
        model: {}
        policy_loss: -0.007635514125771199
        total_loss: 1.472717692454656
        vf_explained_var: 0.9959186911582947
        vf_loss: 1.4804019927978516
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.953333333333333
    gpu_util_percent0: 0.33033333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654149532844943
    mean_env_wait_ms: 1.2272054901808014
    mean_inference_ms: 4.304099137162179
    mean_raw_obs_processing_ms: 0.37774899693306474
  time_since_restore: 3626.008490085602
  time_this_iter_s: 25.418211936950684
  time_total_s: 3626.008490085602
  timers:
    learn_throughput: 8766.515
    learn_time_ms: 18455.68
    sample_throughput: 23559.674
    sample_time_ms: 6867.328
    update_time_ms: 32.251
  timestamp: 1602768993
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    142 |          3626.01 | 22974464 |  285.214 |              329.354 |              148.141 |            775.938 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3176.297654354273
    time_step_min: 2890
  date: 2020-10-15_13-36-59
  done: false
  episode_len_mean: 775.9136434004173
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 285.3883602804651
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 169
  episodes_total: 29714
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2407709188295415e-25
        cur_lr: 5.0e-05
        entropy: 0.10238919717570145
        entropy_coeff: 0.0005000000000000001
        kl: 0.004993245704099536
        model: {}
        policy_loss: -0.007313495540680985
        total_loss: 1.5671208103497822
        vf_explained_var: 0.99493807554245
        vf_loss: 1.5744855006535847
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.730000000000004
    gpu_util_percent0: 0.3323333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465371072883929
    mean_env_wait_ms: 1.2271523798311035
    mean_inference_ms: 4.303851700049722
    mean_raw_obs_processing_ms: 0.37772590287958696
  time_since_restore: 3651.619397878647
  time_this_iter_s: 25.610907793045044
  time_total_s: 3651.619397878647
  timers:
    learn_throughput: 8756.81
    learn_time_ms: 18476.134
    sample_throughput: 23581.567
    sample_time_ms: 6860.952
    update_time_ms: 32.219
  timestamp: 1602769019
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    143 |          3651.62 | 23136256 |  285.388 |              329.354 |              148.141 |            775.914 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3174.707738592679
    time_step_min: 2890
  date: 2020-10-15_13-37-25
  done: false
  episode_len_mean: 775.8964849617786
  episode_reward_max: 329.35353535353556
  episode_reward_mean: 285.63610602806796
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 243
  episodes_total: 29957
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.203854594147707e-26
        cur_lr: 5.0e-05
        entropy: 0.1163597721606493
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041838521137833595
        model: {}
        policy_loss: -0.00829207873903215
        total_loss: 1.821933348973592
        vf_explained_var: 0.995680034160614
        vf_loss: 1.8302835822105408
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040000000000003
    gpu_util_percent0: 0.33499999999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653155936640333
    mean_env_wait_ms: 1.2270788489546185
    mean_inference_ms: 4.303527143560607
    mean_raw_obs_processing_ms: 0.377694395172114
  time_since_restore: 3677.2523238658905
  time_this_iter_s: 25.632925987243652
  time_total_s: 3677.2523238658905
  timers:
    learn_throughput: 8745.509
    learn_time_ms: 18500.01
    sample_throughput: 23626.098
    sample_time_ms: 6848.02
    update_time_ms: 31.252
  timestamp: 1602769045
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    144 |          3677.25 | 23298048 |  285.636 |              329.354 |              148.141 |            775.896 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3173.2655140373
    time_step_min: 2890
  date: 2020-10-15_13-37-51
  done: false
  episode_len_mean: 775.8653897136797
  episode_reward_max: 329.95959595959584
  episode_reward_mean: 285.8581276042504
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 219
  episodes_total: 30176
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1019272970738537e-26
        cur_lr: 5.0e-05
        entropy: 0.09778317684928577
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037119893240742385
        model: {}
        policy_loss: -0.007231836838400341
        total_loss: 1.3955873250961304
        vf_explained_var: 0.9960625767707825
        vf_loss: 1.4028680721918743
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.351724137931036
    gpu_util_percent0: 0.34586206896551724
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652519759870716
    mean_env_wait_ms: 1.227009134448021
    mean_inference_ms: 4.3031947166581554
    mean_raw_obs_processing_ms: 0.37766381063980053
  time_since_restore: 3702.84184551239
  time_this_iter_s: 25.589521646499634
  time_total_s: 3702.84184551239
  timers:
    learn_throughput: 8734.16
    learn_time_ms: 18524.049
    sample_throughput: 23667.068
    sample_time_ms: 6836.166
    update_time_ms: 29.627
  timestamp: 1602769071
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    145 |          3702.84 | 23459840 |  285.858 |               329.96 |              148.141 |            775.865 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3172.122306194515
    time_step_min: 2890
  date: 2020-10-15_13-38-18
  done: false
  episode_len_mean: 775.8349866526053
  episode_reward_max: 329.95959595959584
  episode_reward_mean: 286.0315021153765
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 30343
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5509636485369268e-26
        cur_lr: 5.0e-05
        entropy: 0.09550705862541993
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033874634536914527
        model: {}
        policy_loss: -0.008834953447906932
        total_loss: 1.0436100115378697
        vf_explained_var: 0.9965375065803528
        vf_loss: 1.052492727835973
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.42
    gpu_util_percent0: 0.3443333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652078459507942
    mean_env_wait_ms: 1.2269538272977605
    mean_inference_ms: 4.3029567561763855
    mean_raw_obs_processing_ms: 0.377641405976744
  time_since_restore: 3728.520665168762
  time_this_iter_s: 25.67881965637207
  time_total_s: 3728.520665168762
  timers:
    learn_throughput: 8728.278
    learn_time_ms: 18536.532
    sample_throughput: 23669.896
    sample_time_ms: 6835.349
    update_time_ms: 28.833
  timestamp: 1602769098
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    146 |          3728.52 | 23621632 |  286.032 |               329.96 |              148.141 |            775.835 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3170.5013750654794
    time_step_min: 2890
  date: 2020-10-15_13-38-44
  done: false
  episode_len_mean: 775.7927156215262
  episode_reward_max: 329.95959595959584
  episode_reward_mean: 286.2725803117159
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 243
  episodes_total: 30586
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.754818242684634e-27
        cur_lr: 5.0e-05
        entropy: 0.11048368302484353
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048778927108893795
        model: {}
        policy_loss: -0.008344236218060056
        total_loss: 1.668319433927536
        vf_explained_var: 0.9958657622337341
        vf_loss: 1.6767188807328541
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44666666666667
    gpu_util_percent0: 0.3303333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465151533411523
    mean_env_wait_ms: 1.2268792017840806
    mean_inference_ms: 4.302647397134912
    mean_raw_obs_processing_ms: 0.37761083839181586
  time_since_restore: 3754.0080995559692
  time_this_iter_s: 25.48743438720703
  time_total_s: 3754.0080995559692
  timers:
    learn_throughput: 8725.085
    learn_time_ms: 18543.315
    sample_throughput: 23707.584
    sample_time_ms: 6824.483
    update_time_ms: 26.715
  timestamp: 1602769124
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    147 |          3754.01 | 23783424 |  286.273 |               329.96 |              148.141 |            775.793 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3169.0203471364493
    time_step_min: 2890
  date: 2020-10-15_13-39-10
  done: false
  episode_len_mean: 775.7481173721111
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 286.50472788125353
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 222
  episodes_total: 30808
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.877409121342317e-27
        cur_lr: 5.0e-05
        entropy: 0.09131335591276486
        entropy_coeff: 0.0005000000000000001
        kl: 0.00403425491337354
        model: {}
        policy_loss: -0.0061049360762505485
        total_loss: 1.2523740033308666
        vf_explained_var: 0.9964408278465271
        vf_loss: 1.2585245470205944
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.993103448275864
    gpu_util_percent0: 0.2989655172413794
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650925105654153
    mean_env_wait_ms: 1.2268070689254928
    mean_inference_ms: 4.302323493321773
    mean_raw_obs_processing_ms: 0.3775803856487706
  time_since_restore: 3779.6008627414703
  time_this_iter_s: 25.5927631855011
  time_total_s: 3779.6008627414703
  timers:
    learn_throughput: 8727.968
    learn_time_ms: 18537.189
    sample_throughput: 23713.519
    sample_time_ms: 6822.775
    update_time_ms: 25.251
  timestamp: 1602769150
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    148 |           3779.6 | 23945216 |  286.505 |              330.869 |              148.141 |            775.748 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3167.8467332622117
    time_step_min: 2890
  date: 2020-10-15_13-39-36
  done: false
  episode_len_mean: 775.7142857142857
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 286.6817423630984
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 30975
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9387045606711585e-27
        cur_lr: 5.0e-05
        entropy: 0.0870830478767554
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034361800838572285
        model: {}
        policy_loss: -0.006474501016782597
        total_loss: 0.8203547348578771
        vf_explained_var: 0.9971311688423157
        vf_loss: 0.826872780919075
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.576666666666664
    gpu_util_percent0: 0.37999999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650495991452817
    mean_env_wait_ms: 1.2267505945770707
    mean_inference_ms: 4.302094876246816
    mean_raw_obs_processing_ms: 0.37755850144320485
  time_since_restore: 3805.239762067795
  time_this_iter_s: 25.638899326324463
  time_total_s: 3805.239762067795
  timers:
    learn_throughput: 8726.758
    learn_time_ms: 18539.759
    sample_throughput: 23715.59
    sample_time_ms: 6822.179
    update_time_ms: 25.159
  timestamp: 1602769176
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    149 |          3805.24 | 24107008 |  286.682 |              330.869 |              148.141 |            775.714 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3166.225137944309
    time_step_min: 2890
  date: 2020-10-15_13-40-02
  done: false
  episode_len_mean: 775.6795668610239
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 286.9322830405679
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 239
  episodes_total: 31214
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.693522803355793e-28
        cur_lr: 5.0e-05
        entropy: 0.1014022169013818
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025636512475709
        model: {}
        policy_loss: -0.007145642220469502
        total_loss: 1.2968174417813618
        vf_explained_var: 0.996628999710083
        vf_loss: 1.304013768831889
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.200000000000003
    gpu_util_percent0: 0.3926666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649930342339756
    mean_env_wait_ms: 1.2266754663541477
    mean_inference_ms: 4.301803342000382
    mean_raw_obs_processing_ms: 0.3775285692627222
  time_since_restore: 3830.8323929309845
  time_this_iter_s: 25.592630863189697
  time_total_s: 3830.8323929309845
  timers:
    learn_throughput: 8725.721
    learn_time_ms: 18541.964
    sample_throughput: 23682.482
    sample_time_ms: 6831.716
    update_time_ms: 24.484
  timestamp: 1602769202
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    150 |          3830.83 | 24268800 |  286.932 |              330.869 |              148.141 |             775.68 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3164.692158236718
    time_step_min: 2890
  date: 2020-10-15_13-40-28
  done: false
  episode_len_mean: 775.6446338825625
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 287.1625273666753
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 224
  episodes_total: 31438
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.846761401677896e-28
        cur_lr: 5.0e-05
        entropy: 0.0848199687898159
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031556356116198003
        model: {}
        policy_loss: -0.009742320529767312
        total_loss: 0.9486194103956223
        vf_explained_var: 0.99737948179245
        vf_loss: 0.9584041436513265
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.953333333333333
    gpu_util_percent0: 0.323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464937642260215
    mean_env_wait_ms: 1.226599836414832
    mean_inference_ms: 4.301490296670278
    mean_raw_obs_processing_ms: 0.37749906441717657
  time_since_restore: 3856.3679716587067
  time_this_iter_s: 25.535578727722168
  time_total_s: 3856.3679716587067
  timers:
    learn_throughput: 8728.068
    learn_time_ms: 18536.977
    sample_throughput: 23724.023
    sample_time_ms: 6819.754
    update_time_ms: 26.79
  timestamp: 1602769228
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    151 |          3856.37 | 24430592 |  287.163 |              330.869 |              148.141 |            775.645 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3163.5413934036687
    time_step_min: 2890
  date: 2020-10-15_13-40-54
  done: false
  episode_len_mean: 775.6115171650056
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 287.33704870249727
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 31605
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.423380700838948e-28
        cur_lr: 5.0e-05
        entropy: 0.08219215149680774
        entropy_coeff: 0.0005000000000000001
        kl: 0.003580312574437509
        model: {}
        policy_loss: -0.007695157158498962
        total_loss: 0.9779493461052576
        vf_explained_var: 0.9965912699699402
        vf_loss: 0.9856856067975363
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.824137931034482
    gpu_util_percent0: 0.3389655172413793
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648972780084943
    mean_env_wait_ms: 1.2265439545899373
    mean_inference_ms: 4.301275345450681
    mean_raw_obs_processing_ms: 0.377477824713123
  time_since_restore: 3881.5349159240723
  time_this_iter_s: 25.1669442653656
  time_total_s: 3881.5349159240723
  timers:
    learn_throughput: 8752.199
    learn_time_ms: 18485.868
    sample_throughput: 23637.593
    sample_time_ms: 6844.69
    update_time_ms: 27.311
  timestamp: 1602769254
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    152 |          3881.53 | 24592384 |  287.337 |              330.869 |              148.141 |            775.612 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3161.8918867924526
    time_step_min: 2887
  date: 2020-10-15_13-41-21
  done: false
  episode_len_mean: 775.5683688210539
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 287.58347941445743
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 237
  episodes_total: 31842
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.211690350419474e-28
        cur_lr: 5.0e-05
        entropy: 0.09854660121103127
        entropy_coeff: 0.0005000000000000001
        kl: 0.003806169998521606
        model: {}
        policy_loss: -0.007869369102991186
        total_loss: 1.393313229084015
        vf_explained_var: 0.9963212013244629
        vf_loss: 1.4012318054835002
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.550000000000004
    gpu_util_percent0: 0.32866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648433427233282
    mean_env_wait_ms: 1.2264671164749954
    mean_inference_ms: 4.300992918403709
    mean_raw_obs_processing_ms: 0.377448226492922
  time_since_restore: 3907.4768285751343
  time_this_iter_s: 25.94191265106201
  time_total_s: 3907.4768285751343
  timers:
    learn_throughput: 8743.988
    learn_time_ms: 18503.227
    sample_throughput: 23552.907
    sample_time_ms: 6869.301
    update_time_ms: 28.104
  timestamp: 1602769281
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    153 |          3907.48 | 24754176 |  287.583 |              330.869 |              148.141 |            775.568 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3160.3143597589533
    time_step_min: 2887
  date: 2020-10-15_13-41-47
  done: false
  episode_len_mean: 775.540584364963
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 287.8163432951235
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 227
  episodes_total: 32069
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.05845175209737e-29
        cur_lr: 5.0e-05
        entropy: 0.0831490270793438
        entropy_coeff: 0.0005000000000000001
        kl: 0.003834872778194646
        model: {}
        policy_loss: -0.006749346891107659
        total_loss: 1.0650120973587036
        vf_explained_var: 0.9970338940620422
        vf_loss: 1.0718030085166295
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.861290322580647
    gpu_util_percent0: 0.3651612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647897268000543
    mean_env_wait_ms: 1.22638985245185
    mean_inference_ms: 4.300694331506577
    mean_raw_obs_processing_ms: 0.37741979920204577
  time_since_restore: 3933.547583580017
  time_this_iter_s: 26.070755004882812
  time_total_s: 3933.547583580017
  timers:
    learn_throughput: 8743.896
    learn_time_ms: 18503.423
    sample_throughput: 23442.71
    sample_time_ms: 6901.591
    update_time_ms: 30.072
  timestamp: 1602769307
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    154 |          3933.55 | 24915968 |  287.816 |              330.869 |              148.141 |            775.541 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3159.221128160527
    time_step_min: 2883
  date: 2020-10-15_13-42-14
  done: false
  episode_len_mean: 775.5154175456012
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 287.985682924292
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 32236
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.029225876048685e-29
        cur_lr: 5.0e-05
        entropy: 0.08114595090349515
        entropy_coeff: 0.0005000000000000001
        kl: 0.003774195851292461
        model: {}
        policy_loss: -0.008072483581296789
        total_loss: 0.8922543823719025
        vf_explained_var: 0.9968358874320984
        vf_loss: 0.9003674238920212
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.020000000000003
    gpu_util_percent0: 0.3456666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464750465483579
    mean_env_wait_ms: 1.2263327539845261
    mean_inference_ms: 4.300483501256154
    mean_raw_obs_processing_ms: 0.3773991605449611
  time_since_restore: 3959.3656079769135
  time_this_iter_s: 25.818024396896362
  time_total_s: 3959.3656079769135
  timers:
    learn_throughput: 8743.435
    learn_time_ms: 18504.399
    sample_throughput: 23379.239
    sample_time_ms: 6920.328
    update_time_ms: 32.516
  timestamp: 1602769334
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    155 |          3959.37 | 25077760 |  287.986 |              330.869 |              148.141 |            775.515 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3157.66554606298
    time_step_min: 2883
  date: 2020-10-15_13-42-40
  done: false
  episode_len_mean: 775.479131372247
  episode_reward_max: 330.8686868686866
  episode_reward_mean: 288.22066498964705
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 229
  episodes_total: 32465
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5146129380243426e-29
        cur_lr: 5.0e-05
        entropy: 0.09478919021785259
        entropy_coeff: 0.0005000000000000001
        kl: 0.003536130883730948
        model: {}
        policy_loss: -0.005218010512180626
        total_loss: 1.058271552125613
        vf_explained_var: 0.9971089959144592
        vf_loss: 1.0635369718074799
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28666666666667
    gpu_util_percent0: 0.28366666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646980767923676
    mean_env_wait_ms: 1.2262582675526001
    mean_inference_ms: 4.300224350075473
    mean_raw_obs_processing_ms: 0.37737128077433485
  time_since_restore: 3985.0283060073853
  time_this_iter_s: 25.6626980304718
  time_total_s: 3985.0283060073853
  timers:
    learn_throughput: 8749.899
    learn_time_ms: 18490.728
    sample_throughput: 23373.928
    sample_time_ms: 6921.9
    update_time_ms: 33.34
  timestamp: 1602769360
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    156 |          3985.03 | 25239552 |  288.221 |              330.869 |              148.141 |            775.479 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3156.093603600845
    time_step_min: 2876
  date: 2020-10-15_13-43-06
  done: false
  episode_len_mean: 775.4401394452769
  episode_reward_max: 330.868686868687
  episode_reward_mean: 288.45701935411734
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 236
  episodes_total: 32701
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.573064690121713e-30
        cur_lr: 5.0e-05
        entropy: 0.08605722275873025
        entropy_coeff: 0.0005000000000000001
        kl: 0.004027818911708891
        model: {}
        policy_loss: -0.0069708311930298805
        total_loss: 1.1351486643155415
        vf_explained_var: 0.9967785477638245
        vf_loss: 1.142162521680196
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.031034482758624
    gpu_util_percent0: 0.3120689655172414
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646467994067905
    mean_env_wait_ms: 1.2261760508686192
    mean_inference_ms: 4.299928150828901
    mean_raw_obs_processing_ms: 0.37734280893070216
  time_since_restore: 4010.539032459259
  time_this_iter_s: 25.51072645187378
  time_total_s: 4010.539032459259
  timers:
    learn_throughput: 8746.211
    learn_time_ms: 18498.525
    sample_throughput: 23396.844
    sample_time_ms: 6915.121
    update_time_ms: 34.157
  timestamp: 1602769386
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    157 |          4010.54 | 25401344 |  288.457 |              330.869 |              148.141 |             775.44 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3155.0103271796747
    time_step_min: 2876
  date: 2020-10-15_13-43-32
  done: false
  episode_len_mean: 775.4136241937447
  episode_reward_max: 330.868686868687
  episode_reward_mean: 288.621502539082
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 32868
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7865323450608565e-30
        cur_lr: 5.0e-05
        entropy: 0.08249972698589166
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036210756516084075
        model: {}
        policy_loss: -0.007108744931732265
        total_loss: 0.8950663954019547
        vf_explained_var: 0.9968442916870117
        vf_loss: 0.9022163897752762
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180000000000003
    gpu_util_percent0: 0.31533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646099823410347
    mean_env_wait_ms: 1.2261183997592235
    mean_inference_ms: 4.299727161535354
    mean_raw_obs_processing_ms: 0.37732237177823413
  time_since_restore: 4036.2095811367035
  time_this_iter_s: 25.670548677444458
  time_total_s: 4036.2095811367035
  timers:
    learn_throughput: 8740.434
    learn_time_ms: 18510.752
    sample_throughput: 23424.236
    sample_time_ms: 6907.034
    update_time_ms: 36.002
  timestamp: 1602769412
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    158 |          4036.21 | 25563136 |  288.622 |              330.869 |              148.141 |            775.414 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3153.5723276148983
    time_step_min: 2876
  date: 2020-10-15_13-43-59
  done: false
  episode_len_mean: 775.3746411627836
  episode_reward_max: 330.868686868687
  episode_reward_mean: 288.8408662822588
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 225
  episodes_total: 33093
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8932661725304283e-30
        cur_lr: 5.0e-05
        entropy: 0.09176378386716048
        entropy_coeff: 0.0005000000000000001
        kl: 0.003305242028242598
        model: {}
        policy_loss: -0.007481873588403687
        total_loss: 0.8684664318958918
        vf_explained_var: 0.9976419806480408
        vf_loss: 0.8759941856066386
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.05333333333333
    gpu_util_percent0: 0.325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645590389365537
    mean_env_wait_ms: 1.226042731928298
    mean_inference_ms: 4.299474198161438
    mean_raw_obs_processing_ms: 0.3772962241133757
  time_since_restore: 4061.7625739574432
  time_this_iter_s: 25.552992820739746
  time_total_s: 4061.7625739574432
  timers:
    learn_throughput: 8744.036
    learn_time_ms: 18503.126
    sample_throughput: 23427.199
    sample_time_ms: 6906.161
    update_time_ms: 34.369
  timestamp: 1602769439
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    159 |          4061.76 | 25724928 |  288.841 |              330.869 |              148.141 |            775.375 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3152.004115349955
    time_step_min: 2876
  date: 2020-10-15_13-44-25
  done: false
  episode_len_mean: 775.3344233769351
  episode_reward_max: 330.868686868687
  episode_reward_mean: 289.0788880040051
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 239
  episodes_total: 33332
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.466330862652141e-31
        cur_lr: 5.0e-05
        entropy: 0.08194785006344318
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034339240713355443
        model: {}
        policy_loss: -0.00566804838005434
        total_loss: 0.8331520656744639
        vf_explained_var: 0.997617781162262
        vf_loss: 0.8388610829909643
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.776666666666664
    gpu_util_percent0: 0.3676666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645092332869825
    mean_env_wait_ms: 1.225959943238005
    mean_inference_ms: 4.299190620459416
    mean_raw_obs_processing_ms: 0.3772682962742494
  time_since_restore: 4087.287719964981
  time_this_iter_s: 25.525146007537842
  time_total_s: 4087.287719964981
  timers:
    learn_throughput: 8742.77
    learn_time_ms: 18505.805
    sample_throughput: 23470.434
    sample_time_ms: 6893.439
    update_time_ms: 34.935
  timestamp: 1602769465
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    160 |          4087.29 | 25886720 |  289.079 |              330.869 |              148.141 |            775.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3150.911919421364
    time_step_min: 2876
  date: 2020-10-15_13-44-51
  done: false
  episode_len_mean: 775.3038805970149
  episode_reward_max: 330.868686868687
  episode_reward_mean: 289.2471174430877
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 33500
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.733165431326071e-31
        cur_lr: 5.0e-05
        entropy: 0.07559455931186676
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033612361294217408
        model: {}
        policy_loss: -0.007125152352576454
        total_loss: 0.7582733233769735
        vf_explained_var: 0.9972205758094788
        vf_loss: 0.7654362618923187
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.786666666666672
    gpu_util_percent0: 0.35066666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644722083442024
    mean_env_wait_ms: 1.2259014134050212
    mean_inference_ms: 4.298996845834058
    mean_raw_obs_processing_ms: 0.37724893688114775
  time_since_restore: 4113.04004406929
  time_this_iter_s: 25.752324104309082
  time_total_s: 4113.04004406929
  timers:
    learn_throughput: 8740.354
    learn_time_ms: 18510.92
    sample_throughput: 23406.43
    sample_time_ms: 6912.289
    update_time_ms: 32.033
  timestamp: 1602769491
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    161 |          4113.04 | 26048512 |  289.247 |              330.869 |              148.141 |            775.304 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3149.458829468183
    time_step_min: 2876
  date: 2020-10-15_13-45-17
  done: false
  episode_len_mean: 775.2721314392479
  episode_reward_max: 330.868686868687
  episode_reward_mean: 289.45935645790337
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 219
  episodes_total: 33719
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3665827156630353e-31
        cur_lr: 5.0e-05
        entropy: 0.0876841563731432
        entropy_coeff: 0.0005000000000000001
        kl: 0.004020844819024205
        model: {}
        policy_loss: -0.00800268747843802
        total_loss: 0.9103001157442728
        vf_explained_var: 0.9974185824394226
        vf_loss: 0.9183466583490372
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303333333333335
    gpu_util_percent0: 0.40499999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644237639112415
    mean_env_wait_ms: 1.2258281986590391
    mean_inference_ms: 4.298762127861714
    mean_raw_obs_processing_ms: 0.37722408941216523
  time_since_restore: 4138.569007873535
  time_this_iter_s: 25.528963804244995
  time_total_s: 4138.569007873535
  timers:
    learn_throughput: 8720.924
    learn_time_ms: 18552.163
    sample_throughput: 23454.282
    sample_time_ms: 6898.186
    update_time_ms: 38.66
  timestamp: 1602769517
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    162 |          4138.57 | 26210304 |  289.459 |              330.869 |              148.141 |            775.272 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3147.838074462754
    time_step_min: 2876
  date: 2020-10-15_13-45-43
  done: false
  episode_len_mean: 775.24045340792
  episode_reward_max: 330.868686868687
  episode_reward_mean: 289.69889086656354
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 33965
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1832913578315177e-31
        cur_lr: 5.0e-05
        entropy: 0.08031827832261722
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030675732414238155
        model: {}
        policy_loss: -0.007438127109101818
        total_loss: 0.7285999059677124
        vf_explained_var: 0.9979429841041565
        vf_loss: 0.7360781927903494
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.07931034482759
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643750377515316
    mean_env_wait_ms: 1.2257406613439434
    mean_inference_ms: 4.298484601776927
    mean_raw_obs_processing_ms: 0.37719581376676126
  time_since_restore: 4163.994267463684
  time_this_iter_s: 25.425259590148926
  time_total_s: 4163.994267463684
  timers:
    learn_throughput: 8734.935
    learn_time_ms: 18522.405
    sample_throughput: 23525.682
    sample_time_ms: 6877.25
    update_time_ms: 36.255
  timestamp: 1602769543
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    163 |          4163.99 | 26372096 |  289.699 |              330.869 |              148.141 |             775.24 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3146.787034320915
    time_step_min: 2876
  date: 2020-10-15_13-46-09
  done: false
  episode_len_mean: 775.2195886558069
  episode_reward_max: 330.868686868687
  episode_reward_mean: 289.85721269888626
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 34132
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.916456789157588e-32
        cur_lr: 5.0e-05
        entropy: 0.07516709839304288
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035302324298148355
        model: {}
        policy_loss: -0.008195741581099961
        total_loss: 0.6124037603537241
        vf_explained_var: 0.9977982044219971
        vf_loss: 0.6206370989481608
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.273333333333333
    gpu_util_percent0: 0.3793333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643394115257072
    mean_env_wait_ms: 1.2256815697344128
    mean_inference_ms: 4.298297163280215
    mean_raw_obs_processing_ms: 0.37717694043132277
  time_since_restore: 4189.438168764114
  time_this_iter_s: 25.443901300430298
  time_total_s: 4189.438168764114
  timers:
    learn_throughput: 8743.955
    learn_time_ms: 18503.297
    sample_throughput: 23654.443
    sample_time_ms: 6839.814
    update_time_ms: 36.708
  timestamp: 1602769569
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: e00ea_00000
  
2020-10-15 13:46:10,546	WARNING util.py:136 -- The `process_trial` operation took 0.5023951530456543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    164 |          4189.44 | 26533888 |  289.857 |              330.869 |              148.141 |             775.22 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3145.4641711853537
    time_step_min: 2876
  date: 2020-10-15_13-46-36
  done: false
  episode_len_mean: 775.1809923130678
  episode_reward_max: 330.868686868687
  episode_reward_mean: 290.06004577571673
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 212
  episodes_total: 34344
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.958228394578794e-32
        cur_lr: 5.0e-05
        entropy: 0.07992465731998284
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034288139431737363
        model: {}
        policy_loss: -0.0053378159063868225
        total_loss: 0.6459816147883733
        vf_explained_var: 0.9981258511543274
        vf_loss: 0.6513593842585882
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.070000000000004
    gpu_util_percent0: 0.31299999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642940157396023
    mean_env_wait_ms: 1.2256084782700758
    mean_inference_ms: 4.298075077133244
    mean_raw_obs_processing_ms: 0.3771527884748154
  time_since_restore: 4215.1828520298
  time_this_iter_s: 25.744683265686035
  time_total_s: 4215.1828520298
  timers:
    learn_throughput: 8743.445
    learn_time_ms: 18504.377
    sample_throughput: 23684.0
    sample_time_ms: 6831.278
    update_time_ms: 36.489
  timestamp: 1602769596
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    165 |          4215.18 | 26695680 |   290.06 |              330.869 |              148.141 |            775.181 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3143.8640717696426
    time_step_min: 2876
  date: 2020-10-15_13-47-02
  done: false
  episode_len_mean: 775.1370928115155
  episode_reward_max: 330.868686868687
  episode_reward_mean: 290.3074935264722
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 253
  episodes_total: 34597
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.479114197289397e-32
        cur_lr: 5.0e-05
        entropy: 0.0745039414614439
        entropy_coeff: 0.0005000000000000001
        kl: 0.0027774287736974657
        model: {}
        policy_loss: -0.0054495337123322924
        total_loss: 0.5236395224928856
        vf_explained_var: 0.9985174536705017
        vf_loss: 0.5291263163089752
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36666666666667
    gpu_util_percent0: 0.3376666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642453160036728
    mean_env_wait_ms: 1.2255192185004802
    mean_inference_ms: 4.297804408322112
    mean_raw_obs_processing_ms: 0.3771252818784071
  time_since_restore: 4240.947828531265
  time_this_iter_s: 25.764976501464844
  time_total_s: 4240.947828531265
  timers:
    learn_throughput: 8735.931
    learn_time_ms: 18520.294
    sample_throughput: 23680.358
    sample_time_ms: 6832.329
    update_time_ms: 35.968
  timestamp: 1602769622
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: e00ea_00000
  
2020-10-15 13:47:03,378	WARNING util.py:136 -- The `process_trial` operation took 0.5023105144500732 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    166 |          4240.95 | 26857472 |  290.307 |              330.869 |              148.141 |            775.137 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3142.8206272860803
    time_step_min: 2876
  date: 2020-10-15_13-47-28
  done: false
  episode_len_mean: 775.1105485717574
  episode_reward_max: 330.868686868687
  episode_reward_mean: 290.46440384049345
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 166
  episodes_total: 34763
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.395570986446985e-33
        cur_lr: 5.0e-05
        entropy: 0.0705875214189291
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0070257936604321
        total_loss: .inf
        vf_explained_var: 0.9985671043395996
        vf_loss: 0.3941511735320091
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.660000000000007
    gpu_util_percent0: 0.377
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642108494855324
    mean_env_wait_ms: 1.225459977695414
    mean_inference_ms: 4.297622561976635
    mean_raw_obs_processing_ms: 0.37710715652499033
  time_since_restore: 4266.396694421768
  time_this_iter_s: 25.44886589050293
  time_total_s: 4266.396694421768
  timers:
    learn_throughput: 8746.96
    learn_time_ms: 18496.942
    sample_throughput: 23652.003
    sample_time_ms: 6840.52
    update_time_ms: 43.179
  timestamp: 1602769648
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    167 |           4266.4 | 27019264 |  290.464 |              330.869 |              148.141 |            775.111 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3141.540562203011
    time_step_min: 2876
  date: 2020-10-15_13-47-54
  done: false
  episode_len_mean: 775.0756804666057
  episode_reward_max: 330.8686868686871
  episode_reward_mean: 290.6619217102406
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 213
  episodes_total: 34976
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1093356479670482e-32
        cur_lr: 5.0e-05
        entropy: 0.08293207983175914
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00945990082497398
        total_loss: .inf
        vf_explained_var: 0.997946560382843
        vf_loss: 0.6908512761195501
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57931034482759
    gpu_util_percent0: 0.30241379310344835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641681180373864
    mean_env_wait_ms: 1.2253875774051777
    mean_inference_ms: 4.297413473491753
    mean_raw_obs_processing_ms: 0.3770841849229041
  time_since_restore: 4291.762845516205
  time_this_iter_s: 25.366151094436646
  time_total_s: 4291.762845516205
  timers:
    learn_throughput: 8766.522
    learn_time_ms: 18455.665
    sample_throughput: 23613.927
    sample_time_ms: 6851.55
    update_time_ms: 41.97
  timestamp: 1602769674
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    168 |          4291.76 | 27181056 |  290.662 |              330.869 |              148.141 |            775.076 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3140.016881714318
    time_step_min: 2876
  date: 2020-10-15_13-48-20
  done: false
  episode_len_mean: 775.0434597479278
  episode_reward_max: 330.8686868686871
  episode_reward_mean: 290.8849919084108
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 252
  episodes_total: 35228
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6640034719505716e-32
        cur_lr: 5.0e-05
        entropy: 0.08373476068178813
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009047865363148352
        total_loss: .inf
        vf_explained_var: 0.9973202347755432
        vf_loss: 0.9822424153486887
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.646666666666665
    gpu_util_percent0: 0.38333333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641202280315574
    mean_env_wait_ms: 1.2252972709477397
    mean_inference_ms: 4.29714946452555
    mean_raw_obs_processing_ms: 0.3770567919853956
  time_since_restore: 4316.693459033966
  time_this_iter_s: 24.93061351776123
  time_total_s: 4316.693459033966
  timers:
    learn_throughput: 8791.496
    learn_time_ms: 18403.24
    sample_throughput: 23647.422
    sample_time_ms: 6841.845
    update_time_ms: 41.451
  timestamp: 1602769700
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    169 |          4316.69 | 27342848 |  290.885 |              330.869 |              148.141 |            775.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3138.960908550901
    time_step_min: 2876
  date: 2020-10-15_13-48-46
  done: false
  episode_len_mean: 775.0192965108066
  episode_reward_max: 330.8686868686871
  episode_reward_mean: 291.04150275177267
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 35395
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4960052079258576e-32
        cur_lr: 5.0e-05
        entropy: 0.07049994605282943
        entropy_coeff: 0.0005000000000000001
        kl: 0.003927169367671013
        model: {}
        policy_loss: -0.007383769329559679
        total_loss: 0.5762894327441851
        vf_explained_var: 0.9978378415107727
        vf_loss: 0.5837084551652273
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303333333333338
    gpu_util_percent0: 0.3403333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640872474241595
    mean_env_wait_ms: 1.2252378653192497
    mean_inference_ms: 4.296977001263383
    mean_raw_obs_processing_ms: 0.3770386764200698
  time_since_restore: 4342.355424642563
  time_this_iter_s: 25.6619656085968
  time_total_s: 4342.355424642563
  timers:
    learn_throughput: 8781.821
    learn_time_ms: 18423.515
    sample_throughput: 23665.562
    sample_time_ms: 6836.601
    update_time_ms: 41.305
  timestamp: 1602769726
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    170 |          4342.36 | 27504640 |  291.042 |              330.869 |              148.141 |            775.019 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3137.678148637629
    time_step_min: 2876
  date: 2020-10-15_13-49-13
  done: false
  episode_len_mean: 774.9850021064457
  episode_reward_max: 330.8686868686871
  episode_reward_mean: 291.23382540472846
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 210
  episodes_total: 35605
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2480026039629288e-32
        cur_lr: 5.0e-05
        entropy: 0.08089654520154
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036276256820807853
        model: {}
        policy_loss: -0.00870161322139514
        total_loss: 0.9646257907152176
        vf_explained_var: 0.9970672726631165
        vf_loss: 0.9733678549528122
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666665
    gpu_util_percent0: 0.31799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640477319256368
    mean_env_wait_ms: 1.2251650453808207
    mean_inference_ms: 4.296772923881308
    mean_raw_obs_processing_ms: 0.3770168180609749
  time_since_restore: 4367.861779689789
  time_this_iter_s: 25.506355047225952
  time_total_s: 4367.861779689789
  timers:
    learn_throughput: 8782.826
    learn_time_ms: 18421.406
    sample_throughput: 23751.31
    sample_time_ms: 6811.919
    update_time_ms: 40.959
  timestamp: 1602769753
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: e00ea_00000
  
2020-10-15 13:49:14,078	WARNING util.py:136 -- The `process_trial` operation took 0.5162417888641357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    171 |          4367.86 | 27666432 |  291.234 |              330.869 |              148.141 |            774.985 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3136.1984477078563
    time_step_min: 2876
  date: 2020-10-15_13-49-39
  done: false
  episode_len_mean: 774.9407138873396
  episode_reward_max: 330.8686868686871
  episode_reward_mean: 291.46813505946255
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 255
  episodes_total: 35860
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.240013019814644e-33
        cur_lr: 5.0e-05
        entropy: 0.07843116112053394
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007926601499396687
        total_loss: .inf
        vf_explained_var: 0.998059093952179
        vf_loss: 0.6947798530260721
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.717241379310348
    gpu_util_percent0: 0.33862068965517234
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639984398478492
    mean_env_wait_ms: 1.2250738315791059
    mean_inference_ms: 4.296519177672362
    mean_raw_obs_processing_ms: 0.37698984655510026
  time_since_restore: 4393.322255849838
  time_this_iter_s: 25.46047616004944
  time_total_s: 4393.322255849838
  timers:
    learn_throughput: 8774.689
    learn_time_ms: 18438.487
    sample_throughput: 23813.257
    sample_time_ms: 6794.199
    update_time_ms: 35.452
  timestamp: 1602769779
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 13:49:40,373	WARNING util.py:136 -- The `process_trial` operation took 0.5115346908569336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    172 |          4393.32 | 27828224 |  291.468 |              330.869 |              148.141 |            774.941 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3135.164179519244
    time_step_min: 2876
  date: 2020-10-15_13-50-05
  done: false
  episode_len_mean: 774.9148138895828
  episode_reward_max: 330.8686868686871
  episode_reward_mean: 291.62734346546495
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 167
  episodes_total: 36027
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.360019529721969e-33
        cur_lr: 5.0e-05
        entropy: 0.06605113856494427
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005375528193932648
        total_loss: .inf
        vf_explained_var: 0.9975871443748474
        vf_loss: 0.6480246633291245
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.513333333333332
    gpu_util_percent0: 0.3256666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463966350917303
    mean_env_wait_ms: 1.2250140132426597
    mean_inference_ms: 4.296352651150001
    mean_raw_obs_processing_ms: 0.3769720838581476
  time_since_restore: 4418.672644853592
  time_this_iter_s: 25.350389003753662
  time_total_s: 4418.672644853592
  timers:
    learn_throughput: 8776.232
    learn_time_ms: 18435.247
    sample_throughput: 23832.397
    sample_time_ms: 6788.742
    update_time_ms: 35.502
  timestamp: 1602769805
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    173 |          4418.67 | 27990016 |  291.627 |              330.869 |              148.141 |            774.915 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3133.862877825054
    time_step_min: 2876
  date: 2020-10-15_13-50-31
  done: false
  episode_len_mean: 774.8759520918424
  episode_reward_max: 331.0202020202021
  episode_reward_mean: 291.825169121394
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 209
  episodes_total: 36236
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4040029294582948e-32
        cur_lr: 5.0e-05
        entropy: 0.07599821252127488
        entropy_coeff: 0.0005000000000000001
        kl: 0.004060946016882856
        model: {}
        policy_loss: -0.005916131184979652
        total_loss: 0.7025892386833826
        vf_explained_var: 0.99775630235672
        vf_loss: 0.7085433552662531
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.460000000000004
    gpu_util_percent0: 0.36466666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639278229189048
    mean_env_wait_ms: 1.2249404873185954
    mean_inference_ms: 4.2961499084401344
    mean_raw_obs_processing_ms: 0.3769504045045208
  time_since_restore: 4443.8948748111725
  time_this_iter_s: 25.222229957580566
  time_total_s: 4443.8948748111725
  timers:
    learn_throughput: 8790.132
    learn_time_ms: 18406.095
    sample_throughput: 23829.119
    sample_time_ms: 6789.676
    update_time_ms: 32.884
  timestamp: 1602769831
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    174 |          4443.89 | 28151808 |  291.825 |               331.02 |              148.141 |            774.876 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3132.322414266118
    time_step_min: 2876
  date: 2020-10-15_13-50-57
  done: false
  episode_len_mean: 774.8277156637071
  episode_reward_max: 331.1717171717172
  episode_reward_mean: 292.0609074965373
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 256
  episodes_total: 36492
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.020014647291474e-33
        cur_lr: 5.0e-05
        entropy: 0.07520558622976144
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038086530985310674
        model: {}
        policy_loss: -0.008564828841675384
        total_loss: 0.7744988997777303
        vf_explained_var: 0.9977824091911316
        vf_loss: 0.7831013351678848
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.562068965517245
    gpu_util_percent0: 0.34689655172413797
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8758620689655183
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638798868213088
    mean_env_wait_ms: 1.2248494441330373
    mean_inference_ms: 4.295910792164497
    mean_raw_obs_processing_ms: 0.37692451604537214
  time_since_restore: 4469.41787981987
  time_this_iter_s: 25.52300500869751
  time_total_s: 4469.41787981987
  timers:
    learn_throughput: 8796.592
    learn_time_ms: 18392.577
    sample_throughput: 23860.829
    sample_time_ms: 6780.653
    update_time_ms: 32.159
  timestamp: 1602769857
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: e00ea_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    175 |          4469.42 | 28313600 |  292.061 |              331.172 |              148.141 |            774.828 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3131.250286744224
    time_step_min: 2875
  date: 2020-10-15_13-51-23
  done: false
  episode_len_mean: 774.7886797599564
  episode_reward_max: 331.1717171717172
  episode_reward_mean: 292.22374729289635
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 168
  episodes_total: 36660
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.510007323645737e-33
        cur_lr: 5.0e-05
        entropy: 0.06557841536899407
        entropy_coeff: 0.0005000000000000001
        kl: 0.003728820476680994
        model: {}
        policy_loss: -0.007037234405288473
        total_loss: 0.3900524005293846
        vf_explained_var: 0.9984256625175476
        vf_loss: 0.39712242782115936
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.413333333333338
    gpu_util_percent0: 0.41300000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638479395954662
    mean_env_wait_ms: 1.2247890856931625
    mean_inference_ms: 4.295750034844325
    mean_raw_obs_processing_ms: 0.3769075361720712
  time_since_restore: 4494.750263214111
  time_this_iter_s: 25.332383394241333
  time_total_s: 4494.750263214111
  timers:
    learn_throughput: 8810.76
    learn_time_ms: 18363.001
    sample_throughput: 23904.975
    sample_time_ms: 6768.131
    update_time_ms: 31.736
  timestamp: 1602769883
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: e00ea_00000
  
2020-10-15 13:51:24,554	WARNING util.py:136 -- The `process_trial` operation took 0.5140101909637451 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    176 |          4494.75 | 28475392 |  292.224 |              331.172 |              148.141 |            774.789 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3129.906133753292
    time_step_min: 2875
  date: 2020-10-15_13-51-50
  done: false
  episode_len_mean: 774.7380868433186
  episode_reward_max: 331.1717171717172
  episode_reward_mean: 292.42671103648576
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 211
  episodes_total: 36871
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7550036618228685e-33
        cur_lr: 5.0e-05
        entropy: 0.07281669974327087
        entropy_coeff: 0.0005000000000000001
        kl: 0.003727357465929041
        model: {}
        policy_loss: -0.005805032046434159
        total_loss: 0.5829905892411867
        vf_explained_var: 0.9980922341346741
        vf_loss: 0.5888320257266363
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.573333333333338
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638103143770217
    mean_env_wait_ms: 1.2247144356194655
    mean_inference_ms: 4.295557948038935
    mean_raw_obs_processing_ms: 0.3768862922841776
  time_since_restore: 4520.458950519562
  time_this_iter_s: 25.70868730545044
  time_total_s: 4520.458950519562
  timers:
    learn_throughput: 8794.843
    learn_time_ms: 18396.234
    sample_throughput: 23913.451
    sample_time_ms: 6765.732
    update_time_ms: 25.969
  timestamp: 1602769910
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: e00ea_00000
  
2020-10-15 13:51:50,938	WARNING util.py:136 -- The `process_trial` operation took 0.5008447170257568 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    177 |          4520.46 | 28637184 |  292.427 |              331.172 |              148.141 |            774.738 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3128.341108893803
    time_step_min: 2874
  date: 2020-10-15_13-52-16
  done: false
  episode_len_mean: 774.6898502316561
  episode_reward_max: 331.17171717171726
  episode_reward_mean: 292.6538385144409
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 253
  episodes_total: 37124
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.775018309114342e-34
        cur_lr: 5.0e-05
        entropy: 0.07681776272753875
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007641759924202536
        total_loss: .inf
        vf_explained_var: 0.9978904128074646
        vf_loss: 0.7363540331522623
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14666666666667
    gpu_util_percent0: 0.33966666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637634791212645
    mean_env_wait_ms: 1.224624024151073
    mean_inference_ms: 4.2953256300713285
    mean_raw_obs_processing_ms: 0.3768603400177355
  time_since_restore: 4545.993220567703
  time_this_iter_s: 25.53427004814148
  time_total_s: 4545.993220567703
  timers:
    learn_throughput: 8783.526
    learn_time_ms: 18419.937
    sample_throughput: 23941.748
    sample_time_ms: 6757.736
    update_time_ms: 25.17
  timestamp: 1602769936
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    178 |          4545.99 | 28798976 |  292.654 |              331.172 |              148.141 |             774.69 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3127.2995892727713
    time_step_min: 2874
  date: 2020-10-15_13-52-42
  done: false
  episode_len_mean: 774.6519722199877
  episode_reward_max: 331.1717171717173
  episode_reward_mean: 292.8090055083862
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 169
  episodes_total: 37293
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3162527463671513e-33
        cur_lr: 5.0e-05
        entropy: 0.06888708720604579
        entropy_coeff: 0.0005000000000000001
        kl: 0.003958480607252568
        model: {}
        policy_loss: -0.008244968682750672
        total_loss: 0.4322718655069669
        vf_explained_var: 0.9982401728630066
        vf_loss: 0.4405512710412343
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.686666666666664
    gpu_util_percent0: 0.3106666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463733945841134
    mean_env_wait_ms: 1.224563375161939
    mean_inference_ms: 4.295170026932792
    mean_raw_obs_processing_ms: 0.376843883016314
  time_since_restore: 4571.516189098358
  time_this_iter_s: 25.522968530654907
  time_total_s: 4571.516189098358
  timers:
    learn_throughput: 8761.147
    learn_time_ms: 18466.989
    sample_throughput: 23903.015
    sample_time_ms: 6768.686
    update_time_ms: 25.419
  timestamp: 1602769962
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: e00ea_00000
  
2020-10-15 13:52:43,498	WARNING util.py:136 -- The `process_trial` operation took 0.5210380554199219 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    179 |          4571.52 | 28960768 |  292.809 |              331.172 |              148.141 |            774.652 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3126.0683304417457
    time_step_min: 2874
  date: 2020-10-15_13-53-09
  done: false
  episode_len_mean: 774.6077265577093
  episode_reward_max: 331.1717171717173
  episode_reward_mean: 293.0006565777756
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 214
  episodes_total: 37507
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.581263731835756e-34
        cur_lr: 5.0e-05
        entropy: 0.08224130235612392
        entropy_coeff: 0.0005000000000000001
        kl: 0.004885429788070421
        model: {}
        policy_loss: -0.007224859392105524
        total_loss: 0.6555275668700536
        vf_explained_var: 0.9979074001312256
        vf_loss: 0.6627935568491617
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.979999999999997
    gpu_util_percent0: 0.3583333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636956859860967
    mean_env_wait_ms: 1.2244872885238234
    mean_inference_ms: 4.294980439692531
    mean_raw_obs_processing_ms: 0.376822540357238
  time_since_restore: 4597.0365126132965
  time_this_iter_s: 25.520323514938354
  time_total_s: 4597.0365126132965
  timers:
    learn_throughput: 8774.844
    learn_time_ms: 18438.163
    sample_throughput: 23882.643
    sample_time_ms: 6774.46
    update_time_ms: 24.113
  timestamp: 1602769989
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: e00ea_00000
  
2020-10-15 13:53:09,728	WARNING util.py:136 -- The `process_trial` operation took 0.5040268898010254 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    180 |          4597.04 | 29122560 |  293.001 |              331.172 |              148.141 |            774.608 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3124.7144561701225
    time_step_min: 2874
  date: 2020-10-15_13-53-35
  done: false
  episode_len_mean: 774.5733923085072
  episode_reward_max: 331.1717171717173
  episode_reward_mean: 293.2106345262136
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 249
  episodes_total: 37756
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.290631865917878e-34
        cur_lr: 5.0e-05
        entropy: 0.08150406678517659
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050049750522399945
        model: {}
        policy_loss: -0.0076339625132580595
        total_loss: 0.7481476763884226
        vf_explained_var: 0.9978148937225342
        vf_loss: 0.7558223903179169
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.186206896551724
    gpu_util_percent0: 0.2634482758620689
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.872413793103449
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636515261239688
    mean_env_wait_ms: 1.2243984409158797
    mean_inference_ms: 4.294758302865494
    mean_raw_obs_processing_ms: 0.3767979167356438
  time_since_restore: 4622.49623298645
  time_this_iter_s: 25.459720373153687
  time_total_s: 4622.49623298645
  timers:
    learn_throughput: 8783.095
    learn_time_ms: 18420.84
    sample_throughput: 23834.993
    sample_time_ms: 6788.003
    update_time_ms: 24.283
  timestamp: 1602770015
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: e00ea_00000
  
2020-10-15 13:53:35,952	WARNING util.py:136 -- The `process_trial` operation took 0.5692183971405029 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    181 |           4622.5 | 29284352 |  293.211 |              331.172 |              148.141 |            774.573 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3123.749386268247
    time_step_min: 2872
  date: 2020-10-15_13-54-01
  done: false
  episode_len_mean: 774.5387475280158
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 293.359452135062
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 169
  episodes_total: 37925
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.290631865917878e-34
        cur_lr: 5.0e-05
        entropy: 0.06995855954786141
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032990581433599195
        model: {}
        policy_loss: -0.008678308581390107
        total_loss: 0.5161976193388303
        vf_explained_var: 0.9980010390281677
        vf_loss: 0.5249109069506327
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08333333333333
    gpu_util_percent0: 0.362
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636228165419898
    mean_env_wait_ms: 1.2243379468130908
    mean_inference_ms: 4.294608088721671
    mean_raw_obs_processing_ms: 0.3767819355423082
  time_since_restore: 4647.957943201065
  time_this_iter_s: 25.461710214614868
  time_total_s: 4647.957943201065
  timers:
    learn_throughput: 8787.706
    learn_time_ms: 18411.175
    sample_throughput: 23800.005
    sample_time_ms: 6797.982
    update_time_ms: 22.424
  timestamp: 1602770041
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: e00ea_00000
  
2020-10-15 13:54:02,122	WARNING util.py:136 -- The `process_trial` operation took 0.5329766273498535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    182 |          4647.96 | 29446144 |  293.359 |              331.475 |              148.141 |            774.539 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3122.4912851742965
    time_step_min: 2872
  date: 2020-10-15_13-54-27
  done: false
  episode_len_mean: 774.4934710787142
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 293.54312515262234
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 213
  episodes_total: 38138
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.645315932958939e-34
        cur_lr: 5.0e-05
        entropy: 0.07950389012694359
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036221142703046403
        model: {}
        policy_loss: -0.006147207488538697
        total_loss: 0.9637339065472285
        vf_explained_var: 0.9969474673271179
        vf_loss: 0.9699208786090215
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.310000000000002
    gpu_util_percent0: 0.36599999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635846745023948
    mean_env_wait_ms: 1.2242620957035273
    mean_inference_ms: 4.2944193773699135
    mean_raw_obs_processing_ms: 0.3767607728025542
  time_since_restore: 4673.753927707672
  time_this_iter_s: 25.795984506607056
  time_total_s: 4673.753927707672
  timers:
    learn_throughput: 8774.973
    learn_time_ms: 18437.892
    sample_throughput: 23744.051
    sample_time_ms: 6814.001
    update_time_ms: 23.93
  timestamp: 1602770067
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: e00ea_00000
  
2020-10-15 13:54:28,631	WARNING util.py:136 -- The `process_trial` operation took 0.532099723815918 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    183 |          4673.75 | 29607936 |  293.543 |              331.475 |              148.141 |            774.493 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3121.0808950086057
    time_step_min: 2872
  date: 2020-10-15_13-54-54
  done: false
  episode_len_mean: 774.4509482129832
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 293.75774073968824
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 250
  episodes_total: 38388
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.226579664794695e-35
        cur_lr: 5.0e-05
        entropy: 0.0785625521093607
        entropy_coeff: 0.0005000000000000001
        kl: 0.004517979163210839
        model: {}
        policy_loss: -0.008529699979893243
        total_loss: 0.6240429729223251
        vf_explained_var: 0.9981080889701843
        vf_loss: 0.632611965139707
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44333333333333
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635425745856365
    mean_env_wait_ms: 1.2241733851192127
    mean_inference_ms: 4.294210881492186
    mean_raw_obs_processing_ms: 0.37673710721519016
  time_since_restore: 4699.125903606415
  time_this_iter_s: 25.371975898742676
  time_total_s: 4699.125903606415
  timers:
    learn_throughput: 8765.844
    learn_time_ms: 18457.093
    sample_throughput: 23733.166
    sample_time_ms: 6817.127
    update_time_ms: 24.251
  timestamp: 1602770094
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: e00ea_00000
  
2020-10-15 13:54:54,725	WARNING util.py:136 -- The `process_trial` operation took 0.5457925796508789 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    184 |          4699.13 | 29769728 |  293.758 |              331.475 |              148.141 |            774.451 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3120.0805660132414
    time_step_min: 2872
  date: 2020-10-15_13-55-20
  done: false
  episode_len_mean: 774.4198200067433
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 293.905391283481
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 169
  episodes_total: 38557
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1132898323973477e-35
        cur_lr: 5.0e-05
        entropy: 0.07235608498255412
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006204881676239893
        total_loss: .inf
        vf_explained_var: 0.9979117512702942
        vf_loss: 0.5295939420660337
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29
    gpu_util_percent0: 0.35766666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635147810511065
    mean_env_wait_ms: 1.2241130110919225
    mean_inference_ms: 4.294066647040766
    mean_raw_obs_processing_ms: 0.37672149042505365
  time_since_restore: 4724.99383354187
  time_this_iter_s: 25.867929935455322
  time_total_s: 4724.99383354187
  timers:
    learn_throughput: 8759.672
    learn_time_ms: 18470.099
    sample_throughput: 23656.147
    sample_time_ms: 6839.322
    update_time_ms: 22.53
  timestamp: 1602770120
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 13:55:21,344	WARNING util.py:136 -- The `process_trial` operation took 0.5683913230895996 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    185 |          4724.99 | 29931520 |  293.905 |              331.475 |              148.141 |             774.42 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3118.8497069530868
    time_step_min: 2872
  date: 2020-10-15_13-55-46
  done: false
  episode_len_mean: 774.3736620844402
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 294.08218126380257
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 216
  episodes_total: 38773
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.169934748596024e-35
        cur_lr: 5.0e-05
        entropy: 0.0796990618109703
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035848563614611826
        model: {}
        policy_loss: -0.006379160127835348
        total_loss: 0.9458130051692327
        vf_explained_var: 0.9971521496772766
        vf_loss: 0.9522320081790289
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.643333333333338
    gpu_util_percent0: 0.35533333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634780423626836
    mean_env_wait_ms: 1.2240367747687184
    mean_inference_ms: 4.293889123705724
    mean_raw_obs_processing_ms: 0.3767009001983678
  time_since_restore: 4750.644953966141
  time_this_iter_s: 25.65112042427063
  time_total_s: 4750.644953966141
  timers:
    learn_throughput: 8751.933
    learn_time_ms: 18486.431
    sample_throughput: 23612.521
    sample_time_ms: 6851.958
    update_time_ms: 24.915
  timestamp: 1602770146
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: e00ea_00000
  
2020-10-15 13:55:47,710	WARNING util.py:136 -- The `process_trial` operation took 0.5370879173278809 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    186 |          4750.64 | 30093312 |  294.082 |              331.475 |              148.141 |            774.374 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3117.4917902406487
    time_step_min: 2872
  date: 2020-10-15_13-56-13
  done: false
  episode_len_mean: 774.3212455151205
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 294.2845808158469
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 247
  episodes_total: 39020
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.084967374298012e-35
        cur_lr: 5.0e-05
        entropy: 0.07462168236573537
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007865464285714552
        total_loss: .inf
        vf_explained_var: 0.9977148175239563
        vf_loss: 0.7568746507167816
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.050000000000008
    gpu_util_percent0: 0.3736666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634370167793884
    mean_env_wait_ms: 1.2239484803080467
    mean_inference_ms: 4.293686089628902
    mean_raw_obs_processing_ms: 0.37667750347169415
  time_since_restore: 4776.1212730407715
  time_this_iter_s: 25.476319074630737
  time_total_s: 4776.1212730407715
  timers:
    learn_throughput: 8766.522
    learn_time_ms: 18455.665
    sample_throughput: 23614.867
    sample_time_ms: 6851.277
    update_time_ms: 30.524
  timestamp: 1602770173
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 13:56:13,881	WARNING util.py:136 -- The `process_trial` operation took 0.517064094543457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    187 |          4776.12 | 30255104 |  294.285 |              331.475 |              148.141 |            774.321 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3116.5098730426344
    time_step_min: 2872
  date: 2020-10-15_13-56-39
  done: false
  episode_len_mean: 774.2794151420042
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 294.4315256986926
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 169
  episodes_total: 39189
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.627451061447017e-35
        cur_lr: 5.0e-05
        entropy: 0.06133866682648659
        entropy_coeff: 0.0005000000000000001
        kl: 0.003834791151651492
        model: {}
        policy_loss: -0.0069690271726964665
        total_loss: 0.34655571232239407
        vf_explained_var: 0.9985783100128174
        vf_loss: 0.3535554111003876
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56666666666667
    gpu_util_percent0: 0.2996666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634099703718567
    mean_env_wait_ms: 1.2238882346076358
    mean_inference_ms: 4.293546780113056
    mean_raw_obs_processing_ms: 0.3766621572464309
  time_since_restore: 4801.4483852386475
  time_this_iter_s: 25.327112197875977
  time_total_s: 4801.4483852386475
  timers:
    learn_throughput: 8776.393
    learn_time_ms: 18434.908
    sample_throughput: 23612.321
    sample_time_ms: 6852.016
    update_time_ms: 30.441
  timestamp: 1602770199
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: e00ea_00000
  
2020-10-15 13:56:40,146	WARNING util.py:136 -- The `process_trial` operation took 0.5809640884399414 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    188 |          4801.45 | 30416896 |  294.432 |              331.475 |              148.141 |            774.279 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3115.2492188889173
    time_step_min: 2872
  date: 2020-10-15_13-57-05
  done: false
  episode_len_mean: 774.2404526884721
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 294.6206639974308
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 220
  episodes_total: 39409
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3137255307235084e-35
        cur_lr: 5.0e-05
        entropy: 0.0718292376647393
        entropy_coeff: 0.0005000000000000001
        kl: 0.004334574759316941
        model: {}
        policy_loss: -0.007363657413710219
        total_loss: 0.5311771283547083
        vf_explained_var: 0.9983143210411072
        vf_loss: 0.5385767047603925
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.563333333333336
    gpu_util_percent0: 0.35
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463372452851558
    mean_env_wait_ms: 1.2238112884683274
    mean_inference_ms: 4.293373219312625
    mean_raw_obs_processing_ms: 0.3766421072463691
  time_since_restore: 4827.1761384010315
  time_this_iter_s: 25.727753162384033
  time_total_s: 4827.1761384010315
  timers:
    learn_throughput: 8762.076
    learn_time_ms: 18465.031
    sample_throughput: 23658.405
    sample_time_ms: 6838.669
    update_time_ms: 31.828
  timestamp: 1602770225
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: e00ea_00000
  
2020-10-15 13:57:06,583	WARNING util.py:136 -- The `process_trial` operation took 0.5249922275543213 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    189 |          4827.18 | 30578688 |  294.621 |              331.475 |              148.141 |             774.24 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3113.8751325422872
    time_step_min: 2872
  date: 2020-10-15_13-57-32
  done: false
  episode_len_mean: 774.1941137899728
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 294.8237053782046
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 243
  episodes_total: 39652
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1568627653617542e-35
        cur_lr: 5.0e-05
        entropy: 0.07047714665532112
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032472373762478433
        model: {}
        policy_loss: -0.008083875368659696
        total_loss: 0.4777173548936844
        vf_explained_var: 0.9985159039497375
        vf_loss: 0.4858364736040433
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.050000000000004
    gpu_util_percent0: 0.3373333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146333413514689
    mean_env_wait_ms: 1.223724018694214
    mean_inference_ms: 4.293179044962675
    mean_raw_obs_processing_ms: 0.3766190847234714
  time_since_restore: 4852.91405582428
  time_this_iter_s: 25.73791742324829
  time_total_s: 4852.91405582428
  timers:
    learn_throughput: 8749.669
    learn_time_ms: 18491.213
    sample_throughput: 23655.01
    sample_time_ms: 6839.651
    update_time_ms: 34.178
  timestamp: 1602770252
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: e00ea_00000
  
2020-10-15 13:57:33,069	WARNING util.py:136 -- The `process_trial` operation took 0.5604312419891357 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    190 |          4852.91 | 30740480 |  294.824 |              331.475 |              148.141 |            774.194 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3112.9201608848666
    time_step_min: 2872
  date: 2020-10-15_13-57-58
  done: false
  episode_len_mean: 774.1629250163227
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 294.9637812508086
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 39822
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.784313826808771e-36
        cur_lr: 5.0e-05
        entropy: 0.0654220034678777
        entropy_coeff: 0.0005000000000000001
        kl: 0.0030009144102223217
        model: {}
        policy_loss: -0.007312377689231653
        total_loss: 0.31845803807179135
        vf_explained_var: 0.9987208843231201
        vf_loss: 0.32580312589804333
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.820000000000004
    gpu_util_percent0: 0.36833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633082076846446
    mean_env_wait_ms: 1.2236634852269654
    mean_inference_ms: 4.293044403291696
    mean_raw_obs_processing_ms: 0.3766041467205052
  time_since_restore: 4878.4274933338165
  time_this_iter_s: 25.513437509536743
  time_total_s: 4878.4274933338165
  timers:
    learn_throughput: 8742.169
    learn_time_ms: 18507.077
    sample_throughput: 23698.919
    sample_time_ms: 6826.978
    update_time_ms: 35.332
  timestamp: 1602770278
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: e00ea_00000
  
2020-10-15 13:57:59,312	WARNING util.py:136 -- The `process_trial` operation took 0.5457606315612793 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    191 |          4878.43 | 30902272 |  294.964 |              331.475 |              148.141 |            774.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3111.7144892755364
    time_step_min: 2872
  date: 2020-10-15_13-58-24
  done: false
  episode_len_mean: 774.113824792728
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 295.14920557084184
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 222
  episodes_total: 40044
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8921569134043856e-36
        cur_lr: 5.0e-05
        entropy: 0.07046300855775674
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031601726659573615
        model: {}
        policy_loss: -0.0065882356915002065
        total_loss: 0.4586857110261917
        vf_explained_var: 0.9985098838806152
        vf_loss: 0.4653091753522555
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.87666666666667
    gpu_util_percent0: 0.3463333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632699755972828
    mean_env_wait_ms: 1.2235854431714497
    mean_inference_ms: 4.292878119422153
    mean_raw_obs_processing_ms: 0.376584053652849
  time_since_restore: 4903.8628923892975
  time_this_iter_s: 25.435399055480957
  time_total_s: 4903.8628923892975
  timers:
    learn_throughput: 8741.395
    learn_time_ms: 18508.716
    sample_throughput: 23719.065
    sample_time_ms: 6821.179
    update_time_ms: 35.594
  timestamp: 1602770304
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: e00ea_00000
  
2020-10-15 13:58:25,488	WARNING util.py:136 -- The `process_trial` operation took 0.5492982864379883 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    192 |          4903.86 | 31064064 |  295.149 |              331.475 |              148.141 |            774.114 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3110.4064658814173
    time_step_min: 2872
  date: 2020-10-15_13-58-51
  done: false
  episode_len_mean: 774.0680667262436
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 295.3434952744605
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 240
  episodes_total: 40284
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4460784567021928e-36
        cur_lr: 5.0e-05
        entropy: 0.06992006798585255
        entropy_coeff: 0.0005000000000000001
        kl: 0.005614075887327393
        model: {}
        policy_loss: -0.00772686158597935
        total_loss: 0.4603702425956726
        vf_explained_var: 0.9985339045524597
        vf_loss: 0.46813206374645233
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.526666666666664
    gpu_util_percent0: 0.3393333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632338055182068
    mean_env_wait_ms: 1.2234998210220194
    mean_inference_ms: 4.2926896051685
    mean_raw_obs_processing_ms: 0.37656187171166683
  time_since_restore: 4929.3943412303925
  time_this_iter_s: 25.53144884109497
  time_total_s: 4929.3943412303925
  timers:
    learn_throughput: 8747.979
    learn_time_ms: 18494.787
    sample_throughput: 23756.971
    sample_time_ms: 6810.296
    update_time_ms: 33.814
  timestamp: 1602770331
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: e00ea_00000
  
2020-10-15 13:58:51,758	WARNING util.py:136 -- The `process_trial` operation took 0.5513811111450195 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    193 |          4929.39 | 31225856 |  295.343 |              331.475 |              148.141 |            774.068 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3109.526106107097
    time_step_min: 2872
  date: 2020-10-15_13-59-17
  done: false
  episode_len_mean: 774.0329262866466
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 295.4767078008044
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 40454
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4460784567021928e-36
        cur_lr: 5.0e-05
        entropy: 0.06956416182219982
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041238454869017005
        model: {}
        policy_loss: -0.007431748478362958
        total_loss: 0.52067764600118
        vf_explained_var: 0.9979260563850403
        vf_loss: 0.528144175807635
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48666666666667
    gpu_util_percent0: 0.36800000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463208678569944
    mean_env_wait_ms: 1.2234395796551287
    mean_inference_ms: 4.292559745775442
    mean_raw_obs_processing_ms: 0.37654729359556277
  time_since_restore: 4954.973243236542
  time_this_iter_s: 25.578902006149292
  time_total_s: 4954.973243236542
  timers:
    learn_throughput: 8743.155
    learn_time_ms: 18504.991
    sample_throughput: 23721.773
    sample_time_ms: 6820.401
    update_time_ms: 33.538
  timestamp: 1602770357
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: e00ea_00000
  
2020-10-15 13:59:18,123	WARNING util.py:136 -- The `process_trial` operation took 0.5960195064544678 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    194 |          4954.97 | 31387648 |  295.477 |              331.475 |              148.141 |            774.033 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3108.3515169410202
    time_step_min: 2872
  date: 2020-10-15_13-59-43
  done: false
  episode_len_mean: 773.9869232849102
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 295.6494194954486
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 229
  episodes_total: 40683
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.230392283510964e-37
        cur_lr: 5.0e-05
        entropy: 0.07459443931778272
        entropy_coeff: 0.0005000000000000001
        kl: 0.005008423080046971
        model: {}
        policy_loss: -0.008304327352864979
        total_loss: 0.48738180100917816
        vf_explained_var: 0.998478889465332
        vf_loss: 0.4957234238584836
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.5
    gpu_util_percent0: 0.35866666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631705297717626
    mean_env_wait_ms: 1.223359177320346
    mean_inference_ms: 4.292393648863662
    mean_raw_obs_processing_ms: 0.3765266070567924
  time_since_restore: 4980.521104097366
  time_this_iter_s: 25.547860860824585
  time_total_s: 4980.521104097366
  timers:
    learn_throughput: 8753.222
    learn_time_ms: 18483.708
    sample_throughput: 23771.042
    sample_time_ms: 6806.265
    update_time_ms: 34.786
  timestamp: 1602770383
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: e00ea_00000
  
2020-10-15 13:59:44,398	WARNING util.py:136 -- The `process_trial` operation took 0.529900074005127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    195 |          4980.52 | 31549440 |  295.649 |              331.475 |              148.141 |            773.987 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3107.1671233547
    time_step_min: 2872
  date: 2020-10-15_14-00-09
  done: false
  episode_len_mean: 773.9412699188581
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 295.83714404777083
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 233
  episodes_total: 40916
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.230392283510964e-37
        cur_lr: 5.0e-05
        entropy: 0.06791791009406249
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006622780104711031
        total_loss: .inf
        vf_explained_var: 0.9988462328910828
        vf_loss: 0.35626429071029025
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.529999999999998
    gpu_util_percent0: 0.35700000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631365369563554
    mean_env_wait_ms: 1.223276715432433
    mean_inference_ms: 4.292218227161952
    mean_raw_obs_processing_ms: 0.37650615298606716
  time_since_restore: 5006.047668933868
  time_this_iter_s: 25.526564836502075
  time_total_s: 5006.047668933868
  timers:
    learn_throughput: 8759.812
    learn_time_ms: 18469.803
    sample_throughput: 23770.436
    sample_time_ms: 6806.438
    update_time_ms: 32.94
  timestamp: 1602770409
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:00:10,805	WARNING util.py:136 -- The `process_trial` operation took 0.553809404373169 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    196 |          5006.05 | 31711232 |  295.837 |              331.475 |              148.141 |            773.941 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3106.2662752168403
    time_step_min: 2872
  date: 2020-10-15_14-00-36
  done: false
  episode_len_mean: 773.9068782553668
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 295.9740239369798
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 41086
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0845588425266447e-36
        cur_lr: 5.0e-05
        entropy: 0.06745790255566438
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006399184387798111
        total_loss: .inf
        vf_explained_var: 0.9982962608337402
        vf_loss: 0.4181330626209577
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666665
    gpu_util_percent0: 0.3303333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631120199229747
    mean_env_wait_ms: 1.2232167473198243
    mean_inference_ms: 4.292093026274908
    mean_raw_obs_processing_ms: 0.3764920601146431
  time_since_restore: 5031.699208974838
  time_this_iter_s: 25.65154004096985
  time_total_s: 5031.699208974838
  timers:
    learn_throughput: 8745.794
    learn_time_ms: 18499.406
    sample_throughput: 23783.671
    sample_time_ms: 6802.65
    update_time_ms: 24.978
  timestamp: 1602770436
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:00:37,210	WARNING util.py:136 -- The `process_trial` operation took 0.5404949188232422 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    197 |           5031.7 | 31873024 |  295.974 |              331.475 |              148.141 |            773.907 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3105.10604482132
    time_step_min: 2872
  date: 2020-10-15_14-01-02
  done: false
  episode_len_mean: 773.8628409613476
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 296.1508328193229
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 231
  episodes_total: 41317
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6268382637899673e-36
        cur_lr: 5.0e-05
        entropy: 0.07955875061452389
        entropy_coeff: 0.0005000000000000001
        kl: 0.005836044282962878
        model: {}
        policy_loss: -0.0070904545718804
        total_loss: 0.6382859498262405
        vf_explained_var: 0.9980745911598206
        vf_loss: 0.645416185259819
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.75
    gpu_util_percent0: 0.3756666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630752155383592
    mean_env_wait_ms: 1.2231364645507057
    mean_inference_ms: 4.291930380526833
    mean_raw_obs_processing_ms: 0.37647178953516336
  time_since_restore: 5056.976656675339
  time_this_iter_s: 25.27744770050049
  time_total_s: 5056.976656675339
  timers:
    learn_throughput: 8743.453
    learn_time_ms: 18504.36
    sample_throughput: 23822.238
    sample_time_ms: 6791.637
    update_time_ms: 24.806
  timestamp: 1602770462
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: e00ea_00000
  
2020-10-15 14:01:03,262	WARNING util.py:136 -- The `process_trial` operation took 0.5839717388153076 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    198 |          5056.98 | 32034816 |  296.151 |              331.475 |              148.141 |            773.863 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3103.925239724377
    time_step_min: 2872
  date: 2020-10-15_14-01-28
  done: false
  episode_len_mean: 773.8188841821508
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 296.32950108575903
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 231
  episodes_total: 41548
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6268382637899673e-36
        cur_lr: 5.0e-05
        entropy: 0.07639710046350956
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045099651130537195
        model: {}
        policy_loss: -0.007903536347536525
        total_loss: 0.5081822996338209
        vf_explained_var: 0.9983250498771667
        vf_loss: 0.5161240324378014
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.653333333333332
    gpu_util_percent0: 0.3153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630414105975612
    mean_env_wait_ms: 1.223054560746743
    mean_inference_ms: 4.29176357256924
    mean_raw_obs_processing_ms: 0.3764520923702081
  time_since_restore: 5082.4157910346985
  time_this_iter_s: 25.43913435935974
  time_total_s: 5082.4157910346985
  timers:
    learn_throughput: 8757.582
    learn_time_ms: 18474.507
    sample_throughput: 23813.425
    sample_time_ms: 6794.151
    update_time_ms: 23.407
  timestamp: 1602770488
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: e00ea_00000
  
2020-10-15 14:01:29,501	WARNING util.py:136 -- The `process_trial` operation took 0.6013402938842773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    199 |          5082.42 | 32196608 |   296.33 |              331.475 |              148.141 |            773.819 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3103.116395134007
    time_step_min: 2872
  date: 2020-10-15_14-01-54
  done: false
  episode_len_mean: 773.784486684724
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 296.4529421349816
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 41719
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.134191318949837e-37
        cur_lr: 5.0e-05
        entropy: 0.07940785152216752
        entropy_coeff: 0.0005000000000000001
        kl: 0.005626103336301942
        model: {}
        policy_loss: -0.008854310707344363
        total_loss: 0.45975160350402194
        vf_explained_var: 0.9982516765594482
        vf_loss: 0.4686456024646759
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.713333333333335
    gpu_util_percent0: 0.358
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630165909429851
    mean_env_wait_ms: 1.222994380844734
    mean_inference_ms: 4.291639616106686
    mean_raw_obs_processing_ms: 0.37643830677543966
  time_since_restore: 5107.837104558945
  time_this_iter_s: 25.421313524246216
  time_total_s: 5107.837104558945
  timers:
    learn_throughput: 8768.58
    learn_time_ms: 18451.335
    sample_throughput: 23837.707
    sample_time_ms: 6787.23
    update_time_ms: 21.126
  timestamp: 1602770514
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: e00ea_00000
  
2020-10-15 14:01:55,673	WARNING util.py:136 -- The `process_trial` operation took 0.5549283027648926 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    200 |          5107.84 | 32358400 |  296.453 |              331.475 |              148.141 |            773.784 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3102.0476690365986
    time_step_min: 2872
  date: 2020-10-15_14-02-21
  done: false
  episode_len_mean: 773.7336495376109
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 296.61483290334945
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 237
  episodes_total: 41956
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.134191318949837e-37
        cur_lr: 5.0e-05
        entropy: 0.08580998207132022
        entropy_coeff: 0.0005000000000000001
        kl: 0.004284311008329193
        model: {}
        policy_loss: -0.006498305534478277
        total_loss: 0.890803853670756
        vf_explained_var: 0.997349739074707
        vf_loss: 0.8973450611035029
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.470000000000002
    gpu_util_percent0: 0.3393333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629810975144145
    mean_env_wait_ms: 1.2229129903897333
    mean_inference_ms: 4.291477564226228
    mean_raw_obs_processing_ms: 0.3764175615988384
  time_since_restore: 5133.525583267212
  time_this_iter_s: 25.688478708267212
  time_total_s: 5133.525583267212
  timers:
    learn_throughput: 8764.116
    learn_time_ms: 18460.732
    sample_throughput: 23806.759
    sample_time_ms: 6796.053
    update_time_ms: 19.76
  timestamp: 1602770541
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: e00ea_00000
  
2020-10-15 14:02:22,140	WARNING util.py:136 -- The `process_trial` operation took 0.5829823017120361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    201 |          5133.53 | 32520192 |  296.615 |              331.475 |              148.141 |            773.734 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3100.9691741813003
    time_step_min: 2872
  date: 2020-10-15_14-02-47
  done: false
  episode_len_mean: 773.6931629605045
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 296.7835428391354
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 226
  episodes_total: 42182
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.067095659474918e-37
        cur_lr: 5.0e-05
        entropy: 0.07536411037047704
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037462495965883136
        model: {}
        policy_loss: -0.00816002913416014
        total_loss: 0.4926645929614703
        vf_explained_var: 0.9983667731285095
        vf_loss: 0.5008623078465462
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65333333333333
    gpu_util_percent0: 0.33233333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462949068508606
    mean_env_wait_ms: 1.2228326514218049
    mean_inference_ms: 4.291321946742516
    mean_raw_obs_processing_ms: 0.37639946609006697
  time_since_restore: 5159.116855621338
  time_this_iter_s: 25.591272354125977
  time_total_s: 5159.116855621338
  timers:
    learn_throughput: 8757.872
    learn_time_ms: 18473.894
    sample_throughput: 23803.799
    sample_time_ms: 6796.898
    update_time_ms: 20.045
  timestamp: 1602770567
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: e00ea_00000
  
2020-10-15 14:02:48,490	WARNING util.py:136 -- The `process_trial` operation took 0.5454981327056885 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    202 |          5159.12 | 32681984 |  296.784 |              331.475 |              148.141 |            773.693 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3100.1297092885843
    time_step_min: 2872
  date: 2020-10-15_14-03-13
  done: false
  episode_len_mean: 773.6632508500189
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 296.9125036252209
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 42352
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.033547829737459e-37
        cur_lr: 5.0e-05
        entropy: 0.07515242633720239
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038661473469498255
        model: {}
        policy_loss: -0.0061022363176258905
        total_loss: 0.6301475067933401
        vf_explained_var: 0.9974937438964844
        vf_loss: 0.6362872918446859
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.716666666666665
    gpu_util_percent0: 0.36666666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629241109980926
    mean_env_wait_ms: 1.2227732405240446
    mean_inference_ms: 4.291202410942326
    mean_raw_obs_processing_ms: 0.3763858118832725
  time_since_restore: 5184.4847638607025
  time_this_iter_s: 25.367908239364624
  time_total_s: 5184.4847638607025
  timers:
    learn_throughput: 8767.645
    learn_time_ms: 18453.302
    sample_throughput: 23804.017
    sample_time_ms: 6796.836
    update_time_ms: 20.251
  timestamp: 1602770593
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: e00ea_00000
  
2020-10-15 14:03:14,629	WARNING util.py:136 -- The `process_trial` operation took 0.5699954032897949 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    203 |          5184.48 | 32843776 |  296.913 |              331.475 |              148.141 |            773.663 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3099.0085072382026
    time_step_min: 2872
  date: 2020-10-15_14-03-40
  done: false
  episode_len_mean: 773.6178100201906
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 297.07744297461176
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 242
  episodes_total: 42594
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0167739148687296e-37
        cur_lr: 5.0e-05
        entropy: 0.08207304154833157
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035048171606225273
        model: {}
        policy_loss: -0.0070748961879871786
        total_loss: 0.9778995017210642
        vf_explained_var: 0.9971216320991516
        vf_loss: 0.9850154171387354
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19
    gpu_util_percent0: 0.372
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628888956101663
    mean_env_wait_ms: 1.2226902411985094
    mean_inference_ms: 4.291046999898152
    mean_raw_obs_processing_ms: 0.37636519875179264
  time_since_restore: 5210.170561552048
  time_this_iter_s: 25.685797691345215
  time_total_s: 5210.170561552048
  timers:
    learn_throughput: 8769.697
    learn_time_ms: 18448.983
    sample_throughput: 23757.43
    sample_time_ms: 6810.164
    update_time_ms: 20.32
  timestamp: 1602770620
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: e00ea_00000
  
2020-10-15 14:03:41,111	WARNING util.py:136 -- The `process_trial` operation took 0.5975184440612793 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    204 |          5210.17 | 33005568 |  297.077 |              331.475 |              148.141 |            773.618 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3097.9748667352474
    time_step_min: 2872
  date: 2020-10-15_14-04-06
  done: false
  episode_len_mean: 773.5748119773906
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 297.23837831767486
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 220
  episodes_total: 42814
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.083869574343648e-38
        cur_lr: 5.0e-05
        entropy: 0.0742620900273323
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00633660246967338
        total_loss: .inf
        vf_explained_var: 0.9986245036125183
        vf_loss: 0.4200282618403435
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.040000000000006
    gpu_util_percent0: 0.32633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628585951332795
    mean_env_wait_ms: 1.2226123533517235
    mean_inference_ms: 4.290899772033923
    mean_raw_obs_processing_ms: 0.3763478729215634
  time_since_restore: 5235.914085388184
  time_this_iter_s: 25.743523836135864
  time_total_s: 5235.914085388184
  timers:
    learn_throughput: 8765.813
    learn_time_ms: 18457.159
    sample_throughput: 23713.534
    sample_time_ms: 6822.77
    update_time_ms: 19.363
  timestamp: 1602770646
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:04:07,697	WARNING util.py:136 -- The `process_trial` operation took 0.638615608215332 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    205 |          5235.91 | 33167360 |  297.238 |              331.475 |              148.141 |            773.575 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3097.153738676851
    time_step_min: 2872
  date: 2020-10-15_14-04-33
  done: false
  episode_len_mean: 773.5463068512272
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 297.3591539449398
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 42985
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.62580436151547e-38
        cur_lr: 5.0e-05
        entropy: 0.07588119804859161
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040487930333862705
        model: {}
        policy_loss: -0.005164825881365687
        total_loss: 0.376753605902195
        vf_explained_var: 0.9985045790672302
        vf_loss: 0.3819563686847687
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.073333333333334
    gpu_util_percent0: 0.3226666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628343026783175
    mean_env_wait_ms: 1.2225526623019554
    mean_inference_ms: 4.290783434820069
    mean_raw_obs_processing_ms: 0.37633435702059775
  time_since_restore: 5261.61622095108
  time_this_iter_s: 25.70213556289673
  time_total_s: 5261.61622095108
  timers:
    learn_throughput: 8759.41
    learn_time_ms: 18470.65
    sample_throughput: 23701.022
    sample_time_ms: 6826.372
    update_time_ms: 21.224
  timestamp: 1602770673
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: e00ea_00000
  
2020-10-15 14:04:34,199	WARNING util.py:136 -- The `process_trial` operation took 0.597048282623291 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    206 |          5261.62 | 33329152 |  297.359 |              331.475 |              148.141 |            773.546 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3096.016995855234
    time_step_min: 2872
  date: 2020-10-15_14-04-59
  done: false
  episode_len_mean: 773.4961715515047
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 297.5314665543217
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 244
  episodes_total: 43229
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.812902180757735e-38
        cur_lr: 5.0e-05
        entropy: 0.08048043958842754
        entropy_coeff: 0.0005000000000000001
        kl: 0.004848337732255459
        model: {}
        policy_loss: -0.005338901362847537
        total_loss: 0.658220648765564
        vf_explained_var: 0.9980018734931946
        vf_loss: 0.6635997990767161
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993548387096773
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14628001737828145
    mean_env_wait_ms: 1.2224698829510678
    mean_inference_ms: 4.29063253382988
    mean_raw_obs_processing_ms: 0.37631421736900944
  time_since_restore: 5287.353248357773
  time_this_iter_s: 25.737027406692505
  time_total_s: 5287.353248357773
  timers:
    learn_throughput: 8755.713
    learn_time_ms: 18478.45
    sample_throughput: 23699.938
    sample_time_ms: 6826.685
    update_time_ms: 21.323
  timestamp: 1602770699
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: e00ea_00000
  
2020-10-15 14:05:00,912	WARNING util.py:136 -- The `process_trial` operation took 0.5936646461486816 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    207 |          5287.35 | 33490944 |  297.531 |              331.475 |              148.141 |            773.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3095.0546493410748
    time_step_min: 2872
  date: 2020-10-15_14-05-26
  done: false
  episode_len_mean: 773.4535285181605
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 297.67364781637684
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 217
  episodes_total: 43446
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9064510903788674e-38
        cur_lr: 5.0e-05
        entropy: 0.07853781680266063
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043660082543889684
        model: {}
        policy_loss: -0.00993870955426246
        total_loss: 0.7501898755629858
        vf_explained_var: 0.9975075721740723
        vf_loss: 0.760167861978213
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62333333333333
    gpu_util_percent0: 0.30833333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627709495838354
    mean_env_wait_ms: 1.222393160797522
    mean_inference_ms: 4.2904931900762
    mean_raw_obs_processing_ms: 0.3762975470619727
  time_since_restore: 5312.937653541565
  time_this_iter_s: 25.584405183792114
  time_total_s: 5312.937653541565
  timers:
    learn_throughput: 8754.125
    learn_time_ms: 18481.801
    sample_throughput: 23637.594
    sample_time_ms: 6844.69
    update_time_ms: 21.462
  timestamp: 1602770726
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: e00ea_00000
  
2020-10-15 14:05:27,334	WARNING util.py:136 -- The `process_trial` operation took 0.5716209411621094 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    208 |          5312.94 | 33652736 |  297.674 |              331.475 |              148.141 |            773.454 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3094.4028456683877
    time_step_min: 2872
  date: 2020-10-15_14-05-52
  done: false
  episode_len_mean: 773.4149758121833
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 297.7629163218958
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 43617
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.532255451894337e-39
        cur_lr: 5.0e-05
        entropy: 0.0841427892446518
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037345259722011783
        model: {}
        policy_loss: -0.009477136433512593
        total_loss: 1.4001061817010243
        vf_explained_var: 0.9949766993522644
        vf_loss: 1.4096253911654155
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.026666666666674
    gpu_util_percent0: 0.337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627473359129978
    mean_env_wait_ms: 1.2223341307271567
    mean_inference_ms: 4.290380589352597
    mean_raw_obs_processing_ms: 0.3762844108750516
  time_since_restore: 5338.508223295212
  time_this_iter_s: 25.57056975364685
  time_total_s: 5338.508223295212
  timers:
    learn_throughput: 8755.71
    learn_time_ms: 18478.456
    sample_throughput: 23615.45
    sample_time_ms: 6851.108
    update_time_ms: 21.109
  timestamp: 1602770752
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: e00ea_00000
  
2020-10-15 14:05:53,680	WARNING util.py:136 -- The `process_trial` operation took 0.5672492980957031 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    209 |          5338.51 | 33814528 |  297.763 |              331.475 |              148.141 |            773.415 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3093.3782143427566
    time_step_min: 2872
  date: 2020-10-15_14-06-19
  done: false
  episode_len_mean: 773.3524356607171
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 297.91185326561117
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 252
  episodes_total: 43869
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.7661277259471686e-39
        cur_lr: 5.0e-05
        entropy: 0.07759865807990234
        entropy_coeff: 0.0005000000000000001
        kl: 0.003347744874190539
        model: {}
        policy_loss: -0.006286809672019444
        total_loss: 0.9855185598134995
        vf_explained_var: 0.9973086714744568
        vf_loss: 0.9918441871802012
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55666666666667
    gpu_util_percent0: 0.3366666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14627160184364388
    mean_env_wait_ms: 1.222249672334017
    mean_inference_ms: 4.290234261373914
    mean_raw_obs_processing_ms: 0.3762648280657372
  time_since_restore: 5364.179141521454
  time_this_iter_s: 25.670918226242065
  time_total_s: 5364.179141521454
  timers:
    learn_throughput: 8750.774
    learn_time_ms: 18488.879
    sample_throughput: 23577.919
    sample_time_ms: 6862.014
    update_time_ms: 22.804
  timestamp: 1602770779
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: e00ea_00000
  
2020-10-15 14:06:20,232	WARNING util.py:136 -- The `process_trial` operation took 0.6730151176452637 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    210 |          5364.18 | 33976320 |  297.912 |              331.475 |              148.141 |            773.352 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3092.4356691947864
    time_step_min: 2872
  date: 2020-10-15_14-06-45
  done: false
  episode_len_mean: 773.305444646098
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.0595828979451
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 211
  episodes_total: 44080
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3830638629735843e-39
        cur_lr: 5.0e-05
        entropy: 0.06762261502444744
        entropy_coeff: 0.0005000000000000001
        kl: 0.005001478401633601
        model: {}
        policy_loss: -0.007518673451462139
        total_loss: 0.2675235855082671
        vf_explained_var: 0.9990230202674866
        vf_loss: 0.27507607638835907
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.426666666666673
    gpu_util_percent0: 0.36533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462684659887543
    mean_env_wait_ms: 1.222175437688591
    mean_inference_ms: 4.2900949812334295
    mean_raw_obs_processing_ms: 0.37624844183600875
  time_since_restore: 5389.380083084106
  time_this_iter_s: 25.200941562652588
  time_total_s: 5389.380083084106
  timers:
    learn_throughput: 8775.582
    learn_time_ms: 18436.613
    sample_throughput: 23568.906
    sample_time_ms: 6864.638
    update_time_ms: 22.874
  timestamp: 1602770805
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: e00ea_00000
  
2020-10-15 14:06:46,404	WARNING util.py:136 -- The `process_trial` operation took 0.6013014316558838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    211 |          5389.38 | 34138112 |   298.06 |              331.475 |              148.141 |            773.305 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3091.662994797557
    time_step_min: 2872
  date: 2020-10-15_14-07-11
  done: false
  episode_len_mean: 773.2606661845792
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.1799263538395
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 44252
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3830638629735843e-39
        cur_lr: 5.0e-05
        entropy: 0.06913410437603791
        entropy_coeff: 0.0005000000000000001
        kl: 0.005542270606383681
        model: {}
        policy_loss: -0.00881662366252082
        total_loss: 0.29388072714209557
        vf_explained_var: 0.998832643032074
        vf_loss: 0.3027319187919299
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.836666666666666
    gpu_util_percent0: 0.3543333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626604896714934
    mean_env_wait_ms: 1.2221167453217954
    mean_inference_ms: 4.289985003975482
    mean_raw_obs_processing_ms: 0.3762353124683082
  time_since_restore: 5414.8995752334595
  time_this_iter_s: 25.519492149353027
  time_total_s: 5414.8995752334595
  timers:
    learn_throughput: 8784.41
    learn_time_ms: 18418.083
    sample_throughput: 23536.395
    sample_time_ms: 6874.12
    update_time_ms: 23.79
  timestamp: 1602770831
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: e00ea_00000
  
2020-10-15 14:07:12,800	WARNING util.py:136 -- The `process_trial` operation took 0.6552958488464355 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    212 |           5414.9 | 34299904 |   298.18 |              331.475 |              148.141 |            773.261 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3090.534823577035
    time_step_min: 2872
  date: 2020-10-15_14-07-38
  done: false
  episode_len_mean: 773.2005886449931
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.35120827906565
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 257
  episodes_total: 44509
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3830638629735843e-39
        cur_lr: 5.0e-05
        entropy: 0.0744395771374305
        entropy_coeff: 0.0005000000000000001
        kl: 0.003474539320450276
        model: {}
        policy_loss: -0.0045707199242315255
        total_loss: 0.7181076308091482
        vf_explained_var: 0.9978844523429871
        vf_loss: 0.7227155417203903
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.59666666666667
    gpu_util_percent0: 0.3003333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462630948009553
    mean_env_wait_ms: 1.2220317734898682
    mean_inference_ms: 4.289839706354719
    mean_raw_obs_processing_ms: 0.37621568372372627
  time_since_restore: 5440.622208356857
  time_this_iter_s: 25.722633123397827
  time_total_s: 5440.622208356857
  timers:
    learn_throughput: 8770.014
    learn_time_ms: 18448.316
    sample_throughput: 23518.349
    sample_time_ms: 6879.395
    update_time_ms: 24.062
  timestamp: 1602770858
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: e00ea_00000
  
2020-10-15 14:07:39,343	WARNING util.py:136 -- The `process_trial` operation took 0.5949263572692871 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    213 |          5440.62 | 34461696 |  298.351 |              331.475 |              148.141 |            773.201 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3089.653391537945
    time_step_min: 2872
  date: 2020-10-15_14-08-05
  done: false
  episode_len_mean: 773.1533592771516
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.4839972456721
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 203
  episodes_total: 44712
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1915319314867921e-39
        cur_lr: 5.0e-05
        entropy: 0.07270757916073005
        entropy_coeff: 0.0005000000000000001
        kl: 0.006563837096715967
        model: {}
        policy_loss: -0.006484549856395461
        total_loss: 0.42672254145145416
        vf_explained_var: 0.998443603515625
        vf_loss: 0.43324344356854755
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.926666666666666
    gpu_util_percent0: 0.369
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14626021094615338
    mean_env_wait_ms: 1.2219609224760946
    mean_inference_ms: 4.28971351208321
    mean_raw_obs_processing_ms: 0.3762008448533777
  time_since_restore: 5466.317306518555
  time_this_iter_s: 25.695098161697388
  time_total_s: 5466.317306518555
  timers:
    learn_throughput: 8759.104
    learn_time_ms: 18471.297
    sample_throughput: 23598.236
    sample_time_ms: 6856.106
    update_time_ms: 24.042
  timestamp: 1602770885
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: e00ea_00000
  
2020-10-15 14:08:05,947	WARNING util.py:136 -- The `process_trial` operation took 0.5931107997894287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    214 |          5466.32 | 34623488 |  298.484 |              331.475 |              148.141 |            773.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3088.9263892605477
    time_step_min: 2872
  date: 2020-10-15_14-08-31
  done: false
  episode_len_mean: 773.1163837276656
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.5871764024418
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 174
  episodes_total: 44886
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1915319314867921e-39
        cur_lr: 5.0e-05
        entropy: 0.09076490501562755
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010464428613583246
        total_loss: .inf
        vf_explained_var: 0.9961428642272949
        vf_loss: 1.0475124617417653
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83
    gpu_util_percent0: 0.35866666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625775904312038
    mean_env_wait_ms: 1.22190214871845
    mean_inference_ms: 4.289602381801948
    mean_raw_obs_processing_ms: 0.37618772101292247
  time_since_restore: 5492.036029815674
  time_this_iter_s: 25.71872329711914
  time_total_s: 5492.036029815674
  timers:
    learn_throughput: 8753.511
    learn_time_ms: 18483.097
    sample_throughput: 23651.516
    sample_time_ms: 6840.661
    update_time_ms: 24.102
  timestamp: 1602770911
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:08:32,625	WARNING util.py:136 -- The `process_trial` operation took 0.6337404251098633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    215 |          5492.04 | 34785280 |  298.587 |              331.475 |              148.141 |            773.116 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3088.1740701334397
    time_step_min: 2872
  date: 2020-10-15_14-08-58
  done: false
  episode_len_mean: 773.0494729382585
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.6995466222148
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 270
  episodes_total: 45156
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7872978972301885e-39
        cur_lr: 5.0e-05
        entropy: 0.09963094567259152
        entropy_coeff: 0.0005000000000000001
        kl: 0.00431158432426552
        model: {}
        policy_loss: -0.010129785213697081
        total_loss: 2.0340973834196725
        vf_explained_var: 0.9950043559074402
        vf_loss: 2.0442769626776376
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.630000000000003
    gpu_util_percent0: 0.3096666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625480426530688
    mean_env_wait_ms: 1.2218137988246522
    mean_inference_ms: 4.289456591806741
    mean_raw_obs_processing_ms: 0.3761678940231816
  time_since_restore: 5517.525794267654
  time_this_iter_s: 25.48976445198059
  time_total_s: 5517.525794267654
  timers:
    learn_throughput: 8757.136
    learn_time_ms: 18475.446
    sample_throughput: 23698.17
    sample_time_ms: 6827.194
    update_time_ms: 22.907
  timestamp: 1602770938
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: e00ea_00000
  
2020-10-15 14:08:59,002	WARNING util.py:136 -- The `process_trial` operation took 0.5910129547119141 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    216 |          5517.53 | 34947072 |    298.7 |              331.475 |              148.141 |            773.049 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3087.4672435105067
    time_step_min: 2872
  date: 2020-10-15_14-09-24
  done: false
  episode_len_mean: 773.0127023331717
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.8105420187854
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 190
  episodes_total: 45346
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.936489486150942e-40
        cur_lr: 5.0e-05
        entropy: 0.0753366860250632
        entropy_coeff: 0.0005000000000000001
        kl: 0.00462773294808964
        model: {}
        policy_loss: -0.007855490267199153
        total_loss: 0.6230422556400299
        vf_explained_var: 0.9977187514305115
        vf_loss: 0.6309353907903036
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.836666666666666
    gpu_util_percent0: 0.3459999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14625222497366747
    mean_env_wait_ms: 1.221749868391095
    mean_inference_ms: 4.289340906188127
    mean_raw_obs_processing_ms: 0.376154165409405
  time_since_restore: 5543.070348978043
  time_this_iter_s: 25.544554710388184
  time_total_s: 5543.070348978043
  timers:
    learn_throughput: 8767.678
    learn_time_ms: 18453.233
    sample_throughput: 23691.052
    sample_time_ms: 6829.245
    update_time_ms: 23.008
  timestamp: 1602770964
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: e00ea_00000
  
2020-10-15 14:09:25,369	WARNING util.py:136 -- The `process_trial` operation took 0.6148548126220703 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    217 |          5543.07 | 35108864 |  298.811 |              331.475 |              148.141 |            773.013 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3086.801095010884
    time_step_min: 2872
  date: 2020-10-15_14-09-50
  done: false
  episode_len_mean: 772.9726719536038
  episode_reward_max: 331.47474747474735
  episode_reward_mean: 298.9133988331283
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 45521
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.468244743075471e-40
        cur_lr: 5.0e-05
        entropy: 0.0796454840650161
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0064985233669479685
        total_loss: .inf
        vf_explained_var: 0.9971960186958313
        vf_loss: 0.7674365838368734
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.923333333333336
    gpu_util_percent0: 0.3366666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462496893187451
    mean_env_wait_ms: 1.2216905760738725
    mean_inference_ms: 4.289230363877333
    mean_raw_obs_processing_ms: 0.3761411374615178
  time_since_restore: 5568.5222754478455
  time_this_iter_s: 25.451926469802856
  time_total_s: 5568.5222754478455
  timers:
    learn_throughput: 8766.145
    learn_time_ms: 18456.46
    sample_throughput: 23723.944
    sample_time_ms: 6819.777
    update_time_ms: 23.109
  timestamp: 1602770990
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:09:51,654	WARNING util.py:136 -- The `process_trial` operation took 0.6228094100952148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    218 |          5568.52 | 35270656 |  298.913 |              331.475 |              148.141 |            772.973 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3085.686754098361
    time_step_min: 2872
  date: 2020-10-15_14-10-17
  done: false
  episode_len_mean: 772.9128887141859
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.07757739872534
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 271
  episodes_total: 45792
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.702367114613208e-40
        cur_lr: 5.0e-05
        entropy: 0.08246852519611518
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006752068903400262
        total_loss: .inf
        vf_explained_var: 0.9977770447731018
        vf_loss: 0.7978488951921463
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.276666666666674
    gpu_util_percent0: 0.38533333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624682763853217
    mean_env_wait_ms: 1.2216034460163312
    mean_inference_ms: 4.289090145063257
    mean_raw_obs_processing_ms: 0.3761215773126401
  time_since_restore: 5594.296862125397
  time_this_iter_s: 25.77458667755127
  time_total_s: 5594.296862125397
  timers:
    learn_throughput: 8755.132
    learn_time_ms: 18479.676
    sample_throughput: 23713.1
    sample_time_ms: 6822.895
    update_time_ms: 23.943
  timestamp: 1602771017
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:10:18,293	WARNING util.py:136 -- The `process_trial` operation took 0.6485507488250732 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    219 |           5594.3 | 35432448 |  299.078 |              331.475 |              148.141 |            772.913 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3084.8881704980845
    time_step_min: 2872
  date: 2020-10-15_14-10-43
  done: false
  episode_len_mean: 772.877767627996
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.1967475881088
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 186
  episodes_total: 45978
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0053550671919809e-39
        cur_lr: 5.0e-05
        entropy: 0.07257650978863239
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060853122267872095
        model: {}
        policy_loss: -0.00584165549177366
        total_loss: 0.25479226062695187
        vf_explained_var: 0.9989969730377197
        vf_loss: 0.26067021240790683
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.470000000000002
    gpu_util_percent0: 0.38599999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624440363680408
    mean_env_wait_ms: 1.2215408071608673
    mean_inference_ms: 4.288979519536735
    mean_raw_obs_processing_ms: 0.37610839739285074
  time_since_restore: 5619.736141443253
  time_this_iter_s: 25.439279317855835
  time_total_s: 5619.736141443253
  timers:
    learn_throughput: 8755.485
    learn_time_ms: 18478.931
    sample_throughput: 23785.792
    sample_time_ms: 6802.044
    update_time_ms: 22.526
  timestamp: 1602771043
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: e00ea_00000
  
2020-10-15 14:10:44,571	WARNING util.py:136 -- The `process_trial` operation took 0.6255743503570557 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    220 |          5619.74 | 35594240 |  299.197 |              331.475 |              148.141 |            772.878 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3084.204961401683
    time_step_min: 2872
  date: 2020-10-15_14-11-10
  done: false
  episode_len_mean: 772.8466354694744
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.29945562475143
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 180
  episodes_total: 46158
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0053550671919809e-39
        cur_lr: 5.0e-05
        entropy: 0.07821229907373588
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034758348871643343
        model: {}
        policy_loss: -0.009792836996590873
        total_loss: 0.9645908077557882
        vf_explained_var: 0.9965096116065979
        vf_loss: 0.9744227478901545
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10967741935484
    gpu_util_percent0: 0.3180645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14624208038448372
    mean_env_wait_ms: 1.2214811578737643
    mean_inference_ms: 4.288872276130435
    mean_raw_obs_processing_ms: 0.37609545916941844
  time_since_restore: 5645.359719276428
  time_this_iter_s: 25.62357783317566
  time_total_s: 5645.359719276428
  timers:
    learn_throughput: 8728.872
    learn_time_ms: 18535.269
    sample_throughput: 23839.352
    sample_time_ms: 6786.762
    update_time_ms: 22.644
  timestamp: 1602771070
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: e00ea_00000
  
2020-10-15 14:11:11,227	WARNING util.py:136 -- The `process_trial` operation took 0.6423032283782959 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    221 |          5645.36 | 35756032 |  299.299 |              331.475 |              148.141 |            772.847 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3083.144834425664
    time_step_min: 2872
  date: 2020-10-15_14-11-37
  done: false
  episode_len_mean: 772.803429112997
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.45687021422606
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 268
  episodes_total: 46426
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.026775335959904e-40
        cur_lr: 5.0e-05
        entropy: 0.08003210710982482
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037488870633145175
        model: {}
        policy_loss: -0.007759641739539802
        total_loss: 0.8456386576096216
        vf_explained_var: 0.9975839257240295
        vf_loss: 0.8534383028745651
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.44
    gpu_util_percent0: 0.3466666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462390808575362
    mean_env_wait_ms: 1.2213947424698646
    mean_inference_ms: 4.288734433660737
    mean_raw_obs_processing_ms: 0.3760759964264127
  time_since_restore: 5671.260722398758
  time_this_iter_s: 25.901003122329712
  time_total_s: 5671.260722398758
  timers:
    learn_throughput: 8709.05
    learn_time_ms: 18577.458
    sample_throughput: 23851.326
    sample_time_ms: 6783.355
    update_time_ms: 21.906
  timestamp: 1602771097
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: e00ea_00000
  
2020-10-15 14:11:37,980	WARNING util.py:136 -- The `process_trial` operation took 0.6259558200836182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    222 |          5671.26 | 35917824 |  299.457 |              331.475 |              148.141 |            772.803 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3082.422951382924
    time_step_min: 2872
  date: 2020-10-15_14-12-03
  done: false
  episode_len_mean: 772.7744260888221
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.5658678611909
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 184
  episodes_total: 46610
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.513387667979952e-40
        cur_lr: 5.0e-05
        entropy: 0.07089678632716338
        entropy_coeff: 0.0005000000000000001
        kl: 0.005134369634712736
        model: {}
        policy_loss: -0.008871876731821734
        total_loss: 0.40807371338208515
        vf_explained_var: 0.9984104633331299
        vf_loss: 0.416981041431427
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.113333333333337
    gpu_util_percent0: 0.3306666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462367517979731
    mean_env_wait_ms: 1.2213331593430723
    mean_inference_ms: 4.2886278014156725
    mean_raw_obs_processing_ms: 0.37606323737393066
  time_since_restore: 5696.969998836517
  time_this_iter_s: 25.7092764377594
  time_total_s: 5696.969998836517
  timers:
    learn_throughput: 8705.44
    learn_time_ms: 18585.161
    sample_throughput: 23887.957
    sample_time_ms: 6772.953
    update_time_ms: 23.361
  timestamp: 1602771123
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: e00ea_00000
  
2020-10-15 14:12:04,638	WARNING util.py:136 -- The `process_trial` operation took 0.64505934715271 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    223 |          5696.97 | 36079616 |  299.566 |              331.475 |              148.141 |            772.774 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3081.6785233033174
    time_step_min: 2872
  date: 2020-10-15_14-12-30
  done: false
  episode_len_mean: 772.743583716209
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.6777509036298
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 185
  episodes_total: 46795
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.513387667979952e-40
        cur_lr: 5.0e-05
        entropy: 0.07400198218723138
        entropy_coeff: 0.0005000000000000001
        kl: 0.003370882652234286
        model: {}
        policy_loss: -0.008174905243019262
        total_loss: 0.3609558343887329
        vf_explained_var: 0.9986405372619629
        vf_loss: 0.3691677376627922
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.356666666666666
    gpu_util_percent0: 0.32733333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623437057185873
    mean_env_wait_ms: 1.2212725636524235
    mean_inference_ms: 4.288518009020491
    mean_raw_obs_processing_ms: 0.3760500875260359
  time_since_restore: 5722.460984945297
  time_this_iter_s: 25.490986108779907
  time_total_s: 5722.460984945297
  timers:
    learn_throughput: 8714.229
    learn_time_ms: 18566.416
    sample_throughput: 23896.36
    sample_time_ms: 6770.571
    update_time_ms: 23.374
  timestamp: 1602771150
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: e00ea_00000
  
2020-10-15 14:12:31,105	WARNING util.py:136 -- The `process_trial` operation took 0.6642518043518066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    224 |          5722.46 | 36241408 |  299.678 |              331.475 |              148.141 |            772.744 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3080.609553787911
    time_step_min: 2872
  date: 2020-10-15_14-12-56
  done: false
  episode_len_mean: 772.7072248193795
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.841709702207
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 265
  episodes_total: 47060
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.256693833989976e-40
        cur_lr: 5.0e-05
        entropy: 0.07754559069871902
        entropy_coeff: 0.0005000000000000001
        kl: 0.004600260639563203
        model: {}
        policy_loss: -0.006269255519631163
        total_loss: 0.3884670486052831
        vf_explained_var: 0.9988784790039062
        vf_loss: 0.3947750876347224
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.829032258064522
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14623158214469534
    mean_env_wait_ms: 1.2211875106667818
    mean_inference_ms: 4.288388939170143
    mean_raw_obs_processing_ms: 0.3760314929646688
  time_since_restore: 5748.311616897583
  time_this_iter_s: 25.850631952285767
  time_total_s: 5748.311616897583
  timers:
    learn_throughput: 8714.483
    learn_time_ms: 18565.875
    sample_throughput: 23879.426
    sample_time_ms: 6775.372
    update_time_ms: 30.947
  timestamp: 1602771176
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: e00ea_00000
  
2020-10-15 14:12:57,826	WARNING util.py:136 -- The `process_trial` operation took 0.6552565097808838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    225 |          5748.31 | 36403200 |  299.842 |              331.475 |              148.141 |            772.707 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3079.8684533898304
    time_step_min: 2872
  date: 2020-10-15_14-13-23
  done: false
  episode_len_mean: 772.6842640023708
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 299.9510423228091
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 182
  episodes_total: 47242
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.28346916994988e-41
        cur_lr: 5.0e-05
        entropy: 0.07132120120028655
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037006380540939667
        model: {}
        policy_loss: -0.0063194734393619
        total_loss: 0.3566575621565183
        vf_explained_var: 0.9986047744750977
        vf_loss: 0.3630126987894376
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89666666666667
    gpu_util_percent0: 0.35133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622922681669173
    mean_env_wait_ms: 1.2211267818270217
    mean_inference_ms: 4.288285444263221
    mean_raw_obs_processing_ms: 0.3760190122073595
  time_since_restore: 5773.966267108917
  time_this_iter_s: 25.65465021133423
  time_total_s: 5773.966267108917
  timers:
    learn_throughput: 8715.234
    learn_time_ms: 18564.275
    sample_throughput: 23847.246
    sample_time_ms: 6784.515
    update_time_ms: 29.801
  timestamp: 1602771203
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: e00ea_00000
  
2020-10-15 14:13:24,344	WARNING util.py:136 -- The `process_trial` operation took 0.6404399871826172 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    226 |          5773.97 | 36564992 |  299.951 |              331.475 |              148.141 |            772.684 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3079.134467258953
    time_step_min: 2872
  date: 2020-10-15_14-13-50
  done: false
  episode_len_mean: 772.6566235847266
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.05734653669475
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 187
  episodes_total: 47429
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.14173458497494e-41
        cur_lr: 5.0e-05
        entropy: 0.0780944066743056
        entropy_coeff: 0.0005000000000000001
        kl: 0.006087208360744019
        model: {}
        policy_loss: -0.006522557882514472
        total_loss: 0.5339051162203153
        vf_explained_var: 0.9981060028076172
        vf_loss: 0.5404667233427366
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.426666666666666
    gpu_util_percent0: 0.3589999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622684208064773
    mean_env_wait_ms: 1.2210654956844518
    mean_inference_ms: 4.288177515027383
    mean_raw_obs_processing_ms: 0.3760061380412419
  time_since_restore: 5799.626233100891
  time_this_iter_s: 25.659965991973877
  time_total_s: 5799.626233100891
  timers:
    learn_throughput: 8714.399
    learn_time_ms: 18566.053
    sample_throughput: 23818.854
    sample_time_ms: 6792.602
    update_time_ms: 29.615
  timestamp: 1602771230
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: e00ea_00000
  
2020-10-15 14:13:50,885	WARNING util.py:136 -- The `process_trial` operation took 0.6433351039886475 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    227 |          5799.63 | 36726784 |  300.057 |              331.475 |              148.141 |            772.657 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3078.1279667597005
    time_step_min: 2872
  date: 2020-10-15_14-14-16
  done: false
  episode_len_mean: 772.6182409057553
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.20662649135244
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 266
  episodes_total: 47695
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.14173458497494e-41
        cur_lr: 5.0e-05
        entropy: 0.08098556784292062
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00787337651127018
        total_loss: .inf
        vf_explained_var: 0.9979702830314636
        vf_loss: 0.7340558568636576
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.426666666666673
    gpu_util_percent0: 0.3396666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462240092429134
    mean_env_wait_ms: 1.220980723057879
    mean_inference_ms: 4.288049685544188
    mean_raw_obs_processing_ms: 0.3759878115409349
  time_since_restore: 5825.175837755203
  time_this_iter_s: 25.549604654312134
  time_total_s: 5825.175837755203
  timers:
    learn_throughput: 8714.358
    learn_time_ms: 18566.14
    sample_throughput: 23790.098
    sample_time_ms: 6800.813
    update_time_ms: 30.34
  timestamp: 1602771256
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:14:17,318	WARNING util.py:136 -- The `process_trial` operation took 0.6612250804901123 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    228 |          5825.18 | 36888576 |  300.207 |              331.475 |              148.141 |            772.618 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3077.6064977420974
    time_step_min: 2872
  date: 2020-10-15_14-14-42
  done: false
  episode_len_mean: 772.58831516063
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.2849924232931
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 47874
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.712601877462411e-41
        cur_lr: 5.0e-05
        entropy: 0.08610069379210472
        entropy_coeff: 0.0005000000000000001
        kl: 0.004787414645155271
        model: {}
        policy_loss: -0.008287086025423681
        total_loss: 0.9915568232536316
        vf_explained_var: 0.9965195059776306
        vf_loss: 0.9998869697252909
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.726666666666667
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14622185897684176
    mean_env_wait_ms: 1.2209219447055206
    mean_inference_ms: 4.287953082391728
    mean_raw_obs_processing_ms: 0.3759757063290332
  time_since_restore: 5850.780740261078
  time_this_iter_s: 25.604902505874634
  time_total_s: 5850.780740261078
  timers:
    learn_throughput: 8719.787
    learn_time_ms: 18554.581
    sample_throughput: 23809.584
    sample_time_ms: 6795.247
    update_time_ms: 30.577
  timestamp: 1602771282
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: e00ea_00000
  
2020-10-15 14:14:43,860	WARNING util.py:136 -- The `process_trial` operation took 0.6966984272003174 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    229 |          5850.78 | 37050368 |  300.285 |              331.475 |              148.141 |            772.588 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3077.101849152472
    time_step_min: 2872
  date: 2020-10-15_14-15-09
  done: false
  episode_len_mean: 772.5580267976032
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.3689176216225
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 190
  episodes_total: 48064
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3563009387312055e-41
        cur_lr: 5.0e-05
        entropy: 0.081028929601113
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00879169317583243
        total_loss: .inf
        vf_explained_var: 0.9974358677864075
        vf_loss: 0.7859003841876984
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.953333333333337
    gpu_util_percent0: 0.37066666666666676
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621949458926786
    mean_env_wait_ms: 1.220860240808573
    mean_inference_ms: 4.2878462738960375
    mean_raw_obs_processing_ms: 0.3759628040549918
  time_since_restore: 5876.467336893082
  time_this_iter_s: 25.686596632003784
  time_total_s: 5876.467336893082
  timers:
    learn_throughput: 8716.758
    learn_time_ms: 18561.03
    sample_throughput: 23752.392
    sample_time_ms: 6811.609
    update_time_ms: 30.33
  timestamp: 1602771309
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:15:10,450	WARNING util.py:136 -- The `process_trial` operation took 0.6625502109527588 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    230 |          5876.47 | 37212160 |  300.369 |              331.475 |              148.141 |            772.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3076.136410702452
    time_step_min: 2872
  date: 2020-10-15_14-15-35
  done: false
  episode_len_mean: 772.5180840057935
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.52014558997814
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 266
  episodes_total: 48330
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5344514080968086e-41
        cur_lr: 5.0e-05
        entropy: 0.0732636246830225
        entropy_coeff: 0.0005000000000000001
        kl: 0.004025169027348359
        model: {}
        policy_loss: -0.005726525635206296
        total_loss: 0.6366188774506251
        vf_explained_var: 0.998166024684906
        vf_loss: 0.6423820505539576
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.37333333333334
    gpu_util_percent0: 0.37433333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621680017604513
    mean_env_wait_ms: 1.2207774109749518
    mean_inference_ms: 4.287722121720953
    mean_raw_obs_processing_ms: 0.3759450312777427
  time_since_restore: 5901.911420106888
  time_this_iter_s: 25.444083213806152
  time_total_s: 5901.911420106888
  timers:
    learn_throughput: 8729.359
    learn_time_ms: 18534.235
    sample_throughput: 23735.512
    sample_time_ms: 6816.453
    update_time_ms: 30.194
  timestamp: 1602771335
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: e00ea_00000
  
2020-10-15 14:15:36,785	WARNING util.py:136 -- The `process_trial` operation took 0.6644673347473145 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    231 |          5901.91 | 37373952 |   300.52 |              331.475 |              148.141 |            772.518 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3075.5087074942226
    time_step_min: 2872
  date: 2020-10-15_14-16-02
  done: false
  episode_len_mean: 772.4951346225209
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.6198891566888
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 176
  episodes_total: 48506
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7672257040484043e-41
        cur_lr: 5.0e-05
        entropy: 0.07202009794612725
        entropy_coeff: 0.0005000000000000001
        kl: 0.007398661381254594
        model: {}
        policy_loss: -0.00898748671655388
        total_loss: 0.34340933958689374
        vf_explained_var: 0.9986206889152527
        vf_loss: 0.3524328370889028
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.113333333333337
    gpu_util_percent0: 0.29933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621465149874324
    mean_env_wait_ms: 1.2207192304666359
    mean_inference_ms: 4.2876303136706575
    mean_raw_obs_processing_ms: 0.37593321926689255
  time_since_restore: 5927.336086988449
  time_this_iter_s: 25.42466688156128
  time_total_s: 5927.336086988449
  timers:
    learn_throughput: 8750.546
    learn_time_ms: 18489.362
    sample_throughput: 23738.081
    sample_time_ms: 6815.715
    update_time_ms: 29.378
  timestamp: 1602771362
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: e00ea_00000
  
2020-10-15 14:16:03,105	WARNING util.py:136 -- The `process_trial` operation took 0.6719470024108887 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    232 |          5927.34 | 37535744 |   300.62 |              331.475 |              148.141 |            772.495 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3074.7463879811744
    time_step_min: 2872
  date: 2020-10-15_14-16-28
  done: false
  episode_len_mean: 772.4730897965051
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.73140572235025
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 193
  episodes_total: 48699
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7672257040484043e-41
        cur_lr: 5.0e-05
        entropy: 0.07391504384577274
        entropy_coeff: 0.0005000000000000001
        kl: 0.006261485046707094
        model: {}
        policy_loss: -0.009955024099326693
        total_loss: 0.39595066507657367
        vf_explained_var: 0.9986128807067871
        vf_loss: 0.4059426536162694
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.060000000000002
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14621234083300835
    mean_env_wait_ms: 1.2206573999266987
    mean_inference_ms: 4.287529041514478
    mean_raw_obs_processing_ms: 0.3759205177716277
  time_since_restore: 5952.949780225754
  time_this_iter_s: 25.613693237304688
  time_total_s: 5952.949780225754
  timers:
    learn_throughput: 8764.817
    learn_time_ms: 18459.255
    sample_throughput: 23668.384
    sample_time_ms: 6835.786
    update_time_ms: 27.702
  timestamp: 1602771388
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: e00ea_00000
  
2020-10-15 14:16:29,702	WARNING util.py:136 -- The `process_trial` operation took 0.687960147857666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    233 |          5952.95 | 37697536 |  300.731 |              331.475 |              148.141 |            772.473 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3073.7938104290592
    time_step_min: 2872
  date: 2020-10-15_14-16-55
  done: false
  episode_len_mean: 772.4453362743296
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.87594961934786
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 264
  episodes_total: 48963
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7672257040484043e-41
        cur_lr: 5.0e-05
        entropy: 0.07764661498367786
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009212095629967129
        total_loss: .inf
        vf_explained_var: 0.9987835884094238
        vf_loss: 0.42598840345939
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.166666666666668
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462096692058095
    mean_env_wait_ms: 1.2205747680579697
    mean_inference_ms: 4.287405362618505
    mean_raw_obs_processing_ms: 0.3759029257936116
  time_since_restore: 5978.313388824463
  time_this_iter_s: 25.363608598709106
  time_total_s: 5978.313388824463
  timers:
    learn_throughput: 8773.777
    learn_time_ms: 18440.406
    sample_throughput: 23650.142
    sample_time_ms: 6841.058
    update_time_ms: 27.641
  timestamp: 1602771415
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:16:55,971	WARNING util.py:136 -- The `process_trial` operation took 0.6827361583709717 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    234 |          5978.31 | 37859328 |  300.876 |              331.475 |              148.141 |            772.445 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3073.151539840313
    time_step_min: 2872
  date: 2020-10-15_14-17-21
  done: false
  episode_len_mean: 772.4267572957792
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 300.9706692880206
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 49138
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.650838556072606e-41
        cur_lr: 5.0e-05
        entropy: 0.07373856442670028
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01016906057096397
        total_loss: .inf
        vf_explained_var: 0.9981780052185059
        vf_loss: 0.47990840176741284
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95
    gpu_util_percent0: 0.3303333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462076068530953
    mean_env_wait_ms: 1.2205176455054072
    mean_inference_ms: 4.2873177662896715
    mean_raw_obs_processing_ms: 0.3758915316315772
  time_since_restore: 6003.7741086483
  time_this_iter_s: 25.46071982383728
  time_total_s: 6003.7741086483
  timers:
    learn_throughput: 8784.17
    learn_time_ms: 18418.586
    sample_throughput: 23680.112
    sample_time_ms: 6832.4
    update_time_ms: 19.594
  timestamp: 1602771441
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:17:22,351	WARNING util.py:136 -- The `process_trial` operation took 0.6940395832061768 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    235 |          6003.77 | 38021120 |  300.971 |              331.475 |              148.141 |            772.427 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3072.44427313669
    time_step_min: 2872
  date: 2020-10-15_14-17-47
  done: false
  episode_len_mean: 772.4020188097941
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.0802079903954
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 198
  episodes_total: 49336
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.97625783410891e-41
        cur_lr: 5.0e-05
        entropy: 0.07034075073897839
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00715465092798695
        total_loss: .inf
        vf_explained_var: 0.9986400604248047
        vf_loss: 0.3862982864181201
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.41
    gpu_util_percent0: 0.3103333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146205196893069
    mean_env_wait_ms: 1.220454948774877
    mean_inference_ms: 4.287218537284222
    mean_raw_obs_processing_ms: 0.37587878961606896
  time_since_restore: 6029.200785636902
  time_this_iter_s: 25.426676988601685
  time_total_s: 6029.200785636902
  timers:
    learn_throughput: 8793.409
    learn_time_ms: 18399.236
    sample_throughput: 23665.964
    sample_time_ms: 6836.485
    update_time_ms: 20.247
  timestamp: 1602771467
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:17:48,719	WARNING util.py:136 -- The `process_trial` operation took 0.7100422382354736 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    236 |           6029.2 | 38182912 |   301.08 |              331.475 |              148.141 |            772.402 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3071.5154377043227
    time_step_min: 2872
  date: 2020-10-15_14-18-14
  done: false
  episode_len_mean: 772.3711589644327
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.2170256887776
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 260
  episodes_total: 49596
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.964386751163363e-41
        cur_lr: 5.0e-05
        entropy: 0.07172344935437043
        entropy_coeff: 0.0005000000000000001
        kl: 0.00403733456429715
        model: {}
        policy_loss: -0.00742267042126817
        total_loss: 0.6772976865371069
        vf_explained_var: 0.9980723261833191
        vf_loss: 0.6847562144200007
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.78333333333334
    gpu_util_percent0: 0.3713333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462026380856841
    mean_env_wait_ms: 1.2203734744121753
    mean_inference_ms: 4.287097559613571
    mean_raw_obs_processing_ms: 0.3758616146399827
  time_since_restore: 6054.682501792908
  time_this_iter_s: 25.48171615600586
  time_total_s: 6054.682501792908
  timers:
    learn_throughput: 8797.346
    learn_time_ms: 18391.002
    sample_throughput: 23702.6
    sample_time_ms: 6825.918
    update_time_ms: 20.482
  timestamp: 1602771494
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: e00ea_00000
  
2020-10-15 14:18:15,142	WARNING util.py:136 -- The `process_trial` operation took 0.7022156715393066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    237 |          6054.68 | 38344704 |  301.217 |              331.475 |              148.141 |            772.371 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3070.9348455598456
    time_step_min: 2872
  date: 2020-10-15_14-18-41
  done: false
  episode_len_mean: 772.3524613220816
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.30533788761653
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 174
  episodes_total: 49770
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9821933755816813e-41
        cur_lr: 5.0e-05
        entropy: 0.072005620226264
        entropy_coeff: 0.0005000000000000001
        kl: 0.00632201104114453
        model: {}
        policy_loss: -0.007386253176567455
        total_loss: 0.3548670932650566
        vf_explained_var: 0.9986094832420349
        vf_loss: 0.36228935420513153
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.779999999999998
    gpu_util_percent0: 0.3256666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14620069095199908
    mean_env_wait_ms: 1.2203175338917653
    mean_inference_ms: 4.2870134691939565
    mean_raw_obs_processing_ms: 0.37585059217396627
  time_since_restore: 6080.654979228973
  time_this_iter_s: 25.972477436065674
  time_total_s: 6080.654979228973
  timers:
    learn_throughput: 8779.555
    learn_time_ms: 18428.268
    sample_throughput: 23694.478
    sample_time_ms: 6828.258
    update_time_ms: 19.67
  timestamp: 1602771521
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: e00ea_00000
  
2020-10-15 14:18:42,120	WARNING util.py:136 -- The `process_trial` operation took 0.6871590614318848 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    238 |          6080.65 | 38506496 |  301.305 |              331.475 |              148.141 |            772.352 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3070.2850418619555
    time_step_min: 2872
  date: 2020-10-15_14-19-07
  done: false
  episode_len_mean: 772.3343940121678
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.40576999582777
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 198
  episodes_total: 49968
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9821933755816813e-41
        cur_lr: 5.0e-05
        entropy: 0.07624766106406848
        entropy_coeff: 0.0005000000000000001
        kl: 0.004568818529757361
        model: {}
        policy_loss: -0.009400624461704865
        total_loss: 0.5581521714727083
        vf_explained_var: 0.9983034133911133
        vf_loss: 0.5675908997654915
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.236666666666668
    gpu_util_percent0: 0.29300000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461984134450139
    mean_env_wait_ms: 1.2202558608499496
    mean_inference_ms: 4.286919536798025
    mean_raw_obs_processing_ms: 0.37583838370786254
  time_since_restore: 6106.13823056221
  time_this_iter_s: 25.483251333236694
  time_total_s: 6106.13823056221
  timers:
    learn_throughput: 8784.914
    learn_time_ms: 18417.026
    sample_throughput: 23706.35
    sample_time_ms: 6824.838
    update_time_ms: 21.103
  timestamp: 1602771547
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: e00ea_00000
  
2020-10-15 14:19:08,588	WARNING util.py:136 -- The `process_trial` operation took 0.6451399326324463 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    239 |          6106.14 | 38668288 |  301.406 |              331.475 |              148.141 |            772.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3069.3920615311044
    time_step_min: 2872
  date: 2020-10-15_14-19-34
  done: false
  episode_len_mean: 772.3091502747471
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.5447886928537
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 260
  episodes_total: 50228
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4910966877908407e-41
        cur_lr: 5.0e-05
        entropy: 0.07356726254026096
        entropy_coeff: 0.0005000000000000001
        kl: 0.005262071771236758
        model: {}
        policy_loss: -0.00646398354244108
        total_loss: 0.4288538272182147
        vf_explained_var: 0.9988099932670593
        vf_loss: 0.4353545854489009
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.125806451612906
    gpu_util_percent0: 0.32838709677419353
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461958159089264
    mean_env_wait_ms: 1.2201745643075854
    mean_inference_ms: 4.28679925848197
    mean_raw_obs_processing_ms: 0.37582138934952186
  time_since_restore: 6131.701689958572
  time_this_iter_s: 25.563459396362305
  time_total_s: 6131.701689958572
  timers:
    learn_throughput: 8793.095
    learn_time_ms: 18399.892
    sample_throughput: 23690.504
    sample_time_ms: 6829.403
    update_time_ms: 21.63
  timestamp: 1602771574
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: e00ea_00000
  
2020-10-15 14:19:35,230	WARNING util.py:136 -- The `process_trial` operation took 0.6567668914794922 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    240 |           6131.7 | 38830080 |  301.545 |              331.475 |              148.141 |            772.309 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3068.7896544876885
    time_step_min: 2872
  date: 2020-10-15_14-20-00
  done: false
  episode_len_mean: 772.2935597793738
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.63550648743706
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 174
  episodes_total: 50402
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4910966877908407e-41
        cur_lr: 5.0e-05
        entropy: 0.06997853455444177
        entropy_coeff: 0.0005000000000000001
        kl: 0.004194338631350547
        model: {}
        policy_loss: -0.0077500612824223936
        total_loss: 0.292642908791701
        vf_explained_var: 0.9988572597503662
        vf_loss: 0.30042796085278195
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.643333333333338
    gpu_util_percent0: 0.3156666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880000000000001
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14619390754286396
    mean_env_wait_ms: 1.220119059746094
    mean_inference_ms: 4.286717595078883
    mean_raw_obs_processing_ms: 0.37581065132844305
  time_since_restore: 6157.228465557098
  time_this_iter_s: 25.526775598526
  time_total_s: 6157.228465557098
  timers:
    learn_throughput: 8792.789
    learn_time_ms: 18400.532
    sample_throughput: 23686.637
    sample_time_ms: 6830.518
    update_time_ms: 21.615
  timestamp: 1602771600
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: e00ea_00000
  
2020-10-15 14:20:01,720	WARNING util.py:136 -- The `process_trial` operation took 0.6869568824768066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    241 |          6157.23 | 38991872 |  301.636 |              331.475 |              148.141 |            772.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3068.08318993651
    time_step_min: 2872
  date: 2020-10-15_14-20-27
  done: false
  episode_len_mean: 772.2724254461374
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.7424598747301
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 199
  episodes_total: 50601
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.455483438954203e-42
        cur_lr: 5.0e-05
        entropy: 0.0714947401235501
        entropy_coeff: 0.0005000000000000001
        kl: 0.004852147734103103
        model: {}
        policy_loss: -0.00861293205525726
        total_loss: 0.27964236214756966
        vf_explained_var: 0.9989822506904602
        vf_loss: 0.2882910321156184
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.366666666666667
    gpu_util_percent0: 0.3526666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14619178542591418
    mean_env_wait_ms: 1.2200575800770441
    mean_inference_ms: 4.286626288785968
    mean_raw_obs_processing_ms: 0.37579884334256236
  time_since_restore: 6183.006885528564
  time_this_iter_s: 25.778419971466064
  time_total_s: 6183.006885528564
  timers:
    learn_throughput: 8785.972
    learn_time_ms: 18414.81
    sample_throughput: 23631.783
    sample_time_ms: 6846.373
    update_time_ms: 24.135
  timestamp: 1602771627
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: e00ea_00000
  
2020-10-15 14:20:28,419	WARNING util.py:136 -- The `process_trial` operation took 0.6846263408660889 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    242 |          6183.01 | 39153664 |  301.742 |              331.475 |              148.141 |            772.272 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3067.1853637419076
    time_step_min: 2872
  date: 2020-10-15_14-20-54
  done: false
  episode_len_mean: 772.2443719156131
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.8741813447188
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 260
  episodes_total: 50861
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7277417194771016e-42
        cur_lr: 5.0e-05
        entropy: 0.0761820562183857
        entropy_coeff: 0.0005000000000000001
        kl: 0.005340926581993699
        model: {}
        policy_loss: -0.009085948336481428
        total_loss: 0.6162161280711492
        vf_explained_var: 0.9981803297996521
        vf_loss: 0.6253401587406794
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.760000000000005
    gpu_util_percent0: 0.38333333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618921226842882
    mean_env_wait_ms: 1.2199770281647
    mean_inference_ms: 4.286510533957986
    mean_raw_obs_processing_ms: 0.3757820981665901
  time_since_restore: 6208.7732026577
  time_this_iter_s: 25.766317129135132
  time_total_s: 6208.7732026577
  timers:
    learn_throughput: 8780.066
    learn_time_ms: 18427.197
    sample_throughput: 23627.911
    sample_time_ms: 6847.495
    update_time_ms: 24.515
  timestamp: 1602771654
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: e00ea_00000
  
2020-10-15 14:20:55,269	WARNING util.py:136 -- The `process_trial` operation took 0.7338323593139648 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    243 |          6208.77 | 39315456 |  301.874 |              331.475 |              148.141 |            772.244 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3066.591935989959
    time_step_min: 2872
  date: 2020-10-15_14-21-20
  done: false
  episode_len_mean: 772.2251244268526
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 301.95788943239677
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 51034
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.7277417194771016e-42
        cur_lr: 5.0e-05
        entropy: 0.07301412026087443
        entropy_coeff: 0.0005000000000000001
        kl: 0.003639927541371435
        model: {}
        policy_loss: -0.008030932736195004
        total_loss: 0.622412179907163
        vf_explained_var: 0.9976088404655457
        vf_loss: 0.6304796437422434
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79354838709678
    gpu_util_percent0: 0.3374193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461872693250522
    mean_env_wait_ms: 1.2199219641145875
    mean_inference_ms: 4.286430336253198
    mean_raw_obs_processing_ms: 0.3757715220122573
  time_since_restore: 6234.393775939941
  time_this_iter_s: 25.62057328224182
  time_total_s: 6234.393775939941
  timers:
    learn_throughput: 8768.667
    learn_time_ms: 18451.152
    sample_throughput: 23656.754
    sample_time_ms: 6839.146
    update_time_ms: 24.622
  timestamp: 1602771680
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: e00ea_00000
  
2020-10-15 14:21:21,894	WARNING util.py:136 -- The `process_trial` operation took 0.7021927833557129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    244 |          6234.39 | 39477248 |  301.958 |              331.475 |              148.141 |            772.225 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3065.9169027015414
    time_step_min: 2872
  date: 2020-10-15_14-21-47
  done: false
  episode_len_mean: 772.20513320972
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.0632804082595
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 201
  episodes_total: 51235
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8638708597385508e-42
        cur_lr: 5.0e-05
        entropy: 0.068083131685853
        entropy_coeff: 0.0005000000000000001
        kl: 0.004161515408971657
        model: {}
        policy_loss: -0.0072174800795134315
        total_loss: 0.2721819852789243
        vf_explained_var: 0.9990969300270081
        vf_loss: 0.279433511197567
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.223333333333336
    gpu_util_percent0: 0.382
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618510360517337
    mean_env_wait_ms: 1.2198591763617184
    mean_inference_ms: 4.286339853182461
    mean_raw_obs_processing_ms: 0.3757595125597838
  time_since_restore: 6259.818542957306
  time_this_iter_s: 25.424767017364502
  time_total_s: 6259.818542957306
  timers:
    learn_throughput: 8778.283
    learn_time_ms: 18430.939
    sample_throughput: 23643.465
    sample_time_ms: 6842.99
    update_time_ms: 27.094
  timestamp: 1602771707
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: e00ea_00000
  
2020-10-15 14:21:48,292	WARNING util.py:136 -- The `process_trial` operation took 0.7236042022705078 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    245 |          6259.82 | 39639040 |  302.063 |              331.475 |              148.141 |            772.205 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3065.041904761905
    time_step_min: 2872
  date: 2020-10-15_14-22-13
  done: false
  episode_len_mean: 772.1863396255729
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.1948563942856
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 257
  episodes_total: 51492
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.319354298692754e-43
        cur_lr: 5.0e-05
        entropy: 0.07150793075561523
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039049278323849044
        model: {}
        policy_loss: -0.008618688521285852
        total_loss: 0.40696952243645984
        vf_explained_var: 0.9987781643867493
        vf_loss: 0.415623980263869
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.24666666666667
    gpu_util_percent0: 0.36
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14618261681724498
    mean_env_wait_ms: 1.219781023708363
    mean_inference_ms: 4.286227876722819
    mean_raw_obs_processing_ms: 0.37574356988901775
  time_since_restore: 6285.455183267593
  time_this_iter_s: 25.636640310287476
  time_total_s: 6285.455183267593
  timers:
    learn_throughput: 8767.908
    learn_time_ms: 18452.749
    sample_throughput: 23654.384
    sample_time_ms: 6839.832
    update_time_ms: 26.954
  timestamp: 1602771733
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: e00ea_00000
  
2020-10-15 14:22:14,855	WARNING util.py:136 -- The `process_trial` operation took 0.6913619041442871 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    246 |          6285.46 | 39800832 |  302.195 |              331.475 |              148.141 |            772.186 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3064.4698305084744
    time_step_min: 2872
  date: 2020-10-15_14-22-40
  done: false
  episode_len_mean: 772.1726440474578
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.2823082470828
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 51667
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.659677149346377e-43
        cur_lr: 5.0e-05
        entropy: 0.06721841357648373
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038283863104879856
        model: {}
        policy_loss: -0.0068195051001869915
        total_loss: 0.49809737006823224
        vf_explained_var: 0.9980507493019104
        vf_loss: 0.5049504861235619
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.043333333333337
    gpu_util_percent0: 0.36166666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461807931623746
    mean_env_wait_ms: 1.2197257356809574
    mean_inference_ms: 4.286148664105491
    mean_raw_obs_processing_ms: 0.375733094333772
  time_since_restore: 6310.90074634552
  time_this_iter_s: 25.445563077926636
  time_total_s: 6310.90074634552
  timers:
    learn_throughput: 8770.403
    learn_time_ms: 18447.499
    sample_throughput: 23647.966
    sample_time_ms: 6841.688
    update_time_ms: 26.549
  timestamp: 1602771760
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: e00ea_00000
  
2020-10-15 14:22:41,265	WARNING util.py:136 -- The `process_trial` operation took 0.7281296253204346 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    247 |           6310.9 | 39962624 |  302.282 |              331.475 |              148.141 |            772.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3063.816803025625
    time_step_min: 2872
  date: 2020-10-15_14-23-06
  done: false
  episode_len_mean: 772.1569621717503
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.38220538785464
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 199
  episodes_total: 51866
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3298385746731885e-43
        cur_lr: 5.0e-05
        entropy: 0.06930532989402612
        entropy_coeff: 0.0005000000000000001
        kl: 0.003724145508992175
        model: {}
        policy_loss: -0.0063872516972575495
        total_loss: 0.42990011473496753
        vf_explained_var: 0.9985060691833496
        vf_loss: 0.43632202843825024
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19
    gpu_util_percent0: 0.32866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617855043633315
    mean_env_wait_ms: 1.2196648304564097
    mean_inference_ms: 4.286061662956077
    mean_raw_obs_processing_ms: 0.3757213851867924
  time_since_restore: 6336.608045339584
  time_this_iter_s: 25.70729899406433
  time_total_s: 6336.608045339584
  timers:
    learn_throughput: 8779.462
    learn_time_ms: 18428.465
    sample_throughput: 23672.326
    sample_time_ms: 6834.647
    update_time_ms: 27.612
  timestamp: 1602771786
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: e00ea_00000
  
2020-10-15 14:23:07,994	WARNING util.py:136 -- The `process_trial` operation took 0.6889572143554688 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    248 |          6336.61 | 40124416 |  302.382 |              331.475 |              148.141 |            772.157 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3062.9554749150393
    time_step_min: 2872
  date: 2020-10-15_14-23-33
  done: false
  episode_len_mean: 772.1378225419664
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.5142823922681
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 52125
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1649192873365943e-43
        cur_lr: 5.0e-05
        entropy: 0.0670612311611573
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036400756798684597
        model: {}
        policy_loss: -0.008007041991125638
        total_loss: 0.41649602601925534
        vf_explained_var: 0.9987967014312744
        vf_loss: 0.4245366007089615
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.63
    gpu_util_percent0: 0.351
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617631246362006
    mean_env_wait_ms: 1.219586342814171
    mean_inference_ms: 4.285954523038274
    mean_raw_obs_processing_ms: 0.3757058218500649
  time_since_restore: 6362.211957216263
  time_this_iter_s: 25.603911876678467
  time_total_s: 6362.211957216263
  timers:
    learn_throughput: 8772.293
    learn_time_ms: 18443.524
    sample_throughput: 23675.306
    sample_time_ms: 6833.787
    update_time_ms: 25.196
  timestamp: 1602771813
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: e00ea_00000
  
2020-10-15 14:23:34,532	WARNING util.py:136 -- The `process_trial` operation took 0.6892118453979492 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    249 |          6362.21 | 40286208 |  302.514 |              331.475 |              148.141 |            772.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3062.36259257133
    time_step_min: 2872
  date: 2020-10-15_14-24-00
  done: false
  episode_len_mean: 772.1271534828581
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.6011813193022
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 174
  episodes_total: 52299
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.824596436682971e-44
        cur_lr: 5.0e-05
        entropy: 0.06546812194089095
        entropy_coeff: 0.0005000000000000001
        kl: 0.004044814209919423
        model: {}
        policy_loss: -0.005308200614914919
        total_loss: 0.25617067019144696
        vf_explained_var: 0.9989903569221497
        vf_loss: 0.26151159902413684
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12
    gpu_util_percent0: 0.31733333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617444922632203
    mean_env_wait_ms: 1.2195314703351423
    mean_inference_ms: 4.285876407299657
    mean_raw_obs_processing_ms: 0.3756954845933698
  time_since_restore: 6387.799317121506
  time_this_iter_s: 25.58735990524292
  time_total_s: 6387.799317121506
  timers:
    learn_throughput: 8763.729
    learn_time_ms: 18461.548
    sample_throughput: 23730.863
    sample_time_ms: 6817.788
    update_time_ms: 24.647
  timestamp: 1602771840
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: e00ea_00000
  
2020-10-15 14:24:01,171	WARNING util.py:136 -- The `process_trial` operation took 0.7149159908294678 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    250 |           6387.8 | 40448000 |  302.601 |              331.475 |              148.141 |            772.127 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3061.7027470118383
    time_step_min: 2872
  date: 2020-10-15_14-24-26
  done: false
  episode_len_mean: 772.1130116764128
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.70334519118325
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 200
  episodes_total: 52499
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9122982183414857e-44
        cur_lr: 5.0e-05
        entropy: 0.06768925363818805
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036645220631423094
        model: {}
        policy_loss: -0.006376860859745648
        total_loss: 0.4791041538119316
        vf_explained_var: 0.998319685459137
        vf_loss: 0.48551485190788907
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.81612903225807
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14617215954556828
    mean_env_wait_ms: 1.219470095299989
    mean_inference_ms: 4.2857904283638115
    mean_raw_obs_processing_ms: 0.3756838112258891
  time_since_restore: 6413.571632862091
  time_this_iter_s: 25.772315740585327
  time_total_s: 6413.571632862091
  timers:
    learn_throughput: 8750.426
    learn_time_ms: 18489.615
    sample_throughput: 23752.343
    sample_time_ms: 6811.623
    update_time_ms: 26.482
  timestamp: 1602771866
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: e00ea_00000
  
2020-10-15 14:24:27,947	WARNING util.py:136 -- The `process_trial` operation took 0.7085986137390137 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    251 |          6413.57 | 40609792 |  302.703 |              331.475 |              148.141 |            772.113 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3060.939503737148
    time_step_min: 2872
  date: 2020-10-15_14-24-53
  done: false
  episode_len_mean: 772.1004056410645
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.8200892081021
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 257
  episodes_total: 52756
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4561491091707428e-44
        cur_lr: 5.0e-05
        entropy: 0.07516328617930412
        entropy_coeff: 0.0005000000000000001
        kl: 0.00505304984593143
        model: {}
        policy_loss: -0.009635074685017267
        total_loss: 0.9501806994279226
        vf_explained_var: 0.9973334670066833
        vf_loss: 0.9598533660173416
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.956666666666667
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616999288883234
    mean_env_wait_ms: 1.219392781725333
    mean_inference_ms: 4.28568658414301
    mean_raw_obs_processing_ms: 0.37566876151722806
  time_since_restore: 6439.266457557678
  time_this_iter_s: 25.694824695587158
  time_total_s: 6439.266457557678
  timers:
    learn_throughput: 8747.197
    learn_time_ms: 18496.44
    sample_throughput: 23808.677
    sample_time_ms: 6795.506
    update_time_ms: 24.653
  timestamp: 1602771893
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: e00ea_00000
  
2020-10-15 14:24:54,621	WARNING util.py:136 -- The `process_trial` operation took 0.710853099822998 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    252 |          6439.27 | 40771584 |   302.82 |              331.475 |              148.141 |              772.1 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3060.423471799429
    time_step_min: 2872
  date: 2020-10-15_14-25-20
  done: false
  episode_len_mean: 772.0940091817649
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.9028617206813
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 52931
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4561491091707428e-44
        cur_lr: 5.0e-05
        entropy: 0.06699428955713908
        entropy_coeff: 0.0005000000000000001
        kl: 0.004269946288938324
        model: {}
        policy_loss: -0.006363960371042292
        total_loss: 0.41440332929293316
        vf_explained_var: 0.9984033703804016
        vf_loss: 0.420800785223643
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.196666666666665
    gpu_util_percent0: 0.4066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616824905120113
    mean_env_wait_ms: 1.2193382198156844
    mean_inference_ms: 4.2856115965410595
    mean_raw_obs_processing_ms: 0.37565868206972497
  time_since_restore: 6464.795279741287
  time_this_iter_s: 25.52882218360901
  time_total_s: 6464.795279741287
  timers:
    learn_throughput: 8751.584
    learn_time_ms: 18487.167
    sample_throughput: 23860.084
    sample_time_ms: 6780.865
    update_time_ms: 25.672
  timestamp: 1602771920
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: e00ea_00000
  
2020-10-15 14:25:21,126	WARNING util.py:136 -- The `process_trial` operation took 0.7278566360473633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    253 |           6464.8 | 40933376 |  302.903 |              331.475 |              148.141 |            772.094 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3059.803903100629
    time_step_min: 2872
  date: 2020-10-15_14-25-46
  done: false
  episode_len_mean: 772.0891243788586
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 302.9955400260702
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 197
  episodes_total: 53128
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.280745545853714e-45
        cur_lr: 5.0e-05
        entropy: 0.06988789824148019
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008412929232387492
        total_loss: .inf
        vf_explained_var: 0.9988545775413513
        vf_loss: 0.33579207460085553
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16666666666667
    gpu_util_percent0: 0.3696666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616624180210622
    mean_env_wait_ms: 1.2192780101969376
    mean_inference_ms: 4.285527286375296
    mean_raw_obs_processing_ms: 0.3756475588828669
  time_since_restore: 6490.17084312439
  time_this_iter_s: 25.375563383102417
  time_total_s: 6490.17084312439
  timers:
    learn_throughput: 8761.829
    learn_time_ms: 18465.551
    sample_throughput: 23840.545
    sample_time_ms: 6786.422
    update_time_ms: 25.567
  timestamp: 1602771946
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:25:47,460	WARNING util.py:136 -- The `process_trial` operation took 0.7152707576751709 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    254 |          6490.17 | 41095168 |  302.996 |              331.475 |              148.141 |            772.089 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3059.0238639772047
    time_step_min: 2872
  date: 2020-10-15_14-26-13
  done: false
  episode_len_mean: 772.0910912973438
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.1159903837386
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 258
  episodes_total: 53386
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.092111831878057e-44
        cur_lr: 5.0e-05
        entropy: 0.06903832592070103
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036586751812137663
        model: {}
        policy_loss: -0.007949491269149197
        total_loss: 0.38262133797009784
        vf_explained_var: 0.9989418387413025
        vf_loss: 0.3906053553024928
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.910000000000004
    gpu_util_percent0: 0.3106666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616390486210765
    mean_env_wait_ms: 1.219200760882348
    mean_inference_ms: 4.285426131318992
    mean_raw_obs_processing_ms: 0.37563236618831836
  time_since_restore: 6516.040311098099
  time_this_iter_s: 25.869467973709106
  time_total_s: 6516.040311098099
  timers:
    learn_throughput: 8738.816
    learn_time_ms: 18514.18
    sample_throughput: 23822.634
    sample_time_ms: 6791.525
    update_time_ms: 23.418
  timestamp: 1602771973
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: e00ea_00000
  
2020-10-15 14:26:14,402	WARNING util.py:136 -- The `process_trial` operation took 0.709418535232544 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    255 |          6516.04 | 41256960 |  303.116 |              331.475 |              148.141 |            772.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3058.470889387145
    time_step_min: 2872
  date: 2020-10-15_14-26-39
  done: false
  episode_len_mean: 772.090941339009
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.19901660267976
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 176
  episodes_total: 53562
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.460559159390285e-45
        cur_lr: 5.0e-05
        entropy: 0.06550056363145511
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035187224857509136
        model: {}
        policy_loss: -0.005968147612293251
        total_loss: 0.3257799645264943
        vf_explained_var: 0.9987519383430481
        vf_loss: 0.3317808583378792
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.303333333333335
    gpu_util_percent0: 0.3
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461621934532874
    mean_env_wait_ms: 1.2191464958513478
    mean_inference_ms: 4.28535608877531
    mean_raw_obs_processing_ms: 0.3756226797402272
  time_since_restore: 6541.422525167465
  time_this_iter_s: 25.382214069366455
  time_total_s: 6541.422525167465
  timers:
    learn_throughput: 8744.49
    learn_time_ms: 18502.165
    sample_throughput: 23873.411
    sample_time_ms: 6777.079
    update_time_ms: 22.857
  timestamp: 1602771999
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: e00ea_00000
  
2020-10-15 14:26:40,888	WARNING util.py:136 -- The `process_trial` operation took 0.7158181667327881 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    256 |          6541.42 | 41418752 |  303.199 |              331.475 |              148.141 |            772.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3057.871427773537
    time_step_min: 2872
  date: 2020-10-15_14-27-06
  done: false
  episode_len_mean: 772.0871918891266
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.2913170398057
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 193
  episodes_total: 53755
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7302795796951426e-45
        cur_lr: 5.0e-05
        entropy: 0.07014775772889455
        entropy_coeff: 0.0005000000000000001
        kl: 0.004526332758056621
        model: {}
        policy_loss: -0.005417943291831762
        total_loss: 0.4401865949233373
        vf_explained_var: 0.9986186027526855
        vf_loss: 0.44563961774110794
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180645161290325
    gpu_util_percent0: 0.29096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14616020312796504
    mean_env_wait_ms: 1.2190876782195936
    mean_inference_ms: 4.28527416168075
    mean_raw_obs_processing_ms: 0.37561197499966736
  time_since_restore: 6567.312840461731
  time_this_iter_s: 25.890315294265747
  time_total_s: 6567.312840461731
  timers:
    learn_throughput: 8729.321
    learn_time_ms: 18534.317
    sample_throughput: 23838.338
    sample_time_ms: 6787.05
    update_time_ms: 22.972
  timestamp: 1602772026
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: e00ea_00000
  
2020-10-15 14:27:07,811	WARNING util.py:136 -- The `process_trial` operation took 0.784914493560791 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    257 |          6567.31 | 41580544 |  303.291 |              331.475 |              148.141 |            772.087 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3057.089361150226
    time_step_min: 2872
  date: 2020-10-15_14-27-33
  done: false
  episode_len_mean: 772.0814973895657
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.4128448928132
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 54014
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3651397898475713e-45
        cur_lr: 5.0e-05
        entropy: 0.07401985364655654
        entropy_coeff: 0.0005000000000000001
        kl: 0.007532219441297154
        model: {}
        policy_loss: -0.007929742455113834
        total_loss: 0.3394857446352641
        vf_explained_var: 0.9990690350532532
        vf_loss: 0.3474524940053622
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612904
    gpu_util_percent0: 0.32516129032258057
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615793487552836
    mean_env_wait_ms: 1.2190096860102138
    mean_inference_ms: 4.285174020285522
    mean_raw_obs_processing_ms: 0.37559671847800086
  time_since_restore: 6593.017550468445
  time_this_iter_s: 25.704710006713867
  time_total_s: 6593.017550468445
  timers:
    learn_throughput: 8728.088
    learn_time_ms: 18536.936
    sample_throughput: 23850.44
    sample_time_ms: 6783.606
    update_time_ms: 22.078
  timestamp: 1602772053
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: e00ea_00000
  
2020-10-15 14:27:34,686	WARNING util.py:136 -- The `process_trial` operation took 0.7198328971862793 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    258 |          6593.02 | 41742336 |  303.413 |              331.475 |              148.141 |            772.081 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3056.5127049785788
    time_step_min: 2872
  date: 2020-10-15_14-28-00
  done: false
  episode_len_mean: 772.0771487618556
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.4962644491191
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 180
  episodes_total: 54194
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3651397898475713e-45
        cur_lr: 5.0e-05
        entropy: 0.07038005068898201
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006341001052836266
        total_loss: .inf
        vf_explained_var: 0.9986513257026672
        vf_loss: 0.35842353602250415
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.476666666666667
    gpu_util_percent0: 0.2783333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615625492505716
    mean_env_wait_ms: 1.2189547415698
    mean_inference_ms: 4.285105085364242
    mean_raw_obs_processing_ms: 0.37558703745807154
  time_since_restore: 6618.864349365234
  time_this_iter_s: 25.84679889678955
  time_total_s: 6618.864349365234
  timers:
    learn_throughput: 8719.995
    learn_time_ms: 18554.139
    sample_throughput: 23829.958
    sample_time_ms: 6789.437
    update_time_ms: 22.196
  timestamp: 1602772080
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:28:01,619	WARNING util.py:136 -- The `process_trial` operation took 0.748070240020752 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    259 |          6618.86 | 41904128 |  303.496 |              331.475 |              148.141 |            772.077 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3055.9857560867886
    time_step_min: 2872
  date: 2020-10-15_14-28-27
  done: false
  episode_len_mean: 772.0703738438058
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.5769857230663
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 187
  episodes_total: 54381
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0477096847713574e-45
        cur_lr: 5.0e-05
        entropy: 0.07454227035244305
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007939414663269417
        total_loss: .inf
        vf_explained_var: 0.9971213340759277
        vf_loss: 0.8440263668696085
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.63225806451613
    gpu_util_percent0: 0.384516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615438030056782
    mean_env_wait_ms: 1.2188979796500405
    mean_inference_ms: 4.285027120950981
    mean_raw_obs_processing_ms: 0.3755767287959663
  time_since_restore: 6644.347903966904
  time_this_iter_s: 25.48355460166931
  time_total_s: 6644.347903966904
  timers:
    learn_throughput: 8728.258
    learn_time_ms: 18536.574
    sample_throughput: 23807.037
    sample_time_ms: 6795.974
    update_time_ms: 22.216
  timestamp: 1602772107
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:28:28,230	WARNING util.py:136 -- The `process_trial` operation took 0.711296558380127 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    260 |          6644.35 | 42065920 |  303.577 |              331.475 |              148.141 |             772.07 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3055.2424630938863
    time_step_min: 2872
  date: 2020-10-15_14-28-53
  done: false
  episode_len_mean: 772.0653550512445
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.6851965851784
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 54640
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0715645271570356e-45
        cur_lr: 5.0e-05
        entropy: 0.07970162356893222
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007074590116341521
        total_loss: .inf
        vf_explained_var: 0.9972498416900635
        vf_loss: 1.0096501211325328
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.043333333333333
    gpu_util_percent0: 0.3333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615232947194828
    mean_env_wait_ms: 1.2188211208182864
    mean_inference_ms: 4.284936365688364
    mean_raw_obs_processing_ms: 0.375562204103456
  time_since_restore: 6670.06028175354
  time_this_iter_s: 25.712377786636353
  time_total_s: 6670.06028175354
  timers:
    learn_throughput: 8731.707
    learn_time_ms: 18529.252
    sample_throughput: 23776.058
    sample_time_ms: 6804.829
    update_time_ms: 21.62
  timestamp: 1602772133
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:28:54,953	WARNING util.py:136 -- The `process_trial` operation took 0.7605690956115723 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    261 |          6670.06 | 42227712 |  303.685 |              331.475 |              148.141 |            772.065 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3054.7528840537384
    time_step_min: 2872
  date: 2020-10-15_14-29-20
  done: false
  episode_len_mean: 772.0623244446066
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.76415838242355
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 186
  episodes_total: 54826
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.607346790735553e-45
        cur_lr: 5.0e-05
        entropy: 0.07145859363178413
        entropy_coeff: 0.0005000000000000001
        kl: 0.004392656070801119
        model: {}
        policy_loss: -0.007625243490717064
        total_loss: 0.5496115982532501
        vf_explained_var: 0.9979862570762634
        vf_loss: 0.5572725931803385
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89333333333333
    gpu_util_percent0: 0.31966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14615042772097736
    mean_env_wait_ms: 1.2187642176992675
    mean_inference_ms: 4.284860939574168
    mean_raw_obs_processing_ms: 0.3755520939041091
  time_since_restore: 6695.85867857933
  time_this_iter_s: 25.798396825790405
  time_total_s: 6695.85867857933
  timers:
    learn_throughput: 8725.276
    learn_time_ms: 18542.909
    sample_throughput: 23784.015
    sample_time_ms: 6802.552
    update_time_ms: 21.01
  timestamp: 1602772160
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: e00ea_00000
  
2020-10-15 14:29:21,835	WARNING util.py:136 -- The `process_trial` operation took 0.7186641693115234 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    262 |          6695.86 | 42389504 |  303.764 |              331.475 |              148.141 |            772.062 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3054.2062617111465
    time_step_min: 2872
  date: 2020-10-15_14-29-47
  done: false
  episode_len_mean: 772.057970224137
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.84597570843977
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 185
  episodes_total: 55011
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3036733953677765e-45
        cur_lr: 5.0e-05
        entropy: 0.06821891106665134
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038820880969675877
        model: {}
        policy_loss: -0.009009944934708377
        total_loss: 0.36592913667360943
        vf_explained_var: 0.9986945986747742
        vf_loss: 0.374973197778066
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.783870967741937
    gpu_util_percent0: 0.295483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461485420180005
    mean_env_wait_ms: 1.2187085225716388
    mean_inference_ms: 4.2847836993564234
    mean_raw_obs_processing_ms: 0.37554190593045217
  time_since_restore: 6721.711605548859
  time_this_iter_s: 25.8529269695282
  time_total_s: 6721.711605548859
  timers:
    learn_throughput: 8716.553
    learn_time_ms: 18561.465
    sample_throughput: 23770.226
    sample_time_ms: 6806.498
    update_time_ms: 27.532
  timestamp: 1602772187
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: e00ea_00000
  
2020-10-15 14:29:48,692	WARNING util.py:136 -- The `process_trial` operation took 0.7120554447174072 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    263 |          6721.71 | 42551296 |  303.846 |              331.475 |              148.141 |            772.058 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3053.432050701675
    time_step_min: 2872
  date: 2020-10-15_14-30-14
  done: false
  episode_len_mean: 772.0578645484647
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 303.96074245997363
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 256
  episodes_total: 55267
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1518366976838883e-45
        cur_lr: 5.0e-05
        entropy: 0.07135148160159588
        entropy_coeff: 0.0005000000000000001
        kl: 0.006529244633081059
        model: {}
        policy_loss: -0.007434637123272599
        total_loss: 0.3138115244607131
        vf_explained_var: 0.9992238879203796
        vf_loss: 0.321281844129165
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.43
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614684806902264
    mean_env_wait_ms: 1.2186336162249678
    mean_inference_ms: 4.284704956516722
    mean_raw_obs_processing_ms: 0.3755285078784818
  time_since_restore: 6747.448387622833
  time_this_iter_s: 25.73678207397461
  time_total_s: 6747.448387622833
  timers:
    learn_throughput: 8699.765
    learn_time_ms: 18597.284
    sample_throughput: 23776.264
    sample_time_ms: 6804.77
    update_time_ms: 27.794
  timestamp: 1602772214
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: e00ea_00000
  
2020-10-15 14:30:15,515	WARNING util.py:136 -- The `process_trial` operation took 0.7199652194976807 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    264 |          6747.45 | 42713088 |  303.961 |              331.475 |              148.141 |            772.058 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3052.877345892883
    time_step_min: 2872
  date: 2020-10-15_14-30-41
  done: false
  episode_len_mean: 772.060099534783
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.0466910804465
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 191
  episodes_total: 55458
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1518366976838883e-45
        cur_lr: 5.0e-05
        entropy: 0.06608457118272781
        entropy_coeff: 0.0005000000000000001
        kl: 0.00414250591226543
        model: {}
        policy_loss: -0.007724383705256817
        total_loss: 0.26228128373622894
        vf_explained_var: 0.9990253448486328
        vf_loss: 0.2700387065609296
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.348387096774196
    gpu_util_percent0: 0.3164516129032259
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614471897313214
    mean_env_wait_ms: 1.2185750572034668
    mean_inference_ms: 4.284623046036691
    mean_raw_obs_processing_ms: 0.37551791483385755
  time_since_restore: 6773.015345811844
  time_this_iter_s: 25.56695818901062
  time_total_s: 6773.015345811844
  timers:
    learn_throughput: 8712.196
    learn_time_ms: 18570.749
    sample_throughput: 23788.751
    sample_time_ms: 6801.198
    update_time_ms: 28.029
  timestamp: 1602772241
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: e00ea_00000
  
2020-10-15 14:30:42,344	WARNING util.py:136 -- The `process_trial` operation took 0.7929577827453613 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    265 |          6773.02 | 42874880 |  304.047 |              331.475 |              148.141 |             772.06 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3052.37441542557
    time_step_min: 2872
  date: 2020-10-15_14-31-08
  done: false
  episode_len_mean: 772.0621697401057
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.1251025296643
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 180
  episodes_total: 55638
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.759183488419441e-46
        cur_lr: 5.0e-05
        entropy: 0.0684354944775502
        entropy_coeff: 0.0005000000000000001
        kl: 0.004106684296857566
        model: {}
        policy_loss: -0.006113408492334808
        total_loss: 0.4357297867536545
        vf_explained_var: 0.99859619140625
        vf_loss: 0.44187742471694946
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.759999999999998
    gpu_util_percent0: 0.3213333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.883333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614299344001375
    mean_env_wait_ms: 1.2185207148114714
    mean_inference_ms: 4.284549623957
    mean_raw_obs_processing_ms: 0.3755082640980495
  time_since_restore: 6798.798646211624
  time_this_iter_s: 25.783300399780273
  time_total_s: 6798.798646211624
  timers:
    learn_throughput: 8699.658
    learn_time_ms: 18597.512
    sample_throughput: 23743.108
    sample_time_ms: 6814.272
    update_time_ms: 28.083
  timestamp: 1602772268
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: e00ea_00000
  
2020-10-15 14:31:09,179	WARNING util.py:136 -- The `process_trial` operation took 0.7674150466918945 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    266 |           6798.8 | 43036672 |  304.125 |              331.475 |              148.141 |            772.062 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3051.7060424312954
    time_step_min: 2872
  date: 2020-10-15_14-31-34
  done: false
  episode_len_mean: 772.0684652128022
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.2351411859079
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 259
  episodes_total: 55897
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8795917442097207e-46
        cur_lr: 5.0e-05
        entropy: 0.06861529375116031
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038730999416050813
        model: {}
        policy_loss: -0.006188205617945641
        total_loss: 0.46869293848673504
        vf_explained_var: 0.9986915588378906
        vf_loss: 0.4749154473344485
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0741935483871
    gpu_util_percent0: 0.3051612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14614118352251362
    mean_env_wait_ms: 1.2184455017402538
    mean_inference_ms: 4.284470489527327
    mean_raw_obs_processing_ms: 0.3754945883990894
  time_since_restore: 6824.4051196575165
  time_this_iter_s: 25.606473445892334
  time_total_s: 6824.4051196575165
  timers:
    learn_throughput: 8707.343
    learn_time_ms: 18581.099
    sample_throughput: 23784.29
    sample_time_ms: 6802.474
    update_time_ms: 28.281
  timestamp: 1602772294
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: e00ea_00000
  
2020-10-15 14:31:35,976	WARNING util.py:136 -- The `process_trial` operation took 0.7730977535247803 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    267 |          6824.41 | 43198464 |  304.235 |              331.475 |              148.141 |            772.068 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3051.153603225864
    time_step_min: 2872
  date: 2020-10-15_14-32-01
  done: false
  episode_len_mean: 772.0715291768439
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.319244613224
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 192
  episodes_total: 56089
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4397958721048603e-46
        cur_lr: 5.0e-05
        entropy: 0.06336048152297735
        entropy_coeff: 0.0005000000000000001
        kl: 0.003595349359481285
        model: {}
        policy_loss: -0.0075108537275809795
        total_loss: 0.3200475374857585
        vf_explained_var: 0.998833179473877
        vf_loss: 0.32759007314840954
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.066666666666666
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461390423939736
    mean_env_wait_ms: 1.218386943711173
    mean_inference_ms: 4.284390622531949
    mean_raw_obs_processing_ms: 0.37548440167892705
  time_since_restore: 6849.862169504166
  time_this_iter_s: 25.45704984664917
  time_total_s: 6849.862169504166
  timers:
    learn_throughput: 8725.581
    learn_time_ms: 18542.262
    sample_throughput: 23765.671
    sample_time_ms: 6807.803
    update_time_ms: 28.041
  timestamp: 1602772321
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: e00ea_00000
  
2020-10-15 14:32:02,435	WARNING util.py:136 -- The `process_trial` operation took 0.7402477264404297 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    268 |          6849.86 | 43360256 |  304.319 |              331.475 |              148.141 |            772.072 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3050.6414876209446
    time_step_min: 2872
  date: 2020-10-15_14-32-27
  done: false
  episode_len_mean: 772.0724771620517
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.39584520425535
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 177
  episodes_total: 56266
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.198979360524302e-47
        cur_lr: 5.0e-05
        entropy: 0.06260388220349948
        entropy_coeff: 0.0005000000000000001
        kl: 0.004023471265099943
        model: {}
        policy_loss: -0.006516338130798734
        total_loss: 0.30929727603991825
        vf_explained_var: 0.9988536834716797
        vf_loss: 0.3158449058731397
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.546666666666674
    gpu_util_percent0: 0.33566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461374963172275
    mean_env_wait_ms: 1.2183342581292984
    mean_inference_ms: 4.284321296303458
    mean_raw_obs_processing_ms: 0.37547502106811725
  time_since_restore: 6875.378041982651
  time_this_iter_s: 25.515872478485107
  time_total_s: 6875.378041982651
  timers:
    learn_throughput: 8741.639
    learn_time_ms: 18508.199
    sample_throughput: 23770.98
    sample_time_ms: 6806.282
    update_time_ms: 29.538
  timestamp: 1602772347
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: e00ea_00000
  
2020-10-15 14:32:28,977	WARNING util.py:136 -- The `process_trial` operation took 0.7617621421813965 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    269 |          6875.38 | 43522048 |  304.396 |              331.475 |              148.141 |            772.072 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3049.92110341897
    time_step_min: 2872
  date: 2020-10-15_14-32-54
  done: false
  episode_len_mean: 772.0759363776295
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.50750601501665
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 255
  episodes_total: 56521
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.599489680262151e-47
        cur_lr: 5.0e-05
        entropy: 0.0661237109452486
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007639186359786739
        total_loss: .nan
        vf_explained_var: 0.9990110993385315
        vf_loss: 0.3451392675439517
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54333333333334
    gpu_util_percent0: 0.35366666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613569272931184
    mean_env_wait_ms: 1.2182598076196653
    mean_inference_ms: 4.284244363217452
    mean_raw_obs_processing_ms: 0.375461870477095
  time_since_restore: 6901.084764480591
  time_this_iter_s: 25.706722497940063
  time_total_s: 6901.084764480591
  timers:
    learn_throughput: 8744.292
    learn_time_ms: 18502.585
    sample_throughput: 23707.514
    sample_time_ms: 6824.503
    update_time_ms: 37.571
  timestamp: 1602772374
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:32:55,682	WARNING util.py:136 -- The `process_trial` operation took 0.7372786998748779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    270 |          6901.08 | 43683840 |  304.508 |              331.475 |              148.141 |            772.076 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3049.362850438434
    time_step_min: 2872
  date: 2020-10-15_14-33-21
  done: false
  episode_len_mean: 772.0796001480933
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.59191908506995
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 200
  episodes_total: 56721
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.399234520393228e-47
        cur_lr: 5.0e-05
        entropy: 0.06546107927958171
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005962354325068493
        total_loss: .nan
        vf_explained_var: 0.9990379810333252
        vf_loss: 0.2806297540664673
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.013333333333332
    gpu_util_percent0: 0.3593333333333332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613355102515754
    mean_env_wait_ms: 1.2181998454044318
    mean_inference_ms: 4.2841653113555695
    mean_raw_obs_processing_ms: 0.3754514403056284
  time_since_restore: 6926.717462062836
  time_this_iter_s: 25.632697582244873
  time_total_s: 6926.717462062836
  timers:
    learn_throughput: 8748.35
    learn_time_ms: 18494.001
    sample_throughput: 23706.432
    sample_time_ms: 6824.815
    update_time_ms: 37.45
  timestamp: 1602772401
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:33:22,377	WARNING util.py:136 -- The `process_trial` operation took 0.7997453212738037 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    271 |          6926.72 | 43845632 |  304.592 |              331.475 |              148.141 |             772.08 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3048.897414248021
    time_step_min: 2872
  date: 2020-10-15_14-33-47
  done: false
  episode_len_mean: 772.0821380861984
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.66118365685986
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 56892
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.098851780589841e-47
        cur_lr: 5.0e-05
        entropy: 0.06958399092157681
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007153665375274916
        total_loss: .nan
        vf_explained_var: 0.9987223148345947
        vf_loss: 0.34910532583793
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.266666666666666
    gpu_util_percent0: 0.3063333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613210718198677
    mean_env_wait_ms: 1.2181485754649526
    mean_inference_ms: 4.2841027116192825
    mean_raw_obs_processing_ms: 0.3754426646806342
  time_since_restore: 6952.104971170425
  time_this_iter_s: 25.38750910758972
  time_total_s: 6952.104971170425
  timers:
    learn_throughput: 8765.847
    learn_time_ms: 18457.087
    sample_throughput: 23728.305
    sample_time_ms: 6818.523
    update_time_ms: 37.572
  timestamp: 1602772427
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:33:48,851	WARNING util.py:136 -- The `process_trial` operation took 0.8146512508392334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    272 |           6952.1 | 44007424 |  304.661 |              331.475 |              148.141 |            772.082 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3048.292984483906
    time_step_min: 2872
  date: 2020-10-15_14-34-14
  done: false
  episode_len_mean: 772.0895982080358
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.75526103114316
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 252
  episodes_total: 57144
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2148277670884763e-46
        cur_lr: 5.0e-05
        entropy: 0.07371805608272552
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00824079298278472
        total_loss: .nan
        vf_explained_var: 0.9977874755859375
        vf_loss: 0.822321742773056
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.651612903225804
    gpu_util_percent0: 0.34838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14613041246693495
    mean_env_wait_ms: 1.21807659721643
    mean_inference_ms: 4.2840296992088795
    mean_raw_obs_processing_ms: 0.37543005825601855
  time_since_restore: 6977.789012670517
  time_this_iter_s: 25.684041500091553
  time_total_s: 6977.789012670517
  timers:
    learn_throughput: 8767.706
    learn_time_ms: 18453.174
    sample_throughput: 23746.911
    sample_time_ms: 6813.181
    update_time_ms: 30.237
  timestamp: 1602772454
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:34:15,789	WARNING util.py:136 -- The `process_trial` operation took 0.8161149024963379 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    273 |          6977.79 | 44169216 |  304.755 |              331.475 |              148.141 |             772.09 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3047.7869619082517
    time_step_min: 2872
  date: 2020-10-15_14-34-41
  done: false
  episode_len_mean: 772.092884169413
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.8340030529707
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 207
  episodes_total: 57351
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8222416506327147e-46
        cur_lr: 5.0e-05
        entropy: 0.07264351472258568
        entropy_coeff: 0.0005000000000000001
        kl: 0.006725025208046039
        model: {}
        policy_loss: -0.006983977470857401
        total_loss: 0.5059908082087835
        vf_explained_var: 0.998327910900116
        vf_loss: 0.5130111103256544
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.48666666666667
    gpu_util_percent0: 0.3406666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612820963078862
    mean_env_wait_ms: 1.2180144978538168
    mean_inference_ms: 4.283948251968716
    mean_raw_obs_processing_ms: 0.37541888399022466
  time_since_restore: 7003.141676425934
  time_this_iter_s: 25.35266375541687
  time_total_s: 7003.141676425934
  timers:
    learn_throughput: 8791.138
    learn_time_ms: 18403.989
    sample_throughput: 23738.205
    sample_time_ms: 6815.68
    update_time_ms: 29.905
  timestamp: 1602772481
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: e00ea_00000
  
2020-10-15 14:34:42,208	WARNING util.py:136 -- The `process_trial` operation took 0.8023514747619629 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    274 |          7003.14 | 44331008 |  304.834 |              331.475 |              148.141 |            772.093 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3047.310287235338
    time_step_min: 2872
  date: 2020-10-15_14-35-07
  done: false
  episode_len_mean: 772.0966081952678
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 304.9063927289446
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 57521
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8222416506327147e-46
        cur_lr: 5.0e-05
        entropy: 0.06995581338802974
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053434319561347365
        model: {}
        policy_loss: -0.005060363851953298
        total_loss: 0.1989205926656723
        vf_explained_var: 0.9992356300354004
        vf_loss: 0.2040159416695436
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19
    gpu_util_percent0: 0.3793333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612684170194506
    mean_env_wait_ms: 1.2179641701576565
    mean_inference_ms: 4.283888808546501
    mean_raw_obs_processing_ms: 0.3754107491791646
  time_since_restore: 7028.704218149185
  time_this_iter_s: 25.562541723251343
  time_total_s: 7028.704218149185
  timers:
    learn_throughput: 8791.611
    learn_time_ms: 18402.998
    sample_throughput: 23740.458
    sample_time_ms: 6815.033
    update_time_ms: 30.395
  timestamp: 1602772507
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: e00ea_00000
  
2020-10-15 14:35:08,829	WARNING util.py:136 -- The `process_trial` operation took 0.7882542610168457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    275 |           7028.7 | 44492800 |  304.906 |              331.475 |              148.141 |            772.097 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3046.6451143451145
    time_step_min: 2872
  date: 2020-10-15_14-35-34
  done: false
  episode_len_mean: 772.1015892801496
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.0048154758346
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 241
  episodes_total: 57762
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8222416506327147e-46
        cur_lr: 5.0e-05
        entropy: 0.0718797209362189
        entropy_coeff: 0.0005000000000000001
        kl: 0.005574945981303851
        model: {}
        policy_loss: -0.00777263722072045
        total_loss: 0.32242534806330997
        vf_explained_var: 0.999037504196167
        vf_loss: 0.3302339365084966
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.835483870967746
    gpu_util_percent0: 0.3158064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612514927340572
    mean_env_wait_ms: 1.2178963039953157
    mean_inference_ms: 4.283817616543403
    mean_raw_obs_processing_ms: 0.375398996390759
  time_since_restore: 7054.354650259018
  time_this_iter_s: 25.650432109832764
  time_total_s: 7054.354650259018
  timers:
    learn_throughput: 8794.099
    learn_time_ms: 18397.791
    sample_throughput: 23770.169
    sample_time_ms: 6806.515
    update_time_ms: 30.619
  timestamp: 1602772534
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: e00ea_00000
  
2020-10-15 14:35:35,722	WARNING util.py:136 -- The `process_trial` operation took 0.7763793468475342 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    276 |          7054.35 | 44654592 |  305.005 |              331.475 |              148.141 |            772.102 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3046.0821035917083
    time_step_min: 2872
  date: 2020-10-15_14-36-01
  done: false
  episode_len_mean: 772.1069660750935
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.08941905211384
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 219
  episodes_total: 57981
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8222416506327147e-46
        cur_lr: 5.0e-05
        entropy: 0.07063198958834012
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00794448009886158
        total_loss: .nan
        vf_explained_var: 0.9980829358100891
        vf_loss: 0.603778233130773
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.370000000000005
    gpu_util_percent0: 0.312
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612298453918177
    mean_env_wait_ms: 1.2178296825935457
    mean_inference_ms: 4.283737660384711
    mean_raw_obs_processing_ms: 0.3753871458029213
  time_since_restore: 7080.190029144287
  time_this_iter_s: 25.835378885269165
  time_total_s: 7080.190029144287
  timers:
    learn_throughput: 8785.822
    learn_time_ms: 18415.123
    sample_throughput: 23759.625
    sample_time_ms: 6809.535
    update_time_ms: 31.681
  timestamp: 1602772561
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:36:02,727	WARNING util.py:136 -- The `process_trial` operation took 0.7978787422180176 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    277 |          7080.19 | 44816384 |  305.089 |              331.475 |              148.141 |            772.107 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3045.6821083787922
    time_step_min: 2872
  date: 2020-10-15_14-36-28
  done: false
  episode_len_mean: 772.1119976613417
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.14678503084957
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 58153
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7333624759490712e-46
        cur_lr: 5.0e-05
        entropy: 0.07102684987088044
        entropy_coeff: 0.0005000000000000001
        kl: 0.004471645884526272
        model: {}
        policy_loss: -0.0074847608533067005
        total_loss: 0.5652463138103485
        vf_explained_var: 0.9980535507202148
        vf_loss: 0.5727665945887566
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.974193548387102
    gpu_util_percent0: 0.3396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14612161008056143
    mean_env_wait_ms: 1.2177793292178996
    mean_inference_ms: 4.283677071301166
    mean_raw_obs_processing_ms: 0.37537893649937387
  time_since_restore: 7105.996956586838
  time_this_iter_s: 25.80692744255066
  time_total_s: 7105.996956586838
  timers:
    learn_throughput: 8762.551
    learn_time_ms: 18464.029
    sample_throughput: 23790.278
    sample_time_ms: 6800.761
    update_time_ms: 34.115
  timestamp: 1602772588
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: e00ea_00000
  
2020-10-15 14:36:29,595	WARNING util.py:136 -- The `process_trial` operation took 0.7837715148925781 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    278 |             7106 | 44978176 |  305.147 |              331.475 |              148.141 |            772.112 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3045.078811148058
    time_step_min: 2872
  date: 2020-10-15_14-36-55
  done: false
  episode_len_mean: 772.1149287476021
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.23948013292716
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 231
  episodes_total: 58384
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3666812379745356e-46
        cur_lr: 5.0e-05
        entropy: 0.06978553719818592
        entropy_coeff: 0.0005000000000000001
        kl: 0.004308076148542265
        model: {}
        policy_loss: -0.009534967470244737
        total_loss: 0.40825701753298443
        vf_explained_var: 0.9987632632255554
        vf_loss: 0.4178268760442734
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.826666666666664
    gpu_util_percent0: 0.33799999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611973691857943
    mean_env_wait_ms: 1.217714377021323
    mean_inference_ms: 4.283608817001331
    mean_raw_obs_processing_ms: 0.37536748182417357
  time_since_restore: 7131.502608299255
  time_this_iter_s: 25.505651712417603
  time_total_s: 7131.502608299255
  timers:
    learn_throughput: 8762.948
    learn_time_ms: 18463.193
    sample_throughput: 23793.798
    sample_time_ms: 6799.755
    update_time_ms: 34.072
  timestamp: 1602772615
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: e00ea_00000
  
2020-10-15 14:36:56,225	WARNING util.py:136 -- The `process_trial` operation took 0.8477401733398438 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    279 |           7131.5 | 45139968 |  305.239 |              331.475 |              148.141 |            772.115 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3044.471350520744
    time_step_min: 2872
  date: 2020-10-15_14-37-21
  done: false
  episode_len_mean: 772.1161195659591
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.32927669515755
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 228
  episodes_total: 58612
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.833406189872678e-47
        cur_lr: 5.0e-05
        entropy: 0.06845211610198021
        entropy_coeff: 0.0005000000000000001
        kl: 0.005382750726615389
        model: {}
        policy_loss: -0.00675659623326889
        total_loss: 0.32347995539506275
        vf_explained_var: 0.9989719390869141
        vf_loss: 0.33027078708012897
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.21333333333333
    gpu_util_percent0: 0.31099999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461178250063884
    mean_env_wait_ms: 1.2176459001504154
    mean_inference_ms: 4.283529005816239
    mean_raw_obs_processing_ms: 0.37535572907925097
  time_since_restore: 7157.0291085243225
  time_this_iter_s: 25.52650022506714
  time_total_s: 7157.0291085243225
  timers:
    learn_throughput: 8758.999
    learn_time_ms: 18471.516
    sample_throughput: 23862.62
    sample_time_ms: 6780.144
    update_time_ms: 26.002
  timestamp: 1602772641
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: e00ea_00000
  
2020-10-15 14:37:22,891	WARNING util.py:136 -- The `process_trial` operation took 0.7686219215393066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    280 |          7157.03 | 45301760 |  305.329 |              331.475 |              148.141 |            772.116 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3044.0532175689477
    time_step_min: 2872
  date: 2020-10-15_14-37-48
  done: false
  episode_len_mean: 772.1195604096492
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.3944306801816
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 58782
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.833406189872678e-47
        cur_lr: 5.0e-05
        entropy: 0.0689181424677372
        entropy_coeff: 0.0005000000000000001
        kl: 0.010533542294676105
        model: {}
        policy_loss: -0.006924599789878509
        total_loss: 0.4221002012491226
        vf_explained_var: 0.9984164237976074
        vf_loss: 0.4290592546264331
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706451612903226
    gpu_util_percent0: 0.3448387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611653515686918
    mean_env_wait_ms: 1.2175968438476237
    mean_inference_ms: 4.283473352801432
    mean_raw_obs_processing_ms: 0.3753476644128263
  time_since_restore: 7182.747726917267
  time_this_iter_s: 25.718618392944336
  time_total_s: 7182.747726917267
  timers:
    learn_throughput: 8756.542
    learn_time_ms: 18476.7
    sample_throughput: 23882.916
    sample_time_ms: 6774.382
    update_time_ms: 25.919
  timestamp: 1602772668
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: e00ea_00000
  
2020-10-15 14:37:49,719	WARNING util.py:136 -- The `process_trial` operation took 0.7690229415893555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    281 |          7182.75 | 45463552 |  305.394 |              331.475 |              148.141 |             772.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3043.4746133496133
    time_step_min: 2872
  date: 2020-10-15_14-38-15
  done: false
  episode_len_mean: 772.1203863751906
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.4816132858839
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 228
  episodes_total: 59010
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.833406189872678e-47
        cur_lr: 5.0e-05
        entropy: 0.07008547273774941
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007934603112516925
        total_loss: .nan
        vf_explained_var: 0.9984565377235413
        vf_loss: 0.5261634041865667
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36666666666667
    gpu_util_percent0: 0.35366666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611456469702963
    mean_env_wait_ms: 1.2175316601664754
    mean_inference_ms: 4.283403558927564
    mean_raw_obs_processing_ms: 0.37533637058333563
  time_since_restore: 7208.029676914215
  time_this_iter_s: 25.281949996948242
  time_total_s: 7208.029676914215
  timers:
    learn_throughput: 8771.194
    learn_time_ms: 18445.835
    sample_throughput: 23843.235
    sample_time_ms: 6785.656
    update_time_ms: 26.112
  timestamp: 1602772695
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:38:16,156	WARNING util.py:136 -- The `process_trial` operation took 0.8128259181976318 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    282 |          7208.03 | 45625344 |  305.482 |              331.475 |              148.141 |             772.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3042.9112162162164
    time_step_min: 2872
  date: 2020-10-15_14-38-41
  done: false
  episode_len_mean: 772.1218223557611
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.5648662786674
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 232
  episodes_total: 59242
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0250109284809019e-46
        cur_lr: 5.0e-05
        entropy: 0.07396331864098708
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008296980333398096
        total_loss: .nan
        vf_explained_var: 0.9982966780662537
        vf_loss: 0.559145155052344
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.88666666666667
    gpu_util_percent0: 0.3346666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461127678790504
    mean_env_wait_ms: 1.2174635170463677
    mean_inference_ms: 4.283326929756097
    mean_raw_obs_processing_ms: 0.37532492232614706
  time_since_restore: 7233.682758569717
  time_this_iter_s: 25.65308165550232
  time_total_s: 7233.682758569717
  timers:
    learn_throughput: 8775.354
    learn_time_ms: 18437.091
    sample_throughput: 23827.962
    sample_time_ms: 6790.006
    update_time_ms: 26.551
  timestamp: 1602772721
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:38:42,903	WARNING util.py:136 -- The `process_trial` operation took 0.8057553768157959 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    283 |          7233.68 | 45787136 |  305.565 |              331.475 |              148.141 |            772.122 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3042.4961596766043
    time_step_min: 2872
  date: 2020-10-15_14-39-08
  done: false
  episode_len_mean: 772.1281054332458
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.62717867424004
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 170
  episodes_total: 59412
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5375163927213529e-46
        cur_lr: 5.0e-05
        entropy: 0.06892408306399982
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008592622252763249
        total_loss: .nan
        vf_explained_var: 0.9982779622077942
        vf_loss: 0.47050098329782486
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.66
    gpu_util_percent0: 0.3333333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14611148425946668
    mean_env_wait_ms: 1.2174147145113314
    mean_inference_ms: 4.283272373976168
    mean_raw_obs_processing_ms: 0.3753170268060223
  time_since_restore: 7259.204622507095
  time_this_iter_s: 25.52186393737793
  time_total_s: 7259.204622507095
  timers:
    learn_throughput: 8763.143
    learn_time_ms: 18462.781
    sample_throughput: 23835.604
    sample_time_ms: 6787.829
    update_time_ms: 27.815
  timestamp: 1602772748
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:39:09,497	WARNING util.py:136 -- The `process_trial` operation took 0.7934458255767822 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    284 |           7259.2 | 45948928 |  305.627 |              331.475 |              148.141 |            772.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3041.9464684264403
    time_step_min: 2872
  date: 2020-10-15_14-39-35
  done: false
  episode_len_mean: 772.1282679053544
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.7139576131243
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 221
  episodes_total: 59633
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.306274589082029e-46
        cur_lr: 5.0e-05
        entropy: 0.07447424717247486
        entropy_coeff: 0.0005000000000000001
        kl: 0.008324353800465664
        model: {}
        policy_loss: -0.007595447253455252
        total_loss: 0.27949582661191624
        vf_explained_var: 0.9990952610969543
        vf_loss: 0.2871285031239192
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.663333333333338
    gpu_util_percent0: 0.35866666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610963189535026
    mean_env_wait_ms: 1.2173512819104932
    mean_inference_ms: 4.283209373170925
    mean_raw_obs_processing_ms: 0.3753066516153612
  time_since_restore: 7284.750862836838
  time_this_iter_s: 25.54624032974243
  time_total_s: 7284.750862836838
  timers:
    learn_throughput: 8761.365
    learn_time_ms: 18466.528
    sample_throughput: 23859.262
    sample_time_ms: 6781.098
    update_time_ms: 27.167
  timestamp: 1602772775
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: e00ea_00000
  
2020-10-15 14:39:36,169	WARNING util.py:136 -- The `process_trial` operation took 0.7818639278411865 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    285 |          7284.75 | 46110720 |  305.714 |              331.475 |              148.141 |            772.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3041.3390329104614
    time_step_min: 2872
  date: 2020-10-15_14-40-02
  done: false
  episode_len_mean: 772.1289940037748
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.80350885717434
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 238
  episodes_total: 59871
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.306274589082029e-46
        cur_lr: 5.0e-05
        entropy: 0.07441473690172036
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009997687515957901
        total_loss: .nan
        vf_explained_var: 0.9986114501953125
        vf_loss: 0.47697631766398746
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.64193548387097
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610773776854827
    mean_env_wait_ms: 1.217282687399644
    mean_inference_ms: 4.28312788132038
    mean_raw_obs_processing_ms: 0.3752946077752001
  time_since_restore: 7310.661339759827
  time_this_iter_s: 25.91047692298889
  time_total_s: 7310.661339759827
  timers:
    learn_throughput: 8766.248
    learn_time_ms: 18456.244
    sample_throughput: 23765.234
    sample_time_ms: 6807.928
    update_time_ms: 26.926
  timestamp: 1602772802
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:40:03,184	WARNING util.py:136 -- The `process_trial` operation took 0.8171570301055908 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    286 |          7310.66 | 46272512 |  305.804 |              331.475 |              148.141 |            772.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3040.919716666667
    time_step_min: 2872
  date: 2020-10-15_14-40-28
  done: false
  episode_len_mean: 772.1281935978149
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.87067285223594
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 60042
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.459411883623043e-46
        cur_lr: 5.0e-05
        entropy: 0.0716320766756932
        entropy_coeff: 0.0005000000000000001
        kl: 0.0060297642679264145
        model: {}
        policy_loss: -0.007236705801915377
        total_loss: 0.1814658877750238
        vf_explained_var: 0.999267578125
        vf_loss: 0.18873840818802515
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01
    gpu_util_percent0: 0.3683333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610656175331757
    mean_env_wait_ms: 1.2172340152308403
    mean_inference_ms: 4.283075750006683
    mean_raw_obs_processing_ms: 0.37528676647675907
  time_since_restore: 7336.25682592392
  time_this_iter_s: 25.595486164093018
  time_total_s: 7336.25682592392
  timers:
    learn_throughput: 8779.494
    learn_time_ms: 18428.397
    sample_throughput: 23754.764
    sample_time_ms: 6810.929
    update_time_ms: 26.236
  timestamp: 1602772828
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: e00ea_00000
  
2020-10-15 14:40:29,872	WARNING util.py:136 -- The `process_trial` operation took 0.8055753707885742 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    287 |          7336.26 | 46434304 |  305.871 |              331.475 |              148.141 |            772.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3040.3891988840173
    time_step_min: 2872
  date: 2020-10-15_14-40-55
  done: false
  episode_len_mean: 772.1293936074878
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 305.9500566754875
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 216
  episodes_total: 60258
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.459411883623043e-46
        cur_lr: 5.0e-05
        entropy: 0.0772135779261589
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011326577766643217
        total_loss: .nan
        vf_explained_var: 0.9983220100402832
        vf_loss: 0.5320590784152349
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.53666666666667
    gpu_util_percent0: 0.34800000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610475153206884
    mean_env_wait_ms: 1.2171721467909424
    mean_inference_ms: 4.283013164775751
    mean_raw_obs_processing_ms: 0.37527715605408807
  time_since_restore: 7361.673628091812
  time_this_iter_s: 25.416802167892456
  time_total_s: 7361.673628091812
  timers:
    learn_throughput: 8805.726
    learn_time_ms: 18373.499
    sample_throughput: 23725.125
    sample_time_ms: 6819.437
    update_time_ms: 24.268
  timestamp: 1602772855
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:40:56,392	WARNING util.py:136 -- The `process_trial` operation took 0.8190853595733643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    288 |          7361.67 | 46596096 |   305.95 |              331.475 |              148.141 |            772.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3039.8002315580547
    time_step_min: 2872
  date: 2020-10-15_14-41-22
  done: false
  episode_len_mean: 772.1287395458002
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.0376519817862
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 244
  episodes_total: 60502
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.1891178254345646e-46
        cur_lr: 5.0e-05
        entropy: 0.07516923546791077
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0076768459208930535
        total_loss: .nan
        vf_explained_var: 0.9984288215637207
        vf_loss: 0.5358986258506775
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.276666666666667
    gpu_util_percent0: 0.3816666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14610289211687208
    mean_env_wait_ms: 1.2171028703736628
    mean_inference_ms: 4.282936126953069
    mean_raw_obs_processing_ms: 0.3752649182434759
  time_since_restore: 7387.328722715378
  time_this_iter_s: 25.655094623565674
  time_total_s: 7387.328722715378
  timers:
    learn_throughput: 8804.62
    learn_time_ms: 18375.808
    sample_throughput: 23681.076
    sample_time_ms: 6832.122
    update_time_ms: 22.696
  timestamp: 1602772882
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:41:23,143	WARNING util.py:136 -- The `process_trial` operation took 0.8089282512664795 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    289 |          7387.33 | 46757888 |  306.038 |              331.475 |              148.141 |            772.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3039.391087067672
    time_step_min: 2872
  date: 2020-10-15_14-41-49
  done: false
  episode_len_mean: 772.1233992055775
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.10165455587656
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 60673
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.783676738151847e-46
        cur_lr: 5.0e-05
        entropy: 0.0771762120227019
        entropy_coeff: 0.0005000000000000001
        kl: 0.006358545273542404
        model: {}
        policy_loss: -0.007121290667176557
        total_loss: 0.42796876033147174
        vf_explained_var: 0.998305082321167
        vf_loss: 0.4351286441087723
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.58064516129033
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1461016987260061
    mean_env_wait_ms: 1.2170540156895546
    mean_inference_ms: 4.282883663554545
    mean_raw_obs_processing_ms: 0.3752572638554431
  time_since_restore: 7413.331784486771
  time_this_iter_s: 26.003061771392822
  time_total_s: 7413.331784486771
  timers:
    learn_throughput: 8790.805
    learn_time_ms: 18404.686
    sample_throughput: 23626.305
    sample_time_ms: 6847.96
    update_time_ms: 25.637
  timestamp: 1602772909
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: e00ea_00000
  
2020-10-15 14:41:50,236	WARNING util.py:136 -- The `process_trial` operation took 0.7979745864868164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    290 |          7413.33 | 46919680 |  306.102 |              331.475 |              148.141 |            772.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3038.8841463414633
    time_step_min: 2872
  date: 2020-10-15_14-42-15
  done: false
  episode_len_mean: 772.1196991098118
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.1773536368847
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 213
  episodes_total: 60886
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.783676738151847e-46
        cur_lr: 5.0e-05
        entropy: 0.08471359498798847
        entropy_coeff: 0.0005000000000000001
        kl: 0.00547356562068065
        model: {}
        policy_loss: -0.007631780996841068
        total_loss: 0.6072228153546652
        vf_explained_var: 0.998052179813385
        vf_loss: 0.6148969382047653
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.85666666666667
    gpu_util_percent0: 0.35866666666666663
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609989379620825
    mean_env_wait_ms: 1.2169938092713917
    mean_inference_ms: 4.282822631847312
    mean_raw_obs_processing_ms: 0.3752479044376614
  time_since_restore: 7438.774746894836
  time_this_iter_s: 25.442962408065796
  time_total_s: 7438.774746894836
  timers:
    learn_throughput: 8802.197
    learn_time_ms: 18380.865
    sample_throughput: 23612.882
    sample_time_ms: 6851.853
    update_time_ms: 25.892
  timestamp: 1602772935
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: e00ea_00000
  
2020-10-15 14:42:16,779	WARNING util.py:136 -- The `process_trial` operation took 0.8128983974456787 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    291 |          7438.77 | 47081472 |  306.177 |              331.475 |              148.141 |             772.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3038.3638729743
    time_step_min: 2872
  date: 2020-10-15_14-42-42
  done: false
  episode_len_mean: 772.1166165019957
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.2590294755447
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 246
  episodes_total: 61132
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.783676738151847e-46
        cur_lr: 5.0e-05
        entropy: 0.08165584628780682
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008957524346366577
        total_loss: .inf
        vf_explained_var: 0.9973334670066833
        vf_loss: 0.9263622860113779
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.929999999999996
    gpu_util_percent0: 0.36233333333333323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460981575747317
    mean_env_wait_ms: 1.2169244203426308
    mean_inference_ms: 4.282748085179595
    mean_raw_obs_processing_ms: 0.37523593627871915
  time_since_restore: 7464.240057468414
  time_this_iter_s: 25.46531057357788
  time_total_s: 7464.240057468414
  timers:
    learn_throughput: 8785.72
    learn_time_ms: 18415.338
    sample_throughput: 23642.084
    sample_time_ms: 6843.39
    update_time_ms: 25.943
  timestamp: 1602772962
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:42:43,332	WARNING util.py:136 -- The `process_trial` operation took 0.8014893531799316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    292 |          7464.24 | 47243264 |  306.259 |              331.475 |              148.141 |            772.117 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3037.9711244960254
    time_step_min: 2872
  date: 2020-10-15_14-43-08
  done: false
  episode_len_mean: 772.1173639996738
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.32043953110764
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 61305
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.167551510722777e-45
        cur_lr: 5.0e-05
        entropy: 0.07398833148181438
        entropy_coeff: 0.0005000000000000001
        kl: 0.003951034392230213
        model: {}
        policy_loss: -0.009018175342741111
        total_loss: 0.4341313416759173
        vf_explained_var: 0.9983491897583008
        vf_loss: 0.44318651407957077
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.683870967741935
    gpu_util_percent0: 0.3016129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609696601362748
    mean_env_wait_ms: 1.2168752174658495
    mean_inference_ms: 4.282697634736655
    mean_raw_obs_processing_ms: 0.375228287100791
  time_since_restore: 7489.832823038101
  time_this_iter_s: 25.59276556968689
  time_total_s: 7489.832823038101
  timers:
    learn_throughput: 8786.537
    learn_time_ms: 18413.625
    sample_throughput: 23656.242
    sample_time_ms: 6839.294
    update_time_ms: 25.044
  timestamp: 1602772988
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: e00ea_00000
  
2020-10-15 14:43:10,195	WARNING util.py:136 -- The `process_trial` operation took 0.8127517700195312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    293 |          7489.83 | 47405056 |   306.32 |              331.475 |              148.141 |            772.117 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3037.5222795230115
    time_step_min: 2872
  date: 2020-10-15_14-43-35
  done: false
  episode_len_mean: 772.1208889466925
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.39145613932254
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 206
  episodes_total: 61511
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.837757553613885e-46
        cur_lr: 5.0e-05
        entropy: 0.07790090950826804
        entropy_coeff: 0.0005000000000000001
        kl: 0.004159553907811642
        model: {}
        policy_loss: -0.00712725892663002
        total_loss: 0.7437641968329748
        vf_explained_var: 0.9975894093513489
        vf_loss: 0.7509303937355677
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.099999999999998
    gpu_util_percent0: 0.37566666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460953207219379
    mean_env_wait_ms: 1.2168174615334972
    mean_inference_ms: 4.282637996880951
    mean_raw_obs_processing_ms: 0.37521935865296535
  time_since_restore: 7515.286647558212
  time_this_iter_s: 25.453824520111084
  time_total_s: 7515.286647558212
  timers:
    learn_throughput: 8795.029
    learn_time_ms: 18395.845
    sample_throughput: 23646.623
    sample_time_ms: 6842.076
    update_time_ms: 23.759
  timestamp: 1602773015
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: e00ea_00000
  
2020-10-15 14:43:36,800	WARNING util.py:136 -- The `process_trial` operation took 0.8610937595367432 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    294 |          7515.29 | 47566848 |  306.391 |              331.475 |              148.141 |            772.121 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3036.9653261609255
    time_step_min: 2872
  date: 2020-10-15_14-44-02
  done: false
  episode_len_mean: 772.1260362694301
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.47773803449013
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 249
  episodes_total: 61760
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9188787768069424e-46
        cur_lr: 5.0e-05
        entropy: 0.07432821579277515
        entropy_coeff: 0.0005000000000000001
        kl: 0.004661213490180671
        model: {}
        policy_loss: -0.006840907619334757
        total_loss: 0.6482930779457092
        vf_explained_var: 0.99810391664505
        vf_loss: 0.6551711211601893
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.843333333333337
    gpu_util_percent0: 0.3516666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609366892689227
    mean_env_wait_ms: 1.2167476568451414
    mean_inference_ms: 4.2825689571305245
    mean_raw_obs_processing_ms: 0.37520752629886756
  time_since_restore: 7541.0654883384705
  time_this_iter_s: 25.77884078025818
  time_total_s: 7541.0654883384705
  timers:
    learn_throughput: 8789.06
    learn_time_ms: 18408.34
    sample_throughput: 23617.768
    sample_time_ms: 6850.436
    update_time_ms: 25.173
  timestamp: 1602773042
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: e00ea_00000
  
2020-10-15 14:44:03,670	WARNING util.py:136 -- The `process_trial` operation took 0.7984886169433594 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    295 |          7541.07 | 47728640 |  306.478 |              331.475 |              148.141 |            772.126 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3036.5563220990725
    time_step_min: 2872
  date: 2020-10-15_14-44-29
  done: false
  episode_len_mean: 772.1291655902868
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.54040664981
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 176
  episodes_total: 61936
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4594393884034712e-46
        cur_lr: 5.0e-05
        entropy: 0.07256230463584264
        entropy_coeff: 0.0005000000000000001
        kl: 0.005322571300591032
        model: {}
        policy_loss: -0.005675716408101532
        total_loss: 0.28674472123384476
        vf_explained_var: 0.998906672000885
        vf_loss: 0.2924567237496376
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.741935483870968
    gpu_util_percent0: 0.29258064516129034
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609233881802242
    mean_env_wait_ms: 1.2166981614668881
    mean_inference_ms: 4.282517420891575
    mean_raw_obs_processing_ms: 0.3752000672043956
  time_since_restore: 7566.815256357193
  time_this_iter_s: 25.749768018722534
  time_total_s: 7566.815256357193
  timers:
    learn_throughput: 8787.269
    learn_time_ms: 18412.092
    sample_throughput: 23693.743
    sample_time_ms: 6828.469
    update_time_ms: 27.094
  timestamp: 1602773069
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: e00ea_00000
  
2020-10-15 14:44:30,589	WARNING util.py:136 -- The `process_trial` operation took 0.818333625793457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    296 |          7566.82 | 47890432 |   306.54 |              331.475 |              148.141 |            772.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3036.0773801855194
    time_step_min: 2872
  date: 2020-10-15_14-44-56
  done: false
  episode_len_mean: 772.1312723293315
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.61368553733945
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 202
  episodes_total: 62138
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4594393884034712e-46
        cur_lr: 5.0e-05
        entropy: 0.07685854596396287
        entropy_coeff: 0.0005000000000000001
        kl: 0.00558243024473389
        model: {}
        policy_loss: -0.005385729955984668
        total_loss: 0.34011247009038925
        vf_explained_var: 0.9988688826560974
        vf_loss: 0.34553660452365875
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.586666666666666
    gpu_util_percent0: 0.3520000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14609081457047854
    mean_env_wait_ms: 1.2166414231365397
    mean_inference_ms: 4.282457532860628
    mean_raw_obs_processing_ms: 0.37519151869412565
  time_since_restore: 7592.628669977188
  time_this_iter_s: 25.813413619995117
  time_total_s: 7592.628669977188
  timers:
    learn_throughput: 8775.69
    learn_time_ms: 18436.384
    sample_throughput: 23706.123
    sample_time_ms: 6824.904
    update_time_ms: 26.842
  timestamp: 1602773096
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: e00ea_00000
  
2020-10-15 14:44:57,608	WARNING util.py:136 -- The `process_trial` operation took 0.8172481060028076 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    297 |          7592.63 | 48052224 |  306.614 |              331.475 |              148.141 |            772.131 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3035.529913707375
    time_step_min: 2872
  date: 2020-10-15_14-45-23
  done: false
  episode_len_mean: 772.1341123292941
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.6977483043554
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 250
  episodes_total: 62388
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4594393884034712e-46
        cur_lr: 5.0e-05
        entropy: 0.07700268117090066
        entropy_coeff: 0.0005000000000000001
        kl: 0.004093883093446493
        model: {}
        policy_loss: -0.007902207435108721
        total_loss: 0.5219482084115347
        vf_explained_var: 0.9985023140907288
        vf_loss: 0.5298889155189196
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95806451612903
    gpu_util_percent0: 0.34741935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608927664912558
    mean_env_wait_ms: 1.2165725525310256
    mean_inference_ms: 4.282392246676106
    mean_raw_obs_processing_ms: 0.37517991240316056
  time_since_restore: 7618.324602842331
  time_this_iter_s: 25.695932865142822
  time_total_s: 7618.324602842331
  timers:
    learn_throughput: 8769.928
    learn_time_ms: 18448.498
    sample_throughput: 23654.586
    sample_time_ms: 6839.773
    update_time_ms: 26.514
  timestamp: 1602773123
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: e00ea_00000
  
2020-10-15 14:45:24,503	WARNING util.py:136 -- The `process_trial` operation took 0.8533668518066406 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    298 |          7618.32 | 48214016 |  306.698 |              331.475 |              148.141 |            772.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3035.159357707194
    time_step_min: 2872
  date: 2020-10-15_14-45-49
  done: false
  episode_len_mean: 772.1343817926096
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.75724012274657
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 180
  episodes_total: 62568
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.297196942017356e-47
        cur_lr: 5.0e-05
        entropy: 0.0740035076936086
        entropy_coeff: 0.0005000000000000001
        kl: 0.004243797428595523
        model: {}
        policy_loss: -0.008987964819728708
        total_loss: 0.3962729076544444
        vf_explained_var: 0.998516857624054
        vf_loss: 0.4052978679537773
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08666666666667
    gpu_util_percent0: 0.3853333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608782121907982
    mean_env_wait_ms: 1.2165218398286528
    mean_inference_ms: 4.282340513512363
    mean_raw_obs_processing_ms: 0.3751723937156849
  time_since_restore: 7643.799475669861
  time_this_iter_s: 25.474872827529907
  time_total_s: 7643.799475669861
  timers:
    learn_throughput: 8777.927
    learn_time_ms: 18431.687
    sample_throughput: 23682.631
    sample_time_ms: 6831.673
    update_time_ms: 26.683
  timestamp: 1602773149
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: e00ea_00000
  
2020-10-15 14:45:51,150	WARNING util.py:136 -- The `process_trial` operation took 0.8717014789581299 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    299 |           7643.8 | 48375808 |  306.757 |              331.475 |              148.141 |            772.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3034.767721054813
    time_step_min: 2872
  date: 2020-10-15_14-46-16
  done: false
  episode_len_mean: 772.1359059333375
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.8195673193604
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 196
  episodes_total: 62764
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.648598471008678e-47
        cur_lr: 5.0e-05
        entropy: 0.07658265717327595
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010483136924449354
        total_loss: .nan
        vf_explained_var: 0.9976654648780823
        vf_loss: 0.720120757818222
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.787096774193543
    gpu_util_percent0: 0.34516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460862619706226
    mean_env_wait_ms: 1.2164664633887077
    mean_inference_ms: 4.2822798065001715
    mean_raw_obs_processing_ms: 0.3751639693886591
  time_since_restore: 7669.44913649559
  time_this_iter_s: 25.64966082572937
  time_total_s: 7669.44913649559
  timers:
    learn_throughput: 8788.208
    learn_time_ms: 18410.123
    sample_throughput: 23729.286
    sample_time_ms: 6818.241
    update_time_ms: 24.431
  timestamp: 1602773176
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:46:18,140	WARNING util.py:136 -- The `process_trial` operation took 0.8769068717956543 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    300 |          7669.45 | 48537600 |   306.82 |              331.475 |              148.141 |            772.136 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3034.2354426359666
    time_step_min: 2872
  date: 2020-10-15_14-46-43
  done: false
  episode_len_mean: 772.1362648174302
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.89947413580734
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 253
  episodes_total: 63017
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.472897706513017e-47
        cur_lr: 5.0e-05
        entropy: 0.07965938684840997
        entropy_coeff: 0.0005000000000000001
        kl: 0.010567404019335905
        model: {}
        policy_loss: -0.008897770176796863
        total_loss: 0.7474433978398641
        vf_explained_var: 0.9978836178779602
        vf_loss: 0.7563810050487518
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608478880678688
    mean_env_wait_ms: 1.216398094212753
    mean_inference_ms: 4.282220263885561
    mean_raw_obs_processing_ms: 0.37515273118771914
  time_since_restore: 7695.286217689514
  time_this_iter_s: 25.83708119392395
  time_total_s: 7695.286217689514
  timers:
    learn_throughput: 8782.868
    learn_time_ms: 18421.317
    sample_throughput: 23640.194
    sample_time_ms: 6843.937
    update_time_ms: 24.323
  timestamp: 1602773203
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: e00ea_00000
  
2020-10-15 14:46:45,202	WARNING util.py:136 -- The `process_trial` operation took 0.841590166091919 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    301 |          7695.29 | 48699392 |  306.899 |              331.475 |              148.141 |            772.136 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3033.8792235346273
    time_step_min: 2872
  date: 2020-10-15_14-47-10
  done: false
  episode_len_mean: 772.1390981012659
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.9501310574097
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 183
  episodes_total: 63200
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.472897706513017e-47
        cur_lr: 5.0e-05
        entropy: 0.08323773245016734
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011504423736672228
        total_loss: .nan
        vf_explained_var: 0.9970954060554504
        vf_loss: 0.8368369142214457
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.267741935483876
    gpu_util_percent0: 0.28903225806451616
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608342193677404
    mean_env_wait_ms: 1.2163469144817913
    mean_inference_ms: 4.282168350184007
    mean_raw_obs_processing_ms: 0.3751452950990374
  time_since_restore: 7720.862788200378
  time_this_iter_s: 25.576570510864258
  time_total_s: 7720.862788200378
  timers:
    learn_throughput: 8786.524
    learn_time_ms: 18413.653
    sample_throughput: 23606.431
    sample_time_ms: 6853.725
    update_time_ms: 24.051
  timestamp: 1602773230
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:47:11,921	WARNING util.py:136 -- The `process_trial` operation took 0.8392782211303711 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    302 |          7720.86 | 48861184 |   306.95 |              331.475 |              148.141 |            772.139 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3033.660889057271
    time_step_min: 2872
  date: 2020-10-15_14-47-37
  done: false
  episode_len_mean: 772.1399589840669
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 306.9818105331595
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 190
  episodes_total: 63390
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.209346559769523e-47
        cur_lr: 5.0e-05
        entropy: 0.09023485456903775
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010429409798234701
        total_loss: .nan
        vf_explained_var: 0.9957501292228699
        vf_loss: 1.4097765187422435
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.433333333333334
    gpu_util_percent0: 0.36533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14608217119550143
    mean_env_wait_ms: 1.216293898174194
    mean_inference_ms: 4.282112495022947
    mean_raw_obs_processing_ms: 0.3751373347201986
  time_since_restore: 7746.6206204891205
  time_this_iter_s: 25.757832288742065
  time_total_s: 7746.6206204891205
  timers:
    learn_throughput: 8780.248
    learn_time_ms: 18426.814
    sample_throughput: 23604.856
    sample_time_ms: 6854.183
    update_time_ms: 25.648
  timestamp: 1602773257
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:47:38,958	WARNING util.py:136 -- The `process_trial` operation took 0.8613142967224121 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    303 |          7746.62 | 49022976 |  306.982 |              331.475 |              148.141 |             772.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3033.2622435343133
    time_step_min: 2872
  date: 2020-10-15_14-48-04
  done: false
  episode_len_mean: 772.1418291514133
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.0482611398445
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 257
  episodes_total: 63647
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2314019839654291e-46
        cur_lr: 5.0e-05
        entropy: 0.08060059261818726
        entropy_coeff: 0.0005000000000000001
        kl: 0.00661985968084385
        model: {}
        policy_loss: -0.008031785347460149
        total_loss: 0.7740199814240137
        vf_explained_var: 0.9978832602500916
        vf_loss: 0.7820920497179031
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.62666666666667
    gpu_util_percent0: 0.32399999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460804194289352
    mean_env_wait_ms: 1.216225063583275
    mean_inference_ms: 4.282051703957475
    mean_raw_obs_processing_ms: 0.37512607630978406
  time_since_restore: 7772.156658649445
  time_this_iter_s: 25.536038160324097
  time_total_s: 7772.156658649445
  timers:
    learn_throughput: 8769.307
    learn_time_ms: 18449.804
    sample_throughput: 23634.861
    sample_time_ms: 6845.481
    update_time_ms: 25.803
  timestamp: 1602773284
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: e00ea_00000
  
2020-10-15 14:48:05,738	WARNING util.py:136 -- The `process_trial` operation took 0.8479738235473633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    304 |          7772.16 | 49184768 |  307.048 |              331.475 |              148.141 |            772.142 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3032.8850290014107
    time_step_min: 2872
  date: 2020-10-15_14-48-31
  done: false
  episode_len_mean: 772.1380342148138
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.1038847555643
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 185
  episodes_total: 63832
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2314019839654291e-46
        cur_lr: 5.0e-05
        entropy: 0.07281449375053246
        entropy_coeff: 0.0005000000000000001
        kl: 0.004226222711925705
        model: {}
        policy_loss: -0.007113891323873152
        total_loss: 0.629824216167132
        vf_explained_var: 0.9977335333824158
        vf_loss: 0.6369745035966238
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.31612903225807
    gpu_util_percent0: 0.3545161290322582
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607910635938773
    mean_env_wait_ms: 1.2161736584881504
    mean_inference_ms: 4.282000042261001
    mean_raw_obs_processing_ms: 0.3751188134445686
  time_since_restore: 7797.709631681442
  time_this_iter_s: 25.55297303199768
  time_total_s: 7797.709631681442
  timers:
    learn_throughput: 8781.963
    learn_time_ms: 18423.215
    sample_throughput: 23649.227
    sample_time_ms: 6841.323
    update_time_ms: 24.078
  timestamp: 1602773311
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: e00ea_00000
  
2020-10-15 14:48:32,521	WARNING util.py:136 -- The `process_trial` operation took 0.8808629512786865 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    305 |          7797.71 | 49346560 |  307.104 |              331.475 |              148.141 |            772.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3032.4962485932224
    time_step_min: 2872
  date: 2020-10-15_14-48-58
  done: false
  episode_len_mean: 772.1332750164016
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.1605938796886
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 186
  episodes_total: 64018
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.157009919827146e-47
        cur_lr: 5.0e-05
        entropy: 0.07501682514945666
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00945279817096889
        total_loss: .nan
        vf_explained_var: 0.9981722831726074
        vf_loss: 0.5693151454130808
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.810000000000006
    gpu_util_percent0: 0.34766666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607774804408039
    mean_env_wait_ms: 1.2161220027067199
    mean_inference_ms: 4.281945141749334
    mean_raw_obs_processing_ms: 0.37511104191508504
  time_since_restore: 7823.304481744766
  time_this_iter_s: 25.594850063323975
  time_total_s: 7823.304481744766
  timers:
    learn_throughput: 8789.975
    learn_time_ms: 18406.423
    sample_throughput: 23640.217
    sample_time_ms: 6843.93
    update_time_ms: 29.902
  timestamp: 1602773338
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:48:59,290	WARNING util.py:136 -- The `process_trial` operation took 0.8736090660095215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    306 |           7823.3 | 49508352 |  307.161 |              331.475 |              148.141 |            772.133 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3031.926656495205
    time_step_min: 2872
  date: 2020-10-15_14-49-24
  done: false
  episode_len_mean: 772.1283722811712
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.24335790301825
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 256
  episodes_total: 64274
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.235514879740716e-47
        cur_lr: 5.0e-05
        entropy: 0.07656297522286575
        entropy_coeff: 0.0005000000000000001
        kl: 0.016017493326216936
        model: {}
        policy_loss: -0.009376510347162062
        total_loss: 0.3242144013444583
        vf_explained_var: 0.999083936214447
        vf_loss: 0.33362919837236404
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.848387096774193
    gpu_util_percent0: 0.2890322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607631918822242
    mean_env_wait_ms: 1.2160545671037806
    mean_inference_ms: 4.281887958730196
    mean_raw_obs_processing_ms: 0.3751003652980222
  time_since_restore: 7848.8990297317505
  time_this_iter_s: 25.594547986984253
  time_total_s: 7848.8990297317505
  timers:
    learn_throughput: 8798.661
    learn_time_ms: 18388.253
    sample_throughput: 23654.93
    sample_time_ms: 6839.674
    update_time_ms: 29.781
  timestamp: 1602773364
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: e00ea_00000
  
2020-10-15 14:49:26,239	WARNING util.py:136 -- The `process_trial` operation took 0.8730349540710449 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    307 |           7848.9 | 49670144 |  307.243 |              331.475 |              148.141 |            772.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3031.525798640216
    time_step_min: 2872
  date: 2020-10-15_14-49-51
  done: false
  episode_len_mean: 772.1261479275255
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.30345055168226
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 190
  episodes_total: 64464
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.235514879740716e-47
        cur_lr: 5.0e-05
        entropy: 0.07037679726878802
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009870450613865009
        total_loss: .nan
        vf_explained_var: 0.9988064765930176
        vf_loss: 0.3374406968553861
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.37333333333333
    gpu_util_percent0: 0.3396666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607487364444685
    mean_env_wait_ms: 1.2160018378867032
    mean_inference_ms: 4.281835348299168
    mean_raw_obs_processing_ms: 0.3750928937217207
  time_since_restore: 7874.431772470474
  time_this_iter_s: 25.532742738723755
  time_total_s: 7874.431772470474
  timers:
    learn_throughput: 8791.191
    learn_time_ms: 18403.876
    sample_throughput: 23734.603
    sample_time_ms: 6816.714
    update_time_ms: 29.895
  timestamp: 1602773391
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:49:52,943	WARNING util.py:136 -- The `process_trial` operation took 0.843651533126831 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    308 |          7874.43 | 49831936 |  307.303 |              331.475 |              148.141 |            772.126 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3031.1730361427135
    time_step_min: 2872
  date: 2020-10-15_14-50-18
  done: false
  episode_len_mean: 772.1256670843195
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.3541550359038
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 183
  episodes_total: 64647
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3853272319611074e-46
        cur_lr: 5.0e-05
        entropy: 0.07191268354654312
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008012256124250902
        total_loss: .nan
        vf_explained_var: 0.9977560639381409
        vf_loss: 0.6711632460355759
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93225806451613
    gpu_util_percent0: 0.3425806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460736007260297
    mean_env_wait_ms: 1.2159522196323
    mean_inference_ms: 4.281784584699151
    mean_raw_obs_processing_ms: 0.37508545806021915
  time_since_restore: 7899.962611198425
  time_this_iter_s: 25.53083872795105
  time_total_s: 7899.962611198425
  timers:
    learn_throughput: 8784.026
    learn_time_ms: 18418.889
    sample_throughput: 23745.399
    sample_time_ms: 6813.615
    update_time_ms: 30.1
  timestamp: 1602773418
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:50:19,816	WARNING util.py:136 -- The `process_trial` operation took 0.8650743961334229 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    309 |          7899.96 | 49993728 |  307.354 |              331.475 |              148.141 |            772.126 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3030.645523366071
    time_step_min: 2872
  date: 2020-10-15_14-50-45
  done: false
  episode_len_mean: 772.124204557711
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.4368025021483
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 254
  episodes_total: 64901
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0779908479416615e-46
        cur_lr: 5.0e-05
        entropy: 0.06737800128757954
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006754779351467732
        total_loss: .nan
        vf_explained_var: 0.9987502098083496
        vf_loss: 0.46248626212279004
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.550000000000008
    gpu_util_percent0: 0.3246666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460722169814313
    mean_env_wait_ms: 1.2158842023092933
    mean_inference_ms: 4.281725678898636
    mean_raw_obs_processing_ms: 0.3750749927196673
  time_since_restore: 7925.573939800262
  time_this_iter_s: 25.611328601837158
  time_total_s: 7925.573939800262
  timers:
    learn_throughput: 8784.626
    learn_time_ms: 18417.632
    sample_throughput: 23756.449
    sample_time_ms: 6810.445
    update_time_ms: 29.786
  timestamp: 1602773445
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:50:46,585	WARNING util.py:136 -- The `process_trial` operation took 0.8550419807434082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    310 |          7925.57 | 50155520 |  307.437 |              331.475 |              148.141 |            772.124 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3030.2479017431674
    time_step_min: 2872
  date: 2020-10-15_14-51-12
  done: false
  episode_len_mean: 772.1234945311539
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.49532873282425
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 195
  episodes_total: 65096
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1169862719124917e-46
        cur_lr: 5.0e-05
        entropy: 0.06840776527921359
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007609012269919428
        total_loss: .nan
        vf_explained_var: 0.9983996748924255
        vf_loss: 0.46641577531894046
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69333333333333
    gpu_util_percent0: 0.32833333333333337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14607072841803617
    mean_env_wait_ms: 1.2158312758074041
    mean_inference_ms: 4.2816740393960355
    mean_raw_obs_processing_ms: 0.37506748032953013
  time_since_restore: 7951.162856340408
  time_this_iter_s: 25.588916540145874
  time_total_s: 7951.162856340408
  timers:
    learn_throughput: 8785.428
    learn_time_ms: 18415.95
    sample_throughput: 23840.474
    sample_time_ms: 6786.442
    update_time_ms: 28.791
  timestamp: 1602773472
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:51:13,542	WARNING util.py:136 -- The `process_trial` operation took 0.9104576110839844 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    311 |          7951.16 | 50317312 |  307.495 |              331.475 |              148.141 |            772.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3029.8740342163355
    time_step_min: 2872
  date: 2020-10-15_14-51-39
  done: false
  episode_len_mean: 772.1219474829181
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.5482711107771
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 178
  episodes_total: 65274
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.675479407868738e-46
        cur_lr: 5.0e-05
        entropy: 0.0948285423219204
        entropy_coeff: 0.0005000000000000001
        kl: 0.008288636066329977
        model: {}
        policy_loss: -0.0125155526426776
        total_loss: 0.9561530103286108
        vf_explained_var: 0.9965047240257263
        vf_loss: 0.9687159806489944
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.187096774193552
    gpu_util_percent0: 0.3580645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146069600592335
    mean_env_wait_ms: 1.2157827884241164
    mean_inference_ms: 4.281624305048873
    mean_raw_obs_processing_ms: 0.37506043149975804
  time_since_restore: 7976.699495792389
  time_this_iter_s: 25.53663945198059
  time_total_s: 7976.699495792389
  timers:
    learn_throughput: 8787.531
    learn_time_ms: 18411.543
    sample_throughput: 23842.813
    sample_time_ms: 6785.777
    update_time_ms: 28.808
  timestamp: 1602773499
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: e00ea_00000
  
2020-10-15 14:51:40,284	WARNING util.py:136 -- The `process_trial` operation took 0.9019627571105957 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    312 |           7976.7 | 50479104 |  307.548 |              331.475 |              148.141 |            772.122 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3029.7892406126407
    time_step_min: 2872
  date: 2020-10-15_14-52-05
  done: false
  episode_len_mean: 772.1180240809413
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.5606488360233
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 255
  episodes_total: 65529
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.675479407868738e-46
        cur_lr: 5.0e-05
        entropy: 0.08840800945957501
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009505800029728562
        total_loss: .nan
        vf_explained_var: 0.9935159087181091
        vf_loss: 2.781248470147451
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.376666666666672
    gpu_util_percent0: 0.36500000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460682807624591
    mean_env_wait_ms: 1.2157161506077145
    mean_inference_ms: 4.281570279249202
    mean_raw_obs_processing_ms: 0.37505026583753026
  time_since_restore: 8002.100134134293
  time_this_iter_s: 25.400638341903687
  time_total_s: 8002.100134134293
  timers:
    learn_throughput: 8805.128
    learn_time_ms: 18374.748
    sample_throughput: 23865.27
    sample_time_ms: 6779.391
    update_time_ms: 27.25
  timestamp: 1602773525
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:52:06,847	WARNING util.py:136 -- The `process_trial` operation took 0.850017786026001 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    313 |           8002.1 | 50640896 |  307.561 |              331.475 |              148.141 |            772.118 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3029.410160769746
    time_step_min: 2872
  date: 2020-10-15_14-52-32
  done: false
  episode_len_mean: 772.1190244347747
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.6217360286984
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 197
  episodes_total: 65726
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.013219111803107e-46
        cur_lr: 5.0e-05
        entropy: 0.06965377057592075
        entropy_coeff: 0.0005000000000000001
        kl: 0.004957566425825159
        model: {}
        policy_loss: -0.008044067185740763
        total_loss: 0.3654364049434662
        vf_explained_var: 0.998721182346344
        vf_loss: 0.3735153029362361
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.500000000000004
    gpu_util_percent0: 0.35700000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8700000000000006
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14606673859972122
    mean_env_wait_ms: 1.215663626583951
    mean_inference_ms: 4.281518567996692
    mean_raw_obs_processing_ms: 0.3750426763389322
  time_since_restore: 8027.572736024857
  time_this_iter_s: 25.472601890563965
  time_total_s: 8027.572736024857
  timers:
    learn_throughput: 8814.815
    learn_time_ms: 18354.555
    sample_throughput: 23846.923
    sample_time_ms: 6784.607
    update_time_ms: 27.526
  timestamp: 1602773552
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: e00ea_00000
  
2020-10-15 14:52:33,525	WARNING util.py:136 -- The `process_trial` operation took 0.8885486125946045 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    314 |          8027.57 | 50802688 |  307.622 |              331.475 |              148.141 |            772.119 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3029.064804129973
    time_step_min: 2872
  date: 2020-10-15_14-52-59
  done: false
  episode_len_mean: 772.1197232253953
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.67442137069776
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 176
  episodes_total: 65902
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5066095559015535e-46
        cur_lr: 5.0e-05
        entropy: 0.07234423918028672
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011454348403882856
        total_loss: .nan
        vf_explained_var: 0.9983165860176086
        vf_loss: 0.4747207189599673
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.383333333333333
    gpu_util_percent0: 0.3413333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14606561557787223
    mean_env_wait_ms: 1.215616120545587
    mean_inference_ms: 4.281468694129656
    mean_raw_obs_processing_ms: 0.37503569367595546
  time_since_restore: 8053.3788805007935
  time_this_iter_s: 25.80614447593689
  time_total_s: 8053.3788805007935
  timers:
    learn_throughput: 8802.999
    learn_time_ms: 18379.19
    sample_throughput: 23829.353
    sample_time_ms: 6789.609
    update_time_ms: 30.013
  timestamp: 1602773579
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:53:00,498	WARNING util.py:136 -- The `process_trial` operation took 0.8532979488372803 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    315 |          8053.38 | 50964480 |  307.674 |              331.475 |              148.141 |             772.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3028.5632799382856
    time_step_min: 2872
  date: 2020-10-15_14-53-26
  done: false
  episode_len_mean: 772.1235771620335
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.7527215376293
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 251
  episodes_total: 66153
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.2599143338523306e-46
        cur_lr: 5.0e-05
        entropy: 0.0719055247803529
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038893657037988305
        model: {}
        policy_loss: -0.006022792595710295
        total_loss: 0.4779958476622899
        vf_explained_var: 0.9986402988433838
        vf_loss: 0.484054575363795
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34333333333333
    gpu_util_percent0: 0.294
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460643008079422
    mean_env_wait_ms: 1.2155499659779454
    mean_inference_ms: 4.281415957251691
    mean_raw_obs_processing_ms: 0.37502601805846775
  time_since_restore: 8078.887907981873
  time_this_iter_s: 25.5090274810791
  time_total_s: 8078.887907981873
  timers:
    learn_throughput: 8800.015
    learn_time_ms: 18385.422
    sample_throughput: 23863.614
    sample_time_ms: 6779.861
    update_time_ms: 23.172
  timestamp: 1602773606
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: e00ea_00000
  
2020-10-15 14:53:27,283	WARNING util.py:136 -- The `process_trial` operation took 0.8714280128479004 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    316 |          8078.89 | 51126272 |  307.753 |              331.475 |              148.141 |            772.124 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3028.1775137221784
    time_step_min: 2872
  date: 2020-10-15_14-53-53
  done: false
  episode_len_mean: 772.1283492570602
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.8117012068911
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 205
  episodes_total: 66358
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6299571669261653e-46
        cur_lr: 5.0e-05
        entropy: 0.07045053007702033
        entropy_coeff: 0.0005000000000000001
        kl: 0.004519582997697095
        model: {}
        policy_loss: -0.008022866415558383
        total_loss: 0.6886585603157679
        vf_explained_var: 0.9978020191192627
        vf_loss: 0.6967166562875112
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.732258064516135
    gpu_util_percent0: 0.3093548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14606274542879275
    mean_env_wait_ms: 1.2154960274608624
    mean_inference_ms: 4.281364029286776
    mean_raw_obs_processing_ms: 0.3750181707020993
  time_since_restore: 8104.7489223480225
  time_this_iter_s: 25.861014366149902
  time_total_s: 8104.7489223480225
  timers:
    learn_throughput: 8797.813
    learn_time_ms: 18390.024
    sample_throughput: 23787.735
    sample_time_ms: 6801.488
    update_time_ms: 23.406
  timestamp: 1602773633
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: e00ea_00000
  
2020-10-15 14:53:54,366	WARNING util.py:136 -- The `process_trial` operation took 0.9120473861694336 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    317 |          8104.75 | 51288064 |  307.812 |              331.475 |              148.141 |            772.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3027.8814390350285
    time_step_min: 2872
  date: 2020-10-15_14-54-20
  done: false
  episode_len_mean: 772.129052622086
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.8572813858021
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 66531
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3149785834630826e-46
        cur_lr: 5.0e-05
        entropy: 0.07423956940571468
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010039595169170449
        total_loss: .nan
        vf_explained_var: 0.9973626732826233
        vf_loss: 0.756724938750267
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.112903225806456
    gpu_util_percent0: 0.28580645161290325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14606166965880982
    mean_env_wait_ms: 1.215449575566227
    mean_inference_ms: 4.2813175181943945
    mean_raw_obs_processing_ms: 0.37501143857707686
  time_since_restore: 8130.438339710236
  time_this_iter_s: 25.689417362213135
  time_total_s: 8130.438339710236
  timers:
    learn_throughput: 8796.138
    learn_time_ms: 18393.526
    sample_throughput: 23788.519
    sample_time_ms: 6801.264
    update_time_ms: 23.163
  timestamp: 1602773660
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:54:21,423	WARNING util.py:136 -- The `process_trial` operation took 0.9232642650604248 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    318 |          8130.44 | 51449856 |  307.857 |              331.475 |              148.141 |            772.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3027.4076066595735
    time_step_min: 2872
  date: 2020-10-15_14-54-46
  done: false
  episode_len_mean: 772.1300675422701
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.9305459307558
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 242
  episodes_total: 66773
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.972467875194624e-46
        cur_lr: 5.0e-05
        entropy: 0.07451613495747249
        entropy_coeff: 0.0005000000000000001
        kl: 0.005676861192720632
        model: {}
        policy_loss: -0.007558830121221642
        total_loss: 0.5377996042370796
        vf_explained_var: 0.9984346032142639
        vf_loss: 0.5453956946730614
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.60333333333333
    gpu_util_percent0: 0.35100000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14606040833989015
    mean_env_wait_ms: 1.2153873644377773
    mean_inference_ms: 4.281265877185009
    mean_raw_obs_processing_ms: 0.3750026041428352
  time_since_restore: 8155.911139011383
  time_this_iter_s: 25.47279930114746
  time_total_s: 8155.911139011383
  timers:
    learn_throughput: 8797.772
    learn_time_ms: 18390.11
    sample_throughput: 23804.417
    sample_time_ms: 6796.722
    update_time_ms: 23.219
  timestamp: 1602773686
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: e00ea_00000
  
2020-10-15 14:54:48,069	WARNING util.py:136 -- The `process_trial` operation took 0.8615212440490723 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    319 |          8155.91 | 51611648 |  307.931 |              331.475 |              148.141 |             772.13 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3026.9637911718573
    time_step_min: 2872
  date: 2020-10-15_14-55-13
  done: false
  episode_len_mean: 772.1337125113828
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 307.9950266243429
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 214
  episodes_total: 66987
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.972467875194624e-46
        cur_lr: 5.0e-05
        entropy: 0.07245752401649952
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008151367563793125
        total_loss: .nan
        vf_explained_var: 0.9989468455314636
        vf_loss: 0.3277665326992671
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.806451612903224
    gpu_util_percent0: 0.2958064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14605873897415025
    mean_env_wait_ms: 1.215330102809571
    mean_inference_ms: 4.281212702029625
    mean_raw_obs_processing_ms: 0.37499416765918514
  time_since_restore: 8181.426349878311
  time_this_iter_s: 25.5152108669281
  time_total_s: 8181.426349878311
  timers:
    learn_throughput: 8800.878
    learn_time_ms: 18383.621
    sample_throughput: 23808.438
    sample_time_ms: 6795.574
    update_time_ms: 22.709
  timestamp: 1602773713
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:55:15,068	WARNING util.py:136 -- The `process_trial` operation took 0.9327874183654785 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    320 |          8181.43 | 51773440 |  307.995 |              331.475 |              148.141 |            772.134 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3026.630888882267
    time_step_min: 2872
  date: 2020-10-15_14-55-40
  done: false
  episode_len_mean: 772.1378945801072
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.0480933516224
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 67160
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9587018127919354e-46
        cur_lr: 5.0e-05
        entropy: 0.0729799618323644
        entropy_coeff: 0.0005000000000000001
        kl: 0.006134317062484722
        model: {}
        policy_loss: -0.00935362473440667
        total_loss: 0.2999095047513644
        vf_explained_var: 0.99884033203125
        vf_loss: 0.30929962173104286
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.670000000000005
    gpu_util_percent0: 0.32200000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14605774917392222
    mean_env_wait_ms: 1.2152838372413408
    mean_inference_ms: 4.281169274306141
    mean_raw_obs_processing_ms: 0.3749879552743452
  time_since_restore: 8206.778402090073
  time_this_iter_s: 25.352052211761475
  time_total_s: 8206.778402090073
  timers:
    learn_throughput: 8808.532
    learn_time_ms: 18367.646
    sample_throughput: 23836.631
    sample_time_ms: 6787.536
    update_time_ms: 24.434
  timestamp: 1602773740
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: e00ea_00000
  
2020-10-15 14:55:41,601	WARNING util.py:136 -- The `process_trial` operation took 0.8705246448516846 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    321 |          8206.78 | 51935232 |  308.048 |              331.475 |              148.141 |            772.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3026.1454086556305
    time_step_min: 2872
  date: 2020-10-15_14-56-06
  done: false
  episode_len_mean: 772.1428698606763
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.1230561621678
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 237
  episodes_total: 67397
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9587018127919354e-46
        cur_lr: 5.0e-05
        entropy: 0.06968897643188636
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00899994526601707
        total_loss: .nan
        vf_explained_var: 0.9993743300437927
        vf_loss: 0.20817703132828078
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.370000000000005
    gpu_util_percent0: 0.3346666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.873333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14605652028200397
    mean_env_wait_ms: 1.215223496606676
    mean_inference_ms: 4.281121133097351
    mean_raw_obs_processing_ms: 0.3749795145070913
  time_since_restore: 8232.027364253998
  time_this_iter_s: 25.24896216392517
  time_total_s: 8232.027364253998
  timers:
    learn_throughput: 8815.985
    learn_time_ms: 18352.119
    sample_throughput: 23855.897
    sample_time_ms: 6782.055
    update_time_ms: 24.313
  timestamp: 1602773766
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:56:08,169	WARNING util.py:136 -- The `process_trial` operation took 0.9543759822845459 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    322 |          8232.03 | 52097024 |  308.123 |              331.475 |              148.141 |            772.143 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3025.680315496626
    time_step_min: 2872
  date: 2020-10-15_14-56-33
  done: false
  episode_len_mean: 772.1451388683487
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.1905695124515
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 221
  episodes_total: 67618
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.438052719187903e-46
        cur_lr: 5.0e-05
        entropy: 0.06926616281270981
        entropy_coeff: 0.0005000000000000001
        kl: 0.00430163419029365
        model: {}
        policy_loss: -0.00958193094508412
        total_loss: 0.2918434105813503
        vf_explained_var: 0.999056339263916
        vf_loss: 0.3014599693318208
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.270967741935486
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14605494529377647
    mean_env_wait_ms: 1.2151645178807684
    mean_inference_ms: 4.281067235097659
    mean_raw_obs_processing_ms: 0.3749707334819014
  time_since_restore: 8257.83645439148
  time_this_iter_s: 25.80909013748169
  time_total_s: 8257.83645439148
  timers:
    learn_throughput: 8809.761
    learn_time_ms: 18365.084
    sample_throughput: 23760.99
    sample_time_ms: 6809.144
    update_time_ms: 23.927
  timestamp: 1602773793
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: e00ea_00000
  
2020-10-15 14:56:35,201	WARNING util.py:136 -- The `process_trial` operation took 0.9033830165863037 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    323 |          8257.84 | 52258816 |  308.191 |              331.475 |              148.141 |            772.145 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3025.327020723859
    time_step_min: 2872
  date: 2020-10-15_14-57-01
  done: false
  episode_len_mean: 772.1498303584599
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.2424100870038
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 67790
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2190263595939515e-46
        cur_lr: 5.0e-05
        entropy: 0.06856931249300639
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006655380025279101
        total_loss: .nan
        vf_explained_var: 0.9988470077514648
        vf_loss: 0.31343475977579754
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.529032258064515
    gpu_util_percent0: 0.3567741935483872
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14605396309535357
    mean_env_wait_ms: 1.2151190801582534
    mean_inference_ms: 4.281025156898306
    mean_raw_obs_processing_ms: 0.3749646963491186
  time_since_restore: 8283.784243822098
  time_this_iter_s: 25.947789430618286
  time_total_s: 8283.784243822098
  timers:
    learn_throughput: 8787.576
    learn_time_ms: 18411.449
    sample_throughput: 23766.672
    sample_time_ms: 6807.516
    update_time_ms: 25.561
  timestamp: 1602773821
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:57:02,451	WARNING util.py:136 -- The `process_trial` operation took 0.9798924922943115 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    324 |          8283.78 | 52420608 |  308.242 |              331.475 |              148.141 |             772.15 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3024.8565039573955
    time_step_min: 2872
  date: 2020-10-15_14-57-28
  done: false
  episode_len_mean: 772.1547871089156
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.31383792048945
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 226
  episodes_total: 68016
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.328539539390928e-46
        cur_lr: 5.0e-05
        entropy: 0.07432136436303456
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.003806539432844147
        total_loss: .nan
        vf_explained_var: 0.99946528673172
        vf_loss: 0.1746614227692286
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.816666666666666
    gpu_util_percent0: 0.404
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14605265579409257
    mean_env_wait_ms: 1.215061384798993
    mean_inference_ms: 4.280977162181837
    mean_raw_obs_processing_ms: 0.3749565343508515
  time_since_restore: 8309.519268035889
  time_this_iter_s: 25.735024213790894
  time_total_s: 8309.519268035889
  timers:
    learn_throughput: 8791.418
    learn_time_ms: 18403.402
    sample_throughput: 23755.264
    sample_time_ms: 6810.785
    update_time_ms: 23.236
  timestamp: 1602773848
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:57:29,430	WARNING util.py:136 -- The `process_trial` operation took 0.9227149486541748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    325 |          8309.52 | 52582400 |  308.314 |              331.475 |              148.141 |            772.155 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3024.653124358561
    time_step_min: 2872
  date: 2020-10-15_14-57-55
  done: false
  episode_len_mean: 772.162598171375
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.31751653802127
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 232
  episodes_total: 68248
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.992809309086391e-46
        cur_lr: 5.0e-05
        entropy: 0.12250322910646598
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014054415204251805
        total_loss: .nan
        vf_explained_var: 0.9910045266151428
        vf_loss: 3.6696402629216514
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.158064516129034
    gpu_util_percent0: 0.3738709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14605113084273863
    mean_env_wait_ms: 1.2150002138658251
    mean_inference_ms: 4.280925362882812
    mean_raw_obs_processing_ms: 0.3749478083398377
  time_since_restore: 8335.555548667908
  time_this_iter_s: 26.036280632019043
  time_total_s: 8335.555548667908
  timers:
    learn_throughput: 8781.229
    learn_time_ms: 18424.756
    sample_throughput: 23647.125
    sample_time_ms: 6841.931
    update_time_ms: 22.315
  timestamp: 1602773875
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:57:56,834	WARNING util.py:136 -- The `process_trial` operation took 0.9388453960418701 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    326 |          8335.56 | 52744192 |  308.318 |              331.475 |              148.141 |            772.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3024.902262456674
    time_step_min: 2872
  date: 2020-10-15_14-58-22
  done: false
  episode_len_mean: 772.1725836390476
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.2818234818997
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 171
  episodes_total: 68419
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.489213963629586e-46
        cur_lr: 5.0e-05
        entropy: 0.12239847953120868
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012737719943591705
        total_loss: .inf
        vf_explained_var: 0.9907453656196594
        vf_loss: 3.5304630398750305
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.29677419354839
    gpu_util_percent0: 0.3148387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460501563262196
    mean_env_wait_ms: 1.2149558856409788
    mean_inference_ms: 4.280883393051455
    mean_raw_obs_processing_ms: 0.3749419590010261
  time_since_restore: 8361.515563726425
  time_this_iter_s: 25.960015058517456
  time_total_s: 8361.515563726425
  timers:
    learn_throughput: 8773.809
    learn_time_ms: 18440.338
    sample_throughput: 23672.754
    sample_time_ms: 6834.524
    update_time_ms: 23.487
  timestamp: 1602773902
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:58:24,157	WARNING util.py:136 -- The `process_trial` operation took 0.944817304611206 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    327 |          8361.52 | 52905984 |  308.282 |              331.475 |              148.141 |            772.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3024.8580488089337
    time_step_min: 2872
  date: 2020-10-15_14-58-49
  done: false
  episode_len_mean: 772.1841162072382
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.30321190811327
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 217
  episodes_total: 68636
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1233820945444383e-45
        cur_lr: 5.0e-05
        entropy: 0.09172012905279796
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045496877282857895
        model: {}
        policy_loss: -0.010195306931564119
        total_loss: 2.015899509191513
        vf_explained_var: 0.9948210120201111
        vf_loss: 2.0261406898498535
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.67741935483871
    gpu_util_percent0: 0.33096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14604895268528553
    mean_env_wait_ms: 1.2149016389442724
    mean_inference_ms: 4.28083775899484
    mean_raw_obs_processing_ms: 0.37493437640985067
  time_since_restore: 8387.23448896408
  time_this_iter_s: 25.71892523765564
  time_total_s: 8387.23448896408
  timers:
    learn_throughput: 8777.033
    learn_time_ms: 18433.564
    sample_throughput: 23638.663
    sample_time_ms: 6844.38
    update_time_ms: 23.428
  timestamp: 1602773929
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: e00ea_00000
  
2020-10-15 14:58:51,163	WARNING util.py:136 -- The `process_trial` operation took 0.9623816013336182 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    328 |          8387.23 | 53067776 |  308.303 |              331.475 |              148.141 |            772.184 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3024.5133438902853
    time_step_min: 2872
  date: 2020-10-15_14-59-17
  done: false
  episode_len_mean: 772.1918838475499
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.3639802746157
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 239
  episodes_total: 68875
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.616910472722191e-46
        cur_lr: 5.0e-05
        entropy: 0.07479581174751122
        entropy_coeff: 0.0005000000000000001
        kl: 0.004068696541556467
        model: {}
        policy_loss: -0.008233016997110099
        total_loss: 0.6788337628046671
        vf_explained_var: 0.9979742169380188
        vf_loss: 0.6871041655540466
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.274193548387096
    gpu_util_percent0: 0.3196774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14604749085001648
    mean_env_wait_ms: 1.2148387990666087
    mean_inference_ms: 4.280785332838043
    mean_raw_obs_processing_ms: 0.37492547332786635
  time_since_restore: 8413.09217453003
  time_this_iter_s: 25.857685565948486
  time_total_s: 8413.09217453003
  timers:
    learn_throughput: 8761.454
    learn_time_ms: 18466.341
    sample_throughput: 23625.122
    sample_time_ms: 6848.303
    update_time_ms: 25.134
  timestamp: 1602773957
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: e00ea_00000
  
2020-10-15 14:59:18,438	WARNING util.py:136 -- The `process_trial` operation took 0.9230043888092041 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    329 |          8413.09 | 53229568 |  308.364 |              331.475 |              148.141 |            772.192 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3024.199316001507
    time_step_min: 2872
  date: 2020-10-15_14-59-44
  done: false
  episode_len_mean: 772.1961244351755
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.4099098387421
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 69048
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8084552363610957e-46
        cur_lr: 5.0e-05
        entropy: 0.06845569238066673
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00801796532565883
        total_loss: .nan
        vf_explained_var: 0.9983063340187073
        vf_loss: 0.4660825928052266
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.026666666666664
    gpu_util_percent0: 0.35633333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460465580713767
    mean_env_wait_ms: 1.214794552352115
    mean_inference_ms: 4.280745208068933
    mean_raw_obs_processing_ms: 0.37491945395277604
  time_since_restore: 8438.774718523026
  time_this_iter_s: 25.682543992996216
  time_total_s: 8438.774718523026
  timers:
    learn_throughput: 8755.427
    learn_time_ms: 18479.054
    sample_throughput: 23629.171
    sample_time_ms: 6847.13
    update_time_ms: 25.834
  timestamp: 1602773984
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 14:59:45,402	WARNING util.py:136 -- The `process_trial` operation took 0.9544217586517334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    330 |          8438.77 | 53391360 |   308.41 |              331.475 |              148.141 |            772.196 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3023.79031768734
    time_step_min: 2872
  date: 2020-10-15_15-00-11
  done: false
  episode_len_mean: 772.2008489626196
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.4702405875362
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 213
  episodes_total: 69261
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.212682854541643e-46
        cur_lr: 5.0e-05
        entropy: 0.0712561837087075
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00812847527655928
        total_loss: .nan
        vf_explained_var: 0.9982327818870544
        vf_loss: 0.5823554048935572
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.08709677419355
    gpu_util_percent0: 0.33580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14604532554166608
    mean_env_wait_ms: 1.214740939527455
    mean_inference_ms: 4.280703087468959
    mean_raw_obs_processing_ms: 0.37491241208320153
  time_since_restore: 8464.558663368225
  time_this_iter_s: 25.783944845199585
  time_total_s: 8464.558663368225
  timers:
    learn_throughput: 8744.52
    learn_time_ms: 18502.103
    sample_throughput: 23597.061
    sample_time_ms: 6856.447
    update_time_ms: 25.495
  timestamp: 1602774011
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:00:12,413	WARNING util.py:136 -- The `process_trial` operation took 0.9009783267974854 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    331 |          8464.56 | 53553152 |   308.47 |              331.475 |              148.141 |            772.201 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3023.347993838269
    time_step_min: 2872
  date: 2020-10-15_15-00-37
  done: false
  episode_len_mean: 772.2102067536653
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.5357899673543
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 242
  episodes_total: 69503
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.3190242818124654e-46
        cur_lr: 5.0e-05
        entropy: 0.07183752891918023
        entropy_coeff: 0.0005000000000000001
        kl: 0.004513073052900533
        model: {}
        policy_loss: -0.00915590328319619
        total_loss: 0.42453256497780484
        vf_explained_var: 0.9987502694129944
        vf_loss: 0.43372438848018646
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.006666666666668
    gpu_util_percent0: 0.3153333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14604399408237873
    mean_env_wait_ms: 1.2146791763505143
    mean_inference_ms: 4.280651404187378
    mean_raw_obs_processing_ms: 0.37490351497079655
  time_since_restore: 8489.781113624573
  time_this_iter_s: 25.222450256347656
  time_total_s: 8489.781113624573
  timers:
    learn_throughput: 8747.276
    learn_time_ms: 18496.273
    sample_throughput: 23624.484
    sample_time_ms: 6848.488
    update_time_ms: 26.859
  timestamp: 1602774037
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: e00ea_00000
  
2020-10-15 15:00:39,118	WARNING util.py:136 -- The `process_trial` operation took 0.967627763748169 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    332 |          8489.78 | 53714944 |  308.536 |              331.475 |              148.141 |             772.21 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3023.0415589637546
    time_step_min: 2872
  date: 2020-10-15_15-01-04
  done: false
  episode_len_mean: 772.2167685639656
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.58300824485286
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 69678
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1595121409062327e-46
        cur_lr: 5.0e-05
        entropy: 0.0698900433878104
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005772999351999412
        total_loss: .nan
        vf_explained_var: 0.9989702105522156
        vf_loss: 0.27589015165964764
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.716129032258067
    gpu_util_percent0: 0.28548387096774186
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14604293707630311
    mean_env_wait_ms: 1.2146342380720019
    mean_inference_ms: 4.280612754185814
    mean_raw_obs_processing_ms: 0.3748975673760539
  time_since_restore: 8515.592963695526
  time_this_iter_s: 25.81185007095337
  time_total_s: 8515.592963695526
  timers:
    learn_throughput: 8737.619
    learn_time_ms: 18516.715
    sample_throughput: 23701.351
    sample_time_ms: 6826.278
    update_time_ms: 27.382
  timestamp: 1602774064
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:01:06,272	WARNING util.py:136 -- The `process_trial` operation took 0.9514830112457275 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    333 |          8515.59 | 53876736 |  308.583 |              331.475 |              148.141 |            772.217 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3022.668389553863
    time_step_min: 2872
  date: 2020-10-15_15-01-31
  done: false
  episode_len_mean: 772.2227341670721
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.6387977592369
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 208
  episodes_total: 69886
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.739268211359348e-46
        cur_lr: 5.0e-05
        entropy: 0.07290988725920518
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008128936383097122
        total_loss: .nan
        vf_explained_var: 0.9984171986579895
        vf_loss: 0.5595611209670702
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.61666666666667
    gpu_util_percent0: 0.29300000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14604171168636587
    mean_env_wait_ms: 1.2145816468644184
    mean_inference_ms: 4.280566739478503
    mean_raw_obs_processing_ms: 0.3748907503645184
  time_since_restore: 8541.235985517502
  time_this_iter_s: 25.643021821975708
  time_total_s: 8541.235985517502
  timers:
    learn_throughput: 8748.382
    learn_time_ms: 18493.934
    sample_throughput: 23695.119
    sample_time_ms: 6828.073
    update_time_ms: 25.345
  timestamp: 1602774091
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:01:33,264	WARNING util.py:136 -- The `process_trial` operation took 0.9119679927825928 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    334 |          8541.24 | 54038528 |  308.639 |              331.475 |              148.141 |            772.223 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3022.2220557600663
    time_step_min: 2872
  date: 2020-10-15_15-01-59
  done: false
  episode_len_mean: 772.228981291353
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.7067008206641
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 242
  episodes_total: 70128
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.10890231703902e-46
        cur_lr: 5.0e-05
        entropy: 0.07395489265521367
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008416313474299386
        total_loss: .inf
        vf_explained_var: 0.9987934231758118
        vf_loss: 0.4144664878646533
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.883870967741935
    gpu_util_percent0: 0.31709677419354837
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14604051513833435
    mean_env_wait_ms: 1.214520770071503
    mean_inference_ms: 4.280520805999944
    mean_raw_obs_processing_ms: 0.37488191607387217
  time_since_restore: 8567.014379501343
  time_this_iter_s: 25.778393983840942
  time_total_s: 8567.014379501343
  timers:
    learn_throughput: 8741.167
    learn_time_ms: 18509.199
    sample_throughput: 23740.439
    sample_time_ms: 6815.038
    update_time_ms: 25.57
  timestamp: 1602774119
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:02:00,424	WARNING util.py:136 -- The `process_trial` operation took 0.9639031887054443 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    335 |          8567.01 | 54200320 |  308.707 |              331.475 |              148.141 |            772.229 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3021.867877269881
    time_step_min: 2872
  date: 2020-10-15_15-02-26
  done: false
  episode_len_mean: 772.2325131560233
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.76104667784386
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 182
  episodes_total: 70310
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0663353475558531e-45
        cur_lr: 5.0e-05
        entropy: 0.0685356246928374
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0029470155423041433
        total_loss: .inf
        vf_explained_var: 0.9994575381278992
        vf_loss: 0.14516596868634224
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.229032258064517
    gpu_util_percent0: 0.3396774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603938222207802
    mean_env_wait_ms: 1.214474234388935
    mean_inference_ms: 4.280480814270271
    mean_raw_obs_processing_ms: 0.3748759480040426
  time_since_restore: 8592.838714838028
  time_this_iter_s: 25.82433533668518
  time_total_s: 8592.838714838028
  timers:
    learn_throughput: 8739.636
    learn_time_ms: 18512.442
    sample_throughput: 23826.136
    sample_time_ms: 6790.526
    update_time_ms: 25.615
  timestamp: 1602774146
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:02:27,629	WARNING util.py:136 -- The `process_trial` operation took 0.9564707279205322 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    336 |          8592.84 | 54362112 |  308.761 |              331.475 |              148.141 |            772.233 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3021.482432739244
    time_step_min: 2872
  date: 2020-10-15_15-02-53
  done: false
  episode_len_mean: 772.2380094732961
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.81488667770833
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 204
  episodes_total: 70514
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5995030213337794e-45
        cur_lr: 5.0e-05
        entropy: 0.07193516505261262
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010377523057589618
        total_loss: .inf
        vf_explained_var: 0.9983505606651306
        vf_loss: 0.5044067874550819
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14516129032258
    gpu_util_percent0: 0.2587096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603818651467135
    mean_env_wait_ms: 1.2144230009820676
    mean_inference_ms: 4.280435890655716
    mean_raw_obs_processing_ms: 0.3748694261960385
  time_since_restore: 8618.793175697327
  time_this_iter_s: 25.954460859298706
  time_total_s: 8618.793175697327
  timers:
    learn_throughput: 8737.04
    learn_time_ms: 18517.941
    sample_throughput: 23849.969
    sample_time_ms: 6783.741
    update_time_ms: 24.2
  timestamp: 1602774173
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:02:54,998	WARNING util.py:136 -- The `process_trial` operation took 0.9539382457733154 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    337 |          8618.79 | 54523904 |  308.815 |              331.475 |              148.141 |            772.238 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3021.052973202291
    time_step_min: 2872
  date: 2020-10-15_15-03-20
  done: false
  episode_len_mean: 772.2453326172675
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.87856146723834
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 243
  episodes_total: 70757
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3992545320006693e-45
        cur_lr: 5.0e-05
        entropy: 0.07142757810652256
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0094171120969501
        total_loss: .inf
        vf_explained_var: 0.9988287091255188
        vf_loss: 0.42010635882616043
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.716129032258067
    gpu_util_percent0: 0.3145161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603712529299648
    mean_env_wait_ms: 1.214362673373027
    mean_inference_ms: 4.280392938685569
    mean_raw_obs_processing_ms: 0.3748606786437466
  time_since_restore: 8644.585596323013
  time_this_iter_s: 25.792420625686646
  time_total_s: 8644.585596323013
  timers:
    learn_throughput: 8729.105
    learn_time_ms: 18534.775
    sample_throughput: 23858.295
    sample_time_ms: 6781.373
    update_time_ms: 24.394
  timestamp: 1602774200
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:03:22,171	WARNING util.py:136 -- The `process_trial` operation took 0.9593634605407715 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    338 |          8644.59 | 54685696 |  308.879 |              331.475 |              148.141 |            772.245 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3020.7631170662908
    time_step_min: 2872
  date: 2020-10-15_15-03-47
  done: false
  episode_len_mean: 772.2521073553044
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.9229766299346
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 185
  episodes_total: 70942
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5988817980010035e-45
        cur_lr: 5.0e-05
        entropy: 0.0680135947962602
        entropy_coeff: 0.0005000000000000001
        kl: 0.003927652471854041
        model: {}
        policy_loss: -0.008901356700031707
        total_loss: 0.7659769803285599
        vf_explained_var: 0.9972918629646301
        vf_loss: 0.7749123374621073
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.964516129032265
    gpu_util_percent0: 0.3109677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460358869182575
    mean_env_wait_ms: 1.2143152256718037
    mean_inference_ms: 4.280352070305646
    mean_raw_obs_processing_ms: 0.37485464902762866
  time_since_restore: 8670.334725379944
  time_this_iter_s: 25.749129056930542
  time_total_s: 8670.334725379944
  timers:
    learn_throughput: 8732.085
    learn_time_ms: 18528.45
    sample_throughput: 23866.045
    sample_time_ms: 6779.171
    update_time_ms: 23.105
  timestamp: 1602774227
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: e00ea_00000
  
2020-10-15 15:03:49,159	WARNING util.py:136 -- The `process_trial` operation took 0.9075031280517578 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    339 |          8670.33 | 54847488 |  308.923 |              331.475 |              148.141 |            772.252 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3020.423831092106
    time_step_min: 2872
  date: 2020-10-15_15-04-15
  done: false
  episode_len_mean: 772.2585542778418
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 308.9737847164537
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 192
  episodes_total: 71134
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7994408990005017e-45
        cur_lr: 5.0e-05
        entropy: 0.06590134277939796
        entropy_coeff: 0.0005000000000000001
        kl: 0.00580264887927721
        model: {}
        policy_loss: -0.009993449066920826
        total_loss: 0.3801887035369873
        vf_explained_var: 0.9987556338310242
        vf_loss: 0.3902151013414065
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.664516129032254
    gpu_util_percent0: 0.3351612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603461771820866
    mean_env_wait_ms: 1.2142658167075937
    mean_inference_ms: 4.280307026942345
    mean_raw_obs_processing_ms: 0.3748481614284611
  time_since_restore: 8696.276160001755
  time_this_iter_s: 25.941434621810913
  time_total_s: 8696.276160001755
  timers:
    learn_throughput: 8730.077
    learn_time_ms: 18532.711
    sample_throughput: 23787.351
    sample_time_ms: 6801.598
    update_time_ms: 22.962
  timestamp: 1602774255
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: e00ea_00000
  
2020-10-15 15:04:16,411	WARNING util.py:136 -- The `process_trial` operation took 0.9718706607818604 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    340 |          8696.28 | 55009280 |  308.974 |              331.475 |              148.141 |            772.259 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3019.9903139893468
    time_step_min: 2872
  date: 2020-10-15_15-04-42
  done: false
  episode_len_mean: 772.2692835728895
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.0404524355942
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 248
  episodes_total: 71382
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7994408990005017e-45
        cur_lr: 5.0e-05
        entropy: 0.07052346877753735
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010884198432904668
        total_loss: .inf
        vf_explained_var: 0.9986796975135803
        vf_loss: 0.5048123300075531
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.680645161290325
    gpu_util_percent0: 0.3109677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603372544407284
    mean_env_wait_ms: 1.2142066343019775
    mean_inference_ms: 4.28026875537134
    mean_raw_obs_processing_ms: 0.37484009765768417
  time_since_restore: 8722.130025148392
  time_this_iter_s: 25.853865146636963
  time_total_s: 8722.130025148392
  timers:
    learn_throughput: 8725.946
    learn_time_ms: 18541.486
    sample_throughput: 23791.279
    sample_time_ms: 6800.475
    update_time_ms: 21.478
  timestamp: 1602774282
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:04:43,527	WARNING util.py:136 -- The `process_trial` operation took 0.926811933517456 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    341 |          8722.13 | 55171072 |   309.04 |              331.475 |              148.141 |            772.269 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3019.6479148900476
    time_step_min: 2872
  date: 2020-10-15_15-05-09
  done: false
  episode_len_mean: 772.2763611976584
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.091929169724
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 191
  episodes_total: 71573
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6991613485007524e-45
        cur_lr: 5.0e-05
        entropy: 0.06810679907600085
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009395996972064799
        total_loss: .inf
        vf_explained_var: 0.9990787506103516
        vf_loss: 0.2650070848564307
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.653333333333332
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603246544670279
    mean_env_wait_ms: 1.2141575814932417
    mean_inference_ms: 4.280227458796109
    mean_raw_obs_processing_ms: 0.374833820297156
  time_since_restore: 8747.768463373184
  time_this_iter_s: 25.63843822479248
  time_total_s: 8747.768463373184
  timers:
    learn_throughput: 8702.896
    learn_time_ms: 18590.592
    sample_throughput: 23786.197
    sample_time_ms: 6801.928
    update_time_ms: 20.221
  timestamp: 1602774309
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:05:10,589	WARNING util.py:136 -- The `process_trial` operation took 0.9829716682434082 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    342 |          8747.77 | 55332864 |  309.092 |              331.475 |              148.141 |            772.276 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3019.312500871505
    time_step_min: 2872
  date: 2020-10-15_15-05-36
  done: false
  episode_len_mean: 772.2852962080354
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.14346610044606
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 184
  episodes_total: 71757
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.048742022751129e-45
        cur_lr: 5.0e-05
        entropy: 0.06784364022314548
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008802465017652139
        total_loss: .inf
        vf_explained_var: 0.9991504549980164
        vf_loss: 0.2420136978228887
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93225806451613
    gpu_util_percent0: 0.29967741935483866
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603150730938858
    mean_env_wait_ms: 1.2141108284812547
    mean_inference_ms: 4.280188136141722
    mean_raw_obs_processing_ms: 0.3748278472129344
  time_since_restore: 8773.685821056366
  time_this_iter_s: 25.917357683181763
  time_total_s: 8773.685821056366
  timers:
    learn_throughput: 8702.094
    learn_time_ms: 18592.306
    sample_throughput: 23731.392
    sample_time_ms: 6817.636
    update_time_ms: 21.898
  timestamp: 1602774336
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:05:37,919	WARNING util.py:136 -- The `process_trial` operation took 0.9803798198699951 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    343 |          8773.69 | 55494656 |  309.143 |              331.475 |              148.141 |            772.285 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3018.889423076923
    time_step_min: 2872
  date: 2020-10-15_15-06-03
  done: false
  episode_len_mean: 772.3030412442716
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.2083345607164
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 253
  episodes_total: 72010
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.073113034126693e-45
        cur_lr: 5.0e-05
        entropy: 0.06953172013163567
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007404811889955454
        total_loss: .inf
        vf_explained_var: 0.9990718364715576
        vf_loss: 0.33021068572998047
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.932258064516127
    gpu_util_percent0: 0.3651612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14603044574796026
    mean_env_wait_ms: 1.2140499132864164
    mean_inference_ms: 4.280147186406863
    mean_raw_obs_processing_ms: 0.37481959745025983
  time_since_restore: 8799.331193447113
  time_this_iter_s: 25.64537239074707
  time_total_s: 8799.331193447113
  timers:
    learn_throughput: 8707.619
    learn_time_ms: 18580.509
    sample_throughput: 23694.169
    sample_time_ms: 6828.347
    update_time_ms: 22.269
  timestamp: 1602774363
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:06:04,904	WARNING util.py:136 -- The `process_trial` operation took 0.9995841979980469 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    344 |          8799.33 | 55656448 |  309.208 |              331.475 |              148.141 |            772.303 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3018.5443585266485
    time_step_min: 2872
  date: 2020-10-15_15-06-30
  done: false
  episode_len_mean: 772.3156334829096
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.2596515260635
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 194
  episodes_total: 72204
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.109669551190042e-45
        cur_lr: 5.0e-05
        entropy: 0.06366569952418406
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0066597438174843164
        total_loss: .inf
        vf_explained_var: 0.9991264939308167
        vf_loss: 0.2574012602368991
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.690322580645166
    gpu_util_percent0: 0.2745161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602918709522333
    mean_env_wait_ms: 1.214000747411732
    mean_inference_ms: 4.280106900474983
    mean_raw_obs_processing_ms: 0.37481338810439774
  time_since_restore: 8824.78836107254
  time_this_iter_s: 25.457167625427246
  time_total_s: 8824.78836107254
  timers:
    learn_throughput: 8724.266
    learn_time_ms: 18545.057
    sample_throughput: 23685.472
    sample_time_ms: 6830.854
    update_time_ms: 22.77
  timestamp: 1602774390
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:06:31,894	WARNING util.py:136 -- The `process_trial` operation took 1.0132925510406494 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    345 |          8824.79 | 55818240 |   309.26 |              331.475 |              148.141 |            772.316 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3018.229329388833
    time_step_min: 2872
  date: 2020-10-15_15-06-57
  done: false
  episode_len_mean: 772.3229251578497
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.30748566642967
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 175
  episodes_total: 72379
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3664504326785058e-44
        cur_lr: 5.0e-05
        entropy: 0.06014563050121069
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009673041554682035
        total_loss: .inf
        vf_explained_var: 0.9991492629051208
        vf_loss: 0.2913239647944768
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.812903225806455
    gpu_util_percent0: 0.2867741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602834429298822
    mean_env_wait_ms: 1.2139564484008118
    mean_inference_ms: 4.280069968439802
    mean_raw_obs_processing_ms: 0.3748078310043362
  time_since_restore: 8850.665099143982
  time_this_iter_s: 25.87673807144165
  time_total_s: 8850.665099143982
  timers:
    learn_throughput: 8725.113
    learn_time_ms: 18543.256
    sample_throughput: 23702.244
    sample_time_ms: 6826.02
    update_time_ms: 24.099
  timestamp: 1602774417
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:06:59,101	WARNING util.py:136 -- The `process_trial` operation took 0.9842140674591064 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    346 |          8850.67 | 55980032 |  309.307 |              331.475 |              148.141 |            772.323 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3017.7747881793757
    time_step_min: 2872
  date: 2020-10-15_15-07-24
  done: false
  episode_len_mean: 772.3341457033886
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.3746290197612
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 248
  episodes_total: 72627
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0496756490177594e-44
        cur_lr: 5.0e-05
        entropy: 0.06597766901055972
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006590531188218544
        total_loss: .inf
        vf_explained_var: 0.9993643760681152
        vf_loss: 0.2268995096286138
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95666666666667
    gpu_util_percent0: 0.34400000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602727227730958
    mean_env_wait_ms: 1.2138968253333093
    mean_inference_ms: 4.280030575632899
    mean_raw_obs_processing_ms: 0.3747998438442976
  time_since_restore: 8876.453654527664
  time_this_iter_s: 25.78855538368225
  time_total_s: 8876.453654527664
  timers:
    learn_throughput: 8734.768
    learn_time_ms: 18522.759
    sample_throughput: 23702.583
    sample_time_ms: 6825.923
    update_time_ms: 25.451
  timestamp: 1602774444
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:07:26,349	WARNING util.py:136 -- The `process_trial` operation took 1.0062000751495361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    347 |          8876.45 | 56141824 |  309.375 |              331.475 |              148.141 |            772.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3017.530828410496
    time_step_min: 2872
  date: 2020-10-15_15-07-52
  done: false
  episode_len_mean: 772.359141586116
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.3959055903944
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 205
  episodes_total: 72832
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.074513473526639e-44
        cur_lr: 5.0e-05
        entropy: 0.1080295592546463
        entropy_coeff: 0.0005000000000000001
        kl: 0.00923597541016837
        model: {}
        policy_loss: -0.015032233165887495
        total_loss: 1.7238824168841045
        vf_explained_var: 0.9948099255561829
        vf_loss: 1.7389686803023021
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9741935483871
    gpu_util_percent0: 0.28483870967741937
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602587797277955
    mean_env_wait_ms: 1.2138454384035444
    mean_inference_ms: 4.279989328888426
    mean_raw_obs_processing_ms: 0.3747932069989533
  time_since_restore: 8902.270997285843
  time_this_iter_s: 25.81734275817871
  time_total_s: 8902.270997285843
  timers:
    learn_throughput: 8736.374
    learn_time_ms: 18519.353
    sample_throughput: 23683.764
    sample_time_ms: 6831.347
    update_time_ms: 25.378
  timestamp: 1602774472
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: e00ea_00000
  
2020-10-15 15:07:53,696	WARNING util.py:136 -- The `process_trial` operation took 0.9614424705505371 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    348 |          8902.27 | 56303616 |  309.396 |              331.475 |              148.141 |            772.359 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3017.8886543680273
    time_step_min: 2872
  date: 2020-10-15_15-08-19
  done: false
  episode_len_mean: 772.4143745548189
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.3301285829642
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 172
  episodes_total: 73004
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.074513473526639e-44
        cur_lr: 5.0e-05
        entropy: 0.12585725262761116
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.013489196494144077
        total_loss: .inf
        vf_explained_var: 0.9905107021331787
        vf_loss: 4.174178719520569
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.741935483870968
    gpu_util_percent0: 0.3609677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602505584390987
    mean_env_wait_ms: 1.2138028785785557
    mean_inference_ms: 4.279953355836457
    mean_raw_obs_processing_ms: 0.3747880825110393
  time_since_restore: 8927.912777423859
  time_this_iter_s: 25.641780138015747
  time_total_s: 8927.912777423859
  timers:
    learn_throughput: 8750.306
    learn_time_ms: 18489.867
    sample_throughput: 23659.053
    sample_time_ms: 6838.482
    update_time_ms: 24.608
  timestamp: 1602774499
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:08:20,651	WARNING util.py:136 -- The `process_trial` operation took 0.96848464012146 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    349 |          8927.91 | 56465408 |   309.33 |              331.475 |              148.141 |            772.414 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3017.854821960484
    time_step_min: 2872
  date: 2020-10-15_15-08-46
  done: false
  episode_len_mean: 772.4398044463866
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.35283393281713
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 224
  episodes_total: 73228
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.611770210289958e-44
        cur_lr: 5.0e-05
        entropy: 0.077079176902771
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010085584610351361
        total_loss: .inf
        vf_explained_var: 0.9966525435447693
        vf_loss: 1.2934095958868663
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.000000000000004
    gpu_util_percent0: 0.3443333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602409607146902
    mean_env_wait_ms: 1.213748946877244
    mean_inference_ms: 4.2799180240099535
    mean_raw_obs_processing_ms: 0.3747816416227361
  time_since_restore: 8953.541357040405
  time_this_iter_s: 25.62857961654663
  time_total_s: 8953.541357040405
  timers:
    learn_throughput: 8756.763
    learn_time_ms: 18476.235
    sample_throughput: 23725.154
    sample_time_ms: 6819.429
    update_time_ms: 24.221
  timestamp: 1602774526
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:08:47,706	WARNING util.py:136 -- The `process_trial` operation took 1.016678810119629 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    350 |          8953.54 | 56627200 |  309.353 |              331.475 |              148.141 |             772.44 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3017.5227405843493
    time_step_min: 2872
  date: 2020-10-15_15-09-13
  done: false
  episode_len_mean: 772.4501408987571
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.40900792231514
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 229
  episodes_total: 73457
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.917655315434936e-44
        cur_lr: 5.0e-05
        entropy: 0.06454486679285765
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009423879547587907
        total_loss: .inf
        vf_explained_var: 0.9988207221031189
        vf_loss: 0.3865206663807233
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.593548387096778
    gpu_util_percent0: 0.32387096774193547
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460226113766481
    mean_env_wait_ms: 1.2136916837373002
    mean_inference_ms: 4.279872128168329
    mean_raw_obs_processing_ms: 0.37477357330189437
  time_since_restore: 8979.467011928558
  time_this_iter_s: 25.925654888153076
  time_total_s: 8979.467011928558
  timers:
    learn_throughput: 8752.206
    learn_time_ms: 18485.853
    sample_throughput: 23712.809
    sample_time_ms: 6822.979
    update_time_ms: 25.52
  timestamp: 1602774553
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:09:15,011	WARNING util.py:136 -- The `process_trial` operation took 0.9875659942626953 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    351 |          8979.47 | 56788992 |  309.409 |              331.475 |              148.141 |             772.45 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3017.2376474425178
    time_step_min: 2872
  date: 2020-10-15_15-09-40
  done: false
  episode_len_mean: 772.4595952736656
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.4533011769194
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 73630
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0376482973152404e-43
        cur_lr: 5.0e-05
        entropy: 0.06040082934002081
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007507412228733301
        total_loss: .inf
        vf_explained_var: 0.9989973902702332
        vf_loss: 0.27096077054739
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.119354838709683
    gpu_util_percent0: 0.3067741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602185889827543
    mean_env_wait_ms: 1.213649520797234
    mean_inference_ms: 4.279839768607217
    mean_raw_obs_processing_ms: 0.37476859251450856
  time_since_restore: 9005.184987545013
  time_this_iter_s: 25.717975616455078
  time_total_s: 9005.184987545013
  timers:
    learn_throughput: 8757.794
    learn_time_ms: 18474.059
    sample_throughput: 23685.571
    sample_time_ms: 6830.825
    update_time_ms: 26.239
  timestamp: 1602774580
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:09:42,035	WARNING util.py:136 -- The `process_trial` operation took 0.9525845050811768 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    352 |          9005.18 | 56950784 |  309.453 |              331.475 |              148.141 |             772.46 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3016.892023359484
    time_step_min: 2872
  date: 2020-10-15_15-10-07
  done: false
  episode_len_mean: 772.4695917123705
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.5088258165652
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 215
  episodes_total: 73845
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5564724459728608e-43
        cur_lr: 5.0e-05
        entropy: 0.06521083290378253
        entropy_coeff: 0.0005000000000000001
        kl: 0.005073311544644336
        model: {}
        policy_loss: -0.009990863560233265
        total_loss: 0.3598094806075096
        vf_explained_var: 0.998877227306366
        vf_loss: 0.36983295033375424
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.40666666666667
    gpu_util_percent0: 0.34900000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8766666666666674
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14602058973558266
    mean_env_wait_ms: 1.2135978474865876
    mean_inference_ms: 4.27979871421905
    mean_raw_obs_processing_ms: 0.3747619295413037
  time_since_restore: 9030.85358262062
  time_this_iter_s: 25.6685950756073
  time_total_s: 9030.85358262062
  timers:
    learn_throughput: 8761.106
    learn_time_ms: 18467.075
    sample_throughput: 23749.058
    sample_time_ms: 6812.565
    update_time_ms: 24.651
  timestamp: 1602774607
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: e00ea_00000
  
2020-10-15 15:10:09,083	WARNING util.py:136 -- The `process_trial` operation took 0.9429342746734619 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    353 |          9030.85 | 57112576 |  309.509 |              331.475 |              148.141 |             772.47 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3016.4678894126228
    time_step_min: 2872
  date: 2020-10-15_15-10-34
  done: false
  episode_len_mean: 772.4789897817313
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.56999199778267
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 238
  episodes_total: 74083
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5564724459728608e-43
        cur_lr: 5.0e-05
        entropy: 0.06831922195851803
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008003447321243584
        total_loss: .inf
        vf_explained_var: 0.9989740252494812
        vf_loss: 0.3641115923722585
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612904
    gpu_util_percent0: 0.3416129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460195939111066
    mean_env_wait_ms: 1.213539647133119
    mean_inference_ms: 4.279762050036459
    mean_raw_obs_processing_ms: 0.37475449615294276
  time_since_restore: 9056.670563220978
  time_this_iter_s: 25.816980600357056
  time_total_s: 9056.670563220978
  timers:
    learn_throughput: 8750.563
    learn_time_ms: 18489.324
    sample_throughput: 23778.398
    sample_time_ms: 6804.159
    update_time_ms: 26.208
  timestamp: 1602774634
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:10:36,315	WARNING util.py:136 -- The `process_trial` operation took 0.9987664222717285 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    354 |          9056.67 | 57274368 |   309.57 |              331.475 |              148.141 |            772.479 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3016.1661299667203
    time_step_min: 2872
  date: 2020-10-15_15-11-02
  done: false
  episode_len_mean: 772.4833762001589
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.6168755871832
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 178
  episodes_total: 74261
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3347086689592913e-43
        cur_lr: 5.0e-05
        entropy: 0.06343596739073594
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006191281628465124
        total_loss: .inf
        vf_explained_var: 0.9991413950920105
        vf_loss: 0.24410422767202059
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.138709677419353
    gpu_util_percent0: 0.3145161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14601866512682052
    mean_env_wait_ms: 1.2134960848414622
    mean_inference_ms: 4.279726908703666
    mean_raw_obs_processing_ms: 0.3747492780212807
  time_since_restore: 9082.430365800858
  time_this_iter_s: 25.75980257987976
  time_total_s: 9082.430365800858
  timers:
    learn_throughput: 8744.847
    learn_time_ms: 18501.409
    sample_throughput: 23755.857
    sample_time_ms: 6810.615
    update_time_ms: 26.545
  timestamp: 1602774662
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:11:03,410	WARNING util.py:136 -- The `process_trial` operation took 0.9861161708831787 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    355 |          9082.43 | 57436160 |  309.617 |              331.475 |              148.141 |            772.483 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3015.8032192185738
    time_step_min: 2872
  date: 2020-10-15_15-11-29
  done: false
  episode_len_mean: 772.4847186786626
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.67145267635414
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 209
  episodes_total: 74470
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5020630034389366e-43
        cur_lr: 5.0e-05
        entropy: 0.06718386523425579
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033254650770686567
        model: {}
        policy_loss: -0.006364142871461809
        total_loss: 0.3075382212797801
        vf_explained_var: 0.9990008473396301
        vf_loss: 0.3139359454313914
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.309677419354838
    gpu_util_percent0: 0.2925806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146017469801949
    mean_env_wait_ms: 1.2134455796198826
    mean_inference_ms: 4.279687180844356
    mean_raw_obs_processing_ms: 0.3747431273622588
  time_since_restore: 9108.049167394638
  time_this_iter_s: 25.618801593780518
  time_total_s: 9108.049167394638
  timers:
    learn_throughput: 8761.804
    learn_time_ms: 18465.605
    sample_throughput: 23714.722
    sample_time_ms: 6822.429
    update_time_ms: 25.261
  timestamp: 1602774689
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: e00ea_00000
  
2020-10-15 15:11:30,387	WARNING util.py:136 -- The `process_trial` operation took 1.0093951225280762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    356 |          9108.05 | 57597952 |  309.671 |              331.475 |              148.141 |            772.485 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3015.3793911634325
    time_step_min: 2872
  date: 2020-10-15_15-11-56
  done: false
  episode_len_mean: 772.4884953620045
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.73410489264023
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 239
  episodes_total: 74709
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7510315017194683e-43
        cur_lr: 5.0e-05
        entropy: 0.0676306424041589
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007817863918414028
        total_loss: .inf
        vf_explained_var: 0.9991674423217773
        vf_loss: 0.2784492013355096
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896666666666665
    gpu_util_percent0: 0.31633333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14601653660312724
    mean_env_wait_ms: 1.2133890472623434
    mean_inference_ms: 4.279653766734715
    mean_raw_obs_processing_ms: 0.37473580887146307
  time_since_restore: 9133.910454750061
  time_this_iter_s: 25.861287355422974
  time_total_s: 9133.910454750061
  timers:
    learn_throughput: 8754.609
    learn_time_ms: 18480.78
    sample_throughput: 23738.62
    sample_time_ms: 6815.561
    update_time_ms: 25.459
  timestamp: 1602774716
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:11:57,663	WARNING util.py:136 -- The `process_trial` operation took 0.9719913005828857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    357 |          9133.91 | 57759744 |  309.734 |              331.475 |              148.141 |            772.488 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3015.0694847096233
    time_step_min: 2872
  date: 2020-10-15_15-12-23
  done: false
  episode_len_mean: 772.4929165609603
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.78187169924723
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 184
  episodes_total: 74893
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.626547252579203e-43
        cur_lr: 5.0e-05
        entropy: 0.06925381471713384
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0060667949001071975
        total_loss: .inf
        vf_explained_var: 0.9990759491920471
        vf_loss: 0.24647068108121553
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89032258064516
    gpu_util_percent0: 0.39645161290322584
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460155389514002
    mean_env_wait_ms: 1.2133434771538565
    mean_inference_ms: 4.279617109060295
    mean_raw_obs_processing_ms: 0.37473034987005466
  time_since_restore: 9159.499826908112
  time_this_iter_s: 25.589372158050537
  time_total_s: 9159.499826908112
  timers:
    learn_throughput: 8761.707
    learn_time_ms: 18465.809
    sample_throughput: 23773.82
    sample_time_ms: 6805.469
    update_time_ms: 26.974
  timestamp: 1602774743
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:12:24,635	WARNING util.py:136 -- The `process_trial` operation took 1.0307157039642334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    358 |           9159.5 | 57921536 |  309.782 |              331.475 |              148.141 |            772.493 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3014.729955501079
    time_step_min: 2872
  date: 2020-10-15_15-12-50
  done: false
  episode_len_mean: 772.4978162450067
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.8338303137905
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 207
  episodes_total: 75100
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.939820878868804e-43
        cur_lr: 5.0e-05
        entropy: 0.0694229497263829
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037627233541570604
        model: {}
        policy_loss: -0.008569260671113929
        total_loss: 0.3993022268017133
        vf_explained_var: 0.9987407326698303
        vf_loss: 0.4079061945279439
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.938709677419357
    gpu_util_percent0: 0.34290322580645166
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14601446663055923
    mean_env_wait_ms: 1.2132942065034795
    mean_inference_ms: 4.279580468354035
    mean_raw_obs_processing_ms: 0.37472447230807476
  time_since_restore: 9185.466836214066
  time_this_iter_s: 25.96700930595398
  time_total_s: 9185.466836214066
  timers:
    learn_throughput: 8752.236
    learn_time_ms: 18485.791
    sample_throughput: 23703.584
    sample_time_ms: 6825.634
    update_time_ms: 27.032
  timestamp: 1602774770
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: e00ea_00000
  
2020-10-15 15:12:52,005	WARNING util.py:136 -- The `process_trial` operation took 0.978837251663208 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    359 |          9185.47 | 58083328 |  309.834 |              331.475 |              148.141 |            772.498 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3014.326608672555
    time_step_min: 2872
  date: 2020-10-15_15-13-17
  done: false
  episode_len_mean: 772.5038958280792
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.8977747529855
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 237
  episodes_total: 75337
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.969910439434402e-43
        cur_lr: 5.0e-05
        entropy: 0.06976218211154143
        entropy_coeff: 0.0005000000000000001
        kl: 0.007230045506730676
        model: {}
        policy_loss: -0.006241709255846217
        total_loss: 0.12716811833282313
        vf_explained_var: 0.9996122717857361
        vf_loss: 0.13344470970332623
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612903
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14601350810510874
    mean_env_wait_ms: 1.2132387264236426
    mean_inference_ms: 4.279547932919212
    mean_raw_obs_processing_ms: 0.374717348522933
  time_since_restore: 9211.391410827637
  time_this_iter_s: 25.924574613571167
  time_total_s: 9211.391410827637
  timers:
    learn_throughput: 8743.456
    learn_time_ms: 18504.353
    sample_throughput: 23670.588
    sample_time_ms: 6835.149
    update_time_ms: 28.479
  timestamp: 1602774797
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: e00ea_00000
  
2020-10-15 15:13:19,296	WARNING util.py:136 -- The `process_trial` operation took 1.015209436416626 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    360 |          9211.39 | 58245120 |  309.898 |              331.475 |              148.141 |            772.504 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3014.027304522933
    time_step_min: 2872
  date: 2020-10-15_15-13-45
  done: false
  episode_len_mean: 772.5104734918701
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.94355851294057
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 187
  episodes_total: 75524
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.969910439434402e-43
        cur_lr: 5.0e-05
        entropy: 0.06691247286895911
        entropy_coeff: 0.0005000000000000001
        kl: 0.005066235161696871
        model: {}
        policy_loss: -0.008263408982505402
        total_loss: 0.3172993138432503
        vf_explained_var: 0.998843252658844
        vf_loss: 0.3255961711208026
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.916129032258066
    gpu_util_percent0: 0.31645161290322577
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14601245396973658
    mean_env_wait_ms: 1.2131923249298253
    mean_inference_ms: 4.2795113257527335
    mean_raw_obs_processing_ms: 0.37471182372994344
  time_since_restore: 9237.132021188736
  time_this_iter_s: 25.740610361099243
  time_total_s: 9237.132021188736
  timers:
    learn_throughput: 8754.4
    learn_time_ms: 18481.22
    sample_throughput: 23683.179
    sample_time_ms: 6831.515
    update_time_ms: 27.792
  timestamp: 1602774825
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: e00ea_00000
  
2020-10-15 15:13:46,469	WARNING util.py:136 -- The `process_trial` operation took 1.0707905292510986 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    361 |          9237.13 | 58406912 |  309.944 |              331.475 |              148.141 |             772.51 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3013.7087644190747
    time_step_min: 2872
  date: 2020-10-15_15-14-12
  done: false
  episode_len_mean: 772.5179403879931
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 309.99309124684527
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 199
  episodes_total: 75723
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.969910439434402e-43
        cur_lr: 5.0e-05
        entropy: 0.0717656717946132
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00893119678706474
        total_loss: .inf
        vf_explained_var: 0.9987141489982605
        vf_loss: 0.39659109711647034
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.838709677419352
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14601145316853764
    mean_env_wait_ms: 1.2131443100112194
    mean_inference_ms: 4.27947579636122
    mean_raw_obs_processing_ms: 0.37470602424071586
  time_since_restore: 9262.81880569458
  time_this_iter_s: 25.686784505844116
  time_total_s: 9262.81880569458
  timers:
    learn_throughput: 8759.77
    learn_time_ms: 18469.891
    sample_throughput: 23644.904
    sample_time_ms: 6842.574
    update_time_ms: 28.645
  timestamp: 1602774852
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:14:13,518	WARNING util.py:136 -- The `process_trial` operation took 1.0050785541534424 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    362 |          9262.82 | 58568704 |  309.993 |              331.475 |              148.141 |            772.518 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3013.3217997181355
    time_step_min: 2872
  date: 2020-10-15_15-14-39
  done: false
  episode_len_mean: 772.5295070098072
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.051017647016
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 242
  episodes_total: 75965
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9548656591516033e-43
        cur_lr: 5.0e-05
        entropy: 0.07595368785162766
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047784182243049145
        model: {}
        policy_loss: -0.008959301359330615
        total_loss: 0.3940910796324412
        vf_explained_var: 0.9988947510719299
        vf_loss: 0.40308836350838345
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.113333333333333
    gpu_util_percent0: 0.3843333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14601047978657533
    mean_env_wait_ms: 1.213088313704246
    mean_inference_ms: 4.279441580692561
    mean_raw_obs_processing_ms: 0.37469893635357576
  time_since_restore: 9288.340733289719
  time_this_iter_s: 25.52192759513855
  time_total_s: 9288.340733289719
  timers:
    learn_throughput: 8768.433
    learn_time_ms: 18451.643
    sample_throughput: 23631.318
    sample_time_ms: 6846.508
    update_time_ms: 28.506
  timestamp: 1602774879
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: e00ea_00000
  
2020-10-15 15:14:40,417	WARNING util.py:136 -- The `process_trial` operation took 1.0282564163208008 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    363 |          9288.34 | 58730496 |  310.051 |              331.475 |              148.141 |             772.53 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3013.0137295862733
    time_step_min: 2872
  date: 2020-10-15_15-15-06
  done: false
  episode_len_mean: 772.5382049766923
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.0974905644988
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 190
  episodes_total: 76155
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4774328295758017e-43
        cur_lr: 5.0e-05
        entropy: 0.07145448898275693
        entropy_coeff: 0.0005000000000000001
        kl: 0.005769243851924936
        model: {}
        policy_loss: -0.006032654278290768
        total_loss: 0.302700512111187
        vf_explained_var: 0.9989252090454102
        vf_loss: 0.3087688982486725
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.583870967741934
    gpu_util_percent0: 0.33161290322580644
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14600948855702253
    mean_env_wait_ms: 1.2130421127791873
    mean_inference_ms: 4.2794079496264885
    mean_raw_obs_processing_ms: 0.3746935566921236
  time_since_restore: 9314.335660934448
  time_this_iter_s: 25.994927644729614
  time_total_s: 9314.335660934448
  timers:
    learn_throughput: 8762.16
    learn_time_ms: 18464.853
    sample_throughput: 23644.478
    sample_time_ms: 6842.697
    update_time_ms: 34.246
  timestamp: 1602774906
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: e00ea_00000
  
2020-10-15 15:15:07,799	WARNING util.py:136 -- The `process_trial` operation took 1.0234391689300537 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    364 |          9314.34 | 58892288 |  310.097 |              331.475 |              148.141 |            772.538 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3012.7115576176993
    time_step_min: 2872
  date: 2020-10-15_15-15-33
  done: false
  episode_len_mean: 772.5468573973643
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.14201037332396
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 183
  episodes_total: 76338
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4774328295758017e-43
        cur_lr: 5.0e-05
        entropy: 0.07212106697261333
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008127471732829386
        total_loss: .inf
        vf_explained_var: 0.99888014793396
        vf_loss: 0.3361164405941963
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.067741935483873
    gpu_util_percent0: 0.3148387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.867741935483872
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460085577954849
    mean_env_wait_ms: 1.212998456720172
    mean_inference_ms: 4.279375133122219
    mean_raw_obs_processing_ms: 0.3746883365872803
  time_since_restore: 9340.125715970993
  time_this_iter_s: 25.7900550365448
  time_total_s: 9340.125715970993
  timers:
    learn_throughput: 8766.079
    learn_time_ms: 18456.599
    sample_throughput: 23599.422
    sample_time_ms: 6855.761
    update_time_ms: 33.297
  timestamp: 1602774933
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:15:34,964	WARNING util.py:136 -- The `process_trial` operation took 1.0131359100341797 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    365 |          9340.13 | 59054080 |  310.142 |              331.475 |              148.141 |            772.547 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3012.3014448450645
    time_step_min: 2872
  date: 2020-10-15_15-16-00
  done: false
  episode_len_mean: 772.5552944248597
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.2043248782381
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 252
  episodes_total: 76590
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2161492443637024e-43
        cur_lr: 5.0e-05
        entropy: 0.07322903722524643
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007026744528654187
        total_loss: .inf
        vf_explained_var: 0.9992105960845947
        vf_loss: 0.27141276622811955
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.91666666666667
    gpu_util_percent0: 0.299
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460075575093386
    mean_env_wait_ms: 1.212939754546602
    mean_inference_ms: 4.27934121160275
    mean_raw_obs_processing_ms: 0.37468128707567844
  time_since_restore: 9365.801543951035
  time_this_iter_s: 25.675827980041504
  time_total_s: 9365.801543951035
  timers:
    learn_throughput: 8755.558
    learn_time_ms: 18478.776
    sample_throughput: 23636.987
    sample_time_ms: 6844.866
    update_time_ms: 34.116
  timestamp: 1602774960
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:16:02,132	WARNING util.py:136 -- The `process_trial` operation took 1.0419321060180664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    366 |           9365.8 | 59215872 |  310.204 |              331.475 |              148.141 |            772.555 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3012.0324198949743
    time_step_min: 2872
  date: 2020-10-15_15-16-27
  done: false
  episode_len_mean: 772.5607475418376
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.24311684929006
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 195
  episodes_total: 76785
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.324223866545553e-43
        cur_lr: 5.0e-05
        entropy: 0.07623455425103505
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008771123442177972
        total_loss: .inf
        vf_explained_var: 0.9979798793792725
        vf_loss: 0.6398621698220571
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.048387096774192
    gpu_util_percent0: 0.2996774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1460064526000467
    mean_env_wait_ms: 1.2128927899813142
    mean_inference_ms: 4.279305908436263
    mean_raw_obs_processing_ms: 0.37467561454605985
  time_since_restore: 9391.454001903534
  time_this_iter_s: 25.65245795249939
  time_total_s: 9391.454001903534
  timers:
    learn_throughput: 8772.622
    learn_time_ms: 18442.834
    sample_throughput: 23620.598
    sample_time_ms: 6849.615
    update_time_ms: 32.181
  timestamp: 1602774987
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:16:29,168	WARNING util.py:136 -- The `process_trial` operation took 1.0217106342315674 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    367 |          9391.45 | 59377664 |  310.243 |              331.475 |              148.141 |            772.561 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3011.8074593104884
    time_step_min: 2872
  date: 2020-10-15_15-16-54
  done: false
  episode_len_mean: 772.567445365486
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.2753099427087
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 181
  episodes_total: 76966
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.986335799818329e-43
        cur_lr: 5.0e-05
        entropy: 0.08177258943518002
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010630902873041729
        total_loss: .inf
        vf_explained_var: 0.9970040321350098
        vf_loss: 0.9087313463290533
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55806451612903
    gpu_util_percent0: 0.2880645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14600562938070674
    mean_env_wait_ms: 1.2128498550368294
    mean_inference_ms: 4.279274073126871
    mean_raw_obs_processing_ms: 0.3746706640238612
  time_since_restore: 9417.142482757568
  time_this_iter_s: 25.688480854034424
  time_total_s: 9417.142482757568
  timers:
    learn_throughput: 8769.652
    learn_time_ms: 18449.079
    sample_throughput: 23606.448
    sample_time_ms: 6853.721
    update_time_ms: 30.816
  timestamp: 1602775014
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:16:56,430	WARNING util.py:136 -- The `process_trial` operation took 1.0410048961639404 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    368 |          9417.14 | 59539456 |  310.275 |              331.475 |              148.141 |            772.567 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3011.4587652913124
    time_step_min: 2872
  date: 2020-10-15_15-17-22
  done: false
  episode_len_mean: 772.5732159046755
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.32877800148896
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 244
  episodes_total: 77210
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.479503699727495e-43
        cur_lr: 5.0e-05
        entropy: 0.08188663423061371
        entropy_coeff: 0.0005000000000000001
        kl: 0.004593557018476228
        model: {}
        policy_loss: -0.00657463155221194
        total_loss: 0.778793603181839
        vf_explained_var: 0.9978287220001221
        vf_loss: 0.7854091872771581
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.486666666666665
    gpu_util_percent0: 0.36200000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14600491290346038
    mean_env_wait_ms: 1.2127933986665373
    mean_inference_ms: 4.279243050832125
    mean_raw_obs_processing_ms: 0.37466389487167184
  time_since_restore: 9442.79777956009
  time_this_iter_s: 25.655296802520752
  time_total_s: 9442.79777956009
  timers:
    learn_throughput: 8772.389
    learn_time_ms: 18443.323
    sample_throughput: 23699.428
    sample_time_ms: 6826.831
    update_time_ms: 31.568
  timestamp: 1602775042
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: e00ea_00000
  
2020-10-15 15:17:23,648	WARNING util.py:136 -- The `process_trial` operation took 1.1145060062408447 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    369 |           9442.8 | 59701248 |  310.329 |              331.475 |              148.141 |            772.573 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3011.1811978183887
    time_step_min: 2872
  date: 2020-10-15_15-17-49
  done: false
  episode_len_mean: 772.5781750542524
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.372857175663
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 206
  episodes_total: 77416
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.739751849863748e-43
        cur_lr: 5.0e-05
        entropy: 0.07382067727545898
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01036569154045234
        total_loss: .inf
        vf_explained_var: 0.9987668991088867
        vf_loss: 0.40455785393714905
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.374193548387094
    gpu_util_percent0: 0.375483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14600355829855863
    mean_env_wait_ms: 1.2127441405527684
    mean_inference_ms: 4.279206732508417
    mean_raw_obs_processing_ms: 0.37465796777294935
  time_since_restore: 9468.503587722778
  time_this_iter_s: 25.70580816268921
  time_total_s: 9468.503587722778
  timers:
    learn_throughput: 8781.172
    learn_time_ms: 18424.875
    sample_throughput: 23714.587
    sample_time_ms: 6822.468
    update_time_ms: 30.765
  timestamp: 1602775069
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:17:50,788	WARNING util.py:136 -- The `process_trial` operation took 1.0740151405334473 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    370 |           9468.5 | 59863040 |  310.373 |              331.475 |              148.141 |            772.578 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3010.8891553836233
    time_step_min: 2872
  date: 2020-10-15_15-18-16
  done: false
  episode_len_mean: 772.5834622125992
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.41667994513665
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 176
  episodes_total: 77592
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6096277747956215e-43
        cur_lr: 5.0e-05
        entropy: 0.07771969027817249
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009863333883307254
        total_loss: .inf
        vf_explained_var: 0.9989774823188782
        vf_loss: 0.2830318572620551
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.061290322580643
    gpu_util_percent0: 0.3277419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14600289320015614
    mean_env_wait_ms: 1.2127029506629028
    mean_inference_ms: 4.279177610403993
    mean_raw_obs_processing_ms: 0.3746532596308896
  time_since_restore: 9494.264570236206
  time_this_iter_s: 25.760982513427734
  time_total_s: 9494.264570236206
  timers:
    learn_throughput: 8774.548
    learn_time_ms: 18438.784
    sample_throughput: 23722.555
    sample_time_ms: 6820.176
    update_time_ms: 30.143
  timestamp: 1602775096
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:18:17,997	WARNING util.py:136 -- The `process_trial` operation took 1.0865488052368164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    371 |          9494.26 | 60024832 |  310.417 |              331.475 |              148.141 |            772.583 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3010.531475363474
    time_step_min: 2872
  date: 2020-10-15_15-18-43
  done: false
  episode_len_mean: 772.589300168309
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.4716947071477
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 241
  episodes_total: 77833
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.414441662193434e-43
        cur_lr: 5.0e-05
        entropy: 0.08217498225470383
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009649812813828854
        total_loss: .inf
        vf_explained_var: 0.9983739852905273
        vf_loss: 0.5690510645508766
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.425806451612903
    gpu_util_percent0: 0.31870967741935474
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14600201973898388
    mean_env_wait_ms: 1.2126471250700386
    mean_inference_ms: 4.279148096235431
    mean_raw_obs_processing_ms: 0.3746467576018144
  time_since_restore: 9519.957921743393
  time_this_iter_s: 25.69335150718689
  time_total_s: 9519.957921743393
  timers:
    learn_throughput: 8766.934
    learn_time_ms: 18454.799
    sample_throughput: 23786.253
    sample_time_ms: 6801.912
    update_time_ms: 28.524
  timestamp: 1602775123
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:18:45,186	WARNING util.py:136 -- The `process_trial` operation took 1.0986518859863281 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    372 |          9519.96 | 60186624 |  310.472 |              331.475 |              148.141 |            772.589 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3010.2159605153515
    time_step_min: 2872
  date: 2020-10-15_15-19-10
  done: false
  episode_len_mean: 772.5936551052571
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.5155799024495
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 214
  episodes_total: 78047
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2621662493290152e-42
        cur_lr: 5.0e-05
        entropy: 0.0796112200866143
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008454705181065947
        total_loss: .inf
        vf_explained_var: 0.9987473487854004
        vf_loss: 0.39822080234686535
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.945161290322588
    gpu_util_percent0: 0.2770967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14600068260582968
    mean_env_wait_ms: 1.2125963936492627
    mean_inference_ms: 4.279108206045339
    mean_raw_obs_processing_ms: 0.3746404141842206
  time_since_restore: 9545.695207834244
  time_this_iter_s: 25.73728609085083
  time_total_s: 9545.695207834244
  timers:
    learn_throughput: 8763.559
    learn_time_ms: 18461.905
    sample_throughput: 23786.469
    sample_time_ms: 6801.85
    update_time_ms: 28.353
  timestamp: 1602775150
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:19:12,338	WARNING util.py:136 -- The `process_trial` operation took 1.0144157409667969 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    373 |           9545.7 | 60348416 |  310.516 |              331.475 |              148.141 |            772.594 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3009.9596689605773
    time_step_min: 2872
  date: 2020-10-15_15-19-38
  done: false
  episode_len_mean: 772.5993479928408
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.55387743453474
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 78220
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.893249373993522e-42
        cur_lr: 5.0e-05
        entropy: 0.07652155868709087
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00781566339234511
        total_loss: .inf
        vf_explained_var: 0.9986005425453186
        vf_loss: 0.3911686713496844
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.82580645161291
    gpu_util_percent0: 0.32870967741935486
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599997149743663
    mean_env_wait_ms: 1.2125558902221922
    mean_inference_ms: 4.279078800623427
    mean_raw_obs_processing_ms: 0.37463595840732367
  time_since_restore: 9571.393387317657
  time_this_iter_s: 25.698179483413696
  time_total_s: 9571.393387317657
  timers:
    learn_throughput: 8778.071
    learn_time_ms: 18431.384
    sample_throughput: 23786.637
    sample_time_ms: 6801.802
    update_time_ms: 20.789
  timestamp: 1602775178
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:19:39,604	WARNING util.py:136 -- The `process_trial` operation took 1.139448881149292 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    374 |          9571.39 | 60510208 |  310.554 |              331.475 |              148.141 |            772.599 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3009.580765699128
    time_step_min: 2872
  date: 2020-10-15_15-20-05
  done: false
  episode_len_mean: 772.6074132612741
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.61096394902205
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 234
  episodes_total: 78454
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8398740609902833e-42
        cur_lr: 5.0e-05
        entropy: 0.0756140456845363
        entropy_coeff: 0.0005000000000000001
        kl: 0.00401584553765133
        model: {}
        policy_loss: -0.007617724234781538
        total_loss: 0.3387882982691129
        vf_explained_var: 0.9990687966346741
        vf_loss: 0.3464438319206238
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.325806451612902
    gpu_util_percent0: 0.3290322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599920678272507
    mean_env_wait_ms: 1.212502788014008
    mean_inference_ms: 4.279054078659329
    mean_raw_obs_processing_ms: 0.374630151965298
  time_since_restore: 9597.038826704025
  time_this_iter_s: 25.645439386367798
  time_total_s: 9597.038826704025
  timers:
    learn_throughput: 8773.351
    learn_time_ms: 18441.3
    sample_throughput: 23845.698
    sample_time_ms: 6784.955
    update_time_ms: 20.746
  timestamp: 1602775205
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: e00ea_00000
  
2020-10-15 15:20:06,866	WARNING util.py:136 -- The `process_trial` operation took 1.074781894683838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    375 |          9597.04 | 60672000 |  310.611 |              331.475 |              148.141 |            772.607 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3009.2362366945176
    time_step_min: 2872
  date: 2020-10-15_15-20-32
  done: false
  episode_len_mean: 772.6178582777248
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.66324445599963
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 221
  episodes_total: 78675
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4199370304951417e-42
        cur_lr: 5.0e-05
        entropy: 0.07675149478018284
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007087508407191005
        total_loss: .inf
        vf_explained_var: 0.9993978142738342
        vf_loss: 0.19189606234431267
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7741935483871
    gpu_util_percent0: 0.3487096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599787204251483
    mean_env_wait_ms: 1.2124501439230502
    mean_inference_ms: 4.279012929793397
    mean_raw_obs_processing_ms: 0.3746232069340251
  time_since_restore: 9622.875816583633
  time_this_iter_s: 25.836989879608154
  time_total_s: 9622.875816583633
  timers:
    learn_throughput: 8771.029
    learn_time_ms: 18446.183
    sample_throughput: 23838.276
    sample_time_ms: 6787.068
    update_time_ms: 20.269
  timestamp: 1602775232
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:20:34,125	WARNING util.py:136 -- The `process_trial` operation took 1.047729730606079 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    376 |          9622.88 | 60833792 |  310.663 |              331.475 |              148.141 |            772.618 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3008.9721848590207
    time_step_min: 2872
  date: 2020-10-15_15-20-59
  done: false
  episode_len_mean: 772.6249492694806
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.7003883703104
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 78848
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1299055457427125e-42
        cur_lr: 5.0e-05
        entropy: 0.07968741469085217
        entropy_coeff: 0.0005000000000000001
        kl: 0.007965350640006363
        model: {}
        policy_loss: -0.009856254531769082
        total_loss: 0.43709947417179745
        vf_explained_var: 0.9984302520751953
        vf_loss: 0.4469955737392108
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.453333333333333
    gpu_util_percent0: 0.3570000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599718070653353
    mean_env_wait_ms: 1.2124097895174049
    mean_inference_ms: 4.278986414798411
    mean_raw_obs_processing_ms: 0.3746190813888692
  time_since_restore: 9648.507254838943
  time_this_iter_s: 25.63143825531006
  time_total_s: 9648.507254838943
  timers:
    learn_throughput: 8763.884
    learn_time_ms: 18461.221
    sample_throughput: 23870.175
    sample_time_ms: 6777.998
    update_time_ms: 21.309
  timestamp: 1602775259
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: e00ea_00000
  
2020-10-15 15:21:01,311	WARNING util.py:136 -- The `process_trial` operation took 1.0894498825073242 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    377 |          9648.51 | 60995584 |    310.7 |              331.475 |              148.141 |            772.625 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3008.6518839992914
    time_step_min: 2872
  date: 2020-10-15_15-21-26
  done: false
  episode_len_mean: 772.6346679144115
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.7473977214608
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 228
  episodes_total: 79076
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1299055457427125e-42
        cur_lr: 5.0e-05
        entropy: 0.0795695073902607
        entropy_coeff: 0.0005000000000000001
        kl: 0.006255746042976777
        model: {}
        policy_loss: -0.010711871883055816
        total_loss: 0.5138271649678549
        vf_explained_var: 0.9984510540962219
        vf_loss: 0.524578794836998
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354845
    gpu_util_percent0: 0.3258064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599641599539329
    mean_env_wait_ms: 1.2123583751303881
    mean_inference_ms: 4.278955354446918
    mean_raw_obs_processing_ms: 0.3746131248941731
  time_since_restore: 9674.107213973999
  time_this_iter_s: 25.599959135055542
  time_total_s: 9674.107213973999
  timers:
    learn_throughput: 8769.93
    learn_time_ms: 18448.494
    sample_throughput: 23868.476
    sample_time_ms: 6778.481
    update_time_ms: 22.525
  timestamp: 1602775286
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: e00ea_00000
  
2020-10-15 15:21:28,358	WARNING util.py:136 -- The `process_trial` operation took 1.074948787689209 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    378 |          9674.11 | 61157376 |  310.747 |              331.475 |              148.141 |            772.635 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3008.3131631738984
    time_step_min: 2872
  date: 2020-10-15_15-21-54
  done: false
  episode_len_mean: 772.6469023089243
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.7991608242678
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 225
  episodes_total: 79301
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1299055457427125e-42
        cur_lr: 5.0e-05
        entropy: 0.07805696750680606
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007125407680481051
        total_loss: .inf
        vf_explained_var: 0.9993560314178467
        vf_loss: 0.21011166647076607
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.92580645161291
    gpu_util_percent0: 0.38419354838709685
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599510743000507
    mean_env_wait_ms: 1.2123049995267272
    mean_inference_ms: 4.278920591112723
    mean_raw_obs_processing_ms: 0.37460634471862636
  time_since_restore: 9699.78646326065
  time_this_iter_s: 25.67924928665161
  time_total_s: 9699.78646326065
  timers:
    learn_throughput: 8779.557
    learn_time_ms: 18428.265
    sample_throughput: 23824.795
    sample_time_ms: 6790.908
    update_time_ms: 21.949
  timestamp: 1602775314
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:21:55,489	WARNING util.py:136 -- The `process_trial` operation took 1.082265853881836 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    379 |          9699.79 | 61319168 |  310.799 |              331.475 |              148.141 |            772.647 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3008.076756511777
    time_step_min: 2872
  date: 2020-10-15_15-22-21
  done: false
  episode_len_mean: 772.6563950927965
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.83675572967815
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 174
  episodes_total: 79475
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.194858318614069e-42
        cur_lr: 5.0e-05
        entropy: 0.07725438289344311
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006397557639578129
        total_loss: .inf
        vf_explained_var: 0.9988901615142822
        vf_loss: 0.30581305796901387
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612907
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459943593034922
    mean_env_wait_ms: 1.2122648274374377
    mean_inference_ms: 4.278891960157482
    mean_raw_obs_processing_ms: 0.37460195210563757
  time_since_restore: 9725.490199804306
  time_this_iter_s: 25.703736543655396
  time_total_s: 9725.490199804306
  timers:
    learn_throughput: 8783.261
    learn_time_ms: 18420.493
    sample_throughput: 23833.234
    sample_time_ms: 6788.504
    update_time_ms: 21.594
  timestamp: 1602775341
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:22:22,686	WARNING util.py:136 -- The `process_trial` operation took 1.1013431549072266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    380 |          9725.49 | 61480960 |  310.837 |              331.475 |              148.141 |            772.656 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3007.759877465443
    time_step_min: 2872
  date: 2020-10-15_15-22-48
  done: false
  episode_len_mean: 772.6752788827124
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.8839309740018
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 218
  episodes_total: 79693
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.792287477921104e-42
        cur_lr: 5.0e-05
        entropy: 0.07961513847112656
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007949348102556542
        total_loss: .inf
        vf_explained_var: 0.998441219329834
        vf_loss: 0.5318335915605227
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.809677419354834
    gpu_util_percent0: 0.32806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459933430788787
    mean_env_wait_ms: 1.2122149756699623
    mean_inference_ms: 4.278862796522837
    mean_raw_obs_processing_ms: 0.3745966198685488
  time_since_restore: 9751.547744989395
  time_this_iter_s: 26.05754518508911
  time_total_s: 9751.547744989395
  timers:
    learn_throughput: 8784.471
    learn_time_ms: 18417.957
    sample_throughput: 23760.181
    sample_time_ms: 6809.376
    update_time_ms: 22.13
  timestamp: 1602775368
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:22:50,244	WARNING util.py:136 -- The `process_trial` operation took 1.096038818359375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    381 |          9751.55 | 61642752 |  310.884 |              331.475 |              148.141 |            772.675 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3007.4657619989484
    time_step_min: 2872
  date: 2020-10-15_15-23-16
  done: false
  episode_len_mean: 772.6945222961814
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.92880079509894
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 231
  episodes_total: 79924
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.188431216881656e-42
        cur_lr: 5.0e-05
        entropy: 0.08053093403577805
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00898735326093932
        total_loss: .inf
        vf_explained_var: 0.9981188178062439
        vf_loss: 0.6888174712657928
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.525806451612905
    gpu_util_percent0: 0.33258064516129027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599248009395766
    mean_env_wait_ms: 1.21216222226965
    mean_inference_ms: 4.278831378009546
    mean_raw_obs_processing_ms: 0.37459000849546414
  time_since_restore: 9777.36279129982
  time_this_iter_s: 25.815046310424805
  time_total_s: 9777.36279129982
  timers:
    learn_throughput: 8782.786
    learn_time_ms: 18421.49
    sample_throughput: 23735.416
    sample_time_ms: 6816.48
    update_time_ms: 22.355
  timestamp: 1602775396
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:23:17,502	WARNING util.py:136 -- The `process_trial` operation took 1.0683465003967285 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    382 |          9777.36 | 61804544 |  310.929 |              331.475 |              148.141 |            772.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3007.2048210828702
    time_step_min: 2872
  date: 2020-10-15_15-23-43
  done: false
  episode_len_mean: 772.7091639931591
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.9693457979752
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 183
  episodes_total: 80107
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0782646825322483e-41
        cur_lr: 5.0e-05
        entropy: 0.08138230939706166
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005963674986560363
        total_loss: .inf
        vf_explained_var: 0.9987063407897949
        vf_loss: 0.366973248620828
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.483870967741936
    gpu_util_percent0: 0.32161290322580655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14599161838808272
    mean_env_wait_ms: 1.2121192692073686
    mean_inference_ms: 4.27880056393568
    mean_raw_obs_processing_ms: 0.3745852922709888
  time_since_restore: 9803.205022335052
  time_this_iter_s: 25.842231035232544
  time_total_s: 9803.205022335052
  timers:
    learn_throughput: 8776.568
    learn_time_ms: 18434.54
    sample_throughput: 23740.361
    sample_time_ms: 6815.061
    update_time_ms: 24.279
  timestamp: 1602775423
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:23:44,830	WARNING util.py:136 -- The `process_trial` operation took 1.1017663478851318 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    383 |          9803.21 | 61966336 |  310.969 |              331.475 |              148.141 |            772.709 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3006.9879400991704
    time_step_min: 2872
  date: 2020-10-15_15-24-10
  done: false
  episode_len_mean: 772.7242366887483
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.99162265681184
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 201
  episodes_total: 80308
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6173970237983724e-41
        cur_lr: 5.0e-05
        entropy: 0.10520887436966102
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011874154830972353
        total_loss: .inf
        vf_explained_var: 0.9948276877403259
        vf_loss: 1.800004005432129
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.374193548387098
    gpu_util_percent0: 0.3748387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459907469408624
    mean_env_wait_ms: 1.2120730702502347
    mean_inference_ms: 4.278770573757593
    mean_raw_obs_processing_ms: 0.3745801979086903
  time_since_restore: 9828.84748673439
  time_this_iter_s: 25.64246439933777
  time_total_s: 9828.84748673439
  timers:
    learn_throughput: 8777.552
    learn_time_ms: 18432.475
    sample_throughput: 23722.676
    sample_time_ms: 6820.141
    update_time_ms: 24.872
  timestamp: 1602775450
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:24:12,151	WARNING util.py:136 -- The `process_trial` operation took 1.1177036762237549 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    384 |          9828.85 | 62128128 |  310.992 |              331.475 |              148.141 |            772.724 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3006.9737901222297
    time_step_min: 2872
  date: 2020-10-15_15-24-37
  done: false
  episode_len_mean: 772.7455491272068
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 310.9906837851865
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 238
  episodes_total: 80546
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.426095535697559e-41
        cur_lr: 5.0e-05
        entropy: 0.10042844340205193
        entropy_coeff: 0.0005000000000000001
        kl: 0.005169981236880024
        model: {}
        policy_loss: -0.010442737313345182
        total_loss: 2.0259876449902854
        vf_explained_var: 0.9951948523521423
        vf_loss: 2.0364806254704795
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.873333333333335
    gpu_util_percent0: 0.35999999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8800000000000012
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459898534332153
    mean_env_wait_ms: 1.212019776474644
    mean_inference_ms: 4.278741606508684
    mean_raw_obs_processing_ms: 0.37457387399858016
  time_since_restore: 9854.496735095978
  time_this_iter_s: 25.649248361587524
  time_total_s: 9854.496735095978
  timers:
    learn_throughput: 8775.982
    learn_time_ms: 18435.772
    sample_throughput: 23735.958
    sample_time_ms: 6816.325
    update_time_ms: 25.174
  timestamp: 1602775477
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: e00ea_00000
  
2020-10-15 15:24:39,344	WARNING util.py:136 -- The `process_trial` operation took 1.0762748718261719 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    385 |           9854.5 | 62289920 |  310.991 |              331.475 |              148.141 |            772.746 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3006.8461242951857
    time_step_min: 2872
  date: 2020-10-15_15-25-05
  done: false
  episode_len_mean: 772.7681236607751
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.0150562438486
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 191
  episodes_total: 80737
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.426095535697559e-41
        cur_lr: 5.0e-05
        entropy: 0.08418810864289601
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011978026042925194
        total_loss: .inf
        vf_explained_var: 0.9973821640014648
        vf_loss: 0.8563774625460306
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.87741935483871
    gpu_util_percent0: 0.32225806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14598886411673392
    mean_env_wait_ms: 1.2119753346711382
    mean_inference_ms: 4.278711374923728
    mean_raw_obs_processing_ms: 0.3745690147625736
  time_since_restore: 9880.335726261139
  time_this_iter_s: 25.838991165161133
  time_total_s: 9880.335726261139
  timers:
    learn_throughput: 8770.348
    learn_time_ms: 18447.615
    sample_throughput: 23744.954
    sample_time_ms: 6813.742
    update_time_ms: 24.716
  timestamp: 1602775505
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:25:06,783	WARNING util.py:136 -- The `process_trial` operation took 1.1238336563110352 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    386 |          9880.34 | 62451712 |  311.015 |              331.475 |              148.141 |            772.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3006.600484650668
    time_step_min: 2872
  date: 2020-10-15_15-25-32
  done: false
  episode_len_mean: 772.7899042323139
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.0501755272841
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 80925
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.639143303546338e-41
        cur_lr: 5.0e-05
        entropy: 0.07587097771465778
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010512839207270494
        total_loss: .inf
        vf_explained_var: 0.9983758926391602
        vf_loss: 0.5439048086603483
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56451612903226
    gpu_util_percent0: 0.37612903225806454
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459882797381346
    mean_env_wait_ms: 1.2119326152617094
    mean_inference_ms: 4.278686191644183
    mean_raw_obs_processing_ms: 0.3745645613227877
  time_since_restore: 9905.86345744133
  time_this_iter_s: 25.52773118019104
  time_total_s: 9905.86345744133
  timers:
    learn_throughput: 8779.467
    learn_time_ms: 18428.454
    sample_throughput: 23741.151
    sample_time_ms: 6814.834
    update_time_ms: 31.543
  timestamp: 1602775532
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:25:33,870	WARNING util.py:136 -- The `process_trial` operation took 1.1741700172424316 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    387 |          9905.86 | 62613504 |   311.05 |              331.475 |              148.141 |             772.79 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3006.250653320842
    time_step_min: 2872
  date: 2020-10-15_15-25-59
  done: false
  episode_len_mean: 772.8186925559963
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.10394734123895
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 241
  episodes_total: 81166
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.458714955319508e-41
        cur_lr: 5.0e-05
        entropy: 0.07685982994735241
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006988207231188426
        total_loss: .inf
        vf_explained_var: 0.9992009997367859
        vf_loss: 0.2830427773296833
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.25161290322581
    gpu_util_percent0: 0.30999999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14598724194360732
    mean_env_wait_ms: 1.211878877953772
    mean_inference_ms: 4.278655425522025
    mean_raw_obs_processing_ms: 0.37455807429047705
  time_since_restore: 9931.704660892487
  time_this_iter_s: 25.841203451156616
  time_total_s: 9931.704660892487
  timers:
    learn_throughput: 8767.464
    learn_time_ms: 18453.683
    sample_throughput: 23747.901
    sample_time_ms: 6812.897
    update_time_ms: 32.253
  timestamp: 1602775559
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:26:01,295	WARNING util.py:136 -- The `process_trial` operation took 1.0783095359802246 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    388 |           9931.7 | 62775296 |  311.104 |              331.475 |              148.141 |            772.819 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3005.949634803138
    time_step_min: 2872
  date: 2020-10-15_15-26-27
  done: false
  episode_len_mean: 772.8170533870809
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.15001070085395
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 202
  episodes_total: 81368
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.188072432979261e-41
        cur_lr: 5.0e-05
        entropy: 0.07489911839365959
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005783649297275891
        total_loss: .inf
        vf_explained_var: 0.9991471767425537
        vf_loss: 0.249832966675361
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.66774193548387
    gpu_util_percent0: 0.3422580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8709677419354844
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14598620315277003
    mean_env_wait_ms: 1.2118322431917692
    mean_inference_ms: 4.2786242744555265
    mean_raw_obs_processing_ms: 0.3745529785056468
  time_since_restore: 9957.415004491806
  time_this_iter_s: 25.710343599319458
  time_total_s: 9957.415004491806
  timers:
    learn_throughput: 8761.836
    learn_time_ms: 18465.535
    sample_throughput: 23775.718
    sample_time_ms: 6804.926
    update_time_ms: 32.065
  timestamp: 1602775587
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:26:28,521	WARNING util.py:136 -- The `process_trial` operation took 1.138826847076416 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    389 |          9957.42 | 62937088 |   311.15 |              331.475 |              148.141 |            772.817 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3005.692014182831
    time_step_min: 2872
  date: 2020-10-15_15-26-54
  done: false
  episode_len_mean: 772.8188941617923
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.1887194053623
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 181
  episodes_total: 81549
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2282108649468891e-40
        cur_lr: 5.0e-05
        entropy: 0.082604489599665
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007686604988217975
        total_loss: .inf
        vf_explained_var: 0.9990997910499573
        vf_loss: 0.2515786824127038
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.358064516129033
    gpu_util_percent0: 0.32258064516129026
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459856581974786
    mean_env_wait_ms: 1.211790796519123
    mean_inference_ms: 4.2785978774185445
    mean_raw_obs_processing_ms: 0.3745486239797032
  time_since_restore: 9983.12915301323
  time_this_iter_s: 25.71414852142334
  time_total_s: 9983.12915301323
  timers:
    learn_throughput: 8765.43
    learn_time_ms: 18457.966
    sample_throughput: 23745.598
    sample_time_ms: 6813.558
    update_time_ms: 32.043
  timestamp: 1602775614
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:26:55,724	WARNING util.py:136 -- The `process_trial` operation took 1.1070356369018555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    390 |          9983.13 | 63098880 |  311.189 |              331.475 |              148.141 |            772.819 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3005.3538845462217
    time_step_min: 2872
  date: 2020-10-15_15-27-21
  done: false
  episode_len_mean: 772.8245118536722
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.2402008568243
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 240
  episodes_total: 81789
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8423162974203334e-40
        cur_lr: 5.0e-05
        entropy: 0.078291155397892
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006953711655417767
        total_loss: .inf
        vf_explained_var: 0.999267578125
        vf_loss: 0.2502155862748623
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.963333333333335
    gpu_util_percent0: 0.3056666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459846965080623
    mean_env_wait_ms: 1.211738229530198
    mean_inference_ms: 4.278570939439655
    mean_raw_obs_processing_ms: 0.37454238007251517
  time_since_restore: 10008.728836774826
  time_this_iter_s: 25.59968376159668
  time_total_s: 10008.728836774826
  timers:
    learn_throughput: 8771.44
    learn_time_ms: 18445.318
    sample_throughput: 23829.439
    sample_time_ms: 6789.585
    update_time_ms: 31.699
  timestamp: 1602775641
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:27:22,890	WARNING util.py:136 -- The `process_trial` operation took 1.0928866863250732 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    391 |          10008.7 | 63260672 |   311.24 |              331.475 |              148.141 |            772.825 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3005.067403665471
    time_step_min: 2872
  date: 2020-10-15_15-27-48
  done: false
  episode_len_mean: 772.83497975511
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.2852260346774
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 207
  episodes_total: 81996
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7634744461305007e-40
        cur_lr: 5.0e-05
        entropy: 0.07285610648492973
        entropy_coeff: 0.0005000000000000001
        kl: 0.005780763768901427
        model: {}
        policy_loss: -0.006826274378302817
        total_loss: 0.27266524856289226
        vf_explained_var: 0.9990890622138977
        vf_loss: 0.2795279597242673
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.50645161290323
    gpu_util_percent0: 0.32000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14598368214312796
    mean_env_wait_ms: 1.2116905590098157
    mean_inference_ms: 4.278539102596705
    mean_raw_obs_processing_ms: 0.3745371429938952
  time_since_restore: 10034.575895309448
  time_this_iter_s: 25.847058534622192
  time_total_s: 10034.575895309448
  timers:
    learn_throughput: 8764.183
    learn_time_ms: 18460.591
    sample_throughput: 23842.226
    sample_time_ms: 6785.944
    update_time_ms: 31.739
  timestamp: 1602775668
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: e00ea_00000
  
2020-10-15 15:27:50,378	WARNING util.py:136 -- The `process_trial` operation took 1.1549248695373535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    392 |          10034.6 | 63422464 |  311.285 |              331.475 |              148.141 |            772.835 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3004.833668164274
    time_step_min: 2872
  date: 2020-10-15_15-28-16
  done: false
  episode_len_mean: 772.8499020359485
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.3228137679966
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 177
  episodes_total: 82173
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7634744461305007e-40
        cur_lr: 5.0e-05
        entropy: 0.07418189632395904
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005867400316011147
        total_loss: .inf
        vf_explained_var: 0.9985840916633606
        vf_loss: 0.4075961361328761
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.04193548387097
    gpu_util_percent0: 0.32322580645161286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14598305257196614
    mean_env_wait_ms: 1.2116509879834771
    mean_inference_ms: 4.278514311410083
    mean_raw_obs_processing_ms: 0.3745329506518602
  time_since_restore: 10060.348134279251
  time_this_iter_s: 25.772238969802856
  time_total_s: 10060.348134279251
  timers:
    learn_throughput: 8763.759
    learn_time_ms: 18461.484
    sample_throughput: 23836.527
    sample_time_ms: 6787.566
    update_time_ms: 30.053
  timestamp: 1602775696
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:28:17,719	WARNING util.py:136 -- The `process_trial` operation took 1.1281392574310303 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    393 |          10060.3 | 63584256 |  311.323 |              331.475 |              148.141 |             772.85 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3004.524641517223
    time_step_min: 2872
  date: 2020-10-15_15-28-43
  done: false
  episode_len_mean: 772.8711940099269
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.37028489082434
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 230
  episodes_total: 82403
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.145211669195752e-40
        cur_lr: 5.0e-05
        entropy: 0.0752667598426342
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010223721837974153
        total_loss: .inf
        vf_explained_var: 0.9988625645637512
        vf_loss: 0.3978415901462237
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.606451612903232
    gpu_util_percent0: 0.37000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8838709677419367
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14598231298117811
    mean_env_wait_ms: 1.211599974748527
    mean_inference_ms: 4.278490713732214
    mean_raw_obs_processing_ms: 0.3745274527048437
  time_since_restore: 10086.095154762268
  time_this_iter_s: 25.747020483016968
  time_total_s: 10086.095154762268
  timers:
    learn_throughput: 8764.052
    learn_time_ms: 18460.868
    sample_throughput: 23830.233
    sample_time_ms: 6789.359
    update_time_ms: 37.651
  timestamp: 1602775723
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:28:44,996	WARNING util.py:136 -- The `process_trial` operation took 1.1260826587677002 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    394 |          10086.1 | 63746048 |   311.37 |              331.475 |              148.141 |            772.871 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3004.2363376563617
    time_step_min: 2872
  date: 2020-10-15_15-29-10
  done: false
  episode_len_mean: 772.8914587947665
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.41453788456454
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 220
  episodes_total: 82623
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.217817503793626e-40
        cur_lr: 5.0e-05
        entropy: 0.07347226453324159
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00858213851461187
        total_loss: .inf
        vf_explained_var: 0.9990897178649902
        vf_loss: 0.2980883444348971
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096773
    gpu_util_percent0: 0.3329032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14598105644357862
    mean_env_wait_ms: 1.2115493766681273
    mean_inference_ms: 4.278454253219068
    mean_raw_obs_processing_ms: 0.3745213939605089
  time_since_restore: 10111.942166805267
  time_this_iter_s: 25.847012042999268
  time_total_s: 10111.942166805267
  timers:
    learn_throughput: 8761.973
    learn_time_ms: 18465.247
    sample_throughput: 23810.972
    sample_time_ms: 6794.851
    update_time_ms: 37.477
  timestamp: 1602775750
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:29:12,359	WARNING util.py:136 -- The `process_trial` operation took 1.1302680969238281 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    395 |          10111.9 | 63907840 |  311.415 |              331.475 |              148.141 |            772.891 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.999963748414
    time_step_min: 2872
  date: 2020-10-15_15-29-38
  done: false
  episode_len_mean: 772.9060956314843
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.4528245606909
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 174
  episodes_total: 82797
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.326726255690439e-40
        cur_lr: 5.0e-05
        entropy: 0.07264759267369907
        entropy_coeff: 0.0005000000000000001
        kl: 0.005467351603632172
        model: {}
        policy_loss: -0.0051360738919659825
        total_loss: 0.27293384820222855
        vf_explained_var: 0.999126136302948
        vf_loss: 0.2781062548359235
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.787096774193547
    gpu_util_percent0: 0.35870967741935483
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459804594392222
    mean_env_wait_ms: 1.2115107215721372
    mean_inference_ms: 4.278430709473247
    mean_raw_obs_processing_ms: 0.37451751288219054
  time_since_restore: 10137.705976486206
  time_this_iter_s: 25.76380968093872
  time_total_s: 10137.705976486206
  timers:
    learn_throughput: 8768.592
    learn_time_ms: 18451.309
    sample_throughput: 23794.995
    sample_time_ms: 6799.413
    update_time_ms: 38.485
  timestamp: 1602775778
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: e00ea_00000
  
2020-10-15 15:29:39,677	WARNING util.py:136 -- The `process_trial` operation took 1.1667304039001465 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    396 |          10137.7 | 64069632 |  311.453 |              331.475 |              148.141 |            772.906 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.7020535563643
    time_step_min: 2872
  date: 2020-10-15_15-30-05
  done: false
  episode_len_mean: 772.9285232474102
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.4997268517506
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 223
  episodes_total: 83020
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.326726255690439e-40
        cur_lr: 5.0e-05
        entropy: 0.07344088392953078
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008727618590152511
        total_loss: .inf
        vf_explained_var: 0.9990391731262207
        vf_loss: 0.32745811839898425
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.67096774193549
    gpu_util_percent0: 0.33258064516129027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459795272713447
    mean_env_wait_ms: 1.2114607555391925
    mean_inference_ms: 4.2783999096813865
    mean_raw_obs_processing_ms: 0.37451191581033055
  time_since_restore: 10163.346644639969
  time_this_iter_s: 25.640668153762817
  time_total_s: 10163.346644639969
  timers:
    learn_throughput: 8770.379
    learn_time_ms: 18447.548
    sample_throughput: 23749.747
    sample_time_ms: 6812.367
    update_time_ms: 31.465
  timestamp: 1602775805
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:30:06,829	WARNING util.py:136 -- The `process_trial` operation took 1.1210827827453613 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    397 |          10163.3 | 64231424 |    311.5 |              331.475 |              148.141 |            772.929 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.4001514313873
    time_step_min: 2872
  date: 2020-10-15_15-30-32
  done: false
  episode_len_mean: 772.9525153757447
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.54731269061955
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 228
  episodes_total: 83248
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3990089383535658e-39
        cur_lr: 5.0e-05
        entropy: 0.07176382963856061
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008173348905984312
        total_loss: .inf
        vf_explained_var: 0.9991194605827332
        vf_loss: 0.3044714505473773
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725806451612904
    gpu_util_percent0: 0.32290322580645164
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597857387325947
    mean_env_wait_ms: 1.211409687836622
    mean_inference_ms: 4.278374108221419
    mean_raw_obs_processing_ms: 0.3745061455128581
  time_since_restore: 10189.409390211105
  time_this_iter_s: 26.062745571136475
  time_total_s: 10189.409390211105
  timers:
    learn_throughput: 8771.923
    learn_time_ms: 18444.303
    sample_throughput: 23669.321
    sample_time_ms: 6835.515
    update_time_ms: 31.253
  timestamp: 1602775832
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:30:34,564	WARNING util.py:136 -- The `process_trial` operation took 1.1578426361083984 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    398 |          10189.4 | 64393216 |  311.547 |              331.475 |              148.141 |            772.953 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.1567668045814
    time_step_min: 2872
  date: 2020-10-15_15-31-00
  done: false
  episode_len_mean: 772.9695901806369
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.58286776572237
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 83427
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.098513407530349e-39
        cur_lr: 5.0e-05
        entropy: 0.07460578593115012
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006036521289691639
        total_loss: .inf
        vf_explained_var: 0.9991528987884521
        vf_loss: 0.24085521697998047
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5741935483871
    gpu_util_percent0: 0.297741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597791980344904
    mean_env_wait_ms: 1.2113697019479823
    mean_inference_ms: 4.278347224094099
    mean_raw_obs_processing_ms: 0.374501814174152
  time_since_restore: 10215.108580589294
  time_this_iter_s: 25.699190378189087
  time_total_s: 10215.108580589294
  timers:
    learn_throughput: 8767.542
    learn_time_ms: 18453.519
    sample_throughput: 23670.108
    sample_time_ms: 6835.288
    update_time_ms: 31.458
  timestamp: 1602775860
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:31:01,798	WARNING util.py:136 -- The `process_trial` operation took 1.0971882343292236 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    399 |          10215.1 | 64555008 |  311.583 |              331.475 |              148.141 |             772.97 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.8723651154446
    time_step_min: 2872
  date: 2020-10-15_15-31-27
  done: false
  episode_len_mean: 772.9928615840827
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.62703440565997
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 205
  episodes_total: 83632
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.147770111295523e-39
        cur_lr: 5.0e-05
        entropy: 0.07289749632279079
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0057045849825954065
        total_loss: .inf
        vf_explained_var: 0.9992745518684387
        vf_loss: 0.24697931235035261
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77741935483871
    gpu_util_percent0: 0.34483870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459768525731781
    mean_env_wait_ms: 1.2113233995971797
    mean_inference_ms: 4.278317552249578
    mean_raw_obs_processing_ms: 0.37449699127061803
  time_since_restore: 10240.791816949844
  time_this_iter_s: 25.683236360549927
  time_total_s: 10240.791816949844
  timers:
    learn_throughput: 8766.033
    learn_time_ms: 18456.695
    sample_throughput: 23663.138
    sample_time_ms: 6837.301
    update_time_ms: 31.678
  timestamp: 1602775887
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:31:28,981	WARNING util.py:136 -- The `process_trial` operation took 1.104440450668335 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    400 |          10240.8 | 64716800 |  311.627 |              331.475 |              148.141 |            772.993 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.5584558121777
    time_step_min: 2872
  date: 2020-10-15_15-31-54
  done: false
  episode_len_mean: 773.019566928195
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.67725667232037
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 234
  episodes_total: 83866
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.721655166943285e-39
        cur_lr: 5.0e-05
        entropy: 0.07208011051019032
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00806209706934169
        total_loss: .inf
        vf_explained_var: 0.9994446635246277
        vf_loss: 0.20188860098520914
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.532258064516128
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597622399777632
    mean_env_wait_ms: 1.2112720442605125
    mean_inference_ms: 4.27829481899294
    mean_raw_obs_processing_ms: 0.37449128602354503
  time_since_restore: 10266.455579996109
  time_this_iter_s: 25.66376304626465
  time_total_s: 10266.455579996109
  timers:
    learn_throughput: 8771.811
    learn_time_ms: 18444.537
    sample_throughput: 23632.982
    sample_time_ms: 6846.026
    update_time_ms: 31.412
  timestamp: 1602775914
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:31:56,169	WARNING util.py:136 -- The `process_trial` operation took 1.1250369548797607 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    401 |          10266.5 | 64878592 |  311.677 |              331.475 |              148.141 |             773.02 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.317807958292
    time_step_min: 2872
  date: 2020-10-15_15-32-21
  done: false
  episode_len_mean: 773.0403783237166
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.71466974786244
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 189
  episodes_total: 84055
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.082482750414926e-39
        cur_lr: 5.0e-05
        entropy: 0.07276552046338718
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005171004275325686
        total_loss: .inf
        vf_explained_var: 0.9989134669303894
        vf_loss: 0.3254934574166934
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.845161290322583
    gpu_util_percent0: 0.31161290322580654
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597540224220656
    mean_env_wait_ms: 1.2112298194357825
    mean_inference_ms: 4.2782688219074325
    mean_raw_obs_processing_ms: 0.3744868861184313
  time_since_restore: 10291.953663110733
  time_this_iter_s: 25.498083114624023
  time_total_s: 10291.953663110733
  timers:
    learn_throughput: 8787.048
    learn_time_ms: 18412.555
    sample_throughput: 23646.201
    sample_time_ms: 6842.199
    update_time_ms: 31.611
  timestamp: 1602775941
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:32:23,396	WARNING util.py:136 -- The `process_trial` operation took 1.1514637470245361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    402 |            10292 | 65040384 |  311.715 |              331.475 |              148.141 |             773.04 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.0701756469484
    time_step_min: 2872
  date: 2020-10-15_15-32-48
  done: false
  episode_len_mean: 773.062603121847
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.7510303941547
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 190
  episodes_total: 84245
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0623724125622388e-38
        cur_lr: 5.0e-05
        entropy: 0.07780833914875984
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010570286317185188
        total_loss: .inf
        vf_explained_var: 0.998500406742096
        vf_loss: 0.46663814783096313
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.354838709677423
    gpu_util_percent0: 0.315483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459748293163313
    mean_env_wait_ms: 1.2111873450326411
    mean_inference_ms: 4.278244565649025
    mean_raw_obs_processing_ms: 0.37448280622246993
  time_since_restore: 10317.527246713638
  time_this_iter_s: 25.573583602905273
  time_total_s: 10317.527246713638
  timers:
    learn_throughput: 8798.125
    learn_time_ms: 18389.372
    sample_throughput: 23643.517
    sample_time_ms: 6842.975
    update_time_ms: 32.394
  timestamp: 1602775968
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:32:50,656	WARNING util.py:136 -- The `process_trial` operation took 1.112091302871704 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    403 |          10317.5 | 65202176 |  311.751 |              331.475 |              148.141 |            773.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.145374449339
    time_step_min: 2872
  date: 2020-10-15_15-33-16
  done: false
  episode_len_mean: 773.0853632554506
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.72356761278013
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 241
  episodes_total: 84486
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5935586188433586e-38
        cur_lr: 5.0e-05
        entropy: 0.1254341428478559
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014847879928614324
        total_loss: .inf
        vf_explained_var: 0.9901244640350342
        vf_loss: 4.308443705240886
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.409677419354843
    gpu_util_percent0: 0.30838709677419357
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459739207216872
    mean_env_wait_ms: 1.2111354727370631
    mean_inference_ms: 4.278216843636699
    mean_raw_obs_processing_ms: 0.3744765773537369
  time_since_restore: 10343.394164562225
  time_this_iter_s: 25.866917848587036
  time_total_s: 10343.394164562225
  timers:
    learn_throughput: 8792.717
    learn_time_ms: 18400.683
    sample_throughput: 23651.857
    sample_time_ms: 6840.562
    update_time_ms: 25.918
  timestamp: 1602775996
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:33:18,147	WARNING util.py:136 -- The `process_trial` operation took 1.1887969970703125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    404 |          10343.4 | 65363968 |  311.724 |              331.475 |              148.141 |            773.085 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.7060915384795
    time_step_min: 2872
  date: 2020-10-15_15-33-44
  done: false
  episode_len_mean: 773.1039511596051
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.6333549466611
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 198
  episodes_total: 84684
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3903379282650374e-38
        cur_lr: 5.0e-05
        entropy: 0.14411851639548937
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012426964764017612
        total_loss: .inf
        vf_explained_var: 0.9857611656188965
        vf_loss: 6.803361137708028
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.403225806451616
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597293158679306
    mean_env_wait_ms: 1.2110907921015188
    mean_inference_ms: 4.2781878898691135
    mean_raw_obs_processing_ms: 0.3744718850687145
  time_since_restore: 10369.394001960754
  time_this_iter_s: 25.999837398529053
  time_total_s: 10369.394001960754
  timers:
    learn_throughput: 8786.889
    learn_time_ms: 18412.888
    sample_throughput: 23652.103
    sample_time_ms: 6840.491
    update_time_ms: 27.813
  timestamp: 1602776024
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:33:45,694	WARNING util.py:136 -- The `process_trial` operation took 1.1425719261169434 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    405 |          10369.4 | 65525760 |  311.633 |              331.475 |              148.141 |            773.104 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.199797220028
    time_step_min: 2872
  date: 2020-10-15_15-34-11
  done: false
  episode_len_mean: 773.1236227802458
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.5584331585182
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 84863
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5855068923975575e-38
        cur_lr: 5.0e-05
        entropy: 0.14540975540876389
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0120837939515089
        total_loss: .inf
        vf_explained_var: 0.9868630766868591
        vf_loss: 5.798005024592082
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.625806451612906
    gpu_util_percent0: 0.3045161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597237495707896
    mean_env_wait_ms: 1.211052177493206
    mean_inference_ms: 4.2781667281572
    mean_raw_obs_processing_ms: 0.374467871963023
  time_since_restore: 10395.346286773682
  time_this_iter_s: 25.952284812927246
  time_total_s: 10395.346286773682
  timers:
    learn_throughput: 8782.02
    learn_time_ms: 18423.095
    sample_throughput: 23627.904
    sample_time_ms: 6847.497
    update_time_ms: 28.556
  timestamp: 1602776051
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:34:13,320	WARNING util.py:136 -- The `process_trial` operation took 1.1878082752227783 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    406 |          10395.3 | 65687552 |  311.558 |              331.475 |              148.141 |            773.124 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.750176354402
    time_step_min: 2872
  date: 2020-10-15_15-34-39
  done: false
  episode_len_mean: 773.1467602058802
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.47816421281146
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 235
  episodes_total: 85098
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.378260338596336e-38
        cur_lr: 5.0e-05
        entropy: 0.1322192338605722
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012333009518139685
        total_loss: .inf
        vf_explained_var: 0.9887783527374268
        vf_loss: 5.6945187250773115
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.538709677419355
    gpu_util_percent0: 0.3458064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597171012512725
    mean_env_wait_ms: 1.211002326857882
    mean_inference_ms: 4.278141047860216
    mean_raw_obs_processing_ms: 0.3744625641320976
  time_since_restore: 10421.132113695145
  time_this_iter_s: 25.785826921463013
  time_total_s: 10421.132113695145
  timers:
    learn_throughput: 8774.421
    learn_time_ms: 18439.051
    sample_throughput: 23607.848
    sample_time_ms: 6853.314
    update_time_ms: 29.485
  timestamp: 1602776079
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:34:40,732	WARNING util.py:136 -- The `process_trial` operation took 1.1286485195159912 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    407 |          10421.1 | 65849344 |  311.478 |              331.475 |              148.141 |            773.147 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3004.079875688988
    time_step_min: 2872
  date: 2020-10-15_15-35-06
  done: false
  episode_len_mean: 773.1699292010503
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.43359727242427
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 214
  episodes_total: 85312
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.067390507894504e-38
        cur_lr: 5.0e-05
        entropy: 0.11484989213446777
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01139259192859754
        total_loss: .inf
        vf_explained_var: 0.9915947914123535
        vf_loss: 3.6907278299331665
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.245161290322585
    gpu_util_percent0: 0.3129032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597066459442737
    mean_env_wait_ms: 1.2109561890807512
    mean_inference_ms: 4.278111818619066
    mean_raw_obs_processing_ms: 0.3744577032550179
  time_since_restore: 10446.989389181137
  time_this_iter_s: 25.85727548599243
  time_total_s: 10446.989389181137
  timers:
    learn_throughput: 8778.233
    learn_time_ms: 18431.044
    sample_throughput: 23645.448
    sample_time_ms: 6842.416
    update_time_ms: 29.528
  timestamp: 1602776106
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:35:08,227	WARNING util.py:136 -- The `process_trial` operation took 1.1424140930175781 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    408 |            10447 | 66011136 |  311.434 |              331.475 |              148.141 |             773.17 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3004.253993890664
    time_step_min: 2872
  date: 2020-10-15_15-35-33
  done: false
  episode_len_mean: 773.1892261800316
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.41196311243704
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 173
  episodes_total: 85485
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2101085761841754e-37
        cur_lr: 5.0e-05
        entropy: 0.09727588978906472
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010598072457166078
        total_loss: .inf
        vf_explained_var: 0.9927075505256653
        vf_loss: 2.778196096420288
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4258064516129
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14597004308726014
    mean_env_wait_ms: 1.2109191099948358
    mean_inference_ms: 4.278089034658752
    mean_raw_obs_processing_ms: 0.3744538507406009
  time_since_restore: 10472.625900268555
  time_this_iter_s: 25.636511087417603
  time_total_s: 10472.625900268555
  timers:
    learn_throughput: 8788.751
    learn_time_ms: 18408.986
    sample_throughput: 23636.716
    sample_time_ms: 6844.944
    update_time_ms: 29.517
  timestamp: 1602776133
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:35:35,395	WARNING util.py:136 -- The `process_trial` operation took 1.1471128463745117 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    409 |          10472.6 | 66172928 |  311.412 |              331.475 |              148.141 |            773.189 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3004.091061902483
    time_step_min: 2872
  date: 2020-10-15_15-36-00
  done: false
  episode_len_mean: 773.2130698059714
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.44699147019804
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 224
  episodes_total: 85709
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8151628642762634e-37
        cur_lr: 5.0e-05
        entropy: 0.06931128228704135
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005962236667983234
        total_loss: .inf
        vf_explained_var: 0.9983294010162354
        vf_loss: 0.6151190400123596
    num_steps_sampled: 66334720
    num_steps_trained: 66334720
  iterations_since_restore: 410
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.651612903225807
    gpu_util_percent0: 0.3106451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14596930385942414
    mean_env_wait_ms: 1.2108716691367654
    mean_inference_ms: 4.278063457842771
    mean_raw_obs_processing_ms: 0.3744486224718349
  time_since_restore: 10498.218297243118
  time_this_iter_s: 25.5923969745636
  time_total_s: 10498.218297243118
  timers:
    learn_throughput: 8786.921
    learn_time_ms: 18412.821
    sample_throughput: 23685.72
    sample_time_ms: 6830.783
    update_time_ms: 29.063
  timestamp: 1602776160
  timesteps_since_restore: 0
  timesteps_total: 66334720
  training_iteration: 410
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:36:02,691	WARNING util.py:136 -- The `process_trial` operation took 1.1200048923492432 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    410 |          10498.2 | 66334720 |  311.447 |              331.475 |              148.141 |            773.213 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.7889608121636
    time_step_min: 2872
  date: 2020-10-15_15-36-28
  done: false
  episode_len_mean: 773.2384914354868
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.4939901246688
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 227
  episodes_total: 85936
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.722744296414395e-37
        cur_lr: 5.0e-05
        entropy: 0.06558544561266899
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007099645382065016
        total_loss: .inf
        vf_explained_var: 0.9992725849151611
        vf_loss: 0.24277698372801146
    num_steps_sampled: 66496512
    num_steps_trained: 66496512
  iterations_since_restore: 411
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93225806451613
    gpu_util_percent0: 0.2806451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459683537496464
    mean_env_wait_ms: 1.2108233337007173
    mean_inference_ms: 4.278037747254011
    mean_raw_obs_processing_ms: 0.3744437725951838
  time_since_restore: 10523.836809873581
  time_this_iter_s: 25.618512630462646
  time_total_s: 10523.836809873581
  timers:
    learn_throughput: 8785.827
    learn_time_ms: 18415.114
    sample_throughput: 23717.332
    sample_time_ms: 6821.678
    update_time_ms: 29.09
  timestamp: 1602776188
  timesteps_since_restore: 0
  timesteps_total: 66496512
  training_iteration: 411
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:36:29,924	WARNING util.py:136 -- The `process_trial` operation took 1.1055195331573486 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    411 |          10523.8 | 66496512 |  311.494 |              331.475 |              148.141 |            773.238 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.539583139697
    time_step_min: 2872
  date: 2020-10-15_15-36-55
  done: false
  episode_len_mean: 773.2587035789767
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.5298101436129
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 178
  episodes_total: 86114
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.084116444621592e-37
        cur_lr: 5.0e-05
        entropy: 0.06519884616136551
        entropy_coeff: 0.0005000000000000001
        kl: 0.005893115730335315
        model: {}
        policy_loss: -0.006157647837729503
        total_loss: 0.3292495508988698
        vf_explained_var: 0.9988250136375427
        vf_loss: 0.3354398036996524
    num_steps_sampled: 66658304
    num_steps_trained: 66658304
  iterations_since_restore: 412
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.719354838709677
    gpu_util_percent0: 0.2587096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459676883251534
    mean_env_wait_ms: 1.2107850625213894
    mean_inference_ms: 4.278011241704005
    mean_raw_obs_processing_ms: 0.3744395491647266
  time_since_restore: 10549.581852912903
  time_this_iter_s: 25.7450430393219
  time_total_s: 10549.581852912903
  timers:
    learn_throughput: 8779.94
    learn_time_ms: 18427.46
    sample_throughput: 23712.533
    sample_time_ms: 6823.059
    update_time_ms: 28.639
  timestamp: 1602776215
  timesteps_since_restore: 0
  timesteps_total: 66658304
  training_iteration: 412
  trial_id: e00ea_00000
  
2020-10-15 15:36:57,358	WARNING util.py:136 -- The `process_trial` operation took 1.1646559238433838 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    412 |          10549.6 | 66658304 |   311.53 |              331.475 |              148.141 |            773.259 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3003.2326155486535
    time_step_min: 2872
  date: 2020-10-15_15-37-22
  done: false
  episode_len_mean: 773.2821282116628
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.5735712428599
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 212
  episodes_total: 86326
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.084116444621592e-37
        cur_lr: 5.0e-05
        entropy: 0.06580214637021224
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006228243776907523
        total_loss: .inf
        vf_explained_var: 0.9992146492004395
        vf_loss: 0.27260231350859004
    num_steps_sampled: 66820096
    num_steps_trained: 66820096
  iterations_since_restore: 413
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.161290322580648
    gpu_util_percent0: 0.34032258064516124
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8741935483870975
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14596683165764018
    mean_env_wait_ms: 1.2107393336934493
    mean_inference_ms: 4.277980196680192
    mean_raw_obs_processing_ms: 0.37443452776496533
  time_since_restore: 10575.198588848114
  time_this_iter_s: 25.61673593521118
  time_total_s: 10575.198588848114
  timers:
    learn_throughput: 8775.846
    learn_time_ms: 18436.057
    sample_throughput: 23728.597
    sample_time_ms: 6818.439
    update_time_ms: 29.099
  timestamp: 1602776242
  timesteps_since_restore: 0
  timesteps_total: 66820096
  training_iteration: 413
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:37:24,696	WARNING util.py:136 -- The `process_trial` operation took 1.136986494064331 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    413 |          10575.2 | 66820096 |  311.574 |              331.475 |              148.141 |            773.282 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.921768078417
    time_step_min: 2872
  date: 2020-10-15_15-37-50
  done: false
  episode_len_mean: 773.3095755251057
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.62164298436466
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 228
  episodes_total: 86554
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.126174666932388e-37
        cur_lr: 5.0e-05
        entropy: 0.06409938633441925
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006191426021056638
        total_loss: .inf
        vf_explained_var: 0.9995617866516113
        vf_loss: 0.15562466283639273
    num_steps_sampled: 66981888
    num_steps_trained: 66981888
  iterations_since_restore: 414
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.951612903225815
    gpu_util_percent0: 0.2829032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459660853554813
    mean_env_wait_ms: 1.2106922623462044
    mean_inference_ms: 4.277963943859723
    mean_raw_obs_processing_ms: 0.3744299600444454
  time_since_restore: 10601.165946245193
  time_this_iter_s: 25.967357397079468
  time_total_s: 10601.165946245193
  timers:
    learn_throughput: 8780.459
    learn_time_ms: 18426.371
    sample_throughput: 23662.121
    sample_time_ms: 6837.595
    update_time_ms: 27.674
  timestamp: 1602776270
  timesteps_since_restore: 0
  timesteps_total: 66981888
  training_iteration: 414
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:37:52,243	WARNING util.py:136 -- The `process_trial` operation took 1.1324820518493652 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    414 |          10601.2 | 66981888 |  311.622 |              331.475 |              148.141 |             773.31 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.645144175317
    time_step_min: 2872
  date: 2020-10-15_15-38-18
  done: false
  episode_len_mean: 773.3330220654354
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.6625055982808
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 188
  episodes_total: 86742
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.189262000398582e-37
        cur_lr: 5.0e-05
        entropy: 0.06470040945957105
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.004491995533195829
        total_loss: .inf
        vf_explained_var: 0.9996092319488525
        vf_loss: 0.11197162419557571
    num_steps_sampled: 67143680
    num_steps_trained: 67143680
  iterations_since_restore: 415
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.848387096774196
    gpu_util_percent0: 0.3716129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459652650050943
    mean_env_wait_ms: 1.2106512556629885
    mean_inference_ms: 4.277935688800422
    mean_raw_obs_processing_ms: 0.3744256568219183
  time_since_restore: 10627.21323800087
  time_this_iter_s: 26.04729175567627
  time_total_s: 10627.21323800087
  timers:
    learn_throughput: 8777.335
    learn_time_ms: 18432.93
    sample_throughput: 23636.673
    sample_time_ms: 6844.957
    update_time_ms: 25.793
  timestamp: 1602776298
  timesteps_since_restore: 0
  timesteps_total: 67143680
  training_iteration: 415
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:38:19,966	WARNING util.py:136 -- The `process_trial` operation took 1.1327459812164307 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    415 |          10627.2 | 67143680 |  311.663 |              331.475 |              148.141 |            773.333 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.37552363854
    time_step_min: 2872
  date: 2020-10-15_15-38-46
  done: false
  episode_len_mean: 773.3568684289231
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.7016977700257
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 192
  episodes_total: 86934
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3783893000597875e-36
        cur_lr: 5.0e-05
        entropy: 0.06921708770096302
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009718143170175608
        total_loss: .inf
        vf_explained_var: 0.9992571473121643
        vf_loss: 0.2301995853583018
    num_steps_sampled: 67305472
    num_steps_trained: 67305472
  iterations_since_restore: 416
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.478125
    gpu_util_percent0: 0.31499999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459647053849118
    mean_env_wait_ms: 1.2106107069650371
    mean_inference_ms: 4.277913046867878
    mean_raw_obs_processing_ms: 0.3744216768610889
  time_since_restore: 10653.251528024673
  time_this_iter_s: 26.03829002380371
  time_total_s: 10653.251528024673
  timers:
    learn_throughput: 8772.289
    learn_time_ms: 18443.532
    sample_throughput: 23680.497
    sample_time_ms: 6832.289
    update_time_ms: 24.049
  timestamp: 1602776326
  timesteps_since_restore: 0
  timesteps_total: 67305472
  training_iteration: 416
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:38:47,676	WARNING util.py:136 -- The `process_trial` operation took 1.157231092453003 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    416 |          10653.3 | 67305472 |  311.702 |              331.475 |              148.141 |            773.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3002.056972994066
    time_step_min: 2872
  date: 2020-10-15_15-39-13
  done: false
  episode_len_mean: 773.3877665737458
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.7467676732915
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 237
  episodes_total: 87171
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.067583950089681e-36
        cur_lr: 5.0e-05
        entropy: 0.07139313779771328
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009120446169011606
        total_loss: .inf
        vf_explained_var: 0.9990284442901611
        vf_loss: 0.34849725663661957
    num_steps_sampled: 67467264
    num_steps_trained: 67467264
  iterations_since_restore: 417
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76451612903226
    gpu_util_percent0: 0.29612903225806453
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14596402664203348
    mean_env_wait_ms: 1.2105616253664322
    mean_inference_ms: 4.277892551653641
    mean_raw_obs_processing_ms: 0.3744165548211295
  time_since_restore: 10679.12007188797
  time_this_iter_s: 25.86854386329651
  time_total_s: 10679.12007188797
  timers:
    learn_throughput: 8764.833
    learn_time_ms: 18459.223
    sample_throughput: 23740.544
    sample_time_ms: 6815.008
    update_time_ms: 22.369
  timestamp: 1602776353
  timesteps_since_restore: 0
  timesteps_total: 67467264
  training_iteration: 417
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:39:15,141	WARNING util.py:136 -- The `process_trial` operation took 1.1738331317901611 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    417 |          10679.1 | 67467264 |  311.747 |              331.475 |              148.141 |            773.388 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3001.8138054232318
    time_step_min: 2872
  date: 2020-10-15_15-39-40
  done: false
  episode_len_mean: 773.4105642669108
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.78441621202313
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 199
  episodes_total: 87370
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.101375925134522e-36
        cur_lr: 5.0e-05
        entropy: 0.07257813960313797
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007024682068731636
        total_loss: .inf
        vf_explained_var: 0.9990182518959045
        vf_loss: 0.3083597558240096
    num_steps_sampled: 67629056
    num_steps_trained: 67629056
  iterations_since_restore: 418
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.738709677419358
    gpu_util_percent0: 0.27032258064516124
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14596301149474725
    mean_env_wait_ms: 1.2105184379094671
    mean_inference_ms: 4.277863250162951
    mean_raw_obs_processing_ms: 0.3744119919010305
  time_since_restore: 10704.901100635529
  time_this_iter_s: 25.781028747558594
  time_total_s: 10704.901100635529
  timers:
    learn_throughput: 8771.858
    learn_time_ms: 18444.439
    sample_throughput: 23768.011
    sample_time_ms: 6807.133
    update_time_ms: 21.67
  timestamp: 1602776380
  timesteps_since_restore: 0
  timesteps_total: 67629056
  training_iteration: 418
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:39:42,532	WARNING util.py:136 -- The `process_trial` operation took 1.1474435329437256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    418 |          10704.9 | 67629056 |  311.784 |              331.475 |              148.141 |            773.411 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3001.572514198864
    time_step_min: 2872
  date: 2020-10-15_15-40-08
  done: false
  episode_len_mean: 773.4327405224503
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.8214018331554
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 179
  episodes_total: 87549
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6520638877017834e-36
        cur_lr: 5.0e-05
        entropy: 0.0702230626096328
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005754525111115072
        total_loss: .inf
        vf_explained_var: 0.9995884299278259
        vf_loss: 0.12156858046849568
    num_steps_sampled: 67790848
    num_steps_trained: 67790848
  iterations_since_restore: 419
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.87741935483871
    gpu_util_percent0: 0.2635483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14596258965866912
    mean_env_wait_ms: 1.2104804734405104
    mean_inference_ms: 4.277843894597034
    mean_raw_obs_processing_ms: 0.3744082395474031
  time_since_restore: 10730.668075323105
  time_this_iter_s: 25.766974687576294
  time_total_s: 10730.668075323105
  timers:
    learn_throughput: 8761.313
    learn_time_ms: 18466.638
    sample_throughput: 23807.044
    sample_time_ms: 6795.972
    update_time_ms: 21.67
  timestamp: 1602776408
  timesteps_since_restore: 0
  timesteps_total: 67790848
  training_iteration: 419
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:40:09,888	WARNING util.py:136 -- The `process_trial` operation took 1.1673543453216553 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    419 |          10730.7 | 67790848 |  311.821 |              331.475 |              148.141 |            773.433 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3001.252219702064
    time_step_min: 2872
  date: 2020-10-15_15-40-35
  done: false
  episode_len_mean: 773.4638125291926
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.8676280802075
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 230
  episodes_total: 87779
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.978095831552675e-36
        cur_lr: 5.0e-05
        entropy: 0.06797066579262416
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007121906213190717
        total_loss: .inf
        vf_explained_var: 0.9992842674255371
        vf_loss: 0.2512927899758021
    num_steps_sampled: 67952640
    num_steps_trained: 67952640
  iterations_since_restore: 420
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116129032258062
    gpu_util_percent0: 0.3087096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459618618271931
    mean_env_wait_ms: 1.2104328691984994
    mean_inference_ms: 4.2778221019789235
    mean_raw_obs_processing_ms: 0.37440348293082315
  time_since_restore: 10756.38090467453
  time_this_iter_s: 25.71282935142517
  time_total_s: 10756.38090467453
  timers:
    learn_throughput: 8763.159
    learn_time_ms: 18462.748
    sample_throughput: 23796.543
    sample_time_ms: 6798.971
    update_time_ms: 23.799
  timestamp: 1602776435
  timesteps_since_restore: 0
  timesteps_total: 67952640
  training_iteration: 420
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:40:37,249	WARNING util.py:136 -- The `process_trial` operation took 1.226457118988037 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 29.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    420 |          10756.4 | 67952640 |  311.868 |              331.475 |              148.141 |            773.464 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3000.98651567865
    time_step_min: 2872
  date: 2020-10-15_15-41-03
  done: false
  episode_len_mean: 773.4951475067048
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.9091732131076
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 217
  episodes_total: 87996
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0467143747329013e-35
        cur_lr: 5.0e-05
        entropy: 0.0669532660394907
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005848550203760776
        total_loss: .inf
        vf_explained_var: 0.9991934895515442
        vf_loss: 0.26779238507151604
    num_steps_sampled: 68114432
    num_steps_trained: 68114432
  iterations_since_restore: 421
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.019354838709678
    gpu_util_percent0: 0.3280645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14596091912470263
    mean_env_wait_ms: 1.2103867272436424
    mean_inference_ms: 4.2777945009776
    mean_raw_obs_processing_ms: 0.374398836675562
  time_since_restore: 10782.348524332047
  time_this_iter_s: 25.96761965751648
  time_total_s: 10782.348524332047
  timers:
    learn_throughput: 8753.401
    learn_time_ms: 18483.329
    sample_throughput: 23756.37
    sample_time_ms: 6810.468
    update_time_ms: 26.439
  timestamp: 1602776463
  timesteps_since_restore: 0
  timesteps_total: 68114432
  training_iteration: 421
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:41:04,845	WARNING util.py:136 -- The `process_trial` operation took 1.2167034149169922 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | RUNNING  | 172.17.0.4:75217 |    421 |          10782.3 | 68114432 |  311.909 |              331.475 |              148.141 |            773.495 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_e00ea_00000:
  custom_metrics:
    time_step_max: 3980
    time_step_mean: 3000.765500181554
    time_step_min: 2872
  date: 2020-10-15_15-41-30
  done: true
  episode_len_mean: 773.5206873086083
  episode_reward_max: 331.47474747474774
  episode_reward_mean: 311.94230326401146
  episode_reward_min: 148.1414141414143
  episodes_this_iter: 174
  episodes_total: 88170
  experiment_id: 5ae7fba616f541bc83377a1f99edabde
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5700715620993518e-35
        cur_lr: 5.0e-05
        entropy: 0.06588379355768363
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0058466908837241744
        total_loss: .inf
        vf_explained_var: 0.9994247555732727
        vf_loss: 0.16811162730058035
    num_steps_sampled: 68276224
    num_steps_trained: 68276224
  iterations_since_restore: 422
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290327
    gpu_util_percent0: 0.32741935483870965
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.880645161290323
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 75217
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1459602838077196
    mean_env_wait_ms: 1.2103495008501146
    mean_inference_ms: 4.2777729304624685
    mean_raw_obs_processing_ms: 0.37439512480103343
  time_since_restore: 10808.186980009079
  time_this_iter_s: 25.83845567703247
  time_total_s: 10808.186980009079
  timers:
    learn_throughput: 8748.39
    learn_time_ms: 18493.917
    sample_throughput: 23733.097
    sample_time_ms: 6817.146
    update_time_ms: 28.298
  timestamp: 1602776490
  timesteps_since_restore: 0
  timesteps_total: 68276224
  training_iteration: 422
  trial_id: e00ea_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 15:41:32,520	WARNING util.py:136 -- The `process_trial` operation took 1.327937364578247 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | TERMINATED |       |    422 |          10808.2 | 68276224 |  311.942 |              331.475 |              148.141 |            773.521 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/555.47 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_e00ea_00000 | TERMINATED |       |    422 |          10808.2 | 68276224 |  311.942 |              331.475 |              148.141 |            773.521 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


