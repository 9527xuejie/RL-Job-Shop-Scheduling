2020-10-14 00:09:32,009	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_898e0_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=73342)[0m 2020-10-14 00:09:34,753	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=73338)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73338)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73319)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73319)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73310)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73310)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73343)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73343)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73297)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73297)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73341)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73341)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73287)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73287)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73315)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73315)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73302)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73302)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73353)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73353)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73362)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73362)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73304)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73304)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73299)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73299)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73348)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73348)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73303)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73303)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73301)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73301)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73329)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73329)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73300)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73300)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73285)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73285)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73238)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73238)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73241)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73241)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73231)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73231)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73247)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73247)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73229)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73229)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73243)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73243)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73330)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73330)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73236)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73236)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73227)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73227)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73318)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73318)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73282)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73282)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73312)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73312)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73334)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73334)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73339)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73339)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73326)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73326)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73245)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73245)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73252)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73252)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73311)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73311)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73324)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73324)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73224)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73224)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73260)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73260)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73333)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73333)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73284)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73284)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73337)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73337)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73288)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73288)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73314)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73314)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73280)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73280)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73321)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73321)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73239)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73239)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73358)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73358)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73234)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73234)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73278)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73278)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73296)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73296)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73336)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73336)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73307)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73307)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73257)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73257)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73226)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73226)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73240)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73240)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73309)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73309)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73233)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73233)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73306)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73306)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73223)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73223)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73291)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73291)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73225)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73225)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73246)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73246)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73232)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73232)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73305)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73305)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73235)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73235)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73249)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73249)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73283)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73283)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73322)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73322)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73242)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73242)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73298)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73298)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73228)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73228)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73250)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73250)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73332)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73332)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73248)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73248)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73313)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73313)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73254)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73254)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=73244)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=73244)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 3941
    time_step_mean: 3510.563025210084
    time_step_min: 3241
  date: 2020-10-14_00-10-08
  done: false
  episode_len_mean: 895.2911392405064
  episode_reward_max: 255.28282828282772
  episode_reward_mean: 213.63438179260947
  episode_reward_min: 147.70707070707059
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1733678877353668
        entropy_coeff: 0.0005000000000000001
        kl: 0.004998006935541828
        model: {}
        policy_loss: -0.00916472086282738
        total_loss: 415.6246159871419
        vf_explained_var: 0.5660186409950256
        vf_loss: 415.63336181640625
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.38787878787879
    gpu_util_percent0: 0.2903030303030303
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.53030303030303
    vram_util_percent0: 0.08721935656996334
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1682380277015943
    mean_env_wait_ms: 1.1711913518384491
    mean_inference_ms: 5.633361818761728
    mean_raw_obs_processing_ms: 0.44631907659461745
  time_since_restore: 28.234293937683105
  time_this_iter_s: 28.234293937683105
  time_total_s: 28.234293937683105
  timers:
    learn_throughput: 8382.182
    learn_time_ms: 19301.896
    sample_throughput: 18250.049
    sample_time_ms: 8865.291
    update_time_ms: 28.714
  timestamp: 1602634208
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      1 |          28.2343 | 161792 |  213.634 |              255.283 |              147.707 |            895.291 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3516.126353790614
    time_step_min: 3207
  date: 2020-10-14_00-10-35
  done: false
  episode_len_mean: 894.6867088607595
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 212.48123641478057
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1481354931990306
        entropy_coeff: 0.0005000000000000001
        kl: 0.007063919988771279
        model: {}
        policy_loss: -0.010035032425851872
        total_loss: 99.03858502705891
        vf_explained_var: 0.832775890827179
        vf_loss: 99.04848734537761
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 26.077419354838714
    gpu_util_percent0: 0.30774193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7516129032258054
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16465914028179143
    mean_env_wait_ms: 1.1687936774992504
    mean_inference_ms: 5.443685004502766
    mean_raw_obs_processing_ms: 0.4374144803502042
  time_since_restore: 55.151509046554565
  time_this_iter_s: 26.91721510887146
  time_total_s: 55.151509046554565
  timers:
    learn_throughput: 8411.838
    learn_time_ms: 19233.845
    sample_throughput: 19571.372
    sample_time_ms: 8266.768
    update_time_ms: 34.034
  timestamp: 1602634235
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      2 |          55.1515 | 323584 |  212.481 |              258.919 |              138.768 |            894.687 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3508.733333333333
    time_step_min: 3207
  date: 2020-10-14_00-11-01
  done: false
  episode_len_mean: 886.3544303797469
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 212.8691343817925
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1320596237977345
        entropy_coeff: 0.0005000000000000001
        kl: 0.009627530816942453
        model: {}
        policy_loss: -0.011608108102033535
        total_loss: 40.9521697362264
        vf_explained_var: 0.9179801344871521
        vf_loss: 40.96338144938151
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.65666666666667
    gpu_util_percent0: 0.3383333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1619910196181938
    mean_env_wait_ms: 1.1698870060863082
    mean_inference_ms: 5.282046775931652
    mean_raw_obs_processing_ms: 0.42969059168374646
  time_since_restore: 81.55456328392029
  time_this_iter_s: 26.403054237365723
  time_total_s: 81.55456328392029
  timers:
    learn_throughput: 8423.483
    learn_time_ms: 19207.256
    sample_throughput: 20471.942
    sample_time_ms: 7903.109
    update_time_ms: 29.944
  timestamp: 1602634261
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      3 |          81.5546 | 485376 |  212.869 |              258.919 |              138.768 |            886.354 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3507.1973018549747
    time_step_min: 3207
  date: 2020-10-14_00-11-28
  done: false
  episode_len_mean: 878.8560126582279
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 213.623593530239
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1127084891001384
        entropy_coeff: 0.0005000000000000001
        kl: 0.007927578369465968
        model: {}
        policy_loss: -0.01219729493216922
        total_loss: 31.16670513153076
        vf_explained_var: 0.9360077381134033
        vf_loss: 31.17866579691569
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.945161290322584
    gpu_util_percent0: 0.32161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16003182424934137
    mean_env_wait_ms: 1.1718421839694948
    mean_inference_ms: 5.159217010615094
    mean_raw_obs_processing_ms: 0.42346443814212664
  time_since_restore: 107.81225109100342
  time_this_iter_s: 26.25768780708313
  time_total_s: 107.81225109100342
  timers:
    learn_throughput: 8426.621
    learn_time_ms: 19200.104
    sample_throughput: 21068.054
    sample_time_ms: 7679.494
    update_time_ms: 27.272
  timestamp: 1602634288
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      4 |          107.812 | 647168 |  213.624 |              258.919 |              138.768 |            878.856 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3500.901464713715
    time_step_min: 3207
  date: 2020-10-14_00-11-54
  done: false
  episode_len_mean: 871.9417721518987
  episode_reward_max: 258.9191919191918
  episode_reward_mean: 214.66871244086423
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0736438035964966
        entropy_coeff: 0.0005000000000000001
        kl: 0.008050509185219804
        model: {}
        policy_loss: -0.011381465864057342
        total_loss: 25.45706097284953
        vf_explained_var: 0.9563339352607727
        vf_loss: 25.468174775441486
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.47333333333334
    gpu_util_percent0: 0.34933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.158565330565356
    mean_env_wait_ms: 1.1746841483383954
    mean_inference_ms: 5.06466239841031
    mean_raw_obs_processing_ms: 0.4184873353965
  time_since_restore: 134.03782844543457
  time_this_iter_s: 26.225577354431152
  time_total_s: 134.03782844543457
  timers:
    learn_throughput: 8421.879
    learn_time_ms: 19210.915
    sample_throughput: 21512.153
    sample_time_ms: 7520.958
    update_time_ms: 29.205
  timestamp: 1602634314
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      5 |          134.038 | 808960 |  214.669 |              258.919 |              138.768 |            871.942 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3485.6166822867854
    time_step_min: 3203
  date: 2020-10-14_00-12-20
  done: false
  episode_len_mean: 861.7206148282098
  episode_reward_max: 259.5252525252525
  episode_reward_mean: 216.85017443878195
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 316
  episodes_total: 1106
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0657447179158528
        entropy_coeff: 0.0005000000000000001
        kl: 0.008472384458097318
        model: {}
        policy_loss: -0.011950941659354916
        total_loss: 27.248703002929688
        vf_explained_var: 0.9630305171012878
        vf_loss: 27.260340054829914
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.173333333333336
    gpu_util_percent0: 0.2803333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15654771962091946
    mean_env_wait_ms: 1.1795264670893455
    mean_inference_ms: 4.935660185384854
    mean_raw_obs_processing_ms: 0.4120975599602049
  time_since_restore: 160.1184184551239
  time_this_iter_s: 26.08059000968933
  time_total_s: 160.1184184551239
  timers:
    learn_throughput: 8426.451
    learn_time_ms: 19200.491
    sample_throughput: 21832.373
    sample_time_ms: 7410.647
    update_time_ms: 27.766
  timestamp: 1602634340
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      6 |          160.118 | 970752 |   216.85 |              259.525 |              138.768 |            861.721 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3476.8171428571427
    time_step_min: 3167
  date: 2020-10-14_00-12-46
  done: false
  episode_len_mean: 857.3180379746835
  episode_reward_max: 265.7373737373735
  episode_reward_mean: 218.20381025444308
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 1264
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0459044277668
        entropy_coeff: 0.0005000000000000001
        kl: 0.008025348264103135
        model: {}
        policy_loss: -0.012926413328386843
        total_loss: 15.976893107096354
        vf_explained_var: 0.9690950512886047
        vf_loss: 15.989539941151937
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.903333333333343
    gpu_util_percent0: 0.3143333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15581346605264504
    mean_env_wait_ms: 1.1813488648392123
    mean_inference_ms: 4.888831877452206
    mean_raw_obs_processing_ms: 0.40970965625532896
  time_since_restore: 186.2206621170044
  time_this_iter_s: 26.102243661880493
  time_total_s: 186.2206621170044
  timers:
    learn_throughput: 8430.49
    learn_time_ms: 19191.293
    sample_throughput: 22051.876
    sample_time_ms: 7336.881
    update_time_ms: 26.986
  timestamp: 1602634366
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      7 |          186.221 | 1132544 |  218.204 |              265.737 |              138.768 |            857.318 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3467.940708604483
    time_step_min: 3167
  date: 2020-10-14_00-13-13
  done: false
  episode_len_mean: 854.1075949367089
  episode_reward_max: 265.7373737373735
  episode_reward_mean: 219.32312577249283
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0292503933111827
        entropy_coeff: 0.0005000000000000001
        kl: 0.007071365291873614
        model: {}
        policy_loss: -0.011622646396669248
        total_loss: 13.951191822687784
        vf_explained_var: 0.9717023372650146
        vf_loss: 13.962621847788492
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.07096774193549
    gpu_util_percent0: 0.2896774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15517532907758275
    mean_env_wait_ms: 1.1828528649356405
    mean_inference_ms: 4.848029022598573
    mean_raw_obs_processing_ms: 0.40759496309124166
  time_since_restore: 212.54032683372498
  time_this_iter_s: 26.31966471672058
  time_total_s: 212.54032683372498
  timers:
    learn_throughput: 8425.475
    learn_time_ms: 19202.716
    sample_throughput: 22194.107
    sample_time_ms: 7289.863
    update_time_ms: 26.054
  timestamp: 1602634393
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      8 |           212.54 | 1294336 |  219.323 |              265.737 |              138.768 |            854.108 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3458.6658014276445
    time_step_min: 3148
  date: 2020-10-14_00-13-39
  done: false
  episode_len_mean: 850.2822784810127
  episode_reward_max: 267.8585858585858
  episode_reward_mean: 220.54769211098318
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9820772459109625
        entropy_coeff: 0.0005000000000000001
        kl: 0.007025937355744342
        model: {}
        policy_loss: -0.011879481782671064
        total_loss: 16.12793469429016
        vf_explained_var: 0.9705111384391785
        vf_loss: 16.139602581659954
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.300000000000004
    gpu_util_percent0: 0.3103333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1546152798291009
    mean_env_wait_ms: 1.184391779535016
    mean_inference_ms: 4.812484734354211
    mean_raw_obs_processing_ms: 0.4056701581953657
  time_since_restore: 239.07756400108337
  time_this_iter_s: 26.5372371673584
  time_total_s: 239.07756400108337
  timers:
    learn_throughput: 8415.361
    learn_time_ms: 19225.794
    sample_throughput: 22282.521
    sample_time_ms: 7260.938
    update_time_ms: 27.551
  timestamp: 1602634419
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |      9 |          239.078 | 1456128 |  220.548 |              267.859 |              138.768 |            850.282 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3443.0635433494886
    time_step_min: 3108
  date: 2020-10-14_00-14-06
  done: false
  episode_len_mean: 842.329641350211
  episode_reward_max: 273.9191919191924
  episode_reward_mean: 223.18886651323356
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 316
  episodes_total: 1896
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9527567028999329
        entropy_coeff: 0.0005000000000000001
        kl: 0.0066283421668534475
        model: {}
        policy_loss: -0.010255232860799879
        total_loss: 16.848692893981934
        vf_explained_var: 0.9762053489685059
        vf_loss: 16.85876162846883
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.416129032258066
    gpu_util_percent0: 0.3580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.761290322580645
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15370842881496086
    mean_env_wait_ms: 1.1874040241062431
    mean_inference_ms: 4.755009747285587
    mean_raw_obs_processing_ms: 0.40263601770783386
  time_since_restore: 265.63531160354614
  time_this_iter_s: 26.55774760246277
  time_total_s: 265.63531160354614
  timers:
    learn_throughput: 8406.166
    learn_time_ms: 19246.823
    sample_throughput: 22349.571
    sample_time_ms: 7239.154
    update_time_ms: 27.148
  timestamp: 1602634446
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     10 |          265.635 | 1617920 |  223.189 |              273.919 |              138.768 |             842.33 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3436.9563275434243
    time_step_min: 3108
  date: 2020-10-14_00-14-32
  done: false
  episode_len_mean: 838.5628042843233
  episode_reward_max: 273.9191919191924
  episode_reward_mean: 224.2034856844983
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 2054
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9340264697869619
        entropy_coeff: 0.0005000000000000001
        kl: 0.006637935526669025
        model: {}
        policy_loss: -0.010802200093166903
        total_loss: 11.707856257756552
        vf_explained_var: 0.9762506484985352
        vf_loss: 11.718461910883585
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.99
    gpu_util_percent0: 0.31733333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333325
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15333302291012055
    mean_env_wait_ms: 1.1887382867195782
    mean_inference_ms: 4.731399370114767
    mean_raw_obs_processing_ms: 0.40136621572987113
  time_since_restore: 291.918662071228
  time_this_iter_s: 26.283350467681885
  time_total_s: 291.918662071228
  timers:
    learn_throughput: 8405.283
    learn_time_ms: 19248.845
    sample_throughput: 22977.727
    sample_time_ms: 7041.254
    update_time_ms: 26.207
  timestamp: 1602634472
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     11 |          291.919 | 1779712 |  224.203 |              273.919 |              138.768 |            838.563 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3429.441785549931
    time_step_min: 3108
  date: 2020-10-14_00-14-59
  done: false
  episode_len_mean: 835.3019891500904
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 225.35105119915244
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9185261279344559
        entropy_coeff: 0.0005000000000000001
        kl: 0.006282192383271952
        model: {}
        policy_loss: -0.010594809878966771
        total_loss: 11.78148102760315
        vf_explained_var: 0.9733021855354309
        vf_loss: 11.791906754175821
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.14193548387097
    gpu_util_percent0: 0.36354838709677423
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15299449557973674
    mean_env_wait_ms: 1.1900620391901382
    mean_inference_ms: 4.710093686966072
    mean_raw_obs_processing_ms: 0.4001912704907082
  time_since_restore: 318.28980898857117
  time_this_iter_s: 26.37114691734314
  time_total_s: 318.28980898857117
  timers:
    learn_throughput: 8397.762
    learn_time_ms: 19266.085
    sample_throughput: 23212.858
    sample_time_ms: 6969.93
    update_time_ms: 24.126
  timestamp: 1602634499
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     12 |           318.29 | 1941504 |  225.351 |              283.919 |              138.768 |            835.302 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3416.010212418301
    time_step_min: 3107
  date: 2020-10-14_00-15-25
  done: false
  episode_len_mean: 830.3470044229996
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 227.07201081989982
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 275
  episodes_total: 2487
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8739954978227615
        entropy_coeff: 0.0005000000000000001
        kl: 0.006836044291655223
        model: {}
        policy_loss: -0.011842325504403561
        total_loss: 15.203980127970377
        vf_explained_var: 0.9774308800697327
        vf_loss: 15.215575774510702
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.780000000000005
    gpu_util_percent0: 0.3423333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15248185244778864
    mean_env_wait_ms: 1.1924174232352691
    mean_inference_ms: 4.677867862142955
    mean_raw_obs_processing_ms: 0.39840492181528003
  time_since_restore: 344.2357199192047
  time_this_iter_s: 25.945910930633545
  time_total_s: 344.2357199192047
  timers:
    learn_throughput: 8406.302
    learn_time_ms: 19246.514
    sample_throughput: 23303.852
    sample_time_ms: 6942.715
    update_time_ms: 24.826
  timestamp: 1602634525
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     13 |          344.236 | 2103296 |  227.072 |              283.919 |              138.768 |            830.347 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3407.6898375519454
    time_step_min: 3107
  date: 2020-10-14_00-15-52
  done: false
  episode_len_mean: 827.4717051377513
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 228.25514264010167
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 2686
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8569933970769247
        entropy_coeff: 0.0005000000000000001
        kl: 0.006308569146009783
        model: {}
        policy_loss: -0.010416376687013932
        total_loss: 10.241653362909952
        vf_explained_var: 0.979211151599884
        vf_loss: 10.251867532730103
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.577419354838714
    gpu_util_percent0: 0.34903225806451615
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.152160647445905
    mean_env_wait_ms: 1.1938828978940408
    mean_inference_ms: 4.658069002786138
    mean_raw_obs_processing_ms: 0.3973275973628745
  time_since_restore: 370.95642137527466
  time_this_iter_s: 26.720701456069946
  time_total_s: 370.95642137527466
  timers:
    learn_throughput: 8385.719
    learn_time_ms: 19293.754
    sample_throughput: 23344.616
    sample_time_ms: 6930.592
    update_time_ms: 26.687
  timestamp: 1602634552
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     14 |          370.956 | 2265088 |  228.255 |              283.919 |              138.768 |            827.472 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3403.4976827094474
    time_step_min: 3107
  date: 2020-10-14_00-16-18
  done: false
  episode_len_mean: 825.5502812939521
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 228.9589353450113
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.849844848116239
        entropy_coeff: 0.0005000000000000001
        kl: 0.006415587888720135
        model: {}
        policy_loss: -0.01132429470696176
        total_loss: 10.057613849639893
        vf_explained_var: 0.9778110980987549
        vf_loss: 10.068721373875936
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.68
    gpu_util_percent0: 0.26566666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15192830040885705
    mean_env_wait_ms: 1.1949743396978507
    mean_inference_ms: 4.643654271652619
    mean_raw_obs_processing_ms: 0.396529186256578
  time_since_restore: 397.3089544773102
  time_this_iter_s: 26.352533102035522
  time_total_s: 397.3089544773102
  timers:
    learn_throughput: 8380.411
    learn_time_ms: 19305.975
    sample_throughput: 23345.186
    sample_time_ms: 6930.422
    update_time_ms: 26.54
  timestamp: 1602634578
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     15 |          397.309 | 2426880 |  228.959 |              283.919 |              138.768 |             825.55 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3397.2335669002337
    time_step_min: 3107
  date: 2020-10-14_00-16-45
  done: false
  episode_len_mean: 823.1696310935441
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 229.98873118537148
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 192
  episodes_total: 3036
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8184456924597422
        entropy_coeff: 0.0005000000000000001
        kl: 0.006053957312057416
        model: {}
        policy_loss: -0.00969617662485689
        total_loss: 11.97330617904663
        vf_explained_var: 0.977895200252533
        vf_loss: 11.982805728912354
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.03548387096774
    gpu_util_percent0: 0.31322580645161285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15167261063057133
    mean_env_wait_ms: 1.1962682537930385
    mean_inference_ms: 4.627388333081353
    mean_raw_obs_processing_ms: 0.39560673264032387
  time_since_restore: 423.8524944782257
  time_this_iter_s: 26.543540000915527
  time_total_s: 423.8524944782257
  timers:
    learn_throughput: 8372.552
    learn_time_ms: 19324.097
    sample_throughput: 23258.703
    sample_time_ms: 6956.192
    update_time_ms: 28.147
  timestamp: 1602634605
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     16 |          423.852 | 2588672 |  229.989 |              283.919 |              138.768 |             823.17 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3386.8481244281793
    time_step_min: 3103
  date: 2020-10-14_00-17-11
  done: false
  episode_len_mean: 820.1711874623267
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 231.59162145871008
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 282
  episodes_total: 3318
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7818726052840551
        entropy_coeff: 0.0005000000000000001
        kl: 0.006080446105139951
        model: {}
        policy_loss: -0.010272806757711805
        total_loss: 11.227606932322184
        vf_explained_var: 0.981134831905365
        vf_loss: 11.237662474314371
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.016129032258064
    gpu_util_percent0: 0.35000000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.758064516129031
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15134004193676986
    mean_env_wait_ms: 1.1980461945528702
    mean_inference_ms: 4.606672952368037
    mean_raw_obs_processing_ms: 0.3944832550904024
  time_since_restore: 450.4567265510559
  time_this_iter_s: 26.6042320728302
  time_total_s: 450.4567265510559
  timers:
    learn_throughput: 8366.126
    learn_time_ms: 19338.939
    sample_throughput: 23170.071
    sample_time_ms: 6982.801
    update_time_ms: 35.443
  timestamp: 1602634631
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     17 |          450.457 | 2750464 |  231.592 |              283.919 |              138.768 |            820.171 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3381.3046261274367
    time_step_min: 3094
  date: 2020-10-14_00-17-38
  done: false
  episode_len_mean: 818.4976985040277
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 232.3974584742709
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 3476
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7945885012547175
        entropy_coeff: 0.0005000000000000001
        kl: 0.006646261123629908
        model: {}
        policy_loss: -0.008898436247060696
        total_loss: 9.387492338816324
        vf_explained_var: 0.978131115436554
        vf_loss: 9.396123170852661
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.75333333333333
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15117166435286183
    mean_env_wait_ms: 1.198934689834253
    mean_inference_ms: 4.59610474527776
    mean_raw_obs_processing_ms: 0.3939008168927884
  time_since_restore: 477.08925771713257
  time_this_iter_s: 26.63253116607666
  time_total_s: 477.08925771713257
  timers:
    learn_throughput: 8356.774
    learn_time_ms: 19360.581
    sample_throughput: 23145.522
    sample_time_ms: 6990.207
    update_time_ms: 35.513
  timestamp: 1602634658
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     18 |          477.089 | 2912256 |  232.397 |              283.919 |              138.768 |            818.498 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3375.6286111111112
    time_step_min: 3094
  date: 2020-10-14_00-18-05
  done: false
  episode_len_mean: 816.8419895575707
  episode_reward_max: 283.9191919191916
  episode_reward_mean: 233.2244039737857
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 163
  episodes_total: 3639
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7640985002120336
        entropy_coeff: 0.0005000000000000001
        kl: 0.005924703553318977
        model: {}
        policy_loss: -0.01154794641964448
        total_loss: 9.984641949335733
        vf_explained_var: 0.9783595204353333
        vf_loss: 9.995979229609171
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.17096774193548
    gpu_util_percent0: 0.2912903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15100945732810184
    mean_env_wait_ms: 1.1998537819665152
    mean_inference_ms: 4.585871080598127
    mean_raw_obs_processing_ms: 0.3933342937605061
  time_since_restore: 503.67106771469116
  time_this_iter_s: 26.581809997558594
  time_total_s: 503.67106771469116
  timers:
    learn_throughput: 8353.428
    learn_time_ms: 19368.337
    sample_throughput: 23151.949
    sample_time_ms: 6988.267
    update_time_ms: 33.553
  timestamp: 1602634685
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     19 |          503.671 | 3074048 |  233.224 |              283.919 |              138.768 |            816.842 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3366.6026100307063
    time_step_min: 3068
  date: 2020-10-14_00-18-32
  done: false
  episode_len_mean: 814.0339498353179
  episode_reward_max: 286.1919191919192
  episode_reward_mean: 234.70736245147194
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 308
  episodes_total: 3947
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7212709039449692
        entropy_coeff: 0.0005000000000000001
        kl: 0.005585941020399332
        model: {}
        policy_loss: -0.009148939342897696
        total_loss: 11.774536848068237
        vf_explained_var: 0.9819743633270264
        vf_loss: 11.783488035202026
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.958064516129035
    gpu_util_percent0: 0.3445161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1507251118658706
    mean_env_wait_ms: 1.201453028878967
    mean_inference_ms: 4.568332710197702
    mean_raw_obs_processing_ms: 0.392370221113348
  time_since_restore: 530.109183549881
  time_this_iter_s: 26.43811583518982
  time_total_s: 530.109183549881
  timers:
    learn_throughput: 8358.581
    learn_time_ms: 19356.396
    sample_throughput: 23154.869
    sample_time_ms: 6987.386
    update_time_ms: 33.174
  timestamp: 1602634712
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     20 |          530.109 | 3235840 |  234.707 |              286.192 |              138.768 |            814.034 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3361.2777095109363
    time_step_min: 3068
  date: 2020-10-14_00-18-58
  done: false
  episode_len_mean: 812.6032132424538
  episode_reward_max: 286.1919191919192
  episode_reward_mean: 235.5290219625663
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 161
  episodes_total: 4108
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7334958960612615
        entropy_coeff: 0.0005000000000000001
        kl: 0.00623501290101558
        model: {}
        policy_loss: -0.012696098769083619
        total_loss: 8.210275928179422
        vf_explained_var: 0.9805936217308044
        vf_loss: 8.222715377807617
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.556666666666665
    gpu_util_percent0: 0.35033333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15059211374815384
    mean_env_wait_ms: 1.2022162721877458
    mean_inference_ms: 4.560037288539838
    mean_raw_obs_processing_ms: 0.391920968122286
  time_since_restore: 556.4568021297455
  time_this_iter_s: 26.347618579864502
  time_total_s: 556.4568021297455
  timers:
    learn_throughput: 8361.81
    learn_time_ms: 19348.922
    sample_throughput: 23113.392
    sample_time_ms: 6999.925
    update_time_ms: 34.057
  timestamp: 1602634738
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     21 |          556.457 | 3397632 |  235.529 |              286.192 |              138.768 |            812.603 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3355.935888336882
    time_step_min: 2992
  date: 2020-10-14_00-19-24
  done: false
  episode_len_mean: 811.4566338490389
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 236.30717157510412
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 4266
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7313190599282583
        entropy_coeff: 0.0005000000000000001
        kl: 0.0059486522028843565
        model: {}
        policy_loss: -0.010567928196905996
        total_loss: 8.188967108726501
        vf_explained_var: 0.9800074100494385
        vf_loss: 8.199306011199951
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.732258064516127
    gpu_util_percent0: 0.41516129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1504688215396204
    mean_env_wait_ms: 1.2029208448431266
    mean_inference_ms: 4.552335534794724
    mean_raw_obs_processing_ms: 0.39149529429438223
  time_since_restore: 582.7479071617126
  time_this_iter_s: 26.291105031967163
  time_total_s: 582.7479071617126
  timers:
    learn_throughput: 8369.209
    learn_time_ms: 19331.816
    sample_throughput: 23085.39
    sample_time_ms: 7008.415
    update_time_ms: 34.466
  timestamp: 1602634764
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     22 |          582.748 | 3559424 |  236.307 |              291.495 |              138.768 |            811.457 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3346.508729281768
    time_step_min: 2992
  date: 2020-10-14_00-19-51
  done: false
  episode_len_mean: 809.3422436459247
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 237.67161536486694
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 298
  episodes_total: 4564
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6868849446376165
        entropy_coeff: 0.0005000000000000001
        kl: 0.005426900926977396
        model: {}
        policy_loss: -0.012831605854444206
        total_loss: 10.865561644236246
        vf_explained_var: 0.9829086661338806
        vf_loss: 10.878193855285645
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.69333333333333
    gpu_util_percent0: 0.31599999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15025379806001818
    mean_env_wait_ms: 1.2041828026580925
    mean_inference_ms: 4.538838741827157
    mean_raw_obs_processing_ms: 0.39075940093750106
  time_since_restore: 609.1476483345032
  time_this_iter_s: 26.399741172790527
  time_total_s: 609.1476483345032
  timers:
    learn_throughput: 8359.028
    learn_time_ms: 19355.36
    sample_throughput: 23012.102
    sample_time_ms: 7030.735
    update_time_ms: 33.347
  timestamp: 1602634791
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     23 |          609.148 | 3721216 |  237.672 |              291.495 |              138.768 |            809.342 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3341.208040842374
    time_step_min: 2992
  date: 2020-10-14_00-20-17
  done: false
  episode_len_mean: 808.3808016877637
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 238.4455632272088
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 176
  episodes_total: 4740
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6830945213635763
        entropy_coeff: 0.0005000000000000001
        kl: 0.005740340022991101
        model: {}
        policy_loss: -0.010633966104554323
        total_loss: 7.890702724456787
        vf_explained_var: 0.9825429320335388
        vf_loss: 7.901104132334392
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.02903225806452
    gpu_util_percent0: 0.3696774193548388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1501368170705065
    mean_env_wait_ms: 1.204873684142575
    mean_inference_ms: 4.531668014270378
    mean_raw_obs_processing_ms: 0.3903787695554212
  time_since_restore: 635.3448567390442
  time_this_iter_s: 26.197208404541016
  time_total_s: 635.3448567390442
  timers:
    learn_throughput: 8379.943
    learn_time_ms: 19307.052
    sample_throughput: 22998.495
    sample_time_ms: 7034.895
    update_time_ms: 32.648
  timestamp: 1602634817
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     24 |          635.345 | 3883008 |  238.446 |              291.495 |              138.768 |            808.381 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3336.7478905124512
    time_step_min: 2992
  date: 2020-10-14_00-20-44
  done: false
  episode_len_mean: 807.5679869334422
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 239.13053359235477
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 4898
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6929802199204763
        entropy_coeff: 0.0005000000000000001
        kl: 0.005470787757076323
        model: {}
        policy_loss: -0.010624278977047652
        total_loss: 7.717013875643413
        vf_explained_var: 0.9808815121650696
        vf_loss: 7.727437655131022
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.225806451612904
    gpu_util_percent0: 0.2980645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15003701765302058
    mean_env_wait_ms: 1.2054353984334492
    mean_inference_ms: 4.52547967396834
    mean_raw_obs_processing_ms: 0.39004210475312867
  time_since_restore: 662.2304472923279
  time_this_iter_s: 26.88559055328369
  time_total_s: 662.2304472923279
  timers:
    learn_throughput: 8377.308
    learn_time_ms: 19313.126
    sample_throughput: 22844.266
    sample_time_ms: 7082.39
    update_time_ms: 31.049
  timestamp: 1602634844
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     25 |           662.23 | 4044800 |  239.131 |              291.495 |              138.768 |            807.568 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3330.2917155138725
    time_step_min: 2992
  date: 2020-10-14_00-21-11
  done: false
  episode_len_mean: 806.1334108978089
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 240.13724798890595
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 259
  episodes_total: 5157
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6603399912516276
        entropy_coeff: 0.0005000000000000001
        kl: 0.005722672794945538
        model: {}
        policy_loss: -0.010399677489961809
        total_loss: 11.333871920903524
        vf_explained_var: 0.9813134074211121
        vf_loss: 11.344029506047567
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.333333333333336
    gpu_util_percent0: 0.30100000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14988332228494017
    mean_env_wait_ms: 1.2063112229356259
    mean_inference_ms: 4.516036191026727
    mean_raw_obs_processing_ms: 0.38953461131047895
  time_since_restore: 688.4943618774414
  time_this_iter_s: 26.263914585113525
  time_total_s: 688.4943618774414
  timers:
    learn_throughput: 8379.846
    learn_time_ms: 19307.275
    sample_throughput: 22914.129
    sample_time_ms: 7060.796
    update_time_ms: 29.564
  timestamp: 1602634871
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     26 |          688.494 | 4206592 |  240.137 |              291.495 |              138.768 |            806.133 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3324.14869679355
    time_step_min: 2992
  date: 2020-10-14_00-21-38
  done: false
  episode_len_mean: 805.1243484735667
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 241.03947328835642
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 215
  episodes_total: 5372
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6385591675837835
        entropy_coeff: 0.0005000000000000001
        kl: 0.004849217327622076
        model: {}
        policy_loss: -0.010857531859073788
        total_loss: 7.4299672444661455
        vf_explained_var: 0.9844462871551514
        vf_loss: 7.440659125645955
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.816129032258065
    gpu_util_percent0: 0.34451612903225803
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1497659826648457
    mean_env_wait_ms: 1.2069888362365637
    mean_inference_ms: 4.508637916218328
    mean_raw_obs_processing_ms: 0.38913629614041617
  time_since_restore: 715.0843439102173
  time_this_iter_s: 26.58998203277588
  time_total_s: 715.0843439102173
  timers:
    learn_throughput: 8372.831
    learn_time_ms: 19323.452
    sample_throughput: 22955.545
    sample_time_ms: 7048.057
    update_time_ms: 24.342
  timestamp: 1602634898
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     27 |          715.084 | 4368384 |  241.039 |              291.495 |              138.768 |            805.124 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3319.8796211983245
    time_step_min: 2992
  date: 2020-10-14_00-22-04
  done: false
  episode_len_mean: 804.3748643761302
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 241.64884833872176
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 5530
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6570624609788259
        entropy_coeff: 0.0005000000000000001
        kl: 0.006124866000997524
        model: {}
        policy_loss: -0.01042374266156306
        total_loss: 7.550791382789612
        vf_explained_var: 0.980835497379303
        vf_loss: 7.561237573623657
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.030000000000005
    gpu_util_percent0: 0.2896666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14968383415855652
    mean_env_wait_ms: 1.2074406907714177
    mean_inference_ms: 4.503526140678691
    mean_raw_obs_processing_ms: 0.3888582833586606
  time_since_restore: 741.31671667099
  time_this_iter_s: 26.232372760772705
  time_total_s: 741.31671667099
  timers:
    learn_throughput: 8383.003
    learn_time_ms: 19300.006
    sample_throughput: 23006.89
    sample_time_ms: 7032.328
    update_time_ms: 24.313
  timestamp: 1602634924
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     28 |          741.317 | 4530176 |  241.649 |              291.495 |              138.768 |            804.375 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3314.7637285764254
    time_step_min: 2992
  date: 2020-10-14_00-22-30
  done: false
  episode_len_mean: 803.18273406288
  episode_reward_max: 291.4949494949492
  episode_reward_mean: 242.43614887804574
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 227
  episodes_total: 5757
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6203485031922659
        entropy_coeff: 0.0005000000000000001
        kl: 0.005696180664623777
        model: {}
        policy_loss: -0.009003105558804236
        total_loss: 8.377814809481302
        vf_explained_var: 0.9846324920654297
        vf_loss: 8.38684352238973
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.858064516129033
    gpu_util_percent0: 0.3464516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14957196745679685
    mean_env_wait_ms: 1.20806756402897
    mean_inference_ms: 4.496563737863365
    mean_raw_obs_processing_ms: 0.3884795041777297
  time_since_restore: 767.647716999054
  time_this_iter_s: 26.331000328063965
  time_total_s: 767.647716999054
  timers:
    learn_throughput: 8391.426
    learn_time_ms: 19280.632
    sample_throughput: 23034.974
    sample_time_ms: 7023.755
    update_time_ms: 25.987
  timestamp: 1602634950
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     29 |          767.648 | 4691968 |  242.436 |              291.495 |              138.768 |            803.183 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3308.6626990779546
    time_step_min: 2967
  date: 2020-10-14_00-22-57
  done: false
  episode_len_mean: 802.0041638907395
  episode_reward_max: 295.2828282828282
  episode_reward_mean: 243.3372818794205
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 247
  episodes_total: 6004
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5812346041202545
        entropy_coeff: 0.0005000000000000001
        kl: 0.005257382406853139
        model: {}
        policy_loss: -0.010139185052442675
        total_loss: 8.125032424926758
        vf_explained_var: 0.9838623404502869
        vf_loss: 8.135199348131815
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.62666666666667
    gpu_util_percent0: 0.24933333333333338
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.763333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14945871677226966
    mean_env_wait_ms: 1.2087397235952184
    mean_inference_ms: 4.489440659343038
    mean_raw_obs_processing_ms: 0.3881027012495785
  time_since_restore: 794.1269767284393
  time_this_iter_s: 26.479259729385376
  time_total_s: 794.1269767284393
  timers:
    learn_throughput: 8390.191
    learn_time_ms: 19283.471
    sample_throughput: 23034.641
    sample_time_ms: 7023.856
    update_time_ms: 26.303
  timestamp: 1602634977
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     30 |          794.127 | 4853760 |  243.337 |              295.283 |              138.768 |            802.004 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3304.9157275845173
    time_step_min: 2967
  date: 2020-10-14_00-23-24
  done: false
  episode_len_mean: 801.1801363193769
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 243.90335684006575
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 6162
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.6109522531429926
        entropy_coeff: 0.0005000000000000001
        kl: 0.005762041818040113
        model: {}
        policy_loss: -0.010459269299947968
        total_loss: 6.1143150726954145
        vf_explained_var: 0.9838274121284485
        vf_loss: 6.124791661898295
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.56451612903226
    gpu_util_percent0: 0.2958064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14939051113759907
    mean_env_wait_ms: 1.2091350252953788
    mean_inference_ms: 4.48513654410667
    mean_raw_obs_processing_ms: 0.3878721880436939
  time_since_restore: 820.6919379234314
  time_this_iter_s: 26.564961194992065
  time_total_s: 820.6919379234314
  timers:
    learn_throughput: 8378.021
    learn_time_ms: 19311.481
    sample_throughput: 23061.695
    sample_time_ms: 7015.616
    update_time_ms: 27.21
  timestamp: 1602635004
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     31 |          820.692 | 5015552 |  243.903 |              295.889 |              138.768 |             801.18 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3299.7715456121
    time_step_min: 2967
  date: 2020-10-14_00-23-50
  done: false
  episode_len_mean: 800.0822110867523
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 244.67128693765088
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 224
  episodes_total: 6386
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.586687371134758
        entropy_coeff: 0.0005000000000000001
        kl: 0.005857182511438926
        model: {}
        policy_loss: -0.008998592941982983
        total_loss: 9.322317441304525
        vf_explained_var: 0.9821708798408508
        vf_loss: 9.331316312154135
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.789999999999996
    gpu_util_percent0: 0.3423333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14929786061727807
    mean_env_wait_ms: 1.2096856711968071
    mean_inference_ms: 4.479342700203333
    mean_raw_obs_processing_ms: 0.3875682034215755
  time_since_restore: 846.8766281604767
  time_this_iter_s: 26.184690237045288
  time_total_s: 846.8766281604767
  timers:
    learn_throughput: 8378.914
    learn_time_ms: 19309.424
    sample_throughput: 23093.848
    sample_time_ms: 7005.848
    update_time_ms: 26.915
  timestamp: 1602635030
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     32 |          846.877 | 5177344 |  244.671 |              295.889 |              138.768 |            800.082 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3294.336213430347
    time_step_min: 2967
  date: 2020-10-14_00-24-17
  done: false
  episode_len_mean: 798.8270042194093
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 245.54492026960386
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 6636
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5415168702602386
        entropy_coeff: 0.0005000000000000001
        kl: 0.005145472784837087
        model: {}
        policy_loss: -0.009888297997046417
        total_loss: 7.5816722710927325
        vf_explained_var: 0.9850125908851624
        vf_loss: 7.5915742715199785
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.519354838709678
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7612903225806447
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14920187280408093
    mean_env_wait_ms: 1.210282716912676
    mean_inference_ms: 4.473190915007908
    mean_raw_obs_processing_ms: 0.3872373187541771
  time_since_restore: 873.4083490371704
  time_this_iter_s: 26.531720876693726
  time_total_s: 873.4083490371704
  timers:
    learn_throughput: 8369.081
    learn_time_ms: 19332.11
    sample_throughput: 23132.503
    sample_time_ms: 6994.142
    update_time_ms: 27.354
  timestamp: 1602635057
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     33 |          873.408 | 5339136 |  245.545 |              295.889 |              138.768 |            798.827 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3291.0977054034047
    time_step_min: 2967
  date: 2020-10-14_00-24-44
  done: false
  episode_len_mean: 798.0628495731528
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 246.05275897033334
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 6794
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5788290202617645
        entropy_coeff: 0.0005000000000000001
        kl: 0.00525987334549427
        model: {}
        policy_loss: -0.011562899654866973
        total_loss: 7.844460328420003
        vf_explained_var: 0.9792365431785583
        vf_loss: 7.856049656867981
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.041935483870965
    gpu_util_percent0: 0.32161290322580643
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14914400751173207
    mean_env_wait_ms: 1.2106317666910758
    mean_inference_ms: 4.469513720437136
    mean_raw_obs_processing_ms: 0.3870416698399468
  time_since_restore: 899.82905626297
  time_this_iter_s: 26.42070722579956
  time_total_s: 899.82905626297
  timers:
    learn_throughput: 8362.472
    learn_time_ms: 19347.388
    sample_throughput: 23108.338
    sample_time_ms: 7001.455
    update_time_ms: 26.225
  timestamp: 1602635084
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     34 |          899.829 | 5500928 |  246.053 |              295.889 |              138.768 |            798.063 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3286.5707540420663
    time_step_min: 2967
  date: 2020-10-14_00-25-10
  done: false
  episode_len_mean: 796.9588787706317
  episode_reward_max: 295.88888888888897
  episode_reward_mean: 246.76130830214498
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 7028
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5510778526465098
        entropy_coeff: 0.0005000000000000001
        kl: 0.005085315167283018
        model: {}
        policy_loss: -0.011924359165277565
        total_loss: 9.076769431432089
        vf_explained_var: 0.9827437996864319
        vf_loss: 9.088714996973673
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.383870967741938
    gpu_util_percent0: 0.3132258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14906156087983
    mean_env_wait_ms: 1.2111336986355814
    mean_inference_ms: 4.464284335242698
    mean_raw_obs_processing_ms: 0.3867683543526611
  time_since_restore: 926.1179995536804
  time_this_iter_s: 26.28894329071045
  time_total_s: 926.1179995536804
  timers:
    learn_throughput: 8373.896
    learn_time_ms: 19320.995
    sample_throughput: 23247.96
    sample_time_ms: 6959.406
    update_time_ms: 26.174
  timestamp: 1602635110
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     35 |          926.118 | 5662720 |  246.761 |              295.889 |              138.768 |            796.959 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3281.62373772306
    time_step_min: 2960
  date: 2020-10-14_00-25-37
  done: false
  episode_len_mean: 795.8569069895432
  episode_reward_max: 296.3434343434343
  episode_reward_mean: 247.52184614443837
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 240
  episodes_total: 7268
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5111012508471807
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051787659370650845
        model: {}
        policy_loss: -0.00917195157671813
        total_loss: 6.548242966334025
        vf_explained_var: 0.9863994717597961
        vf_loss: 6.557411591211955
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.566666666666663
    gpu_util_percent0: 0.31333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14898226705662138
    mean_env_wait_ms: 1.211653652421519
    mean_inference_ms: 4.459255980380624
    mean_raw_obs_processing_ms: 0.3864983575045051
  time_since_restore: 952.4991655349731
  time_this_iter_s: 26.381165981292725
  time_total_s: 952.4991655349731
  timers:
    learn_throughput: 8366.428
    learn_time_ms: 19338.24
    sample_throughput: 23270.56
    sample_time_ms: 6952.648
    update_time_ms: 27.48
  timestamp: 1602635137
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     36 |          952.499 | 5824512 |  247.522 |              296.343 |              138.768 |            795.857 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3278.666170299174
    time_step_min: 2947
  date: 2020-10-14_00-26-03
  done: false
  episode_len_mean: 795.1316994344196
  episode_reward_max: 298.3131313131316
  episode_reward_mean: 247.9472179375223
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 7426
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.545738473534584
        entropy_coeff: 0.0005000000000000001
        kl: 0.005450511390032868
        model: {}
        policy_loss: -0.012406253100683292
        total_loss: 5.776481469472249
        vf_explained_var: 0.9846447110176086
        vf_loss: 5.788888335227966
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.922580645161286
    gpu_util_percent0: 0.2954838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14893217533024194
    mean_env_wait_ms: 1.2119702523699356
    mean_inference_ms: 4.456080663054638
    mean_raw_obs_processing_ms: 0.3863299699393656
  time_since_restore: 979.1373100280762
  time_this_iter_s: 26.638144493103027
  time_total_s: 979.1373100280762
  timers:
    learn_throughput: 8367.648
    learn_time_ms: 19335.421
    sample_throughput: 23242.372
    sample_time_ms: 6961.079
    update_time_ms: 25.198
  timestamp: 1602635163
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     37 |          979.137 | 5986304 |  247.947 |              298.313 |              138.768 |            795.132 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3273.961251472706
    time_step_min: 2947
  date: 2020-10-14_00-26-30
  done: false
  episode_len_mean: 794.016671008075
  episode_reward_max: 298.61616161616115
  episode_reward_mean: 248.6355992853779
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 252
  episodes_total: 7678
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.519211878379186
        entropy_coeff: 0.0005000000000000001
        kl: 0.00521182909142226
        model: {}
        policy_loss: -0.009023654982835675
        total_loss: 8.441483894983927
        vf_explained_var: 0.9842881560325623
        vf_loss: 8.450506289800009
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.974193548387092
    gpu_util_percent0: 0.3319354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1488541945819557
    mean_env_wait_ms: 1.2124557598457473
    mean_inference_ms: 4.451231417649878
    mean_raw_obs_processing_ms: 0.38607893968639007
  time_since_restore: 1005.5099358558655
  time_this_iter_s: 26.372625827789307
  time_total_s: 1005.5099358558655
  timers:
    learn_throughput: 8366.683
    learn_time_ms: 19337.652
    sample_throughput: 23211.556
    sample_time_ms: 6970.321
    update_time_ms: 26.739
  timestamp: 1602635190
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     38 |          1005.51 | 6148096 |  248.636 |              298.616 |              138.768 |            794.017 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3270.134206843913
    time_step_min: 2947
  date: 2020-10-14_00-26-56
  done: false
  episode_len_mean: 793.15
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 249.21516430124026
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 222
  episodes_total: 7900
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4856214125951131
        entropy_coeff: 0.0005000000000000001
        kl: 0.005182099722636242
        model: {}
        policy_loss: -0.00891703084926121
        total_loss: 6.5547778606414795
        vf_explained_var: 0.9860953688621521
        vf_loss: 6.563678741455078
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.58
    gpu_util_percent0: 0.267
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14879160293397087
    mean_env_wait_ms: 1.2129021401049132
    mean_inference_ms: 4.447160184647643
    mean_raw_obs_processing_ms: 0.385869247416737
  time_since_restore: 1031.7330660820007
  time_this_iter_s: 26.223130226135254
  time_total_s: 1031.7330660820007
  timers:
    learn_throughput: 8369.362
    learn_time_ms: 19331.461
    sample_throughput: 23223.556
    sample_time_ms: 6966.719
    update_time_ms: 25.386
  timestamp: 1602635216
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     39 |          1031.73 | 6309888 |  249.215 |              303.616 |              138.768 |             793.15 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3267.4345928420003
    time_step_min: 2947
  date: 2020-10-14_00-27-23
  done: false
  episode_len_mean: 792.5824025812857
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 249.58562918838425
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 8058
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.526516318321228
        entropy_coeff: 0.0005000000000000001
        kl: 0.005193507065996528
        model: {}
        policy_loss: -0.011457257942917446
        total_loss: 6.553280353546143
        vf_explained_var: 0.9829640984535217
        vf_loss: 6.564741214116414
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.841935483870973
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14874758950249928
    mean_env_wait_ms: 1.2131890205042295
    mean_inference_ms: 4.444375394891122
    mean_raw_obs_processing_ms: 0.3857250723692635
  time_since_restore: 1058.0795605182648
  time_this_iter_s: 26.346494436264038
  time_total_s: 1058.0795605182648
  timers:
    learn_throughput: 8370.901
    learn_time_ms: 19327.908
    sample_throughput: 23263.135
    sample_time_ms: 6954.867
    update_time_ms: 26.971
  timestamp: 1602635243
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     40 |          1058.08 | 6471680 |  249.586 |              303.616 |              138.768 |            792.582 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3263.2761617380806
    time_step_min: 2947
  date: 2020-10-14_00-27-50
  done: false
  episode_len_mean: 791.6511292647765
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 250.2041219499173
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 266
  episodes_total: 8324
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.49336690952380496
        entropy_coeff: 0.0005000000000000001
        kl: 0.005102361280781527
        model: {}
        policy_loss: -0.009011513475949565
        total_loss: 9.669517993927002
        vf_explained_var: 0.9830517768859863
        vf_loss: 9.678521235783895
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.28387096774193
    gpu_util_percent0: 0.3180645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14867458603516318
    mean_env_wait_ms: 1.2136542404114086
    mean_inference_ms: 4.439922463809307
    mean_raw_obs_processing_ms: 0.3854950820460582
  time_since_restore: 1084.477516412735
  time_this_iter_s: 26.397955894470215
  time_total_s: 1084.477516412735
  timers:
    learn_throughput: 8385.964
    learn_time_ms: 19293.19
    sample_throughput: 23205.042
    sample_time_ms: 6972.278
    update_time_ms: 26.558
  timestamp: 1602635270
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     41 |          1084.48 | 6633472 |  250.204 |              303.616 |              138.768 |            791.651 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3259.8172612739904
    time_step_min: 2947
  date: 2020-10-14_00-28-16
  done: false
  episode_len_mean: 790.9975386779184
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 250.71389705777898
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 208
  episodes_total: 8532
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4733814299106598
        entropy_coeff: 0.0005000000000000001
        kl: 0.004729776371580859
        model: {}
        policy_loss: -0.010568847976780186
        total_loss: 6.403569380442302
        vf_explained_var: 0.9858106970787048
        vf_loss: 6.414138555526733
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.62666666666667
    gpu_util_percent0: 0.26066666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14862418712088268
    mean_env_wait_ms: 1.2140255502040485
    mean_inference_ms: 4.436533920685386
    mean_raw_obs_processing_ms: 0.3853260468358044
  time_since_restore: 1110.679015636444
  time_this_iter_s: 26.201499223709106
  time_total_s: 1110.679015636444
  timers:
    learn_throughput: 8389.801
    learn_time_ms: 19284.366
    sample_throughput: 23171.982
    sample_time_ms: 6982.225
    update_time_ms: 26.636
  timestamp: 1602635296
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     42 |          1110.68 | 6795264 |  250.714 |              303.616 |              138.768 |            790.998 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3257.5926482487575
    time_step_min: 2947
  date: 2020-10-14_00-28-43
  done: false
  episode_len_mean: 790.5673187571922
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 251.05876370145648
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 8690
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.5028343424201012
        entropy_coeff: 0.0005000000000000001
        kl: 0.005660295176009337
        model: {}
        policy_loss: -0.011312471678441701
        total_loss: 5.480809609095256
        vf_explained_var: 0.9858322143554688
        vf_loss: 5.492232163747151
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.132258064516126
    gpu_util_percent0: 0.2961290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7677419354838704
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14858549836688748
    mean_env_wait_ms: 1.2142816161949008
    mean_inference_ms: 4.434070947600301
    mean_raw_obs_processing_ms: 0.3851990592401758
  time_since_restore: 1137.0320131778717
  time_this_iter_s: 26.352997541427612
  time_total_s: 1137.0320131778717
  timers:
    learn_throughput: 8396.067
    learn_time_ms: 19269.975
    sample_throughput: 23223.052
    sample_time_ms: 6966.871
    update_time_ms: 28.066
  timestamp: 1602635323
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     43 |          1137.03 | 6957056 |  251.059 |              303.616 |              138.768 |            790.567 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3253.6264303343055
    time_step_min: 2947
  date: 2020-10-14_00-29-09
  done: false
  episode_len_mean: 789.8943370937116
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 251.6527601492418
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 263
  episodes_total: 8953
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.47802816579739255
        entropy_coeff: 0.0005000000000000001
        kl: 0.005564751607986788
        model: {}
        policy_loss: -0.01014368303124987
        total_loss: 7.744199713071187
        vf_explained_var: 0.9861556887626648
        vf_loss: 7.754443327585856
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.9
    gpu_util_percent0: 0.33566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14852315882773698
    mean_env_wait_ms: 1.214688529402648
    mean_inference_ms: 4.430197911905797
    mean_raw_obs_processing_ms: 0.38499981622859797
  time_since_restore: 1163.4936192035675
  time_this_iter_s: 26.4616060256958
  time_total_s: 1163.4936192035675
  timers:
    learn_throughput: 8388.25
    learn_time_ms: 19287.932
    sample_throughput: 23277.855
    sample_time_ms: 6950.468
    update_time_ms: 29.881
  timestamp: 1602635349
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     44 |          1163.49 | 7118848 |  251.653 |              303.616 |              138.768 |            789.894 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3250.506301369863
    time_step_min: 2940
  date: 2020-10-14_00-29-36
  done: false
  episode_len_mean: 789.403644696639
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 252.1213465955937
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 211
  episodes_total: 9164
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4490925893187523
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058276798808947206
        model: {}
        policy_loss: -0.011005985550582409
        total_loss: 5.937054077784221
        vf_explained_var: 0.9868550300598145
        vf_loss: 5.948138991991679
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.661290322580648
    gpu_util_percent0: 0.3467741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14847604065256265
    mean_env_wait_ms: 1.2150224863876178
    mean_inference_ms: 4.427084097030703
    mean_raw_obs_processing_ms: 0.3848439755135012
  time_since_restore: 1189.8426446914673
  time_this_iter_s: 26.34902548789978
  time_total_s: 1189.8426446914673
  timers:
    learn_throughput: 8381.549
    learn_time_ms: 19303.352
    sample_throughput: 23315.394
    sample_time_ms: 6939.278
    update_time_ms: 30.104
  timestamp: 1602635376
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     45 |          1189.84 | 7280640 |  252.121 |              303.616 |              138.768 |            789.404 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3248.310029085425
    time_step_min: 2940
  date: 2020-10-14_00-30-02
  done: false
  episode_len_mean: 788.9847672173354
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 252.43690498635792
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 158
  episodes_total: 9322
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4758431166410446
        entropy_coeff: 0.0005000000000000001
        kl: 0.005838179378770292
        model: {}
        policy_loss: -0.011624956270679832
        total_loss: 5.875628630320231
        vf_explained_var: 0.9845533967018127
        vf_loss: 5.8873454332351685
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.269999999999996
    gpu_util_percent0: 0.39133333333333326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1484414934438303
    mean_env_wait_ms: 1.2152517283920525
    mean_inference_ms: 4.424880449260468
    mean_raw_obs_processing_ms: 0.3847303793398819
  time_since_restore: 1216.0421001911163
  time_this_iter_s: 26.199455499649048
  time_total_s: 1216.0421001911163
  timers:
    learn_throughput: 8384.707
    learn_time_ms: 19296.083
    sample_throughput: 23352.039
    sample_time_ms: 6928.389
    update_time_ms: 28.528
  timestamp: 1602635402
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     46 |          1216.04 | 7442432 |  252.437 |              303.616 |              138.768 |            788.985 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3244.7780452071997
    time_step_min: 2940
  date: 2020-10-14_00-30-29
  done: false
  episode_len_mean: 788.2295987493486
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 252.98352466825636
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 273
  episodes_total: 9595
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4465416222810745
        entropy_coeff: 0.0005000000000000001
        kl: 0.005149371650380393
        model: {}
        policy_loss: -0.009117977499651412
        total_loss: 7.547347227732341
        vf_explained_var: 0.9863469004631042
        vf_loss: 7.556559681892395
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.290000000000003
    gpu_util_percent0: 0.31366666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14838375544083404
    mean_env_wait_ms: 1.2156298040291595
    mean_inference_ms: 4.4212591743222145
    mean_raw_obs_processing_ms: 0.3845407765722786
  time_since_restore: 1242.2972719669342
  time_this_iter_s: 26.25517177581787
  time_total_s: 1242.2972719669342
  timers:
    learn_throughput: 8388.665
    learn_time_ms: 19286.977
    sample_throughput: 23452.759
    sample_time_ms: 6898.634
    update_time_ms: 28.539
  timestamp: 1602635429
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     47 |           1242.3 | 7604224 |  252.984 |              303.616 |              138.768 |             788.23 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3242.339858563083
    time_step_min: 2938
  date: 2020-10-14_00-30-55
  done: false
  episode_len_mean: 787.7311147407105
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 253.35953244160675
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 9796
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42620716243982315
        entropy_coeff: 0.0005000000000000001
        kl: 0.005318921951887508
        model: {}
        policy_loss: -0.01032576325815171
        total_loss: 5.459895412127177
        vf_explained_var: 0.9876511096954346
        vf_loss: 5.470301270484924
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.758064516129032
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14834354358821653
    mean_env_wait_ms: 1.2159258005540605
    mean_inference_ms: 4.418606626685914
    mean_raw_obs_processing_ms: 0.38440822761084503
  time_since_restore: 1268.6809079647064
  time_this_iter_s: 26.383635997772217
  time_total_s: 1268.6809079647064
  timers:
    learn_throughput: 8383.682
    learn_time_ms: 19298.443
    sample_throughput: 23488.275
    sample_time_ms: 6888.203
    update_time_ms: 27.241
  timestamp: 1602635455
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     48 |          1268.68 | 7766016 |   253.36 |              303.616 |              138.768 |            787.731 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3240.424682395644
    time_step_min: 2938
  date: 2020-10-14_00-31-22
  done: false
  episode_len_mean: 787.3573365471527
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 253.64726505793095
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 161
  episodes_total: 9957
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.45852066328128177
        entropy_coeff: 0.0005000000000000001
        kl: 0.005530392401851714
        model: {}
        policy_loss: -0.010260472452500835
        total_loss: 6.348827560742696
        vf_explained_var: 0.9838187098503113
        vf_loss: 6.3591790199279785
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.36451612903226
    gpu_util_percent0: 0.3335483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1483122037731125
    mean_env_wait_ms: 1.2161401616811047
    mean_inference_ms: 4.416585866355386
    mean_raw_obs_processing_ms: 0.38430250759736184
  time_since_restore: 1295.2812414169312
  time_this_iter_s: 26.60033345222473
  time_total_s: 1295.2812414169312
  timers:
    learn_throughput: 8373.697
    learn_time_ms: 19321.454
    sample_throughput: 23476.208
    sample_time_ms: 6891.743
    update_time_ms: 28.626
  timestamp: 1602635482
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     49 |          1295.28 | 7927808 |  253.647 |              303.616 |              138.768 |            787.357 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3237.025402118478
    time_step_min: 2938
  date: 2020-10-14_00-31-49
  done: false
  episode_len_mean: 786.685979482169
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 254.1688699402427
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 278
  episodes_total: 10235
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.42400458206733066
        entropy_coeff: 0.0005000000000000001
        kl: 0.005758004845120013
        model: {}
        policy_loss: -0.009732729863571876
        total_loss: 9.089882930119833
        vf_explained_var: 0.9840230941772461
        vf_loss: 9.09968360265096
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.166666666666668
    gpu_util_percent0: 0.293
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148260151102214
    mean_env_wait_ms: 1.2164929579486823
    mean_inference_ms: 4.413262532718721
    mean_raw_obs_processing_ms: 0.38412812854486783
  time_since_restore: 1321.6603813171387
  time_this_iter_s: 26.37913990020752
  time_total_s: 1321.6603813171387
  timers:
    learn_throughput: 8369.594
    learn_time_ms: 19330.925
    sample_throughput: 23505.816
    sample_time_ms: 6883.062
    update_time_ms: 28.647
  timestamp: 1602635509
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     50 |          1321.66 | 8089600 |  254.169 |              303.616 |              138.768 |            786.686 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3234.5002406391377
    time_step_min: 2938
  date: 2020-10-14_00-32-15
  done: false
  episode_len_mean: 786.243670886076
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 254.5625937162186
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 10428
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4049416979153951
        entropy_coeff: 0.0005000000000000001
        kl: 0.005929892572263877
        model: {}
        policy_loss: -0.011005657501906777
        total_loss: 4.866203625996907
        vf_explained_var: 0.9882550835609436
        vf_loss: 4.87726350625356
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.729032258064517
    gpu_util_percent0: 0.327741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14822391164043905
    mean_env_wait_ms: 1.2167464728968307
    mean_inference_ms: 4.410939943013635
    mean_raw_obs_processing_ms: 0.38401229997225966
  time_since_restore: 1347.943321943283
  time_this_iter_s: 26.28294062614441
  time_total_s: 1347.943321943283
  timers:
    learn_throughput: 8365.669
    learn_time_ms: 19339.996
    sample_throughput: 23603.619
    sample_time_ms: 6854.542
    update_time_ms: 28.74
  timestamp: 1602635535
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     51 |          1347.94 | 8251392 |  254.563 |              303.616 |              138.768 |            786.244 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3232.345588932057
    time_step_min: 2919
  date: 2020-10-14_00-32-42
  done: false
  episode_len_mean: 785.8859516616315
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 254.876103367512
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 164
  episodes_total: 10592
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4286295870939891
        entropy_coeff: 0.0005000000000000001
        kl: 0.005036363222946723
        model: {}
        policy_loss: -0.009228799298095206
        total_loss: 5.845330556233724
        vf_explained_var: 0.9849271178245544
        vf_loss: 5.8546479145685835
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.430000000000003
    gpu_util_percent0: 0.3136666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14819545750189675
    mean_env_wait_ms: 1.2169491663891547
    mean_inference_ms: 4.409086924730802
    mean_raw_obs_processing_ms: 0.38391513746079625
  time_since_restore: 1374.2133586406708
  time_this_iter_s: 26.270036697387695
  time_total_s: 1374.2133586406708
  timers:
    learn_throughput: 8352.708
    learn_time_ms: 19370.006
    sample_throughput: 23690.876
    sample_time_ms: 6829.296
    update_time_ms: 29.232
  timestamp: 1602635562
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     52 |          1374.21 | 8413184 |  254.876 |              303.616 |              138.768 |            785.886 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3228.687338501292
    time_step_min: 2918
  date: 2020-10-14_00-33-08
  done: false
  episode_len_mean: 785.3101609195402
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 255.41159177986768
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 283
  episodes_total: 10875
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.39254015187422436
        entropy_coeff: 0.0005000000000000001
        kl: 0.0054301081302886205
        model: {}
        policy_loss: -0.007928162871394306
        total_loss: 7.315639098485311
        vf_explained_var: 0.9870920777320862
        vf_loss: 7.323627670605977
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.90666666666667
    gpu_util_percent0: 0.2746666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7633333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481470182324399
    mean_env_wait_ms: 1.2172733494662804
    mean_inference_ms: 4.406012172561705
    mean_raw_obs_processing_ms: 0.38375634069887293
  time_since_restore: 1400.4824647903442
  time_this_iter_s: 26.269106149673462
  time_total_s: 1400.4824647903442
  timers:
    learn_throughput: 8354.51
    learn_time_ms: 19365.827
    sample_throughput: 23671.422
    sample_time_ms: 6834.908
    update_time_ms: 28.734
  timestamp: 1602635588
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     53 |          1400.48 | 8574976 |  255.412 |              303.616 |              138.768 |             785.31 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3226.419199709645
    time_step_min: 2918
  date: 2020-10-14_00-33-35
  done: false
  episode_len_mean: 784.951356238698
  episode_reward_max: 303.6161616161616
  episode_reward_mean: 255.77582789924566
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 185
  episodes_total: 11060
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.38370103389024734
        entropy_coeff: 0.0005000000000000001
        kl: 0.005197393669125934
        model: {}
        policy_loss: -0.009420125027342388
        total_loss: 5.069755752881368
        vf_explained_var: 0.9875216484069824
        vf_loss: 5.079237739245097
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45806451612904
    gpu_util_percent0: 0.34032258064516124
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14811575386900358
    mean_env_wait_ms: 1.217482458323202
    mean_inference_ms: 4.403981511008739
    mean_raw_obs_processing_ms: 0.38365236063203834
  time_since_restore: 1426.686885356903
  time_this_iter_s: 26.204420566558838
  time_total_s: 1426.686885356903
  timers:
    learn_throughput: 8369.544
    learn_time_ms: 19331.042
    sample_throughput: 23664.781
    sample_time_ms: 6836.827
    update_time_ms: 27.148
  timestamp: 1602635615
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     54 |          1426.69 | 8736768 |  255.776 |              303.616 |              138.768 |            784.951 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3224.2455528738715
    time_step_min: 2909
  date: 2020-10-14_00-34-01
  done: false
  episode_len_mean: 784.6326385177267
  episode_reward_max: 304.0707070707074
  episode_reward_mean: 256.1064979025963
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 166
  episodes_total: 11226
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4106709385911624
        entropy_coeff: 0.0005000000000000001
        kl: 0.005636657549378772
        model: {}
        policy_loss: -0.01151395154496034
        total_loss: 5.502703309059143
        vf_explained_var: 0.9857354164123535
        vf_loss: 5.514281590779622
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.883333333333333
    gpu_util_percent0: 0.32533333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14808930325567826
    mean_env_wait_ms: 1.2176676245812932
    mean_inference_ms: 4.402273809143858
    mean_raw_obs_processing_ms: 0.3835621739006759
  time_since_restore: 1452.9231250286102
  time_this_iter_s: 26.236239671707153
  time_total_s: 1452.9231250286102
  timers:
    learn_throughput: 8373.588
    learn_time_ms: 19321.704
    sample_throughput: 23650.194
    sample_time_ms: 6841.043
    update_time_ms: 28.479
  timestamp: 1602635641
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     55 |          1452.92 | 8898560 |  256.106 |              304.071 |              138.768 |            784.633 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3220.7535297193654
    time_step_min: 2909
  date: 2020-10-14_00-34-28
  done: false
  episode_len_mean: 784.0581082254843
  episode_reward_max: 306.3434343434339
  episode_reward_mean: 256.6566770808932
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 287
  episodes_total: 11513
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.3653937478860219
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045549396115044756
        model: {}
        policy_loss: -0.005883191052513818
        total_loss: 8.344082752863565
        vf_explained_var: 0.9852698445320129
        vf_loss: 8.350034753481546
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.832258064516136
    gpu_util_percent0: 0.30000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14804438524457988
    mean_env_wait_ms: 1.2179539923005125
    mean_inference_ms: 4.399437949080208
    mean_raw_obs_processing_ms: 0.38341214488405706
  time_since_restore: 1479.4801878929138
  time_this_iter_s: 26.55706286430359
  time_total_s: 1479.4801878929138
  timers:
    learn_throughput: 8376.126
    learn_time_ms: 19315.852
    sample_throughput: 23548.984
    sample_time_ms: 6870.445
    update_time_ms: 31.175
  timestamp: 1602635668
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     56 |          1479.48 | 9060352 |  256.657 |              306.343 |              138.768 |            784.058 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3218.6061100145885
    time_step_min: 2909
  date: 2020-10-14_00-34-55
  done: false
  episode_len_mean: 783.7340916866233
  episode_reward_max: 306.3434343434339
  episode_reward_mean: 256.99917927133123
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 179
  episodes_total: 11692
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3698219656944275
        entropy_coeff: 0.0005000000000000001
        kl: 0.005350918586676319
        model: {}
        policy_loss: -0.009632504275941756
        total_loss: 4.928918838500977
        vf_explained_var: 0.987260639667511
        vf_loss: 4.938669403394063
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.738709677419358
    gpu_util_percent0: 0.2706451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14801688131066731
    mean_env_wait_ms: 1.2181361686553493
    mean_inference_ms: 4.39766236911213
    mean_raw_obs_processing_ms: 0.3833241844100098
  time_since_restore: 1506.1571009159088
  time_this_iter_s: 26.676913022994995
  time_total_s: 1506.1571009159088
  timers:
    learn_throughput: 8375.17
    learn_time_ms: 19318.056
    sample_throughput: 23448.359
    sample_time_ms: 6899.928
    update_time_ms: 39.631
  timestamp: 1602635695
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     57 |          1506.16 | 9222144 |  256.999 |              306.343 |              138.768 |            783.734 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3216.5854690010997
    time_step_min: 2909
  date: 2020-10-14_00-35-22
  done: false
  episode_len_mean: 783.4070982970832
  episode_reward_max: 306.3434343434339
  episode_reward_mean: 257.3064918277362
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 11862
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.39362389345963794
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050841827954476075
        model: {}
        policy_loss: -0.012481133700930513
        total_loss: 5.161785840988159
        vf_explained_var: 0.9871451258659363
        vf_loss: 5.174400289853414
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.803333333333338
    gpu_util_percent0: 0.30966666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14799181659131994
    mean_env_wait_ms: 1.2183052811277095
    mean_inference_ms: 4.3960591107723
    mean_raw_obs_processing_ms: 0.38323902780870434
  time_since_restore: 1532.5258095264435
  time_this_iter_s: 26.368708610534668
  time_total_s: 1532.5258095264435
  timers:
    learn_throughput: 8376.586
    learn_time_ms: 19314.789
    sample_throughput: 23447.161
    sample_time_ms: 6900.281
    update_time_ms: 40.87
  timestamp: 1602635722
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     58 |          1532.53 | 9383936 |  257.306 |              306.343 |              138.768 |            783.407 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3213.302039468252
    time_step_min: 2909
  date: 2020-10-14_00-35-48
  done: false
  episode_len_mean: 782.942304526749
  episode_reward_max: 307.10101010100993
  episode_reward_mean: 257.8080184561666
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 288
  episodes_total: 12150
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3501675600806872
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049746248017375665
        model: {}
        policy_loss: -0.010083653352921829
        total_loss: 6.506309151649475
        vf_explained_var: 0.9882670044898987
        vf_loss: 6.5165055592854815
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.996774193548394
    gpu_util_percent0: 0.27
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479505361167113
    mean_env_wait_ms: 1.2185598490882164
    mean_inference_ms: 4.393465763197843
    mean_raw_obs_processing_ms: 0.3831033991145646
  time_since_restore: 1558.970431804657
  time_this_iter_s: 26.4446222782135
  time_total_s: 1558.970431804657
  timers:
    learn_throughput: 8383.428
    learn_time_ms: 19299.026
    sample_throughput: 23418.249
    sample_time_ms: 6908.8
    update_time_ms: 40.646
  timestamp: 1602635748
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     59 |          1558.97 | 9545728 |  257.808 |              307.101 |              138.768 |            782.942 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3211.332600732601
    time_step_min: 2909
  date: 2020-10-14_00-36-15
  done: false
  episode_len_mean: 782.6514930217462
  episode_reward_max: 307.10101010100993
  episode_reward_mean: 258.11093899068584
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 174
  episodes_total: 12324
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.35738612711429596
        entropy_coeff: 0.0005000000000000001
        kl: 0.004641391996604701
        model: {}
        policy_loss: -0.00926777491501222
        total_loss: 4.795818567276001
        vf_explained_var: 0.9875019192695618
        vf_loss: 4.8052358229955034
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.574193548387097
    gpu_util_percent0: 0.32903225806451614
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7838709677419353
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14792644426374754
    mean_env_wait_ms: 1.2187142368698078
    mean_inference_ms: 4.391895450127659
    mean_raw_obs_processing_ms: 0.38302453260188046
  time_since_restore: 1585.4539256095886
  time_this_iter_s: 26.48349380493164
  time_total_s: 1585.4539256095886
  timers:
    learn_throughput: 8385.764
    learn_time_ms: 19293.651
    sample_throughput: 23357.005
    sample_time_ms: 6926.916
    update_time_ms: 38.771
  timestamp: 1602635775
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     60 |          1585.45 | 9707520 |  258.111 |              307.101 |              138.768 |            782.651 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3209.3472679130227
    time_step_min: 2876
  date: 2020-10-14_00-36-42
  done: false
  episode_len_mean: 782.3579427291634
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 258.4143151237216
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 12502
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.37262995292743045
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047727123989413185
        model: {}
        policy_loss: -0.009100308564181129
        total_loss: 5.967288215955098
        vf_explained_var: 0.9856808185577393
        vf_loss: 5.97655991713206
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.783870967741937
    gpu_util_percent0: 0.3341935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14790302985307147
    mean_env_wait_ms: 1.2188722379888777
    mean_inference_ms: 4.390377706741319
    mean_raw_obs_processing_ms: 0.3829456935016934
  time_since_restore: 1611.9500939846039
  time_this_iter_s: 26.49616837501526
  time_total_s: 1611.9500939846039
  timers:
    learn_throughput: 8384.428
    learn_time_ms: 19296.725
    sample_throughput: 23299.564
    sample_time_ms: 6943.993
    update_time_ms: 45.225
  timestamp: 1602635802
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     61 |          1611.95 | 9869312 |  258.414 |              309.071 |              138.768 |            782.358 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3206.071540633825
    time_step_min: 2876
  date: 2020-10-14_00-37-09
  done: false
  episode_len_mean: 781.8988034722765
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 258.8943323909305
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 285
  episodes_total: 12787
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3330593133966128
        entropy_coeff: 0.0005000000000000001
        kl: 0.00444906127328674
        model: {}
        policy_loss: -0.00821452463666598
        total_loss: 7.139691313107808
        vf_explained_var: 0.9868228435516357
        vf_loss: 7.148065447807312
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.341935483870966
    gpu_util_percent0: 0.3096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14786527542051428
    mean_env_wait_ms: 1.2190906433108353
    mean_inference_ms: 4.3879856737095295
    mean_raw_obs_processing_ms: 0.3828183597375313
  time_since_restore: 1638.5407824516296
  time_this_iter_s: 26.590688467025757
  time_total_s: 1638.5407824516296
  timers:
    learn_throughput: 8384.372
    learn_time_ms: 19296.855
    sample_throughput: 23221.832
    sample_time_ms: 6967.237
    update_time_ms: 44.823
  timestamp: 1602635829
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     62 |          1638.54 | 10031104 |  258.894 |              309.071 |              138.768 |            781.899 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3204.3154757296584
    time_step_min: 2876
  date: 2020-10-14_00-37-36
  done: false
  episode_len_mean: 781.6361531336833
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 259.16756169287817
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 169
  episodes_total: 12956
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.3515119304259618
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048386941974361735
        model: {}
        policy_loss: -0.009805110188002194
        total_loss: 4.415205558141072
        vf_explained_var: 0.988283634185791
        vf_loss: 4.425182620684306
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.256666666666668
    gpu_util_percent0: 0.30433333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14784318202711624
    mean_env_wait_ms: 1.2192237074023382
    mean_inference_ms: 4.386603577199803
    mean_raw_obs_processing_ms: 0.38274841615857186
  time_since_restore: 1665.0736622810364
  time_this_iter_s: 26.53287982940674
  time_total_s: 1665.0736622810364
  timers:
    learn_throughput: 8379.308
    learn_time_ms: 19308.516
    sample_throughput: 23178.393
    sample_time_ms: 6980.294
    update_time_ms: 45.421
  timestamp: 1602635856
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     63 |          1665.07 | 10192896 |  259.168 |              309.071 |              138.768 |            781.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3202.2302822273073
    time_step_min: 2876
  date: 2020-10-14_00-38-02
  done: false
  episode_len_mean: 781.3836793672523
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 259.4798751835028
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 193
  episodes_total: 13149
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.36590735614299774
        entropy_coeff: 0.0005000000000000001
        kl: 0.005199664505198598
        model: {}
        policy_loss: -0.009823094194871373
        total_loss: 6.801222324371338
        vf_explained_var: 0.9842493534088135
        vf_loss: 6.81122624874115
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.05483870967742
    gpu_util_percent0: 0.29354838709677417
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478208050180332
    mean_env_wait_ms: 1.2193673172469608
    mean_inference_ms: 4.385126273376545
    mean_raw_obs_processing_ms: 0.38267068017486516
  time_since_restore: 1691.5224163532257
  time_this_iter_s: 26.44875407218933
  time_total_s: 1691.5224163532257
  timers:
    learn_throughput: 8366.455
    learn_time_ms: 19338.179
    sample_throughput: 23177.884
    sample_time_ms: 6980.447
    update_time_ms: 47.227
  timestamp: 1602635882
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     64 |          1691.52 | 10354688 |   259.48 |              309.071 |              138.768 |            781.384 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3199.4806427503736
    time_step_min: 2876
  date: 2020-10-14_00-38-29
  done: false
  episode_len_mean: 781.0432222967435
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 259.9165791607106
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 270
  episodes_total: 13419
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3239520415663719
        entropy_coeff: 0.0005000000000000001
        kl: 0.004762139287777245
        model: {}
        policy_loss: -0.007904218912396269
        total_loss: 6.6040810743967695
        vf_explained_var: 0.987603485584259
        vf_loss: 6.612145344416301
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.65483870967742
    gpu_util_percent0: 0.32774193548387087
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14778605241135065
    mean_env_wait_ms: 1.2195581367029735
    mean_inference_ms: 4.383036786942394
    mean_raw_obs_processing_ms: 0.38256194615909
  time_since_restore: 1718.179719209671
  time_this_iter_s: 26.657302856445312
  time_total_s: 1718.179719209671
  timers:
    learn_throughput: 8356.742
    learn_time_ms: 19360.655
    sample_throughput: 23111.265
    sample_time_ms: 7000.569
    update_time_ms: 45.664
  timestamp: 1602635909
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     65 |          1718.18 | 10516480 |  259.917 |              309.071 |              138.768 |            781.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3197.6880950623663
    time_step_min: 2876
  date: 2020-10-14_00-38-56
  done: false
  episode_len_mean: 780.8112304974978
  episode_reward_max: 309.0707070707069
  episode_reward_mean: 260.20380356404786
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 169
  episodes_total: 13588
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.34279561787843704
        entropy_coeff: 0.0005000000000000001
        kl: 0.005283504142425954
        model: {}
        policy_loss: -0.011030456070632985
        total_loss: 4.626668572425842
        vf_explained_var: 0.9871037602424622
        vf_loss: 4.6378694375356035
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.941935483870974
    gpu_util_percent0: 0.3538709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14776606094571523
    mean_env_wait_ms: 1.2196700484285719
    mean_inference_ms: 4.381761624923817
    mean_raw_obs_processing_ms: 0.38249599184896477
  time_since_restore: 1744.5855243206024
  time_this_iter_s: 26.405805110931396
  time_total_s: 1744.5855243206024
  timers:
    learn_throughput: 8361.05
    learn_time_ms: 19350.68
    sample_throughput: 23126.343
    sample_time_ms: 6996.005
    update_time_ms: 51.269
  timestamp: 1602635936
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     66 |          1744.59 | 10678272 |  260.204 |              309.071 |              138.768 |            780.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3195.457063913328
    time_step_min: 2873
  date: 2020-10-14_00-39-22
  done: false
  episode_len_mean: 780.5256670533643
  episode_reward_max: 309.5252525252523
  episode_reward_mean: 260.53601634090325
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 13792
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3453119471669197
        entropy_coeff: 0.0005000000000000001
        kl: 0.006284578509318332
        model: {}
        policy_loss: -0.010803500733648738
        total_loss: 6.535979986190796
        vf_explained_var: 0.9849061965942383
        vf_loss: 6.546955108642578
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.42
    gpu_util_percent0: 0.299
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14774272731121307
    mean_env_wait_ms: 1.2197958431126614
    mean_inference_ms: 4.380307461459509
    mean_raw_obs_processing_ms: 0.3824175085341384
  time_since_restore: 1770.8510437011719
  time_this_iter_s: 26.265519380569458
  time_total_s: 1770.8510437011719
  timers:
    learn_throughput: 8363.016
    learn_time_ms: 19346.131
    sample_throughput: 23218.959
    sample_time_ms: 6968.099
    update_time_ms: 42.833
  timestamp: 1602635962
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     67 |          1770.85 | 10840064 |  260.536 |              309.525 |              138.768 |            780.526 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3192.8586002710995
    time_step_min: 2873
  date: 2020-10-14_00-39-49
  done: false
  episode_len_mean: 780.1807768924302
  episode_reward_max: 312.55555555555554
  episode_reward_mean: 260.92363087333206
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 264
  episodes_total: 14056
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3043691962957382
        entropy_coeff: 0.0005000000000000001
        kl: 0.004952103287602465
        model: {}
        policy_loss: -0.008787163727295896
        total_loss: 5.887337724367778
        vf_explained_var: 0.9885470867156982
        vf_loss: 5.896276235580444
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95161290322581
    gpu_util_percent0: 0.3729032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870956
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14771317628097955
    mean_env_wait_ms: 1.2199682192125028
    mean_inference_ms: 4.378456217739824
    mean_raw_obs_processing_ms: 0.38232237213030323
  time_since_restore: 1797.038854598999
  time_this_iter_s: 26.18781089782715
  time_total_s: 1797.038854598999
  timers:
    learn_throughput: 8369.771
    learn_time_ms: 19330.517
    sample_throughput: 23225.826
    sample_time_ms: 6966.039
    update_time_ms: 41.081
  timestamp: 1602635989
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     68 |          1797.04 | 11001856 |  260.924 |              312.556 |              138.768 |            780.181 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3191.4019884360455
    time_step_min: 2849
  date: 2020-10-14_00-40-16
  done: false
  episode_len_mean: 779.978060614584
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 261.1654005777485
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 14221
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.33161305139462155
        entropy_coeff: 0.0005000000000000001
        kl: 0.005548228432113926
        model: {}
        policy_loss: -0.008611835034874579
        total_loss: 4.937494079271953
        vf_explained_var: 0.9861431121826172
        vf_loss: 4.946271141370137
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.990322580645163
    gpu_util_percent0: 0.28806451612903233
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14769466937949174
    mean_env_wait_ms: 1.2200669880042352
    mean_inference_ms: 4.37731934953179
    mean_raw_obs_processing_ms: 0.382263236130951
  time_since_restore: 1823.686747789383
  time_this_iter_s: 26.64789319038391
  time_total_s: 1823.686747789383
  timers:
    learn_throughput: 8366.266
    learn_time_ms: 19338.616
    sample_throughput: 23191.911
    sample_time_ms: 6976.226
    update_time_ms: 41.458
  timestamp: 1602636016
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     69 |          1823.69 | 11163648 |  261.165 |              313.162 |              138.768 |            779.978 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3189.56353783089
    time_step_min: 2849
  date: 2020-10-14_00-40-43
  done: false
  episode_len_mean: 779.7287971175166
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 261.44417848104104
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 211
  episodes_total: 14432
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3377840941150983
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046046188023562236
        model: {}
        policy_loss: -0.009337110163566345
        total_loss: 6.7480814854304
        vf_explained_var: 0.9852480888366699
        vf_loss: 6.7575870752334595
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.34193548387097
    gpu_util_percent0: 0.28161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8032258064516133
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147672002718048
    mean_env_wait_ms: 1.220178925868579
    mean_inference_ms: 4.3759102519865865
    mean_raw_obs_processing_ms: 0.38218380204758895
  time_since_restore: 1850.424033164978
  time_this_iter_s: 26.737285375595093
  time_total_s: 1850.424033164978
  timers:
    learn_throughput: 8360.552
    learn_time_ms: 19351.833
    sample_throughput: 23159.746
    sample_time_ms: 6985.914
    update_time_ms: 43.414
  timestamp: 1602636043
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     70 |          1850.42 | 11325440 |  261.444 |              313.162 |              138.768 |            779.729 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3187.2540440925536
    time_step_min: 2849
  date: 2020-10-14_00-41-10
  done: false
  episode_len_mean: 779.4633764465623
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 261.81610179397785
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 258
  episodes_total: 14690
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.2946927572290103
        entropy_coeff: 0.0005000000000000001
        kl: 0.004823330091312528
        model: {}
        policy_loss: -0.009921685205578493
        total_loss: 5.918912967046102
        vf_explained_var: 0.9878664016723633
        vf_loss: 5.928981900215149
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.609677419354842
    gpu_util_percent0: 0.33064516129032256
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8516129032258077
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476445795496874
    mean_env_wait_ms: 1.2203302667030436
    mean_inference_ms: 4.374260291505232
    mean_raw_obs_processing_ms: 0.38209815948813813
  time_since_restore: 1876.7975056171417
  time_this_iter_s: 26.373472452163696
  time_total_s: 1876.7975056171417
  timers:
    learn_throughput: 8362.826
    learn_time_ms: 19346.57
    sample_throughput: 23162.505
    sample_time_ms: 6985.082
    update_time_ms: 36.464
  timestamp: 1602636070
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     71 |           1876.8 | 11487232 |  261.816 |              313.162 |              138.768 |            779.463 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3185.827055488052
    time_step_min: 2849
  date: 2020-10-14_00-41-36
  done: false
  episode_len_mean: 779.2945532888979
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.03675412986667
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 163
  episodes_total: 14853
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.31879138201475143
        entropy_coeff: 0.0005000000000000001
        kl: 0.004923491855151951
        model: {}
        policy_loss: -0.00857447545422474
        total_loss: 4.748826503753662
        vf_explained_var: 0.9867516160011292
        vf_loss: 4.75756021340688
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.493333333333336
    gpu_util_percent0: 0.3046666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8233333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762767774230526
    mean_env_wait_ms: 1.2204166134610401
    mean_inference_ms: 4.373223827760095
    mean_raw_obs_processing_ms: 0.3820425294250021
  time_since_restore: 1903.1778762340546
  time_this_iter_s: 26.380370616912842
  time_total_s: 1903.1778762340546
  timers:
    learn_throughput: 8367.92
    learn_time_ms: 19334.792
    sample_throughput: 23171.479
    sample_time_ms: 6982.377
    update_time_ms: 37.992
  timestamp: 1602636096
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     72 |          1903.18 | 11649024 |  262.037 |              313.162 |              138.768 |            779.295 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3183.870568673096
    time_step_min: 2849
  date: 2020-10-14_00-42-03
  done: false
  episode_len_mean: 779.0706514528327
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.34537024751967
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 221
  episodes_total: 15074
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3208874662717183
        entropy_coeff: 0.0005000000000000001
        kl: 0.005192111323898037
        model: {}
        policy_loss: -0.010388918977696449
        total_loss: 6.205375750859578
        vf_explained_var: 0.9863477349281311
        vf_loss: 6.215924978256226
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.332258064516132
    gpu_util_percent0: 0.33677419354838706
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8612903225806465
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760500767301607
    mean_env_wait_ms: 1.2205182146147922
    mean_inference_ms: 4.371854764213998
    mean_raw_obs_processing_ms: 0.38196558142944953
  time_since_restore: 1929.5160446166992
  time_this_iter_s: 26.338168382644653
  time_total_s: 1929.5160446166992
  timers:
    learn_throughput: 8370.76
    learn_time_ms: 19328.234
    sample_throughput: 23213.08
    sample_time_ms: 6969.864
    update_time_ms: 36.411
  timestamp: 1602636123
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     73 |          1929.52 | 11810816 |  262.345 |              313.162 |              138.768 |            779.071 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3181.8615636244685
    time_step_min: 2849
  date: 2020-10-14_00-42-30
  done: false
  episode_len_mean: 778.799008091882
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.6656225528583
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 15324
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.28383104751507443
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046672336369132
        model: {}
        policy_loss: -0.010195812821621075
        total_loss: 6.37613062063853
        vf_explained_var: 0.9868685603141785
        vf_loss: 6.386468291282654
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.845161290322583
    gpu_util_percent0: 0.33774193548387094
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.858064516129034
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14758080480998478
    mean_env_wait_ms: 1.220655093868589
    mean_inference_ms: 4.370371681191482
    mean_raw_obs_processing_ms: 0.3818864218727672
  time_since_restore: 1956.0900070667267
  time_this_iter_s: 26.573962450027466
  time_total_s: 1956.0900070667267
  timers:
    learn_throughput: 8375.584
    learn_time_ms: 19317.101
    sample_throughput: 23140.829
    sample_time_ms: 6991.625
    update_time_ms: 36.243
  timestamp: 1602636150
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     74 |          1956.09 | 11972608 |  262.666 |              313.162 |              138.768 |            778.799 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3180.5587207043895
    time_step_min: 2849
  date: 2020-10-14_00-42-57
  done: false
  episode_len_mean: 778.62092347433
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 262.87419235950074
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 161
  episodes_total: 15485
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.30613749474287033
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050003100574637456
        model: {}
        policy_loss: -0.010951006136262246
        total_loss: 4.720480720202128
        vf_explained_var: 0.9867067337036133
        vf_loss: 4.731584827105205
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.538709677419355
    gpu_util_percent0: 0.31225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14756516936561018
    mean_env_wait_ms: 1.2207314088397676
    mean_inference_ms: 4.369414391711434
    mean_raw_obs_processing_ms: 0.3818346872670139
  time_since_restore: 1982.6533162593842
  time_this_iter_s: 26.56330919265747
  time_total_s: 1982.6533162593842
  timers:
    learn_throughput: 8377.472
    learn_time_ms: 19312.747
    sample_throughput: 23159.201
    sample_time_ms: 6986.079
    update_time_ms: 36.548
  timestamp: 1602636177
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     75 |          1982.65 | 12134400 |  262.874 |              313.162 |              138.768 |            778.621 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3178.5969254321617
    time_step_min: 2849
  date: 2020-10-14_00-43-24
  done: false
  episode_len_mean: 778.3596334945279
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 263.1888611233228
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 231
  episodes_total: 15716
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.3035415733853976
        entropy_coeff: 0.0005000000000000001
        kl: 0.005273402590925495
        model: {}
        policy_loss: -0.01085835959383985
        total_loss: 6.099811514218648
        vf_explained_var: 0.9868482947349548
        vf_loss: 6.110821644465129
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.835483870967742
    gpu_util_percent0: 0.2983870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8612903225806465
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14754295583270077
    mean_env_wait_ms: 1.220832593623729
    mean_inference_ms: 4.368098631726875
    mean_raw_obs_processing_ms: 0.38175941184748186
  time_since_restore: 2009.2728300094604
  time_this_iter_s: 26.619513750076294
  time_total_s: 2009.2728300094604
  timers:
    learn_throughput: 8369.79
    learn_time_ms: 19330.474
    sample_throughput: 23127.725
    sample_time_ms: 6995.587
    update_time_ms: 30.683
  timestamp: 1602636204
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     76 |          2009.27 | 12296192 |  263.189 |              313.162 |              138.768 |             778.36 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3176.516868756675
    time_step_min: 2849
  date: 2020-10-14_00-43-50
  done: false
  episode_len_mean: 778.1283529706693
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 263.49384608177536
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 240
  episodes_total: 15956
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.2723412364721298
        entropy_coeff: 0.0005000000000000001
        kl: 0.00461164415658762
        model: {}
        policy_loss: -0.008690265705808997
        total_loss: 6.037911971410115
        vf_explained_var: 0.9869084358215332
        vf_loss: 6.046738187472026
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.10322580645162
    gpu_util_percent0: 0.35709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8612903225806465
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14752128478567827
    mean_env_wait_ms: 1.2209484524066023
    mean_inference_ms: 4.366748891118287
    mean_raw_obs_processing_ms: 0.3816889196996852
  time_since_restore: 2035.75252699852
  time_this_iter_s: 26.47969698905945
  time_total_s: 2035.75252699852
  timers:
    learn_throughput: 8366.361
    learn_time_ms: 19338.396
    sample_throughput: 23089.769
    sample_time_ms: 7007.086
    update_time_ms: 31.899
  timestamp: 1602636230
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     77 |          2035.75 | 12457984 |  263.494 |              313.162 |              138.768 |            778.128 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3175.1828358208954
    time_step_min: 2849
  date: 2020-10-14_00-44-17
  done: false
  episode_len_mean: 777.9526025187666
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 263.7082513202
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 163
  episodes_total: 16119
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.3022257635990779
        entropy_coeff: 0.0005000000000000001
        kl: 0.005352613128100832
        model: {}
        policy_loss: -0.012025649872763703
        total_loss: 4.878254731496175
        vf_explained_var: 0.9861853718757629
        vf_loss: 4.890431523323059
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.100000000000005
    gpu_util_percent0: 0.2826666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750681331049031
    mean_env_wait_ms: 1.221022136099698
    mean_inference_ms: 4.365853065089045
    mean_raw_obs_processing_ms: 0.38164080054980604
  time_since_restore: 2062.0973575115204
  time_this_iter_s: 26.34483051300049
  time_total_s: 2062.0973575115204
  timers:
    learn_throughput: 8361.027
    learn_time_ms: 19350.732
    sample_throughput: 23081.282
    sample_time_ms: 7009.663
    update_time_ms: 32.156
  timestamp: 1602636257
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     78 |           2062.1 | 12619776 |  263.708 |              313.162 |              138.768 |            777.953 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3173.099877526026
    time_step_min: 2849
  date: 2020-10-14_00-44-44
  done: false
  episode_len_mean: 777.6837925346692
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.0238613145938
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 16369
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2879169334967931
        entropy_coeff: 0.0005000000000000001
        kl: 0.005028028429175417
        model: {}
        policy_loss: -0.00984137509658467
        total_loss: 6.26339606444041
        vf_explained_var: 0.9869596362113953
        vf_loss: 6.273381392161052
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.132258064516126
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.858064516129034
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748468575388338
    mean_env_wait_ms: 1.2211229213823596
    mean_inference_ms: 4.364519981022146
    mean_raw_obs_processing_ms: 0.3815638323029398
  time_since_restore: 2088.8313417434692
  time_this_iter_s: 26.733984231948853
  time_total_s: 2088.8313417434692
  timers:
    learn_throughput: 8351.243
    learn_time_ms: 19373.403
    sample_throughput: 23126.762
    sample_time_ms: 6995.878
    update_time_ms: 30.313
  timestamp: 1602636284
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     79 |          2088.83 | 12781568 |  264.024 |              313.162 |              138.768 |            777.684 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3171.303262839879
    time_step_min: 2849
  date: 2020-10-14_00-45-11
  done: false
  episode_len_mean: 777.4493338959552
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.2888874275335
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 220
  episodes_total: 16589
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.26022494087616604
        entropy_coeff: 0.0005000000000000001
        kl: 0.004336869033674399
        model: {}
        policy_loss: -0.008254327617275218
        total_loss: 5.896654645601909
        vf_explained_var: 0.986591100692749
        vf_loss: 5.905039032300313
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.9741935483871
    gpu_util_percent0: 0.3270967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.854838709677421
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474654916013345
    mean_env_wait_ms: 1.2212159889895537
    mean_inference_ms: 4.363355418787885
    mean_raw_obs_processing_ms: 0.38150140009659356
  time_since_restore: 2115.037270307541
  time_this_iter_s: 26.205928564071655
  time_total_s: 2115.037270307541
  timers:
    learn_throughput: 8365.683
    learn_time_ms: 19339.964
    sample_throughput: 23188.756
    sample_time_ms: 6977.175
    update_time_ms: 28.137
  timestamp: 1602636311
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     80 |          2115.04 | 12943360 |  264.289 |              313.162 |              138.768 |            777.449 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3170.0904032547564
    time_step_min: 2849
  date: 2020-10-14_00-45-38
  done: false
  episode_len_mean: 777.2681310810004
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.5057806622302
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 164
  episodes_total: 16753
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2832312931617101
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045790177925179405
        model: {}
        policy_loss: -0.009791253217069121
        total_loss: 4.2394417723019915
        vf_explained_var: 0.9878109097480774
        vf_loss: 4.249374568462372
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.98709677419355
    gpu_util_percent0: 0.2793548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745182466267903
    mean_env_wait_ms: 1.2212837615296472
    mean_inference_ms: 4.362520350395025
    mean_raw_obs_processing_ms: 0.38145618535143916
  time_since_restore: 2141.6719307899475
  time_this_iter_s: 26.634660482406616
  time_total_s: 2141.6719307899475
  timers:
    learn_throughput: 8355.957
    learn_time_ms: 19362.473
    sample_throughput: 23175.904
    sample_time_ms: 6981.044
    update_time_ms: 27.122
  timestamp: 1602636338
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     81 |          2141.67 | 13105152 |  264.506 |              313.162 |              138.768 |            777.268 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3167.917015136345
    time_step_min: 2849
  date: 2020-10-14_00-46-05
  done: false
  episode_len_mean: 776.9709131507815
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 264.82390243960344
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 265
  episodes_total: 17018
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.26808729271094006
        entropy_coeff: 0.0005000000000000001
        kl: 0.004614657140336931
        model: {}
        policy_loss: -0.00847732342663221
        total_loss: 6.035964131355286
        vf_explained_var: 0.9881357550621033
        vf_loss: 6.044575452804565
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.20967741935484
    gpu_util_percent0: 0.3516129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.858064516129034
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743017312817855
    mean_env_wait_ms: 1.2213840657827
    mean_inference_ms: 4.361238773117835
    mean_raw_obs_processing_ms: 0.38138228527782636
  time_since_restore: 2168.1267278194427
  time_this_iter_s: 26.45479702949524
  time_total_s: 2168.1267278194427
  timers:
    learn_throughput: 8352.919
    learn_time_ms: 19369.517
    sample_throughput: 23173.48
    sample_time_ms: 6981.774
    update_time_ms: 25.634
  timestamp: 1602636365
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     82 |          2168.13 | 13266944 |  264.824 |              313.162 |              138.768 |            776.971 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3166.2549612989583
    time_step_min: 2849
  date: 2020-10-14_00-46-32
  done: false
  episode_len_mean: 776.7420160260133
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.08169372273426
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 204
  episodes_total: 17222
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.24432009706894556
        entropy_coeff: 0.0005000000000000001
        kl: 0.004341511405073106
        model: {}
        policy_loss: -0.007233019404035683
        total_loss: 4.631472627321879
        vf_explained_var: 0.9885926246643066
        vf_loss: 4.638827800750732
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.993548387096777
    gpu_util_percent0: 0.3393548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14741332340235414
    mean_env_wait_ms: 1.221460585494656
    mean_inference_ms: 4.360179828589798
    mean_raw_obs_processing_ms: 0.3813248585458888
  time_since_restore: 2194.7610466480255
  time_this_iter_s: 26.634318828582764
  time_total_s: 2194.7610466480255
  timers:
    learn_throughput: 8344.276
    learn_time_ms: 19389.579
    sample_throughput: 23147.687
    sample_time_ms: 6989.554
    update_time_ms: 27.029
  timestamp: 1602636392
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     83 |          2194.76 | 13428736 |  265.082 |              313.162 |              138.768 |            776.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3164.96455535704
    time_step_min: 2849
  date: 2020-10-14_00-46-58
  done: false
  episode_len_mean: 776.5568717653824
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.27914277914283
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 168
  episodes_total: 17390
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.2776962071657181
        entropy_coeff: 0.0005000000000000001
        kl: 0.004789162892848253
        model: {}
        policy_loss: -0.01052733378795286
        total_loss: 4.960049589474996
        vf_explained_var: 0.9871343970298767
        vf_loss: 4.970715920130412
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.009677419354837
    gpu_util_percent0: 0.3022580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740016196563463
    mean_env_wait_ms: 1.2215250084770533
    mean_inference_ms: 4.359380542023874
    mean_raw_obs_processing_ms: 0.3812798305636692
  time_since_restore: 2221.331498146057
  time_this_iter_s: 26.570451498031616
  time_total_s: 2221.331498146057
  timers:
    learn_throughput: 8343.411
    learn_time_ms: 19391.59
    sample_throughput: 23181.703
    sample_time_ms: 6979.298
    update_time_ms: 25.442
  timestamp: 1602636418
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     84 |          2221.33 | 13590528 |  265.279 |              313.162 |              138.768 |            776.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3162.7364539007094
    time_step_min: 2849
  date: 2020-10-14_00-47-25
  done: false
  episode_len_mean: 776.2942708333334
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.6203320569829
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 274
  episodes_total: 17664
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.25675638020038605
        entropy_coeff: 0.0005000000000000001
        kl: 0.005409693229012191
        model: {}
        policy_loss: -0.008175827337254304
        total_loss: 6.087591091791789
        vf_explained_var: 0.9879869818687439
        vf_loss: 6.095895171165466
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.383870967741935
    gpu_util_percent0: 0.4035483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.858064516129034
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14737886337600076
    mean_env_wait_ms: 1.2216149686116469
    mean_inference_ms: 4.35812910192902
    mean_raw_obs_processing_ms: 0.3812074320957964
  time_since_restore: 2248.0070338249207
  time_this_iter_s: 26.675535678863525
  time_total_s: 2248.0070338249207
  timers:
    learn_throughput: 8347.077
    learn_time_ms: 19383.072
    sample_throughput: 23134.103
    sample_time_ms: 6993.658
    update_time_ms: 29.292
  timestamp: 1602636445
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     85 |          2248.01 | 13752320 |   265.62 |              313.162 |              138.768 |            776.294 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3161.1331462250914
    time_step_min: 2849
  date: 2020-10-14_00-47-52
  done: false
  episode_len_mean: 776.1326873529741
  episode_reward_max: 313.1616161616163
  episode_reward_mean: 265.8626915508847
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 17854
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.24634397650758424
        entropy_coeff: 0.0005000000000000001
        kl: 0.005001329622852306
        model: {}
        policy_loss: -0.009673356369603425
        total_loss: 3.7913096944491067
        vf_explained_var: 0.9898946285247803
        vf_loss: 3.8011061549186707
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.24
    gpu_util_percent0: 0.30033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736465295469445
    mean_env_wait_ms: 1.2216849788926776
    mean_inference_ms: 4.357210684558006
    mean_raw_obs_processing_ms: 0.38115850466967527
  time_since_restore: 2274.4722707271576
  time_this_iter_s: 26.46523690223694
  time_total_s: 2274.4722707271576
  timers:
    learn_throughput: 8350.514
    learn_time_ms: 19375.095
    sample_throughput: 23154.182
    sample_time_ms: 6987.593
    update_time_ms: 27.153
  timestamp: 1602636472
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     86 |          2274.47 | 13914112 |  265.863 |              313.162 |              138.768 |            776.133 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3159.6469182459846
    time_step_min: 2849
  date: 2020-10-14_00-48-19
  done: false
  episode_len_mean: 775.9814219165927
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.09162611025965
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 18032
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.26942487557729083
        entropy_coeff: 0.0005000000000000001
        kl: 0.004710895552610357
        model: {}
        policy_loss: -0.008555350175204998
        total_loss: 4.417007406552632
        vf_explained_var: 0.9881947636604309
        vf_loss: 4.425697326660156
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.23225806451613
    gpu_util_percent0: 0.35193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14735166983173345
    mean_env_wait_ms: 1.2217461663108913
    mean_inference_ms: 4.356413821795899
    mean_raw_obs_processing_ms: 0.38111307553832063
  time_since_restore: 2300.690953731537
  time_this_iter_s: 26.218683004379272
  time_total_s: 2300.690953731537
  timers:
    learn_throughput: 8360.036
    learn_time_ms: 19353.026
    sample_throughput: 23190.812
    sample_time_ms: 6976.556
    update_time_ms: 25.758
  timestamp: 1602636499
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     87 |          2300.69 | 14075904 |  266.092 |              314.828 |              138.768 |            775.981 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3157.477937150991
    time_step_min: 2849
  date: 2020-10-14_00-48-46
  done: false
  episode_len_mean: 775.7841027041792
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.4329031919854
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 273
  episodes_total: 18305
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.24425077935059866
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047048449438686175
        model: {}
        policy_loss: -0.007794409650765981
        total_loss: 4.551439007123311
        vf_explained_var: 0.9909267425537109
        vf_loss: 4.55935553709666
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.353333333333335
    gpu_util_percent0: 0.2893333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8566666666666682
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473322747235593
    mean_env_wait_ms: 1.2218262429882343
    mean_inference_ms: 4.355230242825744
    mean_raw_obs_processing_ms: 0.3810447925089678
  time_since_restore: 2327.0870954990387
  time_this_iter_s: 26.39614176750183
  time_total_s: 2327.0870954990387
  timers:
    learn_throughput: 8362.44
    learn_time_ms: 19347.464
    sample_throughput: 23157.959
    sample_time_ms: 6986.453
    update_time_ms: 25.698
  timestamp: 1602636526
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     88 |          2327.09 | 14237696 |  266.433 |              314.828 |              138.768 |            775.784 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3156.0455900688457
    time_step_min: 2849
  date: 2020-10-14_00-49-13
  done: false
  episode_len_mean: 775.6608785026507
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.6613407689357
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 18486
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.23832658181587854
        entropy_coeff: 0.0005000000000000001
        kl: 0.004391344225344558
        model: {}
        policy_loss: -0.010078011987692056
        total_loss: 4.442067782084147
        vf_explained_var: 0.9879171252250671
        vf_loss: 4.452265024185181
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.680645161290332
    gpu_util_percent0: 0.2474193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731884901129738
    mean_env_wait_ms: 1.2218834191731136
    mean_inference_ms: 4.354419814599209
    mean_raw_obs_processing_ms: 0.3810009795303425
  time_since_restore: 2353.5435569286346
  time_this_iter_s: 26.456461429595947
  time_total_s: 2353.5435569286346
  timers:
    learn_throughput: 8379.362
    learn_time_ms: 19308.391
    sample_throughput: 23119.569
    sample_time_ms: 6998.054
    update_time_ms: 25.638
  timestamp: 1602636553
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     89 |          2353.54 | 14399488 |  266.661 |              314.828 |              138.768 |            775.661 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3154.493719132489
    time_step_min: 2844
  date: 2020-10-14_00-49-39
  done: false
  episode_len_mean: 775.5111694434028
  episode_reward_max: 314.82828282828297
  episode_reward_mean: 266.8942361959987
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 18667
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.26231640080610913
        entropy_coeff: 0.0005000000000000001
        kl: 0.004421331454068422
        model: {}
        policy_loss: -0.01087857196883609
        total_loss: 4.68465252717336
        vf_explained_var: 0.987766444683075
        vf_loss: 4.695662339528401
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.0741935483871
    gpu_util_percent0: 0.2725806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14730663765314034
    mean_env_wait_ms: 1.2219376172205045
    mean_inference_ms: 4.35366406871359
    mean_raw_obs_processing_ms: 0.38095849853759284
  time_since_restore: 2379.935909509659
  time_this_iter_s: 26.39235258102417
  time_total_s: 2379.935909509659
  timers:
    learn_throughput: 8373.486
    learn_time_ms: 19321.942
    sample_throughput: 23107.867
    sample_time_ms: 7001.598
    update_time_ms: 26.501
  timestamp: 1602636579
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     90 |          2379.94 | 14561280 |  266.894 |              314.828 |              138.768 |            775.511 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3152.272914793463
    time_step_min: 2838
  date: 2020-10-14_00-50-06
  done: false
  episode_len_mean: 775.2899292726697
  episode_reward_max: 314.97979797979775
  episode_reward_mean: 267.24438409216197
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 279
  episodes_total: 18946
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.22865770384669304
        entropy_coeff: 0.0005000000000000001
        kl: 0.00466413233273973
        model: {}
        policy_loss: -0.010199747819569893
        total_loss: 4.387278318405151
        vf_explained_var: 0.99107426404953
        vf_loss: 4.397592385609944
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.309677419354845
    gpu_util_percent0: 0.3490322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.858064516129034
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728729094587703
    mean_env_wait_ms: 1.2220076512886435
    mean_inference_ms: 4.352493345233223
    mean_raw_obs_processing_ms: 0.3808897260915545
  time_since_restore: 2406.3859689235687
  time_this_iter_s: 26.450059413909912
  time_total_s: 2406.3859689235687
  timers:
    learn_throughput: 8380.35
    learn_time_ms: 19306.114
    sample_throughput: 23118.554
    sample_time_ms: 6998.362
    update_time_ms: 26.899
  timestamp: 1602636606
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     91 |          2406.39 | 14723072 |  267.244 |               314.98 |              138.768 |             775.29 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3150.9178153991297
    time_step_min: 2838
  date: 2020-10-14_00-50-33
  done: false
  episode_len_mean: 775.1534679359766
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 267.45693360004486
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 172
  episodes_total: 19118
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.23448821902275085
        entropy_coeff: 0.0005000000000000001
        kl: 0.004760275594890118
        model: {}
        policy_loss: -0.011740240413928404
        total_loss: 3.7931633392969766
        vf_explained_var: 0.9892194271087646
        vf_loss: 3.8050206700960794
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.02903225806452
    gpu_util_percent0: 0.35161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727554765998155
    mean_env_wait_ms: 1.2220570721955457
    mean_inference_ms: 4.351792162146644
    mean_raw_obs_processing_ms: 0.3808516846566221
  time_since_restore: 2432.868019104004
  time_this_iter_s: 26.48205018043518
  time_total_s: 2432.868019104004
  timers:
    learn_throughput: 8378.452
    learn_time_ms: 19310.488
    sample_throughput: 23134.696
    sample_time_ms: 6993.479
    update_time_ms: 28.419
  timestamp: 1602636633
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     92 |          2432.87 | 14884864 |  267.457 |              318.768 |              138.768 |            775.153 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3149.4271708683473
    time_step_min: 2838
  date: 2020-10-14_00-51-00
  done: false
  episode_len_mean: 774.982502458974
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 267.6832967036415
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 19317
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.24960773438215256
        entropy_coeff: 0.0005000000000000001
        kl: 0.005254663371791442
        model: {}
        policy_loss: -0.009448278980319932
        total_loss: 5.701354543368022
        vf_explained_var: 0.9859640598297119
        vf_loss: 5.710927724838257
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.79354838709677
    gpu_util_percent0: 0.3251612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14726293343434926
    mean_env_wait_ms: 1.222108978770397
    mean_inference_ms: 4.35102193969016
    mean_raw_obs_processing_ms: 0.38080738494221783
  time_since_restore: 2459.3926599025726
  time_this_iter_s: 26.524640798568726
  time_total_s: 2459.3926599025726
  timers:
    learn_throughput: 8385.26
    learn_time_ms: 19294.81
    sample_throughput: 23126.36
    sample_time_ms: 6995.999
    update_time_ms: 28.795
  timestamp: 1602636660
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     93 |          2459.39 | 15046656 |  267.683 |              318.768 |              138.768 |            774.983 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3147.3726463364715
    time_step_min: 2838
  date: 2020-10-14_00-51-27
  done: false
  episode_len_mean: 774.7720471837819
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 267.9979538014058
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 266
  episodes_total: 19583
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.21555884927511215
        entropy_coeff: 0.0005000000000000001
        kl: 0.004697610507719219
        model: {}
        policy_loss: -0.010029192916893711
        total_loss: 5.045426448186238
        vf_explained_var: 0.9896988868713379
        vf_loss: 5.05556333065033
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.658064516129034
    gpu_util_percent0: 0.3580645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.864516129032259
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472455143717799
    mean_env_wait_ms: 1.222170531879869
    mean_inference_ms: 4.349963284335349
    mean_raw_obs_processing_ms: 0.3807459206310185
  time_since_restore: 2485.995943069458
  time_this_iter_s: 26.603283166885376
  time_total_s: 2485.995943069458
  timers:
    learn_throughput: 8386.504
    learn_time_ms: 19291.947
    sample_throughput: 23092.324
    sample_time_ms: 7006.311
    update_time_ms: 32.086
  timestamp: 1602636687
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     94 |             2486 | 15208448 |  267.998 |              318.768 |              138.768 |            774.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3146.0323169803664
    time_step_min: 2838
  date: 2020-10-14_00-51-54
  done: false
  episode_len_mean: 774.6390886075949
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.20130929548645
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 167
  episodes_total: 19750
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.2268433632949988
        entropy_coeff: 0.0005000000000000001
        kl: 0.004843841656111181
        model: {}
        policy_loss: -0.007885286041224996
        total_loss: 3.483263889948527
        vf_explained_var: 0.9896848201751709
        vf_loss: 3.4912625551223755
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.783870967741937
    gpu_util_percent0: 0.31870967741935485
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14723464987484058
    mean_env_wait_ms: 1.222210116559175
    mean_inference_ms: 4.349310667259152
    mean_raw_obs_processing_ms: 0.3807097235468762
  time_since_restore: 2512.5142164230347
  time_this_iter_s: 26.51827335357666
  time_total_s: 2512.5142164230347
  timers:
    learn_throughput: 8383.731
    learn_time_ms: 19298.33
    sample_throughput: 23183.434
    sample_time_ms: 6978.776
    update_time_ms: 28.574
  timestamp: 1602636714
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     95 |          2512.51 | 15370240 |  268.201 |              318.768 |              138.768 |            774.639 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3144.332212575902
    time_step_min: 2838
  date: 2020-10-14_00-52-21
  done: false
  episode_len_mean: 774.4661925272964
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.4488772327097
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 19966
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.2383594090739886
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050382125191390514
        model: {}
        policy_loss: -0.009273823350667953
        total_loss: 4.70072074731191
        vf_explained_var: 0.9889609217643738
        vf_loss: 4.710113684336345
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.770967741935486
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472212404276107
    mean_env_wait_ms: 1.2222547130508945
    mean_inference_ms: 4.348512392911836
    mean_raw_obs_processing_ms: 0.3806614185533328
  time_since_restore: 2539.3083114624023
  time_this_iter_s: 26.794095039367676
  time_total_s: 2539.3083114624023
  timers:
    learn_throughput: 8374.867
    learn_time_ms: 19318.755
    sample_throughput: 23145.844
    sample_time_ms: 6990.11
    update_time_ms: 28.656
  timestamp: 1602636741
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     96 |          2539.31 | 15532032 |  268.449 |              318.768 |              138.768 |            774.466 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3142.3177403369673
    time_step_min: 2838
  date: 2020-10-14_00-52-48
  done: false
  episode_len_mean: 774.2631188486077
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.7547276514089
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 253
  episodes_total: 20219
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.19948154812057814
        entropy_coeff: 0.0005000000000000001
        kl: 0.004395933899407585
        model: {}
        policy_loss: -0.008341047330759466
        total_loss: 4.934412956237793
        vf_explained_var: 0.9891325831413269
        vf_loss: 4.942853768666585
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.254838709677422
    gpu_util_percent0: 0.39
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.858064516129034
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720619938970958
    mean_env_wait_ms: 1.2223117061967115
    mean_inference_ms: 4.347572359301956
    mean_raw_obs_processing_ms: 0.38060858781747436
  time_since_restore: 2565.9204144477844
  time_this_iter_s: 26.61210298538208
  time_total_s: 2565.9204144477844
  timers:
    learn_throughput: 8366.858
    learn_time_ms: 19337.247
    sample_throughput: 23065.324
    sample_time_ms: 7014.512
    update_time_ms: 30.671
  timestamp: 1602636768
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     97 |          2565.92 | 15693824 |  268.755 |              318.768 |              138.768 |            774.263 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3140.9621510027528
    time_step_min: 2838
  date: 2020-10-14_00-53-15
  done: false
  episode_len_mean: 774.1383505862728
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 268.9533484281068
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 164
  episodes_total: 20383
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.22233347470561662
        entropy_coeff: 0.0005000000000000001
        kl: 0.004189171517888705
        model: {}
        policy_loss: -0.008752714784350246
        total_loss: 4.05763824780782
        vf_explained_var: 0.9878931641578674
        vf_loss: 4.066502054532369
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.574193548387093
    gpu_util_percent0: 0.2887096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719588699711414
    mean_env_wait_ms: 1.222344086746792
    mean_inference_ms: 4.346959694993794
    mean_raw_obs_processing_ms: 0.38057416180990744
  time_since_restore: 2592.6393570899963
  time_this_iter_s: 26.718942642211914
  time_total_s: 2592.6393570899963
  timers:
    learn_throughput: 8356.843
    learn_time_ms: 19360.421
    sample_throughput: 23042.02
    sample_time_ms: 7021.607
    update_time_ms: 30.771
  timestamp: 1602636795
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     98 |          2592.64 | 15855616 |  268.953 |              318.768 |              138.768 |            774.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3139.0598211682377
    time_step_min: 2838
  date: 2020-10-14_00-53-42
  done: false
  episode_len_mean: 773.9408255323277
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 269.2442178980473
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 20617
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.22517057011524835
        entropy_coeff: 0.0005000000000000001
        kl: 0.004362859607984622
        model: {}
        policy_loss: -0.010387252984704295
        total_loss: 4.173368493715922
        vf_explained_var: 0.9902940392494202
        vf_loss: 4.183868249257405
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7258064516129
    gpu_util_percent0: 0.3767741935483872
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14718241254268216
    mean_env_wait_ms: 1.222383103945381
    mean_inference_ms: 4.346141318473105
    mean_raw_obs_processing_ms: 0.38052343228246993
  time_since_restore: 2619.388603448868
  time_this_iter_s: 26.74924635887146
  time_total_s: 2619.388603448868
  timers:
    learn_throughput: 8344.547
    learn_time_ms: 19388.948
    sample_throughput: 23049.616
    sample_time_ms: 7019.293
    update_time_ms: 33.114
  timestamp: 1602636822
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |     99 |          2619.39 | 16017408 |  269.244 |              318.768 |              138.768 |            773.941 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3137.207936965504
    time_step_min: 2837
  date: 2020-10-14_00-54-09
  done: false
  episode_len_mean: 773.7462235649547
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 269.51988304858395
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 236
  episodes_total: 20853
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.1935069573422273
        entropy_coeff: 0.0005000000000000001
        kl: 0.004311881765412788
        model: {}
        policy_loss: -0.008507055084919557
        total_loss: 3.6038972536722818
        vf_explained_var: 0.9915869832038879
        vf_loss: 3.612501045068105
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.70645161290323
    gpu_util_percent0: 0.3312903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147168721769833
    mean_env_wait_ms: 1.2224320539189673
    mean_inference_ms: 4.345305147893856
    mean_raw_obs_processing_ms: 0.3804776917042084
  time_since_restore: 2646.0061955451965
  time_this_iter_s: 26.617592096328735
  time_total_s: 2646.0061955451965
  timers:
    learn_throughput: 8337.243
    learn_time_ms: 19405.936
    sample_throughput: 23033.532
    sample_time_ms: 7024.194
    update_time_ms: 32.552
  timestamp: 1602636849
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    100 |          2646.01 | 16179200 |   269.52 |              318.768 |              138.768 |            773.746 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3135.9166785833454
    time_step_min: 2837
  date: 2020-10-14_00-54-36
  done: false
  episode_len_mean: 773.6080026643829
  episode_reward_max: 318.7676767676766
  episode_reward_mean: 269.7113412169079
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 21018
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.21861098458369574
        entropy_coeff: 0.0005000000000000001
        kl: 0.004833029815927148
        model: {}
        policy_loss: -0.01079400188367193
        total_loss: 3.533422271410624
        vf_explained_var: 0.9896146655082703
        vf_loss: 3.544325590133667
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.661290322580648
    gpu_util_percent0: 0.3145161290322581
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471592365983797
    mean_env_wait_ms: 1.222460405169749
    mean_inference_ms: 4.3447262951132855
    mean_raw_obs_processing_ms: 0.38044476966693314
  time_since_restore: 2672.364652633667
  time_this_iter_s: 26.35845708847046
  time_total_s: 2672.364652633667
  timers:
    learn_throughput: 8338.983
    learn_time_ms: 19401.887
    sample_throughput: 23054.981
    sample_time_ms: 7017.659
    update_time_ms: 32.726
  timestamp: 1602636876
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    101 |          2672.36 | 16340992 |  269.711 |              318.768 |              138.768 |            773.608 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3133.961427965902
    time_step_min: 2833
  date: 2020-10-14_00-55-03
  done: false
  episode_len_mean: 773.4100695750282
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.0156111699925
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 254
  episodes_total: 21272
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21355180690685907
        entropy_coeff: 0.0005000000000000001
        kl: 0.005046109048028787
        model: {}
        policy_loss: -0.010105241177370772
        total_loss: 5.040608127911885
        vf_explained_var: 0.9889450669288635
        vf_loss: 5.0508198738098145
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.935483870967747
    gpu_util_percent0: 0.36451612903225805
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8677419354838722
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714530478705462
    mean_env_wait_ms: 1.2225001577093213
    mean_inference_ms: 4.343909007026651
    mean_raw_obs_processing_ms: 0.38039438613340176
  time_since_restore: 2698.832222700119
  time_this_iter_s: 26.467570066452026
  time_total_s: 2698.832222700119
  timers:
    learn_throughput: 8339.369
    learn_time_ms: 19400.989
    sample_throughput: 23053.088
    sample_time_ms: 7018.235
    update_time_ms: 31.059
  timestamp: 1602636903
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    102 |          2698.83 | 16502784 |  270.016 |              319.374 |              138.768 |             773.41 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3132.2652461768
    time_step_min: 2833
  date: 2020-10-14_00-55-30
  done: false
  episode_len_mean: 773.248801600968
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.27392226354385
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 215
  episodes_total: 21487
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.18353131289283434
        entropy_coeff: 0.0005000000000000001
        kl: 0.004030068579595536
        model: {}
        policy_loss: -0.006411568048254897
        total_loss: 3.7287562092145285
        vf_explained_var: 0.9905366897583008
        vf_loss: 3.735259552796682
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.832258064516132
    gpu_util_percent0: 0.31387096774193546
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14713332862402678
    mean_env_wait_ms: 1.2225336954355355
    mean_inference_ms: 4.343146156338655
    mean_raw_obs_processing_ms: 0.380352226791261
  time_since_restore: 2725.3844232559204
  time_this_iter_s: 26.55220055580139
  time_total_s: 2725.3844232559204
  timers:
    learn_throughput: 8336.068
    learn_time_ms: 19408.67
    sample_throughput: 23065.919
    sample_time_ms: 7014.331
    update_time_ms: 29.223
  timestamp: 1602636930
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    103 |          2725.38 | 16664576 |  270.274 |              319.374 |              138.768 |            773.249 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3130.8655411655873
    time_step_min: 2823
  date: 2020-10-14_00-55-57
  done: false
  episode_len_mean: 773.1197654554688
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.47427364741185
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 172
  episodes_total: 21659
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.21097562834620476
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045883768470957875
        model: {}
        policy_loss: -0.010255051990194866
        total_loss: 3.559800664583842
        vf_explained_var: 0.9896824359893799
        vf_loss: 3.570161203543345
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4741935483871
    gpu_util_percent0: 0.32000000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14712420157495648
    mean_env_wait_ms: 1.2225611801370988
    mean_inference_ms: 4.34258313004672
    mean_raw_obs_processing_ms: 0.38031982094691086
  time_since_restore: 2751.7664000988007
  time_this_iter_s: 26.38197684288025
  time_total_s: 2751.7664000988007
  timers:
    learn_throughput: 8341.19
    learn_time_ms: 19396.752
    sample_throughput: 23092.193
    sample_time_ms: 7006.351
    update_time_ms: 26.388
  timestamp: 1602636957
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    104 |          2751.77 | 16826368 |  270.474 |              319.374 |              138.768 |             773.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3128.907271148485
    time_step_min: 2812
  date: 2020-10-14_00-56-24
  done: false
  episode_len_mean: 772.9262773722628
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 270.77272957678974
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 261
  episodes_total: 21920
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.1978634881476561
        entropy_coeff: 0.0005000000000000001
        kl: 0.004231312137562782
        model: {}
        policy_loss: -0.006590141090176378
        total_loss: 5.831996877988179
        vf_explained_var: 0.9878156185150146
        vf_loss: 5.838685909907023
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.096774193548384
    gpu_util_percent0: 0.34096774193548396
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8612903225806465
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471110261024631
    mean_env_wait_ms: 1.222593247563123
    mean_inference_ms: 4.341785081952479
    mean_raw_obs_processing_ms: 0.3802714566030208
  time_since_restore: 2778.3363099098206
  time_this_iter_s: 26.569909811019897
  time_total_s: 2778.3363099098206
  timers:
    learn_throughput: 8345.474
    learn_time_ms: 19386.797
    sample_throughput: 23017.983
    sample_time_ms: 7028.939
    update_time_ms: 25.956
  timestamp: 1602636984
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    105 |          2778.34 | 16988160 |  270.773 |              319.374 |              138.768 |            772.926 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3127.2984601449275
    time_step_min: 2810
  date: 2020-10-14_00-56-51
  done: false
  episode_len_mean: 772.7865183778651
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 271.01216834012166
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 199
  episodes_total: 22119
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.17703540747364363
        entropy_coeff: 0.0005000000000000001
        kl: 0.004091060778591782
        model: {}
        policy_loss: -0.010252801740231613
        total_loss: 4.546480933825175
        vf_explained_var: 0.9877767562866211
        vf_loss: 4.5568223396937055
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7483870967742
    gpu_util_percent0: 0.3625806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710010118008957
    mean_env_wait_ms: 1.2226183150193202
    mean_inference_ms: 4.341115395548795
    mean_raw_obs_processing_ms: 0.38023421821713704
  time_since_restore: 2804.803294658661
  time_this_iter_s: 26.466984748840332
  time_total_s: 2804.803294658661
  timers:
    learn_throughput: 8360.677
    learn_time_ms: 19351.543
    sample_throughput: 23036.569
    sample_time_ms: 7023.268
    update_time_ms: 33.41
  timestamp: 1602637011
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    106 |           2804.8 | 17149952 |  271.012 |              319.374 |              138.768 |            772.787 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3125.9276859504134
    time_step_min: 2810
  date: 2020-10-14_00-57-18
  done: false
  episode_len_mean: 772.6478052279963
  episode_reward_max: 319.3737373737373
  episode_reward_mean: 271.22524804155074
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 184
  episodes_total: 22303
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.20374412337938944
        entropy_coeff: 0.0005000000000000001
        kl: 0.004298495904852946
        model: {}
        policy_loss: -0.00846323888496651
        total_loss: 5.233591516812642
        vf_explained_var: 0.9859102368354797
        vf_loss: 5.242156624794006
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.799999999999997
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709027054599516
    mean_env_wait_ms: 1.2226412390835562
    mean_inference_ms: 4.340547075740324
    mean_raw_obs_processing_ms: 0.3802013049416009
  time_since_restore: 2831.441397190094
  time_this_iter_s: 26.638102531433105
  time_total_s: 2831.441397190094
  timers:
    learn_throughput: 8354.648
    learn_time_ms: 19365.507
    sample_throughput: 23070.437
    sample_time_ms: 7012.958
    update_time_ms: 32.202
  timestamp: 1602637038
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    107 |          2831.44 | 17311744 |  271.225 |              319.374 |              138.768 |            772.648 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3123.8324605006214
    time_step_min: 2804
  date: 2020-10-14_00-57-45
  done: false
  episode_len_mean: 772.4519073146959
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 271.53983143651294
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 268
  episodes_total: 22571
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.18263191108902296
        entropy_coeff: 0.0005000000000000001
        kl: 0.004251166867713134
        model: {}
        policy_loss: -0.006676604227929299
        total_loss: 5.817436297734578
        vf_explained_var: 0.9875534176826477
        vf_loss: 5.824204405148824
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.529032258064515
    gpu_util_percent0: 0.31129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8612903225806465
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707735845754255
    mean_env_wait_ms: 1.2226667711938022
    mean_inference_ms: 4.339740740728017
    mean_raw_obs_processing_ms: 0.38015338337393234
  time_since_restore: 2858.201281785965
  time_this_iter_s: 26.75988459587097
  time_total_s: 2858.201281785965
  timers:
    learn_throughput: 8359.668
    learn_time_ms: 19353.879
    sample_throughput: 23026.863
    sample_time_ms: 7026.228
    update_time_ms: 33.847
  timestamp: 1602637065
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    108 |           2858.2 | 17473536 |   271.54 |               319.98 |              138.768 |            772.452 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3122.5004622903184
    time_step_min: 2804
  date: 2020-10-14_00-58-12
  done: false
  episode_len_mean: 772.3194883966245
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 271.7522162553808
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 181
  episodes_total: 22752
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.17062795783082643
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039137817414787906
        model: {}
        policy_loss: -0.010251026911040148
        total_loss: 2.7435816526412964
        vf_explained_var: 0.9920342564582825
        vf_loss: 2.7539179921150208
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.31
    gpu_util_percent0: 0.3026666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470682721200986
    mean_env_wait_ms: 1.2226858417986184
    mean_inference_ms: 4.339178138045057
    mean_raw_obs_processing_ms: 0.38012203150622287
  time_since_restore: 2884.490921020508
  time_this_iter_s: 26.289639234542847
  time_total_s: 2884.490921020508
  timers:
    learn_throughput: 8372.474
    learn_time_ms: 19324.277
    sample_throughput: 23076.608
    sample_time_ms: 7011.082
    update_time_ms: 31.566
  timestamp: 1602637092
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    109 |          2884.49 | 17635328 |  271.752 |               319.98 |              138.768 |            772.319 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3120.9862529457973
    time_step_min: 2804
  date: 2020-10-14_00-58-38
  done: false
  episode_len_mean: 772.1804992811398
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 271.97904149322255
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 22953
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.19461134945352873
        entropy_coeff: 0.0005000000000000001
        kl: 0.004375814266192417
        model: {}
        policy_loss: -0.008800105351838283
        total_loss: 4.470666786034902
        vf_explained_var: 0.9886862635612488
        vf_loss: 4.4795641501744585
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.690322580645166
    gpu_util_percent0: 0.29064516129032264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470594770856951
    mean_env_wait_ms: 1.2227046617032096
    mean_inference_ms: 4.338615111899147
    mean_raw_obs_processing_ms: 0.38008854639068784
  time_since_restore: 2910.860347509384
  time_this_iter_s: 26.369426488876343
  time_total_s: 2910.860347509384
  timers:
    learn_throughput: 8377.051
    learn_time_ms: 19313.719
    sample_throughput: 23127.52
    sample_time_ms: 6995.649
    update_time_ms: 31.365
  timestamp: 1602637118
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    110 |          2910.86 | 17797120 |  271.979 |               319.98 |              138.768 |             772.18 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3118.906972730411
    time_step_min: 2804
  date: 2020-10-14_00-59-05
  done: false
  episode_len_mean: 772.0018953262977
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.2870248902986
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 262
  episodes_total: 23215
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.1695228231449922
        entropy_coeff: 0.0005000000000000001
        kl: 0.004864651942625642
        model: {}
        policy_loss: -0.008603686078762015
        total_loss: 3.592876652876536
        vf_explained_var: 0.9920200705528259
        vf_loss: 3.601564943790436
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92258064516129
    gpu_util_percent0: 0.28032258064516136
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8612903225806465
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14704659782047177
    mean_env_wait_ms: 1.2227226288078723
    mean_inference_ms: 4.337834912603506
    mean_raw_obs_processing_ms: 0.3800420501238155
  time_since_restore: 2937.4311184883118
  time_this_iter_s: 26.570770978927612
  time_total_s: 2937.4311184883118
  timers:
    learn_throughput: 8372.571
    learn_time_ms: 19324.052
    sample_throughput: 23099.169
    sample_time_ms: 7004.235
    update_time_ms: 32.896
  timestamp: 1602637145
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    111 |          2937.43 | 17958912 |  272.287 |               319.98 |              138.768 |            772.002 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3117.6138953139725
    time_step_min: 2804
  date: 2020-10-14_00-59-32
  done: false
  episode_len_mean: 771.8905708787685
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.47965867786263
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 23385
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.1722496673464775
        entropy_coeff: 0.0005000000000000001
        kl: 0.004023686614042769
        model: {}
        policy_loss: -0.010478878810924167
        total_loss: 2.8431402444839478
        vf_explained_var: 0.9914758205413818
        vf_loss: 2.853705187638601
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.95161290322581
    gpu_util_percent0: 0.2525806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14703812600851998
    mean_env_wait_ms: 1.2227360157866598
    mean_inference_ms: 4.337343427034162
    mean_raw_obs_processing_ms: 0.38001484301993477
  time_since_restore: 2963.868054628372
  time_this_iter_s: 26.436936140060425
  time_total_s: 2963.868054628372
  timers:
    learn_throughput: 8381.842
    learn_time_ms: 19302.678
    sample_throughput: 23041.124
    sample_time_ms: 7021.88
    update_time_ms: 32.937
  timestamp: 1602637172
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    112 |          2963.87 | 18120704 |   272.48 |               319.98 |              138.768 |            771.891 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3116.1105169340462
    time_step_min: 2804
  date: 2020-10-14_00-59-59
  done: false
  episode_len_mean: 771.7591627473412
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.7119369620958
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 216
  episodes_total: 23601
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.1865410308043162
        entropy_coeff: 0.0005000000000000001
        kl: 0.004090183802569906
        model: {}
        policy_loss: -0.00852372680674307
        total_loss: 4.244714816411336
        vf_explained_var: 0.9897566437721252
        vf_loss: 4.253331899642944
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.735483870967744
    gpu_util_percent0: 0.33806451612903227
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702798486630106
    mean_env_wait_ms: 1.2227446972587792
    mean_inference_ms: 4.336736096053146
    mean_raw_obs_processing_ms: 0.37997681515572057
  time_since_restore: 2990.5611605644226
  time_this_iter_s: 26.693105936050415
  time_total_s: 2990.5611605644226
  timers:
    learn_throughput: 8377.775
    learn_time_ms: 19312.048
    sample_throughput: 23032.437
    sample_time_ms: 7024.528
    update_time_ms: 34.398
  timestamp: 1602637199
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    113 |          2990.56 | 18282496 |  272.712 |               319.98 |              138.768 |            771.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3114.2817788602865
    time_step_min: 2804
  date: 2020-10-14_01-00-27
  done: false
  episode_len_mean: 771.5973503270166
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 272.9948046624216
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 251
  episodes_total: 23852
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.15399332592884699
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035449591426489255
        model: {}
        policy_loss: -0.008280562309664674
        total_loss: 4.150957663853963
        vf_explained_var: 0.9905694127082825
        vf_loss: 4.159315228462219
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.209374999999998
    gpu_util_percent0: 0.27749999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8656249999999996
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470170353146866
    mean_env_wait_ms: 1.2227630704845536
    mean_inference_ms: 4.3360558095253445
    mean_raw_obs_processing_ms: 0.3799389658648159
  time_since_restore: 3017.376530647278
  time_this_iter_s: 26.815370082855225
  time_total_s: 3017.376530647278
  timers:
    learn_throughput: 8374.142
    learn_time_ms: 19320.427
    sample_throughput: 22946.305
    sample_time_ms: 7050.896
    update_time_ms: 33.468
  timestamp: 1602637227
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    114 |          3017.38 | 18444288 |  272.995 |               319.98 |              138.768 |            771.597 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3113.0023771790807
    time_step_min: 2804
  date: 2020-10-14_01-00-54
  done: false
  episode_len_mean: 771.4951076320939
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.1791290092076
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 24017
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.16551268845796585
        entropy_coeff: 0.0005000000000000001
        kl: 0.004297597178568442
        model: {}
        policy_loss: -0.011308001344635462
        total_loss: 2.8338273962338767
        vf_explained_var: 0.9913229942321777
        vf_loss: 2.8452181617418923
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.59032258064516
    gpu_util_percent0: 0.3248387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14700926882325394
    mean_env_wait_ms: 1.2227704727126734
    mean_inference_ms: 4.335599926421999
    mean_raw_obs_processing_ms: 0.37991339319863454
  time_since_restore: 3043.9758298397064
  time_this_iter_s: 26.59929919242859
  time_total_s: 3043.9758298397064
  timers:
    learn_throughput: 8371.029
    learn_time_ms: 19327.611
    sample_throughput: 22971.788
    sample_time_ms: 7043.074
    update_time_ms: 36.626
  timestamp: 1602637254
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    115 |          3043.98 | 18606080 |  273.179 |               319.98 |              138.768 |            771.495 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3111.1840138722596
    time_step_min: 2804
  date: 2020-10-14_01-01-21
  done: false
  episode_len_mean: 771.3350783182193
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.45406663502285
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 243
  episodes_total: 24260
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.17203684026996294
        entropy_coeff: 0.0005000000000000001
        kl: 0.004602224294406672
        model: {}
        policy_loss: -0.00808883226030351
        total_loss: 3.334753175576528
        vf_explained_var: 0.9921055436134338
        vf_loss: 3.342927932739258
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.966666666666672
    gpu_util_percent0: 0.28933333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699876794648148
    mean_env_wait_ms: 1.2227782917551608
    mean_inference_ms: 4.334981427150896
    mean_raw_obs_processing_ms: 0.37987422483622163
  time_since_restore: 3070.3465991020203
  time_this_iter_s: 26.370769262313843
  time_total_s: 3070.3465991020203
  timers:
    learn_throughput: 8369.71
    learn_time_ms: 19330.658
    sample_throughput: 22990.779
    sample_time_ms: 7037.256
    update_time_ms: 29.053
  timestamp: 1602637281
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    116 |          3070.35 | 18767872 |  273.454 |               319.98 |              138.768 |            771.335 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3109.5765769451036
    time_step_min: 2804
  date: 2020-10-14_01-01-47
  done: false
  episode_len_mean: 771.1866857259547
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.70095069543703
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 225
  episodes_total: 24485
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.1411927379667759
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031797124538570642
        model: {}
        policy_loss: -0.00962009271218752
        total_loss: 2.685124695301056
        vf_explained_var: 0.9932982921600342
        vf_loss: 2.6948153177897134
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.425806451612907
    gpu_util_percent0: 0.2938709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698912035442838
    mean_env_wait_ms: 1.2227883166267648
    mean_inference_ms: 4.334375657528333
    mean_raw_obs_processing_ms: 0.3798413776028127
  time_since_restore: 3096.619740009308
  time_this_iter_s: 26.273140907287598
  time_total_s: 3096.619740009308
  timers:
    learn_throughput: 8376.892
    learn_time_ms: 19314.085
    sample_throughput: 23063.361
    sample_time_ms: 7015.11
    update_time_ms: 30.65
  timestamp: 1602637307
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    117 |          3096.62 | 18929664 |  273.701 |               319.98 |              138.768 |            771.187 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3108.3202925045703
    time_step_min: 2804
  date: 2020-10-14_01-02-14
  done: false
  episode_len_mean: 771.0825829480003
  episode_reward_max: 319.97979797979764
  episode_reward_mean: 273.89194819944385
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 169
  episodes_total: 24654
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.16122792412837347
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038139744816968837
        model: {}
        policy_loss: -0.009541969523221875
        total_loss: 3.4975943168004355
        vf_explained_var: 0.9894265532493591
        vf_loss: 3.5072169502576194
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206451612903226
    gpu_util_percent0: 0.34354838709677415
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698160232081836
    mean_env_wait_ms: 1.2227906064502627
    mean_inference_ms: 4.333921584035553
    mean_raw_obs_processing_ms: 0.3798154249161094
  time_since_restore: 3122.8776450157166
  time_this_iter_s: 26.25790500640869
  time_total_s: 3122.8776450157166
  timers:
    learn_throughput: 8386.496
    learn_time_ms: 19291.966
    sample_throughput: 23151.855
    sample_time_ms: 6988.295
    update_time_ms: 28.944
  timestamp: 1602637334
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    118 |          3122.88 | 19091456 |  273.892 |               319.98 |              138.768 |            771.083 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3106.541953121859
    time_step_min: 2801
  date: 2020-10-14_01-02-41
  done: false
  episode_len_mean: 770.9273442517663
  episode_reward_max: 321.0404040404039
  episode_reward_mean: 274.1637254043323
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 258
  episodes_total: 24912
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.16436305890480676
        entropy_coeff: 0.0005000000000000001
        kl: 0.004030725064997871
        model: {}
        policy_loss: -0.008017327233877344
        total_loss: 4.7343523899714155
        vf_explained_var: 0.9895167350769043
        vf_loss: 4.742451747258504
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.190322580645166
    gpu_util_percent0: 0.32516129032258057
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.861290322580646
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697120383193008
    mean_env_wait_ms: 1.2227941622772052
    mean_inference_ms: 4.333312318893969
    mean_raw_obs_processing_ms: 0.3797772225645136
  time_since_restore: 3149.3540716171265
  time_this_iter_s: 26.476426601409912
  time_total_s: 3149.3540716171265
  timers:
    learn_throughput: 8388.409
    learn_time_ms: 19287.567
    sample_throughput: 23110.37
    sample_time_ms: 7000.84
    update_time_ms: 30.413
  timestamp: 1602637361
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    119 |          3149.35 | 19253248 |  274.164 |               321.04 |              138.768 |            770.927 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3105.0814593301434
    time_step_min: 2801
  date: 2020-10-14_01-03-08
  done: false
  episode_len_mean: 770.8063219077193
  episode_reward_max: 321.0404040404039
  episode_reward_mean: 274.3921165554988
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 207
  episodes_total: 25119
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.13733991732199988
        entropy_coeff: 0.0005000000000000001
        kl: 0.00398658006452024
        model: {}
        policy_loss: -0.008998860430438071
        total_loss: 3.066338857014974
        vf_explained_var: 0.9917945265769958
        vf_loss: 3.0754063924153647
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.11333333333334
    gpu_util_percent0: 0.2819999999999999
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696232682278534
    mean_env_wait_ms: 1.2227975509743578
    mean_inference_ms: 4.332765459203745
    mean_raw_obs_processing_ms: 0.3797470527545745
  time_since_restore: 3175.592981815338
  time_this_iter_s: 26.23891019821167
  time_total_s: 3175.592981815338
  timers:
    learn_throughput: 8392.412
    learn_time_ms: 19278.368
    sample_throughput: 23127.188
    sample_time_ms: 6995.749
    update_time_ms: 30.576
  timestamp: 1602637388
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    120 |          3175.59 | 19415040 |  274.392 |               321.04 |              138.768 |            770.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3103.7490300102936
    time_step_min: 2801
  date: 2020-10-14_01-03-34
  done: false
  episode_len_mean: 770.6989761631814
  episode_reward_max: 321.0404040404039
  episode_reward_mean: 274.58665638078213
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 25297
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.15856833880146345
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043513024575077
        model: {}
        policy_loss: -0.010370379663072526
        total_loss: 3.0508464574813843
        vf_explained_var: 0.9910359978675842
        vf_loss: 3.0612961451212564
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45483870967742
    gpu_util_percent0: 0.3461290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14695473012431015
    mean_env_wait_ms: 1.2227978497185077
    mean_inference_ms: 4.332317531702417
    mean_raw_obs_processing_ms: 0.3797201727077129
  time_since_restore: 3201.854770421982
  time_this_iter_s: 26.261788606643677
  time_total_s: 3201.854770421982
  timers:
    learn_throughput: 8397.844
    learn_time_ms: 19265.898
    sample_throughput: 23185.336
    sample_time_ms: 6978.204
    update_time_ms: 28.376
  timestamp: 1602637414
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    121 |          3201.85 | 19576832 |  274.587 |               321.04 |              138.768 |            770.699 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3101.7473750195895
    time_step_min: 2792
  date: 2020-10-14_01-04-01
  done: false
  episode_len_mean: 770.5444979071314
  episode_reward_max: 321.7979797979798
  episode_reward_mean: 274.8853492085507
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 266
  episodes_total: 25563
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.15092444668213525
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040728193513738615
        model: {}
        policy_loss: -0.009207504287284488
        total_loss: 3.7745024959246316
        vf_explained_var: 0.9915598034858704
        vf_loss: 3.7837854425112405
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.070967741935487
    gpu_util_percent0: 0.3787096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.864516129032259
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694429748557436
    mean_env_wait_ms: 1.222795321427813
    mean_inference_ms: 4.3316991662165965
    mean_raw_obs_processing_ms: 0.3796832717100607
  time_since_restore: 3228.1759989261627
  time_this_iter_s: 26.321228504180908
  time_total_s: 3228.1759989261627
  timers:
    learn_throughput: 8390.231
    learn_time_ms: 19283.38
    sample_throughput: 23292.489
    sample_time_ms: 6946.102
    update_time_ms: 30.345
  timestamp: 1602637441
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    122 |          3228.18 | 19738624 |  274.885 |              321.798 |              138.768 |            770.544 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3100.3683064359325
    time_step_min: 2792
  date: 2020-10-14_01-04-28
  done: false
  episode_len_mean: 770.4423778830472
  episode_reward_max: 321.7979797979798
  episode_reward_mean: 275.09578466971493
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 191
  episodes_total: 25754
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.13362999260425568
        entropy_coeff: 0.0005000000000000001
        kl: 0.003501818204919497
        model: {}
        policy_loss: -0.008884420075143376
        total_loss: 2.953603208065033
        vf_explained_var: 0.9916164875030518
        vf_loss: 2.962554454803467
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.367741935483874
    gpu_util_percent0: 0.2993548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469364842530337
    mean_env_wait_ms: 1.2227935400796446
    mean_inference_ms: 4.331213911496438
    mean_raw_obs_processing_ms: 0.37965571965438705
  time_since_restore: 3254.7071895599365
  time_this_iter_s: 26.531190633773804
  time_total_s: 3254.7071895599365
  timers:
    learn_throughput: 8390.845
    learn_time_ms: 19281.967
    sample_throughput: 23347.935
    sample_time_ms: 6929.606
    update_time_ms: 31.021
  timestamp: 1602637468
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    123 |          3254.71 | 19900416 |  275.096 |              321.798 |              138.768 |            770.442 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3098.9696583671107
    time_step_min: 2792
  date: 2020-10-14_01-04-55
  done: false
  episode_len_mean: 770.3282839962998
  episode_reward_max: 321.7979797979798
  episode_reward_mean: 275.30651566544253
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 190
  episodes_total: 25944
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.1557726909716924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004061012587044388
        model: {}
        policy_loss: -0.008608610582693169
        total_loss: 2.6421515742937722
        vf_explained_var: 0.9926905632019043
        vf_loss: 2.6508379379908242
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.393548387096775
    gpu_util_percent0: 0.25322580645161286
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692945698345813
    mean_env_wait_ms: 1.2227901383593198
    mean_inference_ms: 4.330781522265667
    mean_raw_obs_processing_ms: 0.3796293402575095
  time_since_restore: 3280.995648622513
  time_this_iter_s: 26.288459062576294
  time_total_s: 3280.995648622513
  timers:
    learn_throughput: 8392.078
    learn_time_ms: 19279.134
    sample_throughput: 23493.153
    sample_time_ms: 6886.772
    update_time_ms: 31.447
  timestamp: 1602637495
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    124 |             3281 | 20062208 |  275.307 |              321.798 |              138.768 |            770.328 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3097.0838746656477
    time_step_min: 2792
  date: 2020-10-14_01-05-22
  done: false
  episode_len_mean: 770.1792514021901
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 275.59222350561197
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 265
  episodes_total: 26209
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.13781721393267313
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036383712431415915
        model: {}
        policy_loss: -0.008307600607319424
        total_loss: 3.5946669379870095
        vf_explained_var: 0.9922294616699219
        vf_loss: 3.6030433972676597
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.336666666666662
    gpu_util_percent0: 0.308
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8666666666666676
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469191176034615
    mean_env_wait_ms: 1.2227803988844106
    mean_inference_ms: 4.330169143323068
    mean_raw_obs_processing_ms: 0.37959183229511534
  time_since_restore: 3307.3059923648834
  time_this_iter_s: 26.310343742370605
  time_total_s: 3307.3059923648834
  timers:
    learn_throughput: 8390.361
    learn_time_ms: 19283.081
    sample_throughput: 23599.511
    sample_time_ms: 6855.735
    update_time_ms: 28.336
  timestamp: 1602637522
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    125 |          3307.31 | 20224000 |  275.592 |              323.616 |              138.768 |            770.179 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3095.8419614391983
    time_step_min: 2792
  date: 2020-10-14_01-05-49
  done: false
  episode_len_mean: 770.0779929510744
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 275.7806342501836
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 178
  episodes_total: 26387
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.1308093493183454
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037285079791521034
        model: {}
        policy_loss: -0.007945271533875106
        total_loss: 2.71152796347936
        vf_explained_var: 0.9917721748352051
        vf_loss: 2.719538609186808
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20967741935484
    gpu_util_percent0: 0.3138709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691222534880755
    mean_env_wait_ms: 1.2227759719652134
    mean_inference_ms: 4.329751099239449
    mean_raw_obs_processing_ms: 0.3795679075153772
  time_since_restore: 3333.8045535087585
  time_this_iter_s: 26.498561143875122
  time_total_s: 3333.8045535087585
  timers:
    learn_throughput: 8381.321
    learn_time_ms: 19303.879
    sample_throughput: 23639.586
    sample_time_ms: 6844.113
    update_time_ms: 30.539
  timestamp: 1602637549
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    126 |           3333.8 | 20385792 |  275.781 |              323.616 |              138.768 |            770.078 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3094.3565404021388
    time_step_min: 2792
  date: 2020-10-14_01-06-16
  done: false
  episode_len_mean: 769.9681919013423
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 275.9993194341428
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 210
  episodes_total: 26597
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.15123373518387476
        entropy_coeff: 0.0005000000000000001
        kl: 0.00409364519873634
        model: {}
        policy_loss: -0.00796691418994063
        total_loss: 3.045759856700897
        vf_explained_var: 0.9921451210975647
        vf_loss: 3.0538024504979453
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.319354838709682
    gpu_util_percent0: 0.333225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690410155496222
    mean_env_wait_ms: 1.222763666928999
    mean_inference_ms: 4.329282469754693
    mean_raw_obs_processing_ms: 0.3795368275958223
  time_since_restore: 3360.2355823516846
  time_this_iter_s: 26.431028842926025
  time_total_s: 3360.2355823516846
  timers:
    learn_throughput: 8385.422
    learn_time_ms: 19294.436
    sample_throughput: 23563.528
    sample_time_ms: 6866.204
    update_time_ms: 31.839
  timestamp: 1602637576
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    127 |          3360.24 | 20547584 |  275.999 |              323.616 |              138.768 |            769.968 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3092.535434539351
    time_step_min: 2792
  date: 2020-10-14_01-06-43
  done: false
  episode_len_mean: 769.8398823047413
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.2735711241055
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 252
  episodes_total: 26849
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.13039244463046393
        entropy_coeff: 0.0005000000000000001
        kl: 0.004497993310603003
        model: {}
        policy_loss: -0.007785009889630601
        total_loss: 2.6508864561716714
        vf_explained_var: 0.9937828183174133
        vf_loss: 2.6587366461753845
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.038709677419355
    gpu_util_percent0: 0.3083870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.864516129032259
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468952140217954
    mean_env_wait_ms: 1.2227531730001182
    mean_inference_ms: 4.328725209946512
    mean_raw_obs_processing_ms: 0.37950516417260405
  time_since_restore: 3386.973261833191
  time_this_iter_s: 26.737679481506348
  time_total_s: 3386.973261833191
  timers:
    learn_throughput: 8373.642
    learn_time_ms: 19321.581
    sample_throughput: 23500.851
    sample_time_ms: 6884.517
    update_time_ms: 33.556
  timestamp: 1602637603
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    128 |          3386.97 | 20709376 |  276.274 |              323.616 |              138.768 |             769.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3091.328799110452
    time_step_min: 2792
  date: 2020-10-14_01-07-10
  done: false
  episode_len_mean: 769.7559865279989
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.45577392040985
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 27019
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.12892911210656166
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038845023179116347
        model: {}
        policy_loss: -0.009248940439041084
        total_loss: 1.9274434347947438
        vf_explained_var: 0.9939970970153809
        vf_loss: 1.936756859223048
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.2
    gpu_util_percent0: 0.3674193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688894046886986
    mean_env_wait_ms: 1.2227456155901626
    mean_inference_ms: 4.328350486919382
    mean_raw_obs_processing_ms: 0.3794836811809465
  time_since_restore: 3413.3919451236725
  time_this_iter_s: 26.418683290481567
  time_total_s: 3413.3919451236725
  timers:
    learn_throughput: 8364.58
    learn_time_ms: 19342.514
    sample_throughput: 23564.827
    sample_time_ms: 6865.826
    update_time_ms: 32.218
  timestamp: 1602637630
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    129 |          3413.39 | 20871168 |  276.456 |              323.616 |              138.768 |            769.756 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3089.693528828134
    time_step_min: 2792
  date: 2020-10-14_01-07-37
  done: false
  episode_len_mean: 769.6424849552326
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.69739780010576
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 233
  episodes_total: 27252
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.14697920406858125
        entropy_coeff: 0.0005000000000000001
        kl: 0.004123337586255123
        model: {}
        policy_loss: -0.007596952326518173
        total_loss: 3.160941501458486
        vf_explained_var: 0.9924089908599854
        vf_loss: 3.168611983458201
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206666666666663
    gpu_util_percent0: 0.32466666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688079386907443
    mean_env_wait_ms: 1.2227289368692362
    mean_inference_ms: 4.327856372611356
    mean_raw_obs_processing_ms: 0.3794512108768622
  time_since_restore: 3439.6586968898773
  time_this_iter_s: 26.266751766204834
  time_total_s: 3439.6586968898773
  timers:
    learn_throughput: 8363.397
    learn_time_ms: 19345.249
    sample_throughput: 23573.624
    sample_time_ms: 6863.264
    update_time_ms: 34.307
  timestamp: 1602637657
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    130 |          3439.66 | 21032960 |  276.697 |              323.616 |              138.768 |            769.642 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3088.0417167632163
    time_step_min: 2789
  date: 2020-10-14_01-08-04
  done: false
  episode_len_mean: 769.526486211162
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 276.9462977295328
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 234
  episodes_total: 27486
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.12067394703626633
        entropy_coeff: 0.0005000000000000001
        kl: 0.003435036613761137
        model: {}
        policy_loss: -0.008166467906751981
        total_loss: 2.307787001132965
        vf_explained_var: 0.9942525029182434
        vf_loss: 2.3160137931505838
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.654838709677424
    gpu_util_percent0: 0.32161290322580655
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687240547774189
    mean_env_wait_ms: 1.2227140101060239
    mean_inference_ms: 4.327355074841223
    mean_raw_obs_processing_ms: 0.3794223732696783
  time_since_restore: 3465.9895095825195
  time_this_iter_s: 26.330812692642212
  time_total_s: 3465.9895095825195
  timers:
    learn_throughput: 8359.548
    learn_time_ms: 19354.156
    sample_throughput: 23589.455
    sample_time_ms: 6858.658
    update_time_ms: 36.074
  timestamp: 1602637684
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    131 |          3465.99 | 21194752 |  276.946 |              323.616 |              138.768 |            769.526 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3086.855171664494
    time_step_min: 2789
  date: 2020-10-14_01-08-31
  done: false
  episode_len_mean: 769.4495316625076
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.1201008676326
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 165
  episodes_total: 27651
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.12907345096270242
        entropy_coeff: 0.0005000000000000001
        kl: 0.004641744385783871
        model: {}
        policy_loss: -0.008278223656816408
        total_loss: 1.972252627213796
        vf_explained_var: 0.9937462210655212
        vf_loss: 1.9805953900019329
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.812903225806455
    gpu_util_percent0: 0.34580645161290324
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686658849656767
    mean_env_wait_ms: 1.222703037032418
    mean_inference_ms: 4.327008763107837
    mean_raw_obs_processing_ms: 0.37940207436153495
  time_since_restore: 3492.3740377426147
  time_this_iter_s: 26.384528160095215
  time_total_s: 3492.3740377426147
  timers:
    learn_throughput: 8354.057
    learn_time_ms: 19366.878
    sample_throughput: 23613.341
    sample_time_ms: 6851.72
    update_time_ms: 35.692
  timestamp: 1602637711
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    132 |          3492.37 | 21356544 |   277.12 |              323.616 |              138.768 |             769.45 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3085.203969564281
    time_step_min: 2789
  date: 2020-10-14_01-08-57
  done: false
  episode_len_mean: 769.3404537471775
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.3689650890467
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 250
  episodes_total: 27901
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.1399788794418176
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037671815565166375
        model: {}
        policy_loss: -0.010458155030695101
        total_loss: 3.28119424978892
        vf_explained_var: 0.9924675822257996
        vf_loss: 3.2917223374048867
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.793548387096774
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468579051530719
    mean_env_wait_ms: 1.2226804326948706
    mean_inference_ms: 4.3265040073329475
    mean_raw_obs_processing_ms: 0.3793684721588304
  time_since_restore: 3518.6329913139343
  time_this_iter_s: 26.25895357131958
  time_total_s: 3518.6329913139343
  timers:
    learn_throughput: 8364.09
    learn_time_ms: 19343.645
    sample_throughput: 23623.664
    sample_time_ms: 6848.726
    update_time_ms: 33.509
  timestamp: 1602637737
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    133 |          3518.63 | 21518336 |  277.369 |              323.616 |              138.768 |             769.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3083.7306363733487
    time_step_min: 2782
  date: 2020-10-14_01-09-24
  done: false
  episode_len_mean: 769.2385490753912
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.59121980832504
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 219
  episodes_total: 28120
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.11294228211045265
        entropy_coeff: 0.0005000000000000001
        kl: 0.003609710819243143
        model: {}
        policy_loss: -0.00830613449215889
        total_loss: 2.3648006518681846
        vf_explained_var: 0.993797242641449
        vf_loss: 2.3731632828712463
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.693548387096783
    gpu_util_percent0: 0.29
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685071435220667
    mean_env_wait_ms: 1.2226622133372436
    mean_inference_ms: 4.326055297503823
    mean_raw_obs_processing_ms: 0.3793425317561511
  time_since_restore: 3545.135942697525
  time_this_iter_s: 26.5029513835907
  time_total_s: 3545.135942697525
  timers:
    learn_throughput: 8356.334
    learn_time_ms: 19361.601
    sample_throughput: 23620.498
    sample_time_ms: 6849.644
    update_time_ms: 34.941
  timestamp: 1602637764
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    134 |          3545.14 | 21680128 |  277.591 |              323.616 |              138.768 |            769.239 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3082.608757212134
    time_step_min: 2782
  date: 2020-10-14_01-09-51
  done: false
  episode_len_mean: 769.1729939908095
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 277.7624120312349
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 170
  episodes_total: 28290
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.12997115651766458
        entropy_coeff: 0.0005000000000000001
        kl: 0.00404245105649655
        model: {}
        policy_loss: -0.008053736610842558
        total_loss: 2.1743253270785012
        vf_explained_var: 0.9934215545654297
        vf_loss: 2.1824441154797873
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.4
    gpu_util_percent0: 0.35064516129032264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468446304917315
    mean_env_wait_ms: 1.2226447053824103
    mean_inference_ms: 4.325701048311933
    mean_raw_obs_processing_ms: 0.3793206898184413
  time_since_restore: 3571.5805649757385
  time_this_iter_s: 26.4446222782135
  time_total_s: 3571.5805649757385
  timers:
    learn_throughput: 8357.16
    learn_time_ms: 19359.686
    sample_throughput: 23575.683
    sample_time_ms: 6862.664
    update_time_ms: 35.276
  timestamp: 1602637791
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    135 |          3571.58 | 21841920 |  277.762 |              323.616 |              138.768 |            769.173 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3080.922353931402
    time_step_min: 2782
  date: 2020-10-14_01-10-18
  done: false
  episode_len_mean: 769.0851399152453
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 278.0212021097041
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 263
  episodes_total: 28553
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.13214701289931932
        entropy_coeff: 0.0005000000000000001
        kl: 0.003503160609398037
        model: {}
        policy_loss: -0.007317108246146138
        total_loss: 2.9581462740898132
        vf_explained_var: 0.9934385418891907
        vf_loss: 2.9655295610427856
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.548387096774196
    gpu_util_percent0: 0.3209677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14683622380638447
    mean_env_wait_ms: 1.222618032912319
    mean_inference_ms: 4.325213953368856
    mean_raw_obs_processing_ms: 0.37928821680203945
  time_since_restore: 3597.8386404514313
  time_this_iter_s: 26.25807547569275
  time_total_s: 3597.8386404514313
  timers:
    learn_throughput: 8368.97
    learn_time_ms: 19332.367
    sample_throughput: 23587.638
    sample_time_ms: 6859.186
    update_time_ms: 32.87
  timestamp: 1602637818
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | RUNNING  | 172.17.0.4:73342 |    136 |          3597.84 | 22003712 |  278.021 |              323.616 |              138.768 |            769.085 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_898e0_00000:
  custom_metrics:
    time_step_max: 4000
    time_step_mean: 3079.6354518544313
    time_step_min: 2782
  date: 2020-10-14_01-10-45
  done: true
  episode_len_mean: 769.0164151074633
  episode_reward_max: 323.61616161616183
  episode_reward_mean: 278.2208490272411
  episode_reward_min: 138.7676767676767
  episodes_this_iter: 201
  episodes_total: 28754
  experiment_id: a288855555184bb993cbd1509b47e2b7
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.10710334964096546
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034265152062289417
        model: {}
        policy_loss: -0.008674220361475212
        total_loss: 2.136854350566864
        vf_explained_var: 0.9942054748535156
        vf_loss: 2.145582139492035
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.919354838709673
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 73342
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468295568307401
    mean_env_wait_ms: 1.222596755632267
    mean_inference_ms: 4.324806648957641
    mean_raw_obs_processing_ms: 0.37926472569673225
  time_since_restore: 3624.4411323070526
  time_this_iter_s: 26.602491855621338
  time_total_s: 3624.4411323070526
  timers:
    learn_throughput: 8358.037
    learn_time_ms: 19357.656
    sample_throughput: 23606.596
    sample_time_ms: 6853.678
    update_time_ms: 29.704
  timestamp: 1602637845
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: 898e0_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | TERMINATED |       |    137 |          3624.44 | 22165504 |  278.221 |              323.616 |              138.768 |            769.016 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.91 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_898e0_00000 | TERMINATED |       |    137 |          3624.44 | 22165504 |  278.221 |              323.616 |              138.768 |            769.016 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


