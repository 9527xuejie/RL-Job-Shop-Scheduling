2020-10-11 16:58:37,791	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_026bc_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=781)[0m 2020-10-11 16:58:40,537	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=736)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=736)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=737)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=737)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=728)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=728)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=725)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=725)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=722)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=722)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=755)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=755)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=733)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=733)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=729)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=729)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=752)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=752)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=739)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=739)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=765)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=765)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=720)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=720)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=768)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=768)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=743)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=743)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=760)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=760)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=754)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=754)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=767)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=767)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=738)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=738)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=653)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=653)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=660)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=660)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=759)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=759)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=718)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=718)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=677)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=677)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=717)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=717)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=657)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=657)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=713)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=713)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=673)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=673)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=661)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=661)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=749)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=749)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=652)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=652)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=667)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=667)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=651)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=651)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=662)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=662)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=671)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=671)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=757)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=757)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=772)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=772)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=655)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=655)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=753)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=753)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=730)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=730)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=735)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=735)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=678)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=678)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=740)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=740)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=654)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=654)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=742)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=742)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=775)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=775)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=664)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=664)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=645)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=645)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=650)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=650)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=726)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=726)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=716)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=716)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=723)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=723)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=724)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=724)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=748)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=748)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=732)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=732)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=647)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=647)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=714)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=714)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=672)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=672)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=665)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=665)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=674)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=674)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=676)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=676)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=648)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=648)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=746)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=746)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=762)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=762)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=675)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=675)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=663)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=663)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=727)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=727)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=747)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=747)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=750)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=750)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=646)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=646)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=666)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=666)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=721)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=721)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=731)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=731)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=658)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=658)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=741)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=741)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=669)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=669)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=751)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=751)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=719)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=719)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=715)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=715)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=649)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=649)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_026bc_00000:
  custom_metrics: {}
  date: 2020-10-11_16-59-01
  done: false
  episode_len_mean: .nan
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.193138313293457
        entropy_coeff: 0.0001
        kl: 0.007721627131104469
        model: {}
        policy_loss: -0.014717266522347927
        total_loss: 550.752490234375
        vf_explained_var: -0.5167572498321533
        vf_loss: 550.7658081054688
    num_steps_sampled: 60672
    num_steps_trained: 60672
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.52777777777778
    gpu_util_percent0: 0.19111111111111112
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.322222222222222
    vram_util_percent0: 0.07616340512371998
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  time_since_restore: 15.379424095153809
  time_this_iter_s: 15.379424095153809
  time_total_s: 15.379424095153809
  timers:
    learn_throughput: 6778.233
    learn_time_ms: 8951.005
    sample_throughput: 9520.682
    sample_time_ms: 6372.653
    update_time_ms: 31.641
  timestamp: 1602435541
  timesteps_since_restore: 0
  timesteps_total: 60672
  training_iteration: 1
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.4/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |    ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      1 |          15.3794 | 60672 |      nan |                  nan |                  nan |                nan |
+-------------------------+----------+----------------+--------+------------------+-------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 3929
    time_step_mean: 3605.2025316455697
    time_step_min: 3274
  date: 2020-10-11_16-59-15
  done: false
  episode_len_mean: 886.6582278481013
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 219.77739419511545
  episode_reward_min: 170.71717171717202
  episodes_this_iter: 79
  episodes_total: 79
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1673843383789062
        entropy_coeff: 0.0001
        kl: 0.006144535727798939
        model: {}
        policy_loss: -0.013755168556235731
        total_loss: 535.4020385742188
        vf_explained_var: 0.31667008996009827
        vf_loss: 535.4146850585937
    num_steps_sampled: 121344
    num_steps_trained: 121344
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 35.40625
    gpu_util_percent0: 0.291875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14139310718461384
    mean_env_wait_ms: 0.6594393114955946
    mean_inference_ms: 5.786879393724597
    mean_raw_obs_processing_ms: 0.31053984093424364
  time_since_restore: 30.00749135017395
  time_this_iter_s: 14.628067255020142
  time_total_s: 30.00749135017395
  timers:
    learn_throughput: 6876.416
    learn_time_ms: 8823.201
    sample_throughput: 9907.822
    sample_time_ms: 6123.646
    update_time_ms: 26.228
  timestamp: 1602435555
  timesteps_since_restore: 0
  timesteps_total: 121344
  training_iteration: 2
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      2 |          30.0075 | 121344 |  219.777 |               269.96 |              170.717 |            886.658 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 3967
    time_step_mean: 3626.0632911392404
    time_step_min: 3274
  date: 2020-10-11_16-59-29
  done: false
  episode_len_mean: 882.9303797468355
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 216.61667305971088
  episode_reward_min: 164.95959595959556
  episodes_this_iter: 79
  episodes_total: 158
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1481393575668335
        entropy_coeff: 0.0001
        kl: 0.006490523274987936
        model: {}
        policy_loss: -0.015036692284047604
        total_loss: 317.61456909179685
        vf_explained_var: 0.6613558530807495
        vf_loss: 317.6284118652344
    num_steps_sampled: 182016
    num_steps_trained: 182016
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.21875
    gpu_util_percent0: 0.27749999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1376011871806702
    mean_env_wait_ms: 0.65697408938611
    mean_inference_ms: 5.601800678039541
    mean_raw_obs_processing_ms: 0.3018418567999206
  time_since_restore: 43.56244921684265
  time_this_iter_s: 13.554957866668701
  time_total_s: 43.56244921684265
  timers:
    learn_throughput: 6938.015
    learn_time_ms: 8744.865
    sample_throughput: 10604.921
    sample_time_ms: 5721.118
    update_time_ms: 23.489
  timestamp: 1602435569
  timesteps_since_restore: 0
  timesteps_total: 182016
  training_iteration: 3
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      3 |          43.5624 | 182016 |  216.617 |               269.96 |               164.96 |             882.93 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3631.721518987342
    time_step_min: 3274
  date: 2020-10-11_16-59-42
  done: false
  episode_len_mean: 877.1476793248945
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 215.75936580999857
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 237
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.133927011489868
        entropy_coeff: 0.0001
        kl: 0.006118734646588564
        model: {}
        policy_loss: -0.015129859372973443
        total_loss: 199.39960327148438
        vf_explained_var: 0.7791659235954285
        vf_loss: 199.41362609863282
    num_steps_sampled: 242688
    num_steps_trained: 242688
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.2875
    gpu_util_percent0: 0.275625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13503358299140059
    mean_env_wait_ms: 0.6562085482944982
    mean_inference_ms: 5.451710511353671
    mean_raw_obs_processing_ms: 0.2959462430564311
  time_since_restore: 57.086549043655396
  time_this_iter_s: 13.524099826812744
  time_total_s: 57.086549043655396
  timers:
    learn_throughput: 6926.651
    learn_time_ms: 8759.211
    sample_throughput: 11116.323
    sample_time_ms: 5457.92
    update_time_ms: 23.213
  timestamp: 1602435582
  timesteps_since_restore: 0
  timesteps_total: 242688
  training_iteration: 4
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      4 |          57.0865 | 242688 |  215.759 |               269.96 |               116.02 |            877.148 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3621.4493670886077
    time_step_min: 3274
  date: 2020-10-11_16-59-56
  done: false
  episode_len_mean: 871.5411392405064
  episode_reward_max: 269.9595959595958
  episode_reward_mean: 217.31575246132192
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 316
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.1258406400680543
        entropy_coeff: 0.0001
        kl: 0.006548813544213772
        model: {}
        policy_loss: -0.01587002072483301
        total_loss: 101.75677337646485
        vf_explained_var: 0.8550761342048645
        vf_loss: 101.77144622802734
    num_steps_sampled: 303360
    num_steps_trained: 303360
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.02
    gpu_util_percent0: 0.23533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1331496329246873
    mean_env_wait_ms: 0.6561343711571395
    mean_inference_ms: 5.33575357092479
    mean_raw_obs_processing_ms: 0.2916587229824319
  time_since_restore: 70.43287014961243
  time_this_iter_s: 13.346321105957031
  time_total_s: 70.43287014961243
  timers:
    learn_throughput: 6933.037
    learn_time_ms: 8751.143
    sample_throughput: 11489.877
    sample_time_ms: 5280.474
    update_time_ms: 22.896
  timestamp: 1602435596
  timesteps_since_restore: 0
  timesteps_total: 303360
  training_iteration: 5
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      5 |          70.4329 | 303360 |  217.316 |               269.96 |               116.02 |            871.541 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3609.5620253164557
    time_step_min: 3226
  date: 2020-10-11_17-00-09
  done: false
  episode_len_mean: 867.2329113924051
  episode_reward_max: 277.23232323232315
  episode_reward_mean: 219.1168648510419
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 395
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.112362313270569
        entropy_coeff: 0.0001
        kl: 0.00727074546739459
        model: {}
        policy_loss: -0.01704823523759842
        total_loss: 71.98140106201171
        vf_explained_var: 0.8918706774711609
        vf_loss: 71.99710693359376
    num_steps_sampled: 364032
    num_steps_trained: 364032
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.06666666666666
    gpu_util_percent0: 0.2773333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13169782524460721
    mean_env_wait_ms: 0.6562732240953844
    mean_inference_ms: 5.243888396728066
    mean_raw_obs_processing_ms: 0.28834797829255926
  time_since_restore: 83.63684868812561
  time_this_iter_s: 13.203978538513184
  time_total_s: 83.63684868812561
  timers:
    learn_throughput: 6941.258
    learn_time_ms: 8740.779
    sample_throughput: 11796.885
    sample_time_ms: 5143.053
    update_time_ms: 23.807
  timestamp: 1602435609
  timesteps_since_restore: 0
  timesteps_total: 364032
  training_iteration: 6
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      6 |          83.6368 | 364032 |  219.117 |              277.232 |               116.02 |            867.233 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3593.96835443038
    time_step_min: 3187
  date: 2020-10-11_17-00-22
  done: false
  episode_len_mean: 862.3860759493671
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 221.47954225802314
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 474
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0965974807739258
        entropy_coeff: 0.0001
        kl: 0.006363364960998296
        model: {}
        policy_loss: -0.01626312769949436
        total_loss: 60.179150390625
        vf_explained_var: 0.905462920665741
        vf_loss: 60.19425048828125
    num_steps_sampled: 424704
    num_steps_trained: 424704
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.506249999999998
    gpu_util_percent0: 0.285
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.13051962303679418
    mean_env_wait_ms: 0.6564648785133232
    mean_inference_ms: 5.169321832948338
    mean_raw_obs_processing_ms: 0.285696496830229
  time_since_restore: 96.93772292137146
  time_this_iter_s: 13.30087423324585
  time_total_s: 96.93772292137146
  timers:
    learn_throughput: 6941.471
    learn_time_ms: 8740.51
    sample_throughput: 12015.943
    sample_time_ms: 5049.292
    update_time_ms: 26.033
  timestamp: 1602435622
  timesteps_since_restore: 0
  timesteps_total: 424704
  training_iteration: 7
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      7 |          96.9377 | 424704 |   221.48 |              283.141 |               116.02 |            862.386 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3587.43115942029
    time_step_min: 3187
  date: 2020-10-11_17-00-36
  done: false
  episode_len_mean: 857.5561594202899
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 222.47002635046098
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 552
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0854148864746094
        entropy_coeff: 0.0001
        kl: 0.006640845071524381
        model: {}
        policy_loss: -0.017912944313138723
        total_loss: 57.62085189819336
        vf_explained_var: 0.9167647361755371
        vf_loss: 57.637541961669925
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.573333333333334
    gpu_util_percent0: 0.3013333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12955019394660422
    mean_env_wait_ms: 0.6567035286597551
    mean_inference_ms: 5.10780576146441
    mean_raw_obs_processing_ms: 0.2835224457246449
  time_since_restore: 110.21923828125
  time_this_iter_s: 13.28151535987854
  time_total_s: 110.21923828125
  timers:
    learn_throughput: 6946.856
    learn_time_ms: 8733.735
    sample_throughput: 12167.434
    sample_time_ms: 4986.425
    update_time_ms: 24.747
  timestamp: 1602435636
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 8
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      8 |          110.219 | 485376 |   222.47 |              283.141 |               116.02 |            857.556 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3579.633914421553
    time_step_min: 3187
  date: 2020-10-11_17-00-49
  done: false
  episode_len_mean: 853.2773375594295
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 223.65142710784536
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 631
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0685611724853517
        entropy_coeff: 0.0001
        kl: 0.006435621529817581
        model: {}
        policy_loss: -0.017283295653760432
        total_loss: 42.61400680541992
        vf_explained_var: 0.9355558156967163
        vf_loss: 42.63011016845703
    num_steps_sampled: 546048
    num_steps_trained: 546048
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.233333333333334
    gpu_util_percent0: 0.24999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12873939539819357
    mean_env_wait_ms: 0.6570926674537437
    mean_inference_ms: 5.055372251295585
    mean_raw_obs_processing_ms: 0.28168143541478796
  time_since_restore: 123.51551365852356
  time_this_iter_s: 13.29627537727356
  time_total_s: 123.51551365852356
  timers:
    learn_throughput: 6951.822
    learn_time_ms: 8727.496
    sample_throughput: 12285.41
    sample_time_ms: 4938.541
    update_time_ms: 25.414
  timestamp: 1602435649
  timesteps_since_restore: 0
  timesteps_total: 546048
  training_iteration: 9
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |      9 |          123.516 | 546048 |  223.651 |              283.141 |               116.02 |            853.277 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3572.5885714285714
    time_step_min: 3187
  date: 2020-10-11_17-01-02
  done: false
  episode_len_mean: 848.9885714285714
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 224.71890331890322
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 69
  episodes_total: 700
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0564856290817262
        entropy_coeff: 0.0001
        kl: 0.0069581371732056144
        model: {}
        policy_loss: -0.017678908724337816
        total_loss: 35.82207412719727
        vf_explained_var: 0.9436900019645691
        vf_loss: 35.83846588134766
    num_steps_sampled: 606720
    num_steps_trained: 606720
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.813333333333336
    gpu_util_percent0: 0.24533333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1281282692235597
    mean_env_wait_ms: 0.6575528979963373
    mean_inference_ms: 5.015573764189788
    mean_raw_obs_processing_ms: 0.28030612139761873
  time_since_restore: 136.74131965637207
  time_this_iter_s: 13.22580599784851
  time_total_s: 136.74131965637207
  timers:
    learn_throughput: 6956.957
    learn_time_ms: 8721.055
    sample_throughput: 12392.006
    sample_time_ms: 4896.059
    update_time_ms: 24.733
  timestamp: 1602435662
  timesteps_since_restore: 0
  timesteps_total: 606720
  training_iteration: 10
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     10 |          136.741 | 606720 |  224.719 |              283.141 |               116.02 |            848.989 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3569.1739707835327
    time_step_min: 3187
  date: 2020-10-11_17-01-16
  done: false
  episode_len_mean: 846.2270916334661
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 225.236267053
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 53
  episodes_total: 753
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0427685260772706
        entropy_coeff: 0.0001
        kl: 0.006341448985040188
        model: {}
        policy_loss: -0.016971242520958184
        total_loss: 35.50624237060547
        vf_explained_var: 0.9447349309921265
        vf_loss: 35.52204895019531
    num_steps_sampled: 667392
    num_steps_trained: 667392
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.59375
    gpu_util_percent0: 0.25625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1276699134349628
    mean_env_wait_ms: 0.6578412103983874
    mean_inference_ms: 4.986686074052503
    mean_raw_obs_processing_ms: 0.27922876785869416
  time_since_restore: 150.19758987426758
  time_this_iter_s: 13.456270217895508
  time_total_s: 150.19758987426758
  timers:
    learn_throughput: 6967.795
    learn_time_ms: 8707.489
    sample_throughput: 12867.96
    sample_time_ms: 4714.967
    update_time_ms: 25.936
  timestamp: 1602435676
  timesteps_since_restore: 0
  timesteps_total: 667392
  training_iteration: 11
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     11 |          150.198 | 667392 |  225.236 |              283.141 |               116.02 |            846.227 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3563.3507462686566
    time_step_min: 3187
  date: 2020-10-11_17-01-29
  done: false
  episode_len_mean: 844.0559701492538
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 226.11857379767818
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 51
  episodes_total: 804
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0320799827575684
        entropy_coeff: 0.0001
        kl: 0.006349154934287071
        model: {}
        policy_loss: -0.018102188082411885
        total_loss: 23.2988582611084
        vf_explained_var: 0.9564861059188843
        vf_loss: 23.315794372558592
    num_steps_sampled: 728064
    num_steps_trained: 728064
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.3
    gpu_util_percent0: 0.29399999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12729220368801417
    mean_env_wait_ms: 0.6581903403580992
    mean_inference_ms: 4.962231905891922
    mean_raw_obs_processing_ms: 0.2783622963949977
  time_since_restore: 163.5104215145111
  time_this_iter_s: 13.31283164024353
  time_total_s: 163.5104215145111
  timers:
    learn_throughput: 6967.94
    learn_time_ms: 8707.309
    sample_throughput: 13237.355
    sample_time_ms: 4583.393
    update_time_ms: 26.013
  timestamp: 1602435689
  timesteps_since_restore: 0
  timesteps_total: 728064
  training_iteration: 12
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     12 |           163.51 | 728064 |  226.119 |              283.141 |               116.02 |            844.056 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3557.2402745995423
    time_step_min: 3187
  date: 2020-10-11_17-01-43
  done: false
  episode_len_mean: 841.3981693363844
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 227.04440283845307
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 70
  episodes_total: 874
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.033131980895996
        entropy_coeff: 0.0001
        kl: 0.006343710143119097
        model: {}
        policy_loss: -0.017480041179805995
        total_loss: 19.233250427246094
        vf_explained_var: 0.9637705087661743
        vf_loss: 19.249564743041994
    num_steps_sampled: 788736
    num_steps_trained: 788736
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.04375
    gpu_util_percent0: 0.35375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1268073318819917
    mean_env_wait_ms: 0.6585009651002863
    mean_inference_ms: 4.931275118854507
    mean_raw_obs_processing_ms: 0.277197864846723
  time_since_restore: 177.0922496318817
  time_this_iter_s: 13.581828117370605
  time_total_s: 177.0922496318817
  timers:
    learn_throughput: 6945.004
    learn_time_ms: 8736.065
    sample_throughput: 13320.298
    sample_time_ms: 4554.853
    update_time_ms: 28.102
  timestamp: 1602435703
  timesteps_since_restore: 0
  timesteps_total: 788736
  training_iteration: 13
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     13 |          177.092 | 788736 |  227.044 |              283.141 |               116.02 |            841.398 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3549.9789029535864
    time_step_min: 3187
  date: 2020-10-11_17-01-57
  done: false
  episode_len_mean: 838.9367088607595
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 228.1446106635979
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 74
  episodes_total: 948
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 1.0167401790618897
        entropy_coeff: 0.0001
        kl: 0.007002000138163567
        model: {}
        policy_loss: -0.018312944937497376
        total_loss: 17.715657043457032
        vf_explained_var: 0.9672698974609375
        vf_loss: 17.732671356201173
    num_steps_sampled: 849408
    num_steps_trained: 849408
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.760000000000005
    gpu_util_percent0: 0.2226666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1263729924910855
    mean_env_wait_ms: 0.6589192968582249
    mean_inference_ms: 4.902394685225931
    mean_raw_obs_processing_ms: 0.2761162943614284
  time_since_restore: 190.64881777763367
  time_this_iter_s: 13.556568145751953
  time_total_s: 190.64881777763367
  timers:
    learn_throughput: 6946.764
    learn_time_ms: 8733.851
    sample_throughput: 13309.031
    sample_time_ms: 4558.709
    update_time_ms: 29.363
  timestamp: 1602435717
  timesteps_since_restore: 0
  timesteps_total: 849408
  training_iteration: 14
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     14 |          190.649 | 849408 |  228.145 |              283.141 |               116.02 |            838.937 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3543.60564751704
    time_step_min: 3187
  date: 2020-10-11_17-02-10
  done: false
  episode_len_mean: 836.4985394352483
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 229.11025542671103
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1027
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9943946719169616
        entropy_coeff: 0.0001
        kl: 0.007912519946694374
        model: {}
        policy_loss: -0.018420965038239957
        total_loss: 17.297443771362303
        vf_explained_var: 0.9709770083427429
        vf_loss: 17.314381790161132
    num_steps_sampled: 910080
    num_steps_trained: 910080
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.94
    gpu_util_percent0: 0.2826666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12594017657923615
    mean_env_wait_ms: 0.659305030339765
    mean_inference_ms: 4.874427786972293
    mean_raw_obs_processing_ms: 0.27504244830536523
  time_since_restore: 203.84446048736572
  time_this_iter_s: 13.195642709732056
  time_total_s: 203.84446048736572
  timers:
    learn_throughput: 6955.805
    learn_time_ms: 8722.498
    sample_throughput: 13324.007
    sample_time_ms: 4553.585
    update_time_ms: 30.825
  timestamp: 1602435730
  timesteps_since_restore: 0
  timesteps_total: 910080
  training_iteration: 15
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     15 |          203.844 | 910080 |   229.11 |              283.141 |               116.02 |            836.499 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3535.533453887884
    time_step_min: 3187
  date: 2020-10-11_17-02-23
  done: false
  episode_len_mean: 834.1925858951175
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 230.33331506749218
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1106
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9890294790267944
        entropy_coeff: 0.0001
        kl: 0.0057542574591934684
        model: {}
        policy_loss: -0.016859600320458412
        total_loss: 22.078242111206055
        vf_explained_var: 0.9638479948043823
        vf_loss: 22.09404945373535
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.687500000000004
    gpu_util_percent0: 0.3475
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12555329935408693
    mean_env_wait_ms: 0.6596651500430735
    mean_inference_ms: 4.849441887170735
    mean_raw_obs_processing_ms: 0.27407412912739776
  time_since_restore: 217.08992195129395
  time_this_iter_s: 13.245461463928223
  time_total_s: 217.08992195129395
  timers:
    learn_throughput: 6954.648
    learn_time_ms: 8723.949
    sample_throughput: 13321.246
    sample_time_ms: 4554.529
    update_time_ms: 32.039
  timestamp: 1602435743
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 16
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     16 |           217.09 | 970752 |  230.333 |              283.141 |               116.02 |            834.193 |
+-------------------------+----------+----------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3527.42194092827
    time_step_min: 3187
  date: 2020-10-11_17-02-37
  done: false
  episode_len_mean: 832.0430379746836
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 231.56233218258527
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1185
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9727253675460815
        entropy_coeff: 0.0001
        kl: 0.006370769906789064
        model: {}
        policy_loss: -0.01666537211276591
        total_loss: 16.41226272583008
        vf_explained_var: 0.971873939037323
        vf_loss: 16.42775077819824
    num_steps_sampled: 1031424
    num_steps_trained: 1031424
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.48
    gpu_util_percent0: 0.2706666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1252036805967254
    mean_env_wait_ms: 0.6600153911857586
    mean_inference_ms: 4.826919305472011
    mean_raw_obs_processing_ms: 0.2731918768036771
  time_since_restore: 230.42932963371277
  time_this_iter_s: 13.339407682418823
  time_total_s: 230.42932963371277
  timers:
    learn_throughput: 6952.795
    learn_time_ms: 8726.275
    sample_throughput: 13313.186
    sample_time_ms: 4557.286
    update_time_ms: 30.342
  timestamp: 1602435757
  timesteps_since_restore: 0
  timesteps_total: 1031424
  training_iteration: 17
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     17 |          230.429 | 1031424 |  231.562 |              283.141 |               116.02 |            832.043 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3521.1756329113923
    time_step_min: 3187
  date: 2020-10-11_17-02-50
  done: false
  episode_len_mean: 830.375
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 232.50874248817277
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1264
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9600818872451782
        entropy_coeff: 0.0001
        kl: 0.006693596951663494
        model: {}
        policy_loss: -0.017358003184199333
        total_loss: 18.817015075683592
        vf_explained_var: 0.9695835113525391
        vf_loss: 18.833131790161133
    num_steps_sampled: 1092096
    num_steps_trained: 1092096
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.733333333333338
    gpu_util_percent0: 0.34600000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12488728605683397
    mean_env_wait_ms: 0.6603641407403074
    mean_inference_ms: 4.806553875100527
    mean_raw_obs_processing_ms: 0.27238777046650775
  time_since_restore: 243.72431063652039
  time_this_iter_s: 13.294981002807617
  time_total_s: 243.72431063652039
  timers:
    learn_throughput: 6954.415
    learn_time_ms: 8724.242
    sample_throughput: 13305.188
    sample_time_ms: 4560.026
    update_time_ms: 30.601
  timestamp: 1602435770
  timesteps_since_restore: 0
  timesteps_total: 1092096
  training_iteration: 18
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     18 |          243.724 | 1092096 |  232.509 |              283.141 |               116.02 |            830.375 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3516.3827252419956
    time_step_min: 3187
  date: 2020-10-11_17-03-03
  done: false
  episode_len_mean: 829.0067014147431
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 233.23494061989956
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1343
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9440988779067994
        entropy_coeff: 0.0001
        kl: 0.007389881648123264
        model: {}
        policy_loss: -0.01705184131860733
        total_loss: 15.939278411865235
        vf_explained_var: 0.9752111434936523
        vf_loss: 15.954946899414063
    num_steps_sampled: 1152768
    num_steps_trained: 1152768
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.933333333333334
    gpu_util_percent0: 0.31799999999999995
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12460007468221047
    mean_env_wait_ms: 0.6606953229772926
    mean_inference_ms: 4.787980775447648
    mean_raw_obs_processing_ms: 0.27164873071747714
  time_since_restore: 256.81789088249207
  time_this_iter_s: 13.09358024597168
  time_total_s: 256.81789088249207
  timers:
    learn_throughput: 6966.637
    learn_time_ms: 8708.937
    sample_throughput: 13319.323
    sample_time_ms: 4555.187
    update_time_ms: 29.529
  timestamp: 1602435783
  timesteps_since_restore: 0
  timesteps_total: 1152768
  training_iteration: 19
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     19 |          256.818 | 1152768 |  233.235 |              283.141 |               116.02 |            829.007 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3510.7897327707456
    time_step_min: 3187
  date: 2020-10-11_17-03-16
  done: false
  episode_len_mean: 828.0991561181435
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 234.08236372160414
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1422
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9336613178253174
        entropy_coeff: 0.0001
        kl: 0.006462276726961136
        model: {}
        policy_loss: -0.017750857584178447
        total_loss: 12.616872406005859
        vf_explained_var: 0.9794406890869141
        vf_loss: 12.63342456817627
    num_steps_sampled: 1213440
    num_steps_trained: 1213440
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.399999999999995
    gpu_util_percent0: 0.22866666666666668
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12433871771384056
    mean_env_wait_ms: 0.6609975010505394
    mean_inference_ms: 4.770966782179893
    mean_raw_obs_processing_ms: 0.2709713101827392
  time_since_restore: 270.0122151374817
  time_this_iter_s: 13.194324254989624
  time_total_s: 270.0122151374817
  timers:
    learn_throughput: 6974.611
    learn_time_ms: 8698.98
    sample_throughput: 13304.036
    sample_time_ms: 4560.42
    update_time_ms: 29.269
  timestamp: 1602435796
  timesteps_since_restore: 0
  timesteps_total: 1213440
  training_iteration: 20
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     20 |          270.012 | 1213440 |  234.082 |              283.141 |               116.02 |            828.099 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3505.7934710193204
    time_step_min: 3187
  date: 2020-10-11_17-03-30
  done: false
  episode_len_mean: 827.3597601598934
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 234.83937307788065
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1501
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.9111692786216736
        entropy_coeff: 0.0001
        kl: 0.007237232103943825
        model: {}
        policy_loss: -0.016399275790899993
        total_loss: 14.045680236816406
        vf_explained_var: 0.9778404235839844
        vf_loss: 14.060722923278808
    num_steps_sampled: 1274112
    num_steps_trained: 1274112
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.48125
    gpu_util_percent0: 0.3275
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12409882801027933
    mean_env_wait_ms: 0.6612764529702897
    mean_inference_ms: 4.755368290032092
    mean_raw_obs_processing_ms: 0.27034911433274705
  time_since_restore: 283.349814414978
  time_this_iter_s: 13.337599277496338
  time_total_s: 283.349814414978
  timers:
    learn_throughput: 6983.477
    learn_time_ms: 8687.936
    sample_throughput: 13301.706
    sample_time_ms: 4561.22
    update_time_ms: 27.641
  timestamp: 1602435810
  timesteps_since_restore: 0
  timesteps_total: 1274112
  training_iteration: 21
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     21 |           283.35 | 1274112 |  234.839 |              283.141 |               116.02 |             827.36 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3500.7069620253164
    time_step_min: 3187
  date: 2020-10-11_17-03-43
  done: false
  episode_len_mean: 826.9227848101266
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 235.61005625879037
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1580
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8991423726081849
        entropy_coeff: 0.0001
        kl: 0.005821902584284544
        model: {}
        policy_loss: -0.01675959237618372
        total_loss: 11.816276931762696
        vf_explained_var: 0.9811431169509888
        vf_loss: 11.83196144104004
    num_steps_sampled: 1334784
    num_steps_trained: 1334784
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.719999999999995
    gpu_util_percent0: 0.2286666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12387815189193017
    mean_env_wait_ms: 0.6615237001273359
    mean_inference_ms: 4.740992831409041
    mean_raw_obs_processing_ms: 0.2697732457553119
  time_since_restore: 296.7076737880707
  time_this_iter_s: 13.357859373092651
  time_total_s: 296.7076737880707
  timers:
    learn_throughput: 6987.615
    learn_time_ms: 8682.791
    sample_throughput: 13277.64
    sample_time_ms: 4569.487
    update_time_ms: 28.66
  timestamp: 1602435823
  timesteps_since_restore: 0
  timesteps_total: 1334784
  training_iteration: 22
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     22 |          296.708 | 1334784 |   235.61 |              283.141 |               116.02 |            826.923 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3496.0596745027124
    time_step_min: 3187
  date: 2020-10-11_17-03-57
  done: false
  episode_len_mean: 826.6172393007836
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 236.3141907319122
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1659
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8894487977027893
        entropy_coeff: 0.0001
        kl: 0.00622055558487773
        model: {}
        policy_loss: -0.016685663908720016
        total_loss: 12.154040336608887
        vf_explained_var: 0.9802371859550476
        vf_loss: 12.169570541381836
    num_steps_sampled: 1395456
    num_steps_trained: 1395456
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.50666666666667
    gpu_util_percent0: 0.19799999999999998
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1236729369644576
    mean_env_wait_ms: 0.6617411790625801
    mean_inference_ms: 4.727684491997126
    mean_raw_obs_processing_ms: 0.2692407899763037
  time_since_restore: 309.91317200660706
  time_this_iter_s: 13.205498218536377
  time_total_s: 309.91317200660706
  timers:
    learn_throughput: 7012.712
    learn_time_ms: 8651.717
    sample_throughput: 13295.632
    sample_time_ms: 4563.303
    update_time_ms: 28.202
  timestamp: 1602435837
  timesteps_since_restore: 0
  timesteps_total: 1395456
  training_iteration: 23
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     23 |          309.913 | 1395456 |  236.314 |              283.141 |               116.02 |            826.617 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3493.070195627158
    time_step_min: 3187
  date: 2020-10-11_17-04-10
  done: false
  episode_len_mean: 826.337169159954
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 236.7671420766932
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1738
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8807594299316406
        entropy_coeff: 0.0001
        kl: 0.005982859991490841
        model: {}
        policy_loss: -0.016763895750045776
        total_loss: 12.645524978637695
        vf_explained_var: 0.9802120923995972
        vf_loss: 12.661180877685547
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.38
    gpu_util_percent0: 0.3566666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12348224097718613
    mean_env_wait_ms: 0.6619382011319972
    mean_inference_ms: 4.71532222357583
    mean_raw_obs_processing_ms: 0.26875014661287316
  time_since_restore: 323.0878391265869
  time_this_iter_s: 13.174667119979858
  time_total_s: 323.0878391265869
  timers:
    learn_throughput: 7031.926
    learn_time_ms: 8628.077
    sample_throughput: 13333.391
    sample_time_ms: 4550.38
    update_time_ms: 26.201
  timestamp: 1602435850
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 24
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     24 |          323.088 | 1456128 |  236.767 |              283.141 |               116.02 |            826.337 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3489.458998348927
    time_step_min: 3187
  date: 2020-10-11_17-04-23
  done: false
  episode_len_mean: 826.1761144744083
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 237.31429317945543
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 1817
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8732481479644776
        entropy_coeff: 0.0001
        kl: 0.00523865120485425
        model: {}
        policy_loss: -0.015956384362652898
        total_loss: 13.58818016052246
        vf_explained_var: 0.9780164957046509
        vf_loss: 13.603175735473632
    num_steps_sampled: 1516800
    num_steps_trained: 1516800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.700000000000003
    gpu_util_percent0: 0.34
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12330553343040648
    mean_env_wait_ms: 0.6621108754100793
    mean_inference_ms: 4.703810505705495
    mean_raw_obs_processing_ms: 0.26829601225477917
  time_since_restore: 336.33845686912537
  time_this_iter_s: 13.250617742538452
  time_total_s: 336.33845686912537
  timers:
    learn_throughput: 7033.733
    learn_time_ms: 8625.861
    sample_throughput: 13332.396
    sample_time_ms: 4550.72
    update_time_ms: 26.117
  timestamp: 1602435863
  timesteps_since_restore: 0
  timesteps_total: 1516800
  training_iteration: 25
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     25 |          336.338 | 1516800 |  237.314 |              283.141 |               116.02 |            826.176 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3486.208443271768
    time_step_min: 3187
  date: 2020-10-11_17-04-37
  done: false
  episode_len_mean: 825.932981530343
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 237.80680152447957
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 1895
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8687670111656189
        entropy_coeff: 0.0001
        kl: 0.005749080330133438
        model: {}
        policy_loss: -0.016765564773231746
        total_loss: 11.381985664367676
        vf_explained_var: 0.9814356565475464
        vf_loss: 11.397688293457032
    num_steps_sampled: 1577472
    num_steps_trained: 1577472
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.36
    gpu_util_percent0: 0.24933333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666667
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12314222187902779
    mean_env_wait_ms: 0.6622646304447253
    mean_inference_ms: 4.693127454510115
    mean_raw_obs_processing_ms: 0.26787595363416766
  time_since_restore: 349.5179297924042
  time_this_iter_s: 13.179472923278809
  time_total_s: 349.5179297924042
  timers:
    learn_throughput: 7043.71
    learn_time_ms: 8613.643
    sample_throughput: 13311.825
    sample_time_ms: 4557.752
    update_time_ms: 24.508
  timestamp: 1602435877
  timesteps_since_restore: 0
  timesteps_total: 1577472
  training_iteration: 26
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     26 |          349.518 | 1577472 |  237.807 |              283.141 |               116.02 |            825.933 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3483.598268839104
    time_step_min: 3187
  date: 2020-10-11_17-04-50
  done: false
  episode_len_mean: 825.6252545824847
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 238.2022824991256
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 69
  episodes_total: 1964
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8563682079315186
        entropy_coeff: 0.0001
        kl: 0.005709396209567785
        model: {}
        policy_loss: -0.017698555067181588
        total_loss: 10.851219177246094
        vf_explained_var: 0.9818906784057617
        vf_loss: 10.867861557006837
    num_steps_sampled: 1638144
    num_steps_trained: 1638144
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.55
    gpu_util_percent0: 0.231875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.41875
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12300450957363136
    mean_env_wait_ms: 0.662381469958399
    mean_inference_ms: 4.684059264178332
    mean_raw_obs_processing_ms: 0.2675313680389422
  time_since_restore: 363.0079343318939
  time_this_iter_s: 13.490004539489746
  time_total_s: 363.0079343318939
  timers:
    learn_throughput: 7051.274
    learn_time_ms: 8604.403
    sample_throughput: 13249.447
    sample_time_ms: 4579.21
    update_time_ms: 27.539
  timestamp: 1602435890
  timesteps_since_restore: 0
  timesteps_total: 1638144
  training_iteration: 27
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     27 |          363.008 | 1638144 |  238.202 |              283.141 |               116.02 |            825.625 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3481.9901429275506
    time_step_min: 3187
  date: 2020-10-11_17-05-03
  done: false
  episode_len_mean: 825.3652045342533
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 238.44593794027003
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 65
  episodes_total: 2029
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8387965321540832
        entropy_coeff: 0.0001
        kl: 0.006011899933218956
        model: {}
        policy_loss: -0.01685419175773859
        total_loss: 10.585357475280762
        vf_explained_var: 0.9818204045295715
        vf_loss: 10.601093292236328
    num_steps_sampled: 1698816
    num_steps_trained: 1698816
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.026666666666664
    gpu_util_percent0: 0.22999999999999998
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1228824097955642
    mean_env_wait_ms: 0.6625040349074989
    mean_inference_ms: 4.675965391201938
    mean_raw_obs_processing_ms: 0.2672116066326146
  time_since_restore: 376.3195638656616
  time_this_iter_s: 13.3116295337677
  time_total_s: 376.3195638656616
  timers:
    learn_throughput: 7055.026
    learn_time_ms: 8599.826
    sample_throughput: 13239.407
    sample_time_ms: 4582.683
    update_time_ms: 29.656
  timestamp: 1602435903
  timesteps_since_restore: 0
  timesteps_total: 1698816
  training_iteration: 28
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     28 |           376.32 | 1698816 |  238.446 |              283.141 |               116.02 |            825.365 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3479.421254188607
    time_step_min: 3187
  date: 2020-10-11_17-05-17
  done: false
  episode_len_mean: 825.2077549066539
  episode_reward_max: 283.1414141414142
  episode_reward_mean: 238.83516350677664
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 60
  episodes_total: 2089
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8409842371940612
        entropy_coeff: 0.0001
        kl: 0.006058331951498986
        model: {}
        policy_loss: -0.017604468390345573
        total_loss: 8.032962226867676
        vf_explained_var: 0.9850233197212219
        vf_loss: 8.049439334869385
    num_steps_sampled: 1759488
    num_steps_trained: 1759488
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.68125
    gpu_util_percent0: 0.26937500000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.41875
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12277932262810817
    mean_env_wait_ms: 0.6625992969574942
    mean_inference_ms: 4.669076231131903
    mean_raw_obs_processing_ms: 0.26692853495607544
  time_since_restore: 389.8285422325134
  time_this_iter_s: 13.508978366851807
  time_total_s: 389.8285422325134
  timers:
    learn_throughput: 7043.591
    learn_time_ms: 8613.787
    sample_throughput: 13183.275
    sample_time_ms: 4602.195
    update_time_ms: 37.347
  timestamp: 1602435917
  timesteps_since_restore: 0
  timesteps_total: 1759488
  training_iteration: 29
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     29 |          389.829 | 1759488 |  238.835 |              283.141 |               116.02 |            825.208 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3477.6100511865984
    time_step_min: 3169
  date: 2020-10-11_17-05-30
  done: false
  episode_len_mean: 825.1763610981852
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 239.10958820405065
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 60
  episodes_total: 2149
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8300541996955871
        entropy_coeff: 0.0001
        kl: 0.006526672095060348
        model: {}
        policy_loss: -0.017102842684835196
        total_loss: 9.383557319641113
        vf_explained_var: 0.9828168153762817
        vf_loss: 9.399437713623048
    num_steps_sampled: 1820160
    num_steps_trained: 1820160
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.73333333333333
    gpu_util_percent0: 0.332
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12267485402360942
    mean_env_wait_ms: 0.6626792219208739
    mean_inference_ms: 4.66255957919075
    mean_raw_obs_processing_ms: 0.2666819290505635
  time_since_restore: 403.05150151252747
  time_this_iter_s: 13.222959280014038
  time_total_s: 403.05150151252747
  timers:
    learn_throughput: 7040.228
    learn_time_ms: 8617.902
    sample_throughput: 13186.546
    sample_time_ms: 4601.053
    update_time_ms: 37.438
  timestamp: 1602435930
  timesteps_since_restore: 0
  timesteps_total: 1820160
  training_iteration: 30
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     30 |          403.052 | 1820160 |   239.11 |              285.869 |               116.02 |            825.176 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3474.5842342342344
    time_step_min: 3169
  date: 2020-10-11_17-05-44
  done: false
  episode_len_mean: 825.2738738738739
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 239.56804531804525
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 71
  episodes_total: 2220
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8318358898162842
        entropy_coeff: 0.0001
        kl: 0.005705495923757553
        model: {}
        policy_loss: -0.016056472854688763
        total_loss: 9.408385086059571
        vf_explained_var: 0.9828255772590637
        vf_loss: 9.423383331298828
    num_steps_sampled: 1880832
    num_steps_trained: 1880832
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.313333333333333
    gpu_util_percent0: 0.3413333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4133333333333327
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12255653491395617
    mean_env_wait_ms: 0.6627406637559171
    mean_inference_ms: 4.65496188070215
    mean_raw_obs_processing_ms: 0.2663740012872749
  time_since_restore: 416.3603506088257
  time_this_iter_s: 13.308849096298218
  time_total_s: 416.3603506088257
  timers:
    learn_throughput: 7041.357
    learn_time_ms: 8616.521
    sample_throughput: 13194.007
    sample_time_ms: 4598.451
    update_time_ms: 37.986
  timestamp: 1602435944
  timesteps_since_restore: 0
  timesteps_total: 1880832
  training_iteration: 31
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     31 |           416.36 | 1880832 |  239.568 |              285.869 |               116.02 |            825.274 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3471.0850043591977
    time_step_min: 3169
  date: 2020-10-11_17-05-57
  done: false
  episode_len_mean: 825.2794245858762
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 240.0982316627477
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 74
  episodes_total: 2294
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8104691743850708
        entropy_coeff: 0.0001
        kl: 0.005790698435157537
        model: {}
        policy_loss: -0.016167748929001392
        total_loss: 10.063735961914062
        vf_explained_var: 0.982425332069397
        vf_loss: 10.078826713562012
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.24666666666667
    gpu_util_percent0: 0.25666666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12244163131758412
    mean_env_wait_ms: 0.6627908978419998
    mean_inference_ms: 4.647463726012324
    mean_raw_obs_processing_ms: 0.2660749303235379
  time_since_restore: 429.5823678970337
  time_this_iter_s: 13.222017288208008
  time_total_s: 429.5823678970337
  timers:
    learn_throughput: 7041.731
    learn_time_ms: 8616.063
    sample_throughput: 13232.635
    sample_time_ms: 4585.028
    update_time_ms: 38.399
  timestamp: 1602435957
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 32
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     32 |          429.582 | 1941504 |  240.098 |              285.869 |               116.02 |            825.279 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3468.250105440742
    time_step_min: 3169
  date: 2020-10-11_17-06-11
  done: false
  episode_len_mean: 825.065795023197
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 240.52776180190762
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 77
  episodes_total: 2371
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.8031792521476746
        entropy_coeff: 0.0001
        kl: 0.005553171876817942
        model: {}
        policy_loss: -0.01612788438796997
        total_loss: 8.300242805480957
        vf_explained_var: 0.9860474467277527
        vf_loss: 8.31534023284912
    num_steps_sampled: 2002176
    num_steps_trained: 2002176
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 29.581250000000004
    gpu_util_percent0: 0.35562499999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4124999999999996
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1223281700627921
    mean_env_wait_ms: 0.6628588726792333
    mean_inference_ms: 4.640094658273114
    mean_raw_obs_processing_ms: 0.2657817092715322
  time_since_restore: 442.9096932411194
  time_this_iter_s: 13.327325344085693
  time_total_s: 442.9096932411194
  timers:
    learn_throughput: 7030.852
    learn_time_ms: 8629.395
    sample_throughput: 13232.848
    sample_time_ms: 4584.954
    update_time_ms: 36.654
  timestamp: 1602435971
  timesteps_since_restore: 0
  timesteps_total: 2002176
  training_iteration: 33
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     33 |           442.91 | 2002176 |  240.528 |              285.869 |               116.02 |            825.066 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3465.450204081633
    time_step_min: 3169
  date: 2020-10-11_17-06-24
  done: false
  episode_len_mean: 824.8004081632653
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 240.95198928056067
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2450
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7909312963485717
        entropy_coeff: 0.0001
        kl: 0.005124823935329914
        model: {}
        policy_loss: -0.01604766147211194
        total_loss: 9.810696792602538
        vf_explained_var: 0.9840415716171265
        vf_loss: 9.825798606872558
    num_steps_sampled: 2062848
    num_steps_trained: 2062848
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.133333333333333
    gpu_util_percent0: 0.30199999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12221712290621335
    mean_env_wait_ms: 0.662918483655754
    mean_inference_ms: 4.632864690694645
    mean_raw_obs_processing_ms: 0.2654895156539146
  time_since_restore: 456.11310720443726
  time_this_iter_s: 13.203413963317871
  time_total_s: 456.11310720443726
  timers:
    learn_throughput: 7023.159
    learn_time_ms: 8638.847
    sample_throughput: 13256.401
    sample_time_ms: 4576.808
    update_time_ms: 36.91
  timestamp: 1602435984
  timesteps_since_restore: 0
  timesteps_total: 2062848
  training_iteration: 34
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     34 |          456.113 | 2062848 |  240.952 |              285.869 |               116.02 |              824.8 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3462.2349683544303
    time_step_min: 3169
  date: 2020-10-11_17-06-37
  done: false
  episode_len_mean: 824.4497626582279
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 241.4391462089246
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 2528
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7874290704727173
        entropy_coeff: 0.0001
        kl: 0.00535367289558053
        model: {}
        policy_loss: -0.015816283226013184
        total_loss: 8.599153900146485
        vf_explained_var: 0.9853988885879517
        vf_loss: 8.613978385925293
    num_steps_sampled: 2123520
    num_steps_trained: 2123520
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.49333333333334
    gpu_util_percent0: 0.216
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4066666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12211555584709413
    mean_env_wait_ms: 0.6629740046312342
    mean_inference_ms: 4.626136688473584
    mean_raw_obs_processing_ms: 0.26521712181088564
  time_since_restore: 469.2933053970337
  time_this_iter_s: 13.180198192596436
  time_total_s: 469.2933053970337
  timers:
    learn_throughput: 7022.106
    learn_time_ms: 8640.143
    sample_throughput: 13255.844
    sample_time_ms: 4577.0
    update_time_ms: 35.12
  timestamp: 1602435997
  timesteps_since_restore: 0
  timesteps_total: 2123520
  training_iteration: 35
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     35 |          469.293 | 2123520 |  241.439 |              285.869 |               116.02 |             824.45 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3459.7096279248176
    time_step_min: 3169
  date: 2020-10-11_17-06-50
  done: false
  episode_len_mean: 824.2305331799002
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 241.82177354674468
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2607
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7673699378967285
        entropy_coeff: 0.0001
        kl: 0.005303710792213678
        model: {}
        policy_loss: -0.015531449741683901
        total_loss: 9.45707950592041
        vf_explained_var: 0.9850137829780579
        vf_loss: 9.47162685394287
    num_steps_sampled: 2184192
    num_steps_trained: 2184192
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.100000000000005
    gpu_util_percent0: 0.35733333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.3999999999999995
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12201606510947806
    mean_env_wait_ms: 0.6630275192758508
    mean_inference_ms: 4.619606166424895
    mean_raw_obs_processing_ms: 0.2649526561353501
  time_since_restore: 482.20643401145935
  time_this_iter_s: 12.91312861442566
  time_total_s: 482.20643401145935
  timers:
    learn_throughput: 7048.69
    learn_time_ms: 8607.557
    sample_throughput: 13243.701
    sample_time_ms: 4581.197
    update_time_ms: 35.662
  timestamp: 1602436010
  timesteps_since_restore: 0
  timesteps_total: 2184192
  training_iteration: 36
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     36 |          482.206 | 2184192 |  241.822 |              285.869 |               116.02 |            824.231 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3457.1046165301564
    time_step_min: 3169
  date: 2020-10-11_17-07-04
  done: false
  episode_len_mean: 823.8406552494415
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 242.21647224290552
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2686
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7685246229171753
        entropy_coeff: 0.0001
        kl: 0.005332937464118004
        model: {}
        policy_loss: -0.015074999816715718
        total_loss: 10.110424613952636
        vf_explained_var: 0.9831159710884094
        vf_loss: 10.124510192871094
    num_steps_sampled: 2244864
    num_steps_trained: 2244864
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.986666666666665
    gpu_util_percent0: 0.21733333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.413333333333333
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12192045698792538
    mean_env_wait_ms: 0.6630821228645726
    mean_inference_ms: 4.613375373327867
    mean_raw_obs_processing_ms: 0.2647002996524587
  time_since_restore: 495.5734956264496
  time_this_iter_s: 13.367061614990234
  time_total_s: 495.5734956264496
  timers:
    learn_throughput: 7053.22
    learn_time_ms: 8602.028
    sample_throughput: 13260.153
    sample_time_ms: 4575.513
    update_time_ms: 34.164
  timestamp: 1602436024
  timesteps_since_restore: 0
  timesteps_total: 2244864
  training_iteration: 37
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     37 |          495.573 | 2244864 |  242.216 |              285.869 |               116.02 |            823.841 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3454.723327305606
    time_step_min: 3169
  date: 2020-10-11_17-07-17
  done: false
  episode_len_mean: 823.6517179023508
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 242.57727364056467
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2765
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7509182333946228
        entropy_coeff: 0.0001
        kl: 0.00532805984839797
        model: {}
        policy_loss: -0.01556666032411158
        total_loss: 8.812029457092285
        vf_explained_var: 0.9855332374572754
        vf_loss: 8.826605606079102
    num_steps_sampled: 2305536
    num_steps_trained: 2305536
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.399999999999995
    gpu_util_percent0: 0.30666666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1218289547870796
    mean_env_wait_ms: 0.6631316070995541
    mean_inference_ms: 4.607434521026551
    mean_raw_obs_processing_ms: 0.26445964874058325
  time_since_restore: 508.811297416687
  time_this_iter_s: 13.237801790237427
  time_total_s: 508.811297416687
  timers:
    learn_throughput: 7055.281
    learn_time_ms: 8599.516
    sample_throughput: 13273.23
    sample_time_ms: 4571.005
    update_time_ms: 33.96
  timestamp: 1602436037
  timesteps_since_restore: 0
  timesteps_total: 2305536
  training_iteration: 38
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     38 |          508.811 | 2305536 |  242.577 |              285.869 |               116.02 |            823.652 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3452.709563994374
    time_step_min: 3169
  date: 2020-10-11_17-07-31
  done: false
  episode_len_mean: 823.4556962025316
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 242.8823892937816
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2844
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.2
        cur_lr: 5.0e-05
        entropy: 0.7447753310203552
        entropy_coeff: 0.0001
        kl: 0.0048968297429382805
        model: {}
        policy_loss: -0.015580352582037448
        total_loss: 9.111823654174804
        vf_explained_var: 0.9850462079048157
        vf_loss: 9.126498985290528
    num_steps_sampled: 2366208
    num_steps_trained: 2366208
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.513333333333332
    gpu_util_percent0: 0.2633333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666667
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12174184795210517
    mean_env_wait_ms: 0.6631790300195834
    mean_inference_ms: 4.6017615142201835
    mean_raw_obs_processing_ms: 0.2642305875492525
  time_since_restore: 522.0965356826782
  time_this_iter_s: 13.285238265991211
  time_total_s: 522.0965356826782
  timers:
    learn_throughput: 7052.68
    learn_time_ms: 8602.688
    sample_throughput: 13331.865
    sample_time_ms: 4550.901
    update_time_ms: 28.603
  timestamp: 1602436051
  timesteps_since_restore: 0
  timesteps_total: 2366208
  training_iteration: 39
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     39 |          522.097 | 2366208 |  242.882 |              285.869 |               116.02 |            823.456 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3450.620595278823
    time_step_min: 3169
  date: 2020-10-11_17-07-44
  done: false
  episode_len_mean: 823.1758467328087
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 243.19889970522874
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 2923
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7417478799819947
        entropy_coeff: 0.0001
        kl: 0.005580193921923637
        model: {}
        policy_loss: -0.014510815404355526
        total_loss: 12.874164962768555
        vf_explained_var: 0.9789665341377258
        vf_loss: 12.88819179534912
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 30.2375
    gpu_util_percent0: 0.279375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1216586542633823
    mean_env_wait_ms: 0.6632265024916639
    mean_inference_ms: 4.596327036509114
    mean_raw_obs_processing_ms: 0.26401309805925205
  time_since_restore: 535.3937585353851
  time_this_iter_s: 13.29722285270691
  time_total_s: 535.3937585353851
  timers:
    learn_throughput: 7046.039
    learn_time_ms: 8610.795
    sample_throughput: 13341.934
    sample_time_ms: 4547.467
    update_time_ms: 31.597
  timestamp: 1602436064
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 40
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     40 |          535.394 | 2426880 |  243.199 |              285.869 |               116.02 |            823.176 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3448.715856095936
    time_step_min: 3169
  date: 2020-10-11_17-07-57
  done: false
  episode_len_mean: 822.8854097268488
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 243.48749655112073
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 79
  episodes_total: 3002
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7339844465255737
        entropy_coeff: 0.0001
        kl: 0.006111154239624739
        model: {}
        policy_loss: -0.015608488768339156
        total_loss: 11.980018806457519
        vf_explained_var: 0.9802266359329224
        vf_loss: 11.995088958740235
    num_steps_sampled: 2487552
    num_steps_trained: 2487552
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.42
    gpu_util_percent0: 0.23133333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.42
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12157892967965228
    mean_env_wait_ms: 0.663273490904163
    mean_inference_ms: 4.591119657704121
    mean_raw_obs_processing_ms: 0.2638063039447684
  time_since_restore: 548.6631767749786
  time_this_iter_s: 13.269418239593506
  time_total_s: 548.6631767749786
  timers:
    learn_throughput: 7052.88
    learn_time_ms: 8602.443
    sample_throughput: 13331.597
    sample_time_ms: 4550.993
    update_time_ms: 32.097
  timestamp: 1602436077
  timesteps_since_restore: 0
  timesteps_total: 2487552
  training_iteration: 41
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     41 |          548.663 | 2487552 |  243.487 |              285.869 |               116.02 |            822.885 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3446.6886363636363
    time_step_min: 3169
  date: 2020-10-11_17-08-11
  done: false
  episode_len_mean: 822.4435064935064
  episode_reward_max: 285.86868686868667
  episode_reward_mean: 243.7946510560146
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 78
  episodes_total: 3080
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7262521624565125
        entropy_coeff: 0.0001
        kl: 0.006154146790504455
        model: {}
        policy_loss: -0.015755283087491988
        total_loss: 9.44112319946289
        vf_explained_var: 0.9838287234306335
        vf_loss: 9.456335830688477
    num_steps_sampled: 2548224
    num_steps_trained: 2548224
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.84
    gpu_util_percent0: 0.22133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12150454386287002
    mean_env_wait_ms: 0.6633217404919456
    mean_inference_ms: 4.586176429609928
    mean_raw_obs_processing_ms: 0.26361004253853243
  time_since_restore: 561.9079780578613
  time_this_iter_s: 13.24480128288269
  time_total_s: 561.9079780578613
  timers:
    learn_throughput: 7057.202
    learn_time_ms: 8597.175
    sample_throughput: 13307.3
    sample_time_ms: 4559.302
    update_time_ms: 30.279
  timestamp: 1602436091
  timesteps_since_restore: 0
  timesteps_total: 2548224
  training_iteration: 42
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     42 |          561.908 | 2548224 |  243.795 |              285.869 |               116.02 |            822.444 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3444.746907706946
    time_step_min: 3165
  date: 2020-10-11_17-08-24
  done: false
  episode_len_mean: 822.0837297811609
  episode_reward_max: 286.47474747474723
  episode_reward_mean: 244.08885236763436
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 73
  episodes_total: 3153
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7186455488204956
        entropy_coeff: 0.0001
        kl: 0.006436989177018404
        model: {}
        policy_loss: -0.016508414037525655
        total_loss: 9.405962371826172
        vf_explained_var: 0.983354926109314
        vf_loss: 9.421898651123048
    num_steps_sampled: 2608896
    num_steps_trained: 2608896
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 31.86666666666667
    gpu_util_percent0: 0.29133333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.406666666666666
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12143571320545399
    mean_env_wait_ms: 0.6633688809883619
    mean_inference_ms: 4.5816863029749255
    mean_raw_obs_processing_ms: 0.26344034611318246
  time_since_restore: 575.0996234416962
  time_this_iter_s: 13.191645383834839
  time_total_s: 575.0996234416962
  timers:
    learn_throughput: 7071.932
    learn_time_ms: 8579.268
    sample_throughput: 13294.24
    sample_time_ms: 4563.781
    update_time_ms: 30.251
  timestamp: 1602436104
  timesteps_since_restore: 0
  timesteps_total: 2608896
  training_iteration: 43
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     43 |            575.1 | 2608896 |  244.089 |              286.475 |               116.02 |            822.084 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3442.7816377171216
    time_step_min: 3165
  date: 2020-10-11_17-08-37
  done: false
  episode_len_mean: 821.6752481389578
  episode_reward_max: 286.47474747474723
  episode_reward_mean: 244.3866205479108
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 71
  episodes_total: 3224
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7102178335189819
        entropy_coeff: 0.0001
        kl: 0.006092663202434778
        model: {}
        policy_loss: -0.016193848475813864
        total_loss: 7.7973559379577635
        vf_explained_var: 0.9857897758483887
        vf_loss: 7.813011932373047
    num_steps_sampled: 2669568
    num_steps_trained: 2669568
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.887500000000003
    gpu_util_percent0: 0.2425
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.40625
    vram_util_percent0: 0.09732699245654312
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12137278802646505
    mean_env_wait_ms: 0.6634230624932012
    mean_inference_ms: 4.57756997437402
    mean_raw_obs_processing_ms: 0.2632741801390405
  time_since_restore: 588.4316520690918
  time_this_iter_s: 13.33202862739563
  time_total_s: 588.4316520690918
  timers:
    learn_throughput: 7073.867
    learn_time_ms: 8576.921
    sample_throughput: 13274.721
    sample_time_ms: 4570.491
    update_time_ms: 31.606
  timestamp: 1602436117
  timesteps_since_restore: 0
  timesteps_total: 2669568
  training_iteration: 44
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc            |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | RUNNING  | 172.17.0.4:781 |     44 |          588.432 | 2669568 |  244.387 |              286.475 |               116.02 |            821.675 |
+-------------------------+----------+----------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_026bc_00000:
  custom_metrics:
    time_step_max: 4290
    time_step_mean: 3441.1525629360026
    time_step_min: 3165
  date: 2020-10-11_17-08-51
  done: true
  episode_len_mean: 821.3342432514407
  episode_reward_max: 286.47474747474723
  episode_reward_mean: 244.6334500602016
  episode_reward_min: 116.02020202020218
  episodes_this_iter: 73
  episodes_total: 3297
  experiment_id: 4c0ffe358e0b4717af216334c2d4e494
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.1
        cur_lr: 5.0e-05
        entropy: 0.7077718257904053
        entropy_coeff: 0.0001
        kl: 0.006186609528958798
        model: {}
        policy_loss: -0.015196176012977958
        total_loss: 7.407955741882324
        vf_explained_var: 0.9867888689041138
        vf_loss: 7.422603893280029
    num_steps_sampled: 2730240
    num_steps_trained: 2730240
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 32.04
    gpu_util_percent0: 0.21333333333333335
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.4266666666666663
    vram_util_percent0: 0.09732699245654314
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 781
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.12130648337287785
    mean_env_wait_ms: 0.6634709343990309
    mean_inference_ms: 4.573401399284867
    mean_raw_obs_processing_ms: 0.2631097375270568
  time_since_restore: 601.6401448249817
  time_this_iter_s: 13.208492755889893
  time_total_s: 601.6401448249817
  timers:
    learn_throughput: 7075.695
    learn_time_ms: 8574.705
    sample_throughput: 13262.134
    sample_time_ms: 4574.829
    update_time_ms: 31.714
  timestamp: 1602436131
  timesteps_since_restore: 0
  timesteps_total: 2730240
  training_iteration: 45
  trial_id: 026bc_00000
  
== Status ==
Memory usage on this node: 25.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | TERMINATED |       |     45 |           601.64 | 2730240 |  244.633 |              286.475 |               116.02 |            821.334 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.35 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_026bc_00000 | TERMINATED |       |     45 |           601.64 | 2730240 |  244.633 |              286.475 |               116.02 |            821.334 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Traceback (most recent call last):
  File "train.py", line 72, in <module>
    train_func()
  File "train.py", line 57, in train_func
    result = analysis.dataframe(None, 'min').to_dict('index')[0]
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 89, in dataframe
    metric = self._validate_metric(metric)
  File "/root/miniconda3/lib/python3.8/site-packages/ray/tune/analysis/experiment_analysis.py", line 64, in _validate_metric
    raise ValueError(
ValueError: No `metric` has been passed and  `default_metric` has not been set. Please specify the `metric` parameter.
