2020-10-15 00:08:57,604	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.5/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_9f737_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=10000)[0m 2020-10-15 00:09:00,384	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=9944)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9944)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9983)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9983)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9919)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9919)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9947)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9947)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9959)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9959)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9974)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9974)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9943)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9943)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9993)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9993)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9948)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9948)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9997)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9997)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9981)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9981)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9937)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9937)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9946)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9946)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9955)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9955)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9992)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9992)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9990)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9990)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9971)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9971)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9881)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9881)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9923)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9923)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9880)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9880)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9863)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9863)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9933)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9933)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9879)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9879)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9867)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9867)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9921)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9921)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9913)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9913)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9930)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9930)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9932)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9932)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9941)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9941)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9865)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9865)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9918)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9918)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9870)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9870)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9866)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9866)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9989)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9989)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9892)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9892)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9885)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9885)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9929)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9929)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9876)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9876)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9934)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9934)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9887)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9887)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9976)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9976)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9968)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9968)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9878)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9878)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9860)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9860)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=10002)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=10002)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9862)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9862)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9886)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9886)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9957)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9957)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9985)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9985)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9861)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9861)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9916)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9916)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9920)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9920)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9882)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9882)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9871)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9871)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9999)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9999)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9906)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9906)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9998)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9998)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9883)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9883)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9972)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9972)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9868)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9868)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9936)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9936)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9893)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9893)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9953)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9953)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9874)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9874)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9910)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9910)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9931)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9931)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9864)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9864)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9962)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9962)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9873)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9873)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9986)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9986)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9877)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9877)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9888)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9888)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9977)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9977)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9956)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9956)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9872)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9872)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9890)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9890)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9942)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9942)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9884)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9884)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=9965)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=9965)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4248
    time_step_mean: 3616.3166666666666
    time_step_min: 3355
  date: 2020-10-15_00-09-34
  done: false
  episode_len_mean: 904.8481012658228
  episode_reward_max: 246.595959595959
  episode_reward_mean: 201.8721391126452
  episode_reward_min: 106.74747474747424
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.169209361076355
        entropy_coeff: 0.0005000000000000001
        kl: 0.004283593191454808
        model: {}
        policy_loss: -0.00855300908733625
        total_loss: 369.24242401123047
        vf_explained_var: 0.588043749332428
        vf_loss: 369.2507044474284
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 28.308823529411764
    gpu_util_percent0: 0.24882352941176467
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.5588235294117645
    vram_util_percent0: 0.08639283853915461
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.17094790254635883
    mean_env_wait_ms: 1.187593961211037
    mean_inference_ms: 5.793155877448114
    mean_raw_obs_processing_ms: 0.45294038525846647
  time_since_restore: 28.730098485946655
  time_this_iter_s: 28.730098485946655
  time_total_s: 28.730098485946655
  timers:
    learn_throughput: 8235.586
    learn_time_ms: 19645.475
    sample_throughput: 17931.92
    sample_time_ms: 9022.57
    update_time_ms: 23.628
  timestamp: 1602720574
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      1 |          28.7301 | 161792 |  201.872 |              246.596 |              106.747 |            904.848 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4331
    time_step_mean: 3626.0323741007196
    time_step_min: 3254
  date: 2020-10-15_00-10-02
  done: false
  episode_len_mean: 904.6550632911392
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 199.26425648893976
  episode_reward_min: 94.17171717171675
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1384726762771606
        entropy_coeff: 0.0005000000000000001
        kl: 0.007555847250235577
        model: {}
        policy_loss: -0.009366699125772962
        total_loss: 89.9572073618571
        vf_explained_var: 0.8351356983184814
        vf_loss: 89.96638933817546
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.44516129032258
    gpu_util_percent0: 0.3219354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7516129032258054
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16657399115353677
    mean_env_wait_ms: 1.1831481429772845
    mean_inference_ms: 5.565220439953701
    mean_raw_obs_processing_ms: 0.44252000661692986
  time_since_restore: 56.011972188949585
  time_this_iter_s: 27.28187370300293
  time_total_s: 56.011972188949585
  timers:
    learn_throughput: 8282.647
    learn_time_ms: 19533.852
    sample_throughput: 19259.64
    sample_time_ms: 8400.572
    update_time_ms: 29.519
  timestamp: 1602720602
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      2 |           56.012 | 323584 |  199.264 |              257.354 |              94.1717 |            904.655 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4331
    time_step_mean: 3633.0733944954127
    time_step_min: 3254
  date: 2020-10-15_00-10-28
  done: false
  episode_len_mean: 899.1413502109705
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 199.64480245492877
  episode_reward_min: 94.17171717171675
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.123505175113678
        entropy_coeff: 0.0005000000000000001
        kl: 0.00928467131840686
        model: {}
        policy_loss: -0.012755329177404443
        total_loss: 44.79078483581543
        vf_explained_var: 0.9124049544334412
        vf_loss: 44.803174336751304
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.758064516129036
    gpu_util_percent0: 0.2732258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1634911009489243
    mean_env_wait_ms: 1.1822619467178406
    mean_inference_ms: 5.379016913162651
    mean_raw_obs_processing_ms: 0.43375167973580975
  time_since_restore: 82.89162373542786
  time_this_iter_s: 26.87965154647827
  time_total_s: 82.89162373542786
  timers:
    learn_throughput: 8267.332
    learn_time_ms: 19570.039
    sample_throughput: 20254.708
    sample_time_ms: 7987.871
    update_time_ms: 27.577
  timestamp: 1602720628
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      3 |          82.8916 | 485376 |  199.645 |              257.354 |              94.1717 |            899.141 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3626.841750841751
    time_step_min: 3254
  date: 2020-10-15_00-10-55
  done: false
  episode_len_mean: 894.5094936708861
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 200.8474939266076
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1135420103867848
        entropy_coeff: 0.0005000000000000001
        kl: 0.008282240596599877
        model: {}
        policy_loss: -0.011898660280470116
        total_loss: 39.95386600494385
        vf_explained_var: 0.9208271503448486
        vf_loss: 39.96549320220947
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20645161290323
    gpu_util_percent0: 0.35032258064516125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16128984587099632
    mean_env_wait_ms: 1.1825670485868078
    mean_inference_ms: 5.240050006026633
    mean_raw_obs_processing_ms: 0.4269047385789676
  time_since_restore: 109.48796939849854
  time_this_iter_s: 26.59634566307068
  time_total_s: 109.48796939849854
  timers:
    learn_throughput: 8261.622
    learn_time_ms: 19583.563
    sample_throughput: 20975.271
    sample_time_ms: 7713.464
    update_time_ms: 29.326
  timestamp: 1602720655
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      4 |          109.488 | 647168 |  200.847 |              257.354 |              74.7778 |            894.509 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3616.928191489362
    time_step_min: 3254
  date: 2020-10-15_00-11-22
  done: false
  episode_len_mean: 890.3
  episode_reward_max: 257.35353535353516
  episode_reward_mean: 202.15541490857922
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0863073368867238
        entropy_coeff: 0.0005000000000000001
        kl: 0.00802083076753964
        model: {}
        policy_loss: -0.010518901399336755
        total_loss: 31.891902287801106
        vf_explained_var: 0.937171459197998
        vf_loss: 31.902161121368408
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.08333333333333
    gpu_util_percent0: 0.3816666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15964628838752715
    mean_env_wait_ms: 1.1832958500383928
    mean_inference_ms: 5.133695330779668
    mean_raw_obs_processing_ms: 0.42144444414659077
  time_since_restore: 135.958069562912
  time_this_iter_s: 26.470100164413452
  time_total_s: 135.958069562912
  timers:
    learn_throughput: 8261.697
    learn_time_ms: 19583.387
    sample_throughput: 21481.859
    sample_time_ms: 7531.564
    update_time_ms: 31.048
  timestamp: 1602720682
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      5 |          135.958 | 808960 |  202.155 |              257.354 |              74.7778 |              890.3 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3608.4322033898306
    time_step_min: 3244
  date: 2020-10-15_00-11-48
  done: false
  episode_len_mean: 885.3126272912424
  episode_reward_max: 258.8686868686869
  episode_reward_mean: 203.83381678290004
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 982
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0417972803115845
        entropy_coeff: 0.0005000000000000001
        kl: 0.00803802243899554
        model: {}
        policy_loss: -0.011555042661105594
        total_loss: 29.145253658294678
        vf_explained_var: 0.9571675658226013
        vf_loss: 29.156526406606037
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.180645161290325
    gpu_util_percent0: 0.2558064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7580645161290316
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1581226443615389
    mean_env_wait_ms: 1.1848186427157779
    mean_inference_ms: 5.0345378567290675
    mean_raw_obs_processing_ms: 0.4164582485386112
  time_since_restore: 162.61888885498047
  time_this_iter_s: 26.66081929206848
  time_total_s: 162.61888885498047
  timers:
    learn_throughput: 8248.343
    learn_time_ms: 19615.09
    sample_throughput: 21838.199
    sample_time_ms: 7408.67
    update_time_ms: 33.076
  timestamp: 1602720708
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.0/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      6 |          162.619 | 970752 |  203.834 |              258.869 |              74.7778 |            885.313 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3593.012234910277
    time_step_min: 3212
  date: 2020-10-15_00-12-15
  done: false
  episode_len_mean: 879.5348101265823
  episode_reward_max: 263.7171717171712
  episode_reward_mean: 206.26859576780438
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 282
  episodes_total: 1264
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.052981048822403
        entropy_coeff: 0.0005000000000000001
        kl: 0.00756885576993227
        model: {}
        policy_loss: -0.013547717448091134
        total_loss: 24.316144307454426
        vf_explained_var: 0.9633116722106934
        vf_loss: 24.329460938771565
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.013333333333335
    gpu_util_percent0: 0.39966666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7599999999999993
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1565206306828766
    mean_env_wait_ms: 1.1865071185278862
    mean_inference_ms: 4.932341414609288
    mean_raw_obs_processing_ms: 0.4112464006082905
  time_since_restore: 189.00014066696167
  time_this_iter_s: 26.3812518119812
  time_total_s: 189.00014066696167
  timers:
    learn_throughput: 8252.218
    learn_time_ms: 19605.881
    sample_throughput: 22120.116
    sample_time_ms: 7314.247
    update_time_ms: 32.916
  timestamp: 1602720735
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      7 |              189 | 1132544 |  206.269 |              263.717 |              74.7778 |            879.535 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3584.440751445087
    time_step_min: 3212
  date: 2020-10-15_00-12-41
  done: false
  episode_len_mean: 875.0407876230661
  episode_reward_max: 263.7171717171712
  episode_reward_mean: 207.88383838383817
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.025655210018158
        entropy_coeff: 0.0005000000000000001
        kl: 0.007761195845281084
        model: {}
        policy_loss: -0.012660732085350901
        total_loss: 18.453331629435223
        vf_explained_var: 0.9655329585075378
        vf_loss: 18.465729077657063
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34516129032258
    gpu_util_percent0: 0.3109677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15583748938428565
    mean_env_wait_ms: 1.187454669747146
    mean_inference_ms: 4.888001078753845
    mean_raw_obs_processing_ms: 0.40903534366246874
  time_since_restore: 215.54513716697693
  time_this_iter_s: 26.54499650001526
  time_total_s: 215.54513716697693
  timers:
    learn_throughput: 8250.801
    learn_time_ms: 19609.248
    sample_throughput: 22305.403
    sample_time_ms: 7253.489
    update_time_ms: 33.038
  timestamp: 1602720761
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      8 |          215.545 | 1294336 |  207.884 |              263.717 |              74.7778 |            875.041 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3574.6420233463036
    time_step_min: 3212
  date: 2020-10-15_00-13-08
  done: false
  episode_len_mean: 870.4873417721519
  episode_reward_max: 267.20202020201987
  episode_reward_mean: 209.55290244214274
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 1580
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0090048958857853
        entropy_coeff: 0.0005000000000000001
        kl: 0.007344354099283616
        model: {}
        policy_loss: -0.011860693940737596
        total_loss: 18.446773687998455
        vf_explained_var: 0.963460385799408
        vf_loss: 18.458404541015625
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64
    gpu_util_percent0: 0.3076666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15524327475942012
    mean_env_wait_ms: 1.1884924449180287
    mean_inference_ms: 4.849254921164308
    mean_raw_obs_processing_ms: 0.40706853865381953
  time_since_restore: 242.03585982322693
  time_this_iter_s: 26.49072265625
  time_total_s: 242.03585982322693
  timers:
    learn_throughput: 8254.068
    learn_time_ms: 19601.486
    sample_throughput: 22439.126
    sample_time_ms: 7210.263
    update_time_ms: 32.63
  timestamp: 1602720788
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |      9 |          242.036 | 1456128 |  209.553 |              267.202 |              74.7778 |            870.487 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3564.078730904818
    time_step_min: 3179
  date: 2020-10-15_00-13-35
  done: false
  episode_len_mean: 866.4586206896552
  episode_reward_max: 268.71717171717165
  episode_reward_mean: 211.2001044932078
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 160
  episodes_total: 1740
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9668994247913361
        entropy_coeff: 0.0005000000000000001
        kl: 0.007379486147935192
        model: {}
        policy_loss: -0.011729328679696968
        total_loss: 17.816092491149902
        vf_explained_var: 0.9671626687049866
        vf_loss: 17.82756741841634
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290323
    gpu_util_percent0: 0.2751612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15471082439687983
    mean_env_wait_ms: 1.1895678207898561
    mean_inference_ms: 4.814675146427402
    mean_raw_obs_processing_ms: 0.40527460129288456
  time_since_restore: 268.6315803527832
  time_this_iter_s: 26.595720529556274
  time_total_s: 268.6315803527832
  timers:
    learn_throughput: 8247.771
    learn_time_ms: 19616.452
    sample_throughput: 22580.808
    sample_time_ms: 7165.022
    update_time_ms: 32.679
  timestamp: 1602720815
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     10 |          268.632 | 1617920 |    211.2 |              268.717 |              74.7778 |            866.459 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3544.0879284649777
    time_step_min: 3179
  date: 2020-10-15_00-14-01
  done: false
  episode_len_mean: 859.2252559726962
  episode_reward_max: 274.02020202020196
  episode_reward_mean: 214.2406414215286
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 311
  episodes_total: 2051
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9493600130081177
        entropy_coeff: 0.0005000000000000001
        kl: 0.006886738818138838
        model: {}
        policy_loss: -0.010961758438497782
        total_loss: 22.52125628789266
        vf_explained_var: 0.9688637852668762
        vf_loss: 22.53200387954712
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.676666666666666
    gpu_util_percent0: 0.388
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1538476932801404
    mean_env_wait_ms: 1.1917891571692483
    mean_inference_ms: 4.75937967278483
    mean_raw_obs_processing_ms: 0.4024835159644878
  time_since_restore: 295.1137487888336
  time_this_iter_s: 26.482168436050415
  time_total_s: 295.1137487888336
  timers:
    learn_throughput: 8250.411
    learn_time_ms: 19610.174
    sample_throughput: 23297.817
    sample_time_ms: 6944.513
    update_time_ms: 33.227
  timestamp: 1602720841
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     11 |          295.114 | 1779712 |  214.241 |               274.02 |              74.7778 |            859.225 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3534.241030358786
    time_step_min: 3150
  date: 2020-10-15_00-14-28
  done: false
  episode_len_mean: 856.0664556962025
  episode_reward_max: 274.02020202020196
  episode_reward_mean: 215.7192677224321
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 161
  episodes_total: 2212
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9360186457633972
        entropy_coeff: 0.0005000000000000001
        kl: 0.006805013477181395
        model: {}
        policy_loss: -0.011606539716012776
        total_loss: 14.837892452875773
        vf_explained_var: 0.9714512228965759
        vf_loss: 14.849286794662476
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.287096774193547
    gpu_util_percent0: 0.25870967741935486
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783870967741935
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15348148417292845
    mean_env_wait_ms: 1.1927904740123547
    mean_inference_ms: 4.73572338575409
    mean_raw_obs_processing_ms: 0.40127073417341513
  time_since_restore: 321.9000060558319
  time_this_iter_s: 26.78625726699829
  time_total_s: 321.9000060558319
  timers:
    learn_throughput: 8235.81
    learn_time_ms: 19644.942
    sample_throughput: 23584.373
    sample_time_ms: 6860.136
    update_time_ms: 31.91
  timestamp: 1602720868
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     12 |            321.9 | 1941504 |  215.719 |               274.02 |              74.7778 |            856.066 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3524.1796740994855
    time_step_min: 3150
  date: 2020-10-15_00-14-55
  done: false
  episode_len_mean: 854.0987341772152
  episode_reward_max: 274.02020202020196
  episode_reward_mean: 217.11411584196375
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 2370
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.917464887102445
        entropy_coeff: 0.0005000000000000001
        kl: 0.007187117318001886
        model: {}
        policy_loss: -0.01065789075801149
        total_loss: 11.536466916402182
        vf_explained_var: 0.9758577346801758
        vf_loss: 11.5468643506368
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97096774193549
    gpu_util_percent0: 0.343225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15315363962124803
    mean_env_wait_ms: 1.1937145687701551
    mean_inference_ms: 4.714687194935809
    mean_raw_obs_processing_ms: 0.400195768623255
  time_since_restore: 348.6009407043457
  time_this_iter_s: 26.700934648513794
  time_total_s: 348.6009407043457
  timers:
    learn_throughput: 8233.567
    learn_time_ms: 19650.293
    sample_throughput: 23665.369
    sample_time_ms: 6836.657
    update_time_ms: 31.561
  timestamp: 1602720895
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     13 |          348.601 | 2103296 |  217.114 |               274.02 |              74.7778 |            854.099 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3515.129614767255
    time_step_min: 3137
  date: 2020-10-15_00-15-22
  done: false
  episode_len_mean: 851.9537549407114
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 218.34401325508028
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 160
  episodes_total: 2530
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8839244097471237
        entropy_coeff: 0.0005000000000000001
        kl: 0.006774009283011158
        model: {}
        policy_loss: -0.011904717515183924
        total_loss: 13.830247561136881
        vf_explained_var: 0.9751328825950623
        vf_loss: 13.84191664059957
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.13333333333333
    gpu_util_percent0: 0.35033333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15285127764483544
    mean_env_wait_ms: 1.194592846137878
    mean_inference_ms: 4.6952842156997745
    mean_raw_obs_processing_ms: 0.3991805966812884
  time_since_restore: 375.15163135528564
  time_this_iter_s: 26.55069065093994
  time_total_s: 375.15163135528564
  timers:
    learn_throughput: 8237.468
    learn_time_ms: 19640.987
    sample_throughput: 23652.986
    sample_time_ms: 6840.236
    update_time_ms: 31.176
  timestamp: 1602720922
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     14 |          375.152 | 2265088 |  218.344 |              275.081 |              74.7778 |            851.954 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3500.4163090128754
    time_step_min: 3137
  date: 2020-10-15_00-15-48
  done: false
  episode_len_mean: 847.9442484121383
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 220.51883335828273
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 304
  episodes_total: 2834
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8736105312903722
        entropy_coeff: 0.0005000000000000001
        kl: 0.0062687892544393735
        model: {}
        policy_loss: -0.010797865291048462
        total_loss: 14.933183749516806
        vf_explained_var: 0.9792088866233826
        vf_loss: 14.943791627883911
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.54193548387097
    gpu_util_percent0: 0.3800000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7548387096774185
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15235260251439564
    mean_env_wait_ms: 1.1962167763160128
    mean_inference_ms: 4.663330613865914
    mean_raw_obs_processing_ms: 0.39753328623294293
  time_since_restore: 401.94057154655457
  time_this_iter_s: 26.78894019126892
  time_total_s: 401.94057154655457
  timers:
    learn_throughput: 8232.988
    learn_time_ms: 19651.673
    sample_throughput: 23582.39
    sample_time_ms: 6860.713
    update_time_ms: 30.771
  timestamp: 1602720948
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     15 |          401.941 | 2426880 |  220.519 |              275.081 |              74.7778 |            847.944 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3493.861673414305
    time_step_min: 3137
  date: 2020-10-15_00-16-15
  done: false
  episode_len_mean: 846.0689540306462
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 221.5660805254408
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 168
  episodes_total: 3002
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8535224894682566
        entropy_coeff: 0.0005000000000000001
        kl: 0.006619990609275798
        model: {}
        policy_loss: -0.010143043616532546
        total_loss: 11.625003178914389
        vf_explained_var: 0.9782131314277649
        vf_loss: 11.634911298751831
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.370967741935484
    gpu_util_percent0: 0.3490322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15211568552633428
    mean_env_wait_ms: 1.196988007284087
    mean_inference_ms: 4.647983633394699
    mean_raw_obs_processing_ms: 0.39674340010106607
  time_since_restore: 428.43650460243225
  time_this_iter_s: 26.495933055877686
  time_total_s: 428.43650460243225
  timers:
    learn_throughput: 8248.401
    learn_time_ms: 19614.953
    sample_throughput: 23511.963
    sample_time_ms: 6881.263
    update_time_ms: 28.856
  timestamp: 1602720975
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     16 |          428.437 | 2588672 |  221.566 |              275.081 |              74.7778 |            846.069 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3487.5397181294043
    time_step_min: 3137
  date: 2020-10-15_00-16-42
  done: false
  episode_len_mean: 844.1161392405063
  episode_reward_max: 275.08080808080814
  episode_reward_mean: 222.54211417977223
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 3160
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8495060205459595
        entropy_coeff: 0.0005000000000000001
        kl: 0.006349431467242539
        model: {}
        policy_loss: -0.012107226405836021
        total_loss: 10.89086365699768
        vf_explained_var: 0.9773384928703308
        vf_loss: 10.902761061986288
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17419354838709
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.151907690990476
    mean_env_wait_ms: 1.1976845917150558
    mean_inference_ms: 4.634606806620557
    mean_raw_obs_processing_ms: 0.39603891572763283
  time_since_restore: 455.04881954193115
  time_this_iter_s: 26.6123149394989
  time_total_s: 455.04881954193115
  timers:
    learn_throughput: 8248.781
    learn_time_ms: 19614.05
    sample_throughput: 23457.097
    sample_time_ms: 6897.358
    update_time_ms: 35.326
  timestamp: 1602721002
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     17 |          455.049 | 2750464 |  222.542 |              275.081 |              74.7778 |            844.116 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3480.486388384755
    time_step_min: 3137
  date: 2020-10-15_00-17-08
  done: false
  episode_len_mean: 841.9829545454545
  episode_reward_max: 277.35353535353516
  episode_reward_mean: 223.7197543617997
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 3344
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8174087206522623
        entropy_coeff: 0.0005000000000000001
        kl: 0.007216097398971518
        model: {}
        policy_loss: -0.011498963499131301
        total_loss: 13.150360743204752
        vf_explained_var: 0.9775157570838928
        vf_loss: 13.16154670715332
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.01333333333334
    gpu_util_percent0: 0.3830000000000001
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7666666666666657
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15168161615333584
    mean_env_wait_ms: 1.1984862688164801
    mean_inference_ms: 4.620082219016671
    mean_raw_obs_processing_ms: 0.3952751533121484
  time_since_restore: 481.6438024044037
  time_this_iter_s: 26.594982862472534
  time_total_s: 481.6438024044037
  timers:
    learn_throughput: 8251.544
    learn_time_ms: 19607.481
    sample_throughput: 23418.907
    sample_time_ms: 6908.606
    update_time_ms: 34.446
  timestamp: 1602721028
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     18 |          481.644 | 2912256 |   223.72 |              277.354 |              74.7778 |            841.983 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3468.8776077885955
    time_step_min: 3121
  date: 2020-10-15_00-17-35
  done: false
  episode_len_mean: 839.2174511423067
  episode_reward_max: 277.5050505050504
  episode_reward_mean: 225.50545087539297
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 289
  episodes_total: 3633
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8106405337651571
        entropy_coeff: 0.0005000000000000001
        kl: 0.005711381828101973
        model: {}
        policy_loss: -0.009296051983255893
        total_loss: 12.59952704111735
        vf_explained_var: 0.9803258776664734
        vf_loss: 12.608657042185465
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.75483870967743
    gpu_util_percent0: 0.28483870967741937
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.151367585970707
    mean_env_wait_ms: 1.1996596875649765
    mean_inference_ms: 4.599951609180639
    mean_raw_obs_processing_ms: 0.3942011080200939
  time_since_restore: 508.0756661891937
  time_this_iter_s: 26.43186378479004
  time_total_s: 508.0756661891937
  timers:
    learn_throughput: 8250.274
    learn_time_ms: 19610.5
    sample_throughput: 23450.034
    sample_time_ms: 6899.436
    update_time_ms: 33.618
  timestamp: 1602721055
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     19 |          508.076 | 3074048 |  225.505 |              277.505 |              74.7778 |            839.217 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3462.9064997336177
    time_step_min: 3121
  date: 2020-10-15_00-18-02
  done: false
  episode_len_mean: 837.8349156118144
  episode_reward_max: 278.565656565657
  episode_reward_mean: 226.38830552359022
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 3792
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8071876466274261
        entropy_coeff: 0.0005000000000000001
        kl: 0.0058333837271978455
        model: {}
        policy_loss: -0.01144625450069725
        total_loss: 8.772553523381552
        vf_explained_var: 0.9817249774932861
        vf_loss: 8.783819993336996
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.870000000000005
    gpu_util_percent0: 0.35566666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15120915467600324
    mean_env_wait_ms: 1.2002451882085081
    mean_inference_ms: 4.589855027835156
    mean_raw_obs_processing_ms: 0.39367051451352464
  time_since_restore: 534.6363084316254
  time_this_iter_s: 26.56064224243164
  time_total_s: 534.6363084316254
  timers:
    learn_throughput: 8254.886
    learn_time_ms: 19599.544
    sample_throughput: 23424.257
    sample_time_ms: 6907.028
    update_time_ms: 32.648
  timestamp: 1602721082
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     20 |          534.636 | 3235840 |  226.388 |              278.566 |              74.7778 |            837.835 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3456.5080500894455
    time_step_min: 3121
  date: 2020-10-15_00-18-29
  done: false
  episode_len_mean: 836.770437863832
  episode_reward_max: 278.565656565657
  episode_reward_mean: 227.26982045205267
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 3951
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7912125339110693
        entropy_coeff: 0.0005000000000000001
        kl: 0.006185428200600048
        model: {}
        policy_loss: -0.010615241718672527
        total_loss: 9.660934766133627
        vf_explained_var: 0.9788644313812256
        vf_loss: 9.671326955159506
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.906451612903226
    gpu_util_percent0: 0.31483870967741934
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15106098070703067
    mean_env_wait_ms: 1.2008025745108575
    mean_inference_ms: 4.5803060522886225
    mean_raw_obs_processing_ms: 0.3931655106518138
  time_since_restore: 561.2219786643982
  time_this_iter_s: 26.585670232772827
  time_total_s: 561.2219786643982
  timers:
    learn_throughput: 8249.6
    learn_time_ms: 19612.104
    sample_throughput: 23437.636
    sample_time_ms: 6903.085
    update_time_ms: 33.204
  timestamp: 1602721109
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     21 |          561.222 | 3397632 |   227.27 |              278.566 |              74.7778 |             836.77 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3447.855593056895
    time_step_min: 3114
  date: 2020-10-15_00-18-55
  done: false
  episode_len_mean: 835.048972766364
  episode_reward_max: 279.6262626262625
  episode_reward_mean: 228.53481783916556
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 4186
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.759943018356959
        entropy_coeff: 0.0005000000000000001
        kl: 0.006279778007107477
        model: {}
        policy_loss: -0.010360730404499918
        total_loss: 12.314131418863932
        vf_explained_var: 0.9803383946418762
        vf_loss: 12.324244101842245
    num_steps_sampled: 3559424
    num_steps_trained: 3559424
  iterations_since_restore: 22
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.436666666666667
    gpu_util_percent0: 0.2933333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15086190942658512
    mean_env_wait_ms: 1.2016401480627996
    mean_inference_ms: 4.56720711910717
    mean_raw_obs_processing_ms: 0.39248960888174184
  time_since_restore: 587.5729217529297
  time_this_iter_s: 26.350943088531494
  time_total_s: 587.5729217529297
  timers:
    learn_throughput: 8262.667
    learn_time_ms: 19581.087
    sample_throughput: 23481.431
    sample_time_ms: 6890.21
    update_time_ms: 33.127
  timestamp: 1602721135
  timesteps_since_restore: 0
  timesteps_total: 3559424
  training_iteration: 22
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     22 |          587.573 | 3559424 |  228.535 |              279.626 |              74.7778 |            835.049 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3440.1703534777653
    time_step_min: 3114
  date: 2020-10-15_00-19-22
  done: false
  episode_len_mean: 833.3927198733891
  episode_reward_max: 279.6262626262625
  episode_reward_mean: 229.59495246381965
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 237
  episodes_total: 4423
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7440361777941386
        entropy_coeff: 0.0005000000000000001
        kl: 0.005954018871610363
        model: {}
        policy_loss: -0.011595885191733638
        total_loss: 9.518235127131144
        vf_explained_var: 0.9834404587745667
        vf_loss: 9.52960737546285
    num_steps_sampled: 3721216
    num_steps_trained: 3721216
  iterations_since_restore: 23
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.338709677419356
    gpu_util_percent0: 0.35290322580645167
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322573
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15067230088102995
    mean_env_wait_ms: 1.2023678744851813
    mean_inference_ms: 4.555219834186362
    mean_raw_obs_processing_ms: 0.3918540535437323
  time_since_restore: 614.0264339447021
  time_this_iter_s: 26.45351219177246
  time_total_s: 614.0264339447021
  timers:
    learn_throughput: 8269.942
    learn_time_ms: 19563.862
    sample_throughput: 23520.798
    sample_time_ms: 6878.678
    update_time_ms: 36.543
  timestamp: 1602721162
  timesteps_since_restore: 0
  timesteps_total: 3721216
  training_iteration: 23
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     23 |          614.026 | 3721216 |  229.595 |              279.626 |              74.7778 |            833.393 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3435.027068661972
    time_step_min: 3114
  date: 2020-10-15_00-19-49
  done: false
  episode_len_mean: 832.1865997381057
  episode_reward_max: 279.6262626262625
  episode_reward_mean: 230.303748969397
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 4582
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7401020477215449
        entropy_coeff: 0.0005000000000000001
        kl: 0.006353482875662546
        model: {}
        policy_loss: -0.009180035073465357
        total_loss: 8.316651423772177
        vf_explained_var: 0.9818807244300842
        vf_loss: 8.325566053390503
    num_steps_sampled: 3883008
    num_steps_trained: 3883008
  iterations_since_restore: 24
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.948387096774198
    gpu_util_percent0: 0.42258064516129035
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15055418741000465
    mean_env_wait_ms: 1.2028429459209469
    mean_inference_ms: 4.54767478635704
    mean_raw_obs_processing_ms: 0.3914611923107602
  time_since_restore: 640.9122281074524
  time_this_iter_s: 26.885794162750244
  time_total_s: 640.9122281074524
  timers:
    learn_throughput: 8257.066
    learn_time_ms: 19594.369
    sample_throughput: 23539.465
    sample_time_ms: 6873.223
    update_time_ms: 43.879
  timestamp: 1602721189
  timesteps_since_restore: 0
  timesteps_total: 3883008
  training_iteration: 24
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     24 |          640.912 | 3883008 |  230.304 |              279.626 |              74.7778 |            832.187 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3430.264319049639
    time_step_min: 3074
  date: 2020-10-15_00-20-16
  done: false
  episode_len_mean: 830.9013047138047
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 231.0115485664727
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 170
  episodes_total: 4752
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.7222304195165634
        entropy_coeff: 0.0005000000000000001
        kl: 0.005677717078166704
        model: {}
        policy_loss: -0.010766375771102807
        total_loss: 10.416849772135416
        vf_explained_var: 0.9797022938728333
        vf_loss: 10.427409569422403
    num_steps_sampled: 4044800
    num_steps_trained: 4044800
  iterations_since_restore: 25
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.046666666666667
    gpu_util_percent0: 0.36000000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15043898011218082
    mean_env_wait_ms: 1.2033502737367148
    mean_inference_ms: 4.540036643312593
    mean_raw_obs_processing_ms: 0.3910651932139323
  time_since_restore: 667.6709923744202
  time_this_iter_s: 26.758764266967773
  time_total_s: 667.6709923744202
  timers:
    learn_throughput: 8257.378
    learn_time_ms: 19593.629
    sample_throughput: 23555.598
    sample_time_ms: 6868.516
    update_time_ms: 44.475
  timestamp: 1602721216
  timesteps_since_restore: 0
  timesteps_total: 4044800
  training_iteration: 25
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     25 |          667.671 | 4044800 |  231.012 |              284.626 |              74.7778 |            830.901 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3422.069
    time_step_min: 3074
  date: 2020-10-15_00-20-42
  done: false
  episode_len_mean: 829.0049622866217
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 232.20574943560246
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 286
  episodes_total: 5038
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6987026333808899
        entropy_coeff: 0.0005000000000000001
        kl: 0.005648046149872243
        model: {}
        policy_loss: -0.011446453854053592
        total_loss: 10.859757820765177
        vf_explained_var: 0.9836289882659912
        vf_loss: 10.870988845825195
    num_steps_sampled: 4206592
    num_steps_trained: 4206592
  iterations_since_restore: 26
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.274193548387096
    gpu_util_percent0: 0.25451612903225806
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7645161290322577
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1502514455112009
    mean_env_wait_ms: 1.2042070041085082
    mean_inference_ms: 4.528036541020236
    mean_raw_obs_processing_ms: 0.3904635008607498
  time_since_restore: 694.2627158164978
  time_this_iter_s: 26.591723442077637
  time_total_s: 694.2627158164978
  timers:
    learn_throughput: 8248.673
    learn_time_ms: 19614.307
    sample_throughput: 23600.52
    sample_time_ms: 6855.442
    update_time_ms: 46.228
  timestamp: 1602721242
  timesteps_since_restore: 0
  timesteps_total: 4206592
  training_iteration: 26
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     26 |          694.263 | 4206592 |  232.206 |              284.626 |              74.7778 |            829.005 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3417.3390649149924
    time_step_min: 3074
  date: 2020-10-15_00-21-09
  done: false
  episode_len_mean: 827.6762562332183
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 232.9396612848856
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 5214
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6922043909629186
        entropy_coeff: 0.0005000000000000001
        kl: 0.005940411356277764
        model: {}
        policy_loss: -0.012036716704945624
        total_loss: 8.719095865885416
        vf_explained_var: 0.9824223518371582
        vf_loss: 8.730884631474813
    num_steps_sampled: 4368384
    num_steps_trained: 4368384
  iterations_since_restore: 27
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.563333333333336
    gpu_util_percent0: 0.3833333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.776666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15014408678413085
    mean_env_wait_ms: 1.2046750421332824
    mean_inference_ms: 4.52123137827616
    mean_raw_obs_processing_ms: 0.39011747789952794
  time_since_restore: 720.7837166786194
  time_this_iter_s: 26.521000862121582
  time_total_s: 720.7837166786194
  timers:
    learn_throughput: 8246.143
    learn_time_ms: 19620.324
    sample_throughput: 23629.145
    sample_time_ms: 6847.137
    update_time_ms: 38.881
  timestamp: 1602721269
  timesteps_since_restore: 0
  timesteps_total: 4368384
  training_iteration: 27
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     27 |          720.784 | 4368384 |   232.94 |              284.626 |              74.7778 |            827.676 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3413.4507966260544
    time_step_min: 3074
  date: 2020-10-15_00-21-36
  done: false
  episode_len_mean: 826.4688256095292
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 233.53257871850826
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 159
  episodes_total: 5373
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6884655207395554
        entropy_coeff: 0.0005000000000000001
        kl: 0.006287429481744766
        model: {}
        policy_loss: -0.010294094875765344
        total_loss: 8.898841540018717
        vf_explained_var: 0.9810356497764587
        vf_loss: 8.908851226170858
    num_steps_sampled: 4530176
    num_steps_trained: 4530176
  iterations_since_restore: 28
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.64193548387097
    gpu_util_percent0: 0.2532258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.774193548387097
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15005254849974523
    mean_env_wait_ms: 1.2050986641669674
    mean_inference_ms: 4.515346872416021
    mean_raw_obs_processing_ms: 0.38981877841672213
  time_since_restore: 747.2981333732605
  time_this_iter_s: 26.514416694641113
  time_total_s: 747.2981333732605
  timers:
    learn_throughput: 8246.387
    learn_time_ms: 19619.743
    sample_throughput: 23655.773
    sample_time_ms: 6839.43
    update_time_ms: 38.345
  timestamp: 1602721296
  timesteps_since_restore: 0
  timesteps_total: 4530176
  training_iteration: 28
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     28 |          747.298 | 4530176 |  233.533 |              284.626 |              74.7778 |            826.469 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3407.133273703041
    time_step_min: 3074
  date: 2020-10-15_00-22-02
  done: false
  episode_len_mean: 824.4921819474058
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 234.46217505545854
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 255
  episodes_total: 5628
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6605731795231501
        entropy_coeff: 0.0005000000000000001
        kl: 0.005549012799747288
        model: {}
        policy_loss: -0.012337252808113893
        total_loss: 12.105455001195272
        vf_explained_var: 0.9811687469482422
        vf_loss: 12.11756706237793
    num_steps_sampled: 4691968
    num_steps_trained: 4691968
  iterations_since_restore: 29
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5
    gpu_util_percent0: 0.3351612903225807
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14991296195508375
    mean_env_wait_ms: 1.2058094924332714
    mean_inference_ms: 4.5063341523252
    mean_raw_obs_processing_ms: 0.38937121499757177
  time_since_restore: 773.9206173419952
  time_this_iter_s: 26.62248396873474
  time_total_s: 773.9206173419952
  timers:
    learn_throughput: 8244.421
    learn_time_ms: 19624.423
    sample_throughput: 23638.951
    sample_time_ms: 6844.297
    update_time_ms: 38.29
  timestamp: 1602721322
  timesteps_since_restore: 0
  timesteps_total: 4691968
  training_iteration: 29
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     29 |          773.921 | 4691968 |  234.462 |              284.626 |              74.7778 |            824.492 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3401.373450413223
    time_step_min: 3074
  date: 2020-10-15_00-22-29
  done: false
  episode_len_mean: 822.993670886076
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 235.34133327804207
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 5846
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6386907746394476
        entropy_coeff: 0.0005000000000000001
        kl: 0.005903509367878239
        model: {}
        policy_loss: -0.00886095035281187
        total_loss: 8.310452580451965
        vf_explained_var: 0.9841588139533997
        vf_loss: 8.319042523701986
    num_steps_sampled: 4853760
    num_steps_trained: 4853760
  iterations_since_restore: 30
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.526666666666664
    gpu_util_percent0: 0.2903333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14980273466502003
    mean_env_wait_ms: 1.206371132161415
    mean_inference_ms: 4.499379202132717
    mean_raw_obs_processing_ms: 0.38901758777884804
  time_since_restore: 800.46417927742
  time_this_iter_s: 26.543561935424805
  time_total_s: 800.46417927742
  timers:
    learn_throughput: 8246.702
    learn_time_ms: 19618.995
    sample_throughput: 23630.261
    sample_time_ms: 6846.814
    update_time_ms: 38.314
  timestamp: 1602721349
  timesteps_since_restore: 0
  timesteps_total: 4853760
  training_iteration: 30
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     30 |          800.464 | 4853760 |  235.341 |              284.626 |              74.7778 |            822.994 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3397.6528662420383
    time_step_min: 3074
  date: 2020-10-15_00-22-56
  done: false
  episode_len_mean: 821.9566955363091
  episode_reward_max: 284.6262626262628
  episode_reward_mean: 235.89486133823235
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 158
  episodes_total: 6004
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6477035929759344
        entropy_coeff: 0.0005000000000000001
        kl: 0.005589286796748638
        model: {}
        policy_loss: -0.010180077918145495
        total_loss: 9.052189270655314
        vf_explained_var: 0.9800416827201843
        vf_loss: 9.062134265899658
    num_steps_sampled: 5015552
    num_steps_trained: 5015552
  iterations_since_restore: 31
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.816129032258072
    gpu_util_percent0: 0.377741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1497264991074802
    mean_env_wait_ms: 1.2067697315054715
    mean_inference_ms: 4.494496828184678
    mean_raw_obs_processing_ms: 0.38877167591528417
  time_since_restore: 827.0254950523376
  time_this_iter_s: 26.561315774917603
  time_total_s: 827.0254950523376
  timers:
    learn_throughput: 8247.543
    learn_time_ms: 19616.993
    sample_throughput: 23631.74
    sample_time_ms: 6846.385
    update_time_ms: 36.992
  timestamp: 1602721376
  timesteps_since_restore: 0
  timesteps_total: 5015552
  training_iteration: 31
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     31 |          827.025 | 5015552 |  235.895 |              284.626 |              74.7778 |            821.957 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3392.510992563854
    time_step_min: 3074
  date: 2020-10-15_00-23-23
  done: false
  episode_len_mean: 820.6823586118252
  episode_reward_max: 287.5050505050501
  episode_reward_mean: 236.66034866661465
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 220
  episodes_total: 6224
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6274365981419882
        entropy_coeff: 0.0005000000000000001
        kl: 0.005409304557057719
        model: {}
        policy_loss: -0.01206011434745354
        total_loss: 10.890360116958618
        vf_explained_var: 0.9811581969261169
        vf_loss: 10.90219267209371
    num_steps_sampled: 5177344
    num_steps_trained: 5177344
  iterations_since_restore: 32
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.540000000000003
    gpu_util_percent0: 0.4133333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1496272170752398
    mean_env_wait_ms: 1.2073281473864224
    mean_inference_ms: 4.487896939140712
    mean_raw_obs_processing_ms: 0.3884465743267189
  time_since_restore: 853.4982194900513
  time_this_iter_s: 26.472724437713623
  time_total_s: 853.4982194900513
  timers:
    learn_throughput: 8241.238
    learn_time_ms: 19632.003
    sample_throughput: 23647.666
    sample_time_ms: 6841.775
    update_time_ms: 38.065
  timestamp: 1602721403
  timesteps_since_restore: 0
  timesteps_total: 5177344
  training_iteration: 32
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     32 |          853.498 | 5177344 |   236.66 |              287.505 |              74.7778 |            820.682 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3387.1559390547263
    time_step_min: 3043
  date: 2020-10-15_00-23-49
  done: false
  episode_len_mean: 819.4449768160742
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 237.48224126894905
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 246
  episodes_total: 6470
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6052823215723038
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051997468108311296
        model: {}
        policy_loss: -0.011560283159875931
        total_loss: 9.271020571390787
        vf_explained_var: 0.9844850897789001
        vf_loss: 9.282363414764404
    num_steps_sampled: 5339136
    num_steps_trained: 5339136
  iterations_since_restore: 33
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.383870967741935
    gpu_util_percent0: 0.3341935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14951933098583015
    mean_env_wait_ms: 1.207910835622344
    mean_inference_ms: 4.481224142853916
    mean_raw_obs_processing_ms: 0.38810189864292555
  time_since_restore: 880.0135300159454
  time_this_iter_s: 26.515310525894165
  time_total_s: 880.0135300159454
  timers:
    learn_throughput: 8239.738
    learn_time_ms: 19635.576
    sample_throughput: 23638.387
    sample_time_ms: 6844.46
    update_time_ms: 35.337
  timestamp: 1602721429
  timesteps_since_restore: 0
  timesteps_total: 5339136
  training_iteration: 33
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     33 |          880.014 | 5339136 |  237.482 |              289.323 |              74.7778 |            819.445 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3383.511821764171
    time_step_min: 3043
  date: 2020-10-15_00-24-16
  done: false
  episode_len_mean: 818.700120554551
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 238.00715564323153
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 166
  episodes_total: 6636
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.6068141659100851
        entropy_coeff: 0.0005000000000000001
        kl: 0.005375104529472689
        model: {}
        policy_loss: -0.012939858686877415
        total_loss: 6.613195101420085
        vf_explained_var: 0.9857916831970215
        vf_loss: 6.625900705655416
    num_steps_sampled: 5500928
    num_steps_trained: 5500928
  iterations_since_restore: 34
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14193548387097
    gpu_util_percent0: 0.37677419354838715
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1494494331495515
    mean_env_wait_ms: 1.2082782935775978
    mean_inference_ms: 4.476798886075526
    mean_raw_obs_processing_ms: 0.3878821205186627
  time_since_restore: 906.5565230846405
  time_this_iter_s: 26.54299306869507
  time_total_s: 906.5565230846405
  timers:
    learn_throughput: 8245.425
    learn_time_ms: 19622.033
    sample_throughput: 23684.678
    sample_time_ms: 6831.083
    update_time_ms: 27.985
  timestamp: 1602721456
  timesteps_since_restore: 0
  timesteps_total: 5500928
  training_iteration: 34
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     34 |          906.557 | 5500928 |  238.007 |              289.323 |              74.7778 |              818.7 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3379.7174265899366
    time_step_min: 3043
  date: 2020-10-15_00-24-43
  done: false
  episode_len_mean: 817.8542920029347
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 238.5968711324543
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 6815
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.596410483121872
        entropy_coeff: 0.0005000000000000001
        kl: 0.005001630789289872
        model: {}
        policy_loss: -0.012441018969790699
        total_loss: 8.19734021027883
        vf_explained_var: 0.9839007258415222
        vf_loss: 8.209579269091288
    num_steps_sampled: 5662720
    num_steps_trained: 5662720
  iterations_since_restore: 35
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.450000000000003
    gpu_util_percent0: 0.3293333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14938065943350515
    mean_env_wait_ms: 1.2086613134584605
    mean_inference_ms: 4.472240344281545
    mean_raw_obs_processing_ms: 0.38765448835780575
  time_since_restore: 933.0349502563477
  time_this_iter_s: 26.478427171707153
  time_total_s: 933.0349502563477
  timers:
    learn_throughput: 8251.478
    learn_time_ms: 19607.64
    sample_throughput: 23728.339
    sample_time_ms: 6818.514
    update_time_ms: 26.73
  timestamp: 1602721483
  timesteps_since_restore: 0
  timesteps_total: 5662720
  training_iteration: 35
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     35 |          933.035 | 5662720 |  238.597 |              289.323 |              74.7778 |            817.854 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3374.4842642472354
    time_step_min: 3043
  date: 2020-10-15_00-25-09
  done: false
  episode_len_mean: 816.5851663846588
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 239.42137961681104
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 277
  episodes_total: 7092
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.5682249665260315
        entropy_coeff: 0.0005000000000000001
        kl: 0.004860180236088733
        model: {}
        policy_loss: -0.010135394695680588
        total_loss: 9.11709451675415
        vf_explained_var: 0.9856011867523193
        vf_loss: 9.127027670542398
    num_steps_sampled: 5824512
    num_steps_trained: 5824512
  iterations_since_restore: 36
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.322580645161292
    gpu_util_percent0: 0.24064516129032254
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14927413650853796
    mean_env_wait_ms: 1.2092703439440418
    mean_inference_ms: 4.465589418769661
    mean_raw_obs_processing_ms: 0.38732513688372366
  time_since_restore: 959.6594631671906
  time_this_iter_s: 26.624512910842896
  time_total_s: 959.6594631671906
  timers:
    learn_throughput: 8250.197
    learn_time_ms: 19610.683
    sample_throughput: 23724.507
    sample_time_ms: 6819.615
    update_time_ms: 24.843
  timestamp: 1602721509
  timesteps_since_restore: 0
  timesteps_total: 5824512
  training_iteration: 36
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     36 |          959.659 | 5824512 |  239.421 |              289.323 |              74.7778 |            816.585 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3371.3118948824344
    time_step_min: 3043
  date: 2020-10-15_00-25-36
  done: false
  episode_len_mean: 815.8380572372042
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 239.95424664921083
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 7268
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5694400072097778
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050308507634326816
        model: {}
        policy_loss: -0.010465515117781857
        total_loss: 6.280025005340576
        vf_explained_var: 0.986748456954956
        vf_loss: 6.290523727734883
    num_steps_sampled: 5986304
    num_steps_trained: 5986304
  iterations_since_restore: 37
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.812903225806455
    gpu_util_percent0: 0.21290322580645163
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903227
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1492102137733837
    mean_env_wait_ms: 1.2096191088434303
    mean_inference_ms: 4.461593384739127
    mean_raw_obs_processing_ms: 0.3871272303613737
  time_since_restore: 986.5910856723785
  time_this_iter_s: 26.93162250518799
  time_total_s: 986.5910856723785
  timers:
    learn_throughput: 8238.535
    learn_time_ms: 19638.442
    sample_throughput: 23690.002
    sample_time_ms: 6829.548
    update_time_ms: 27.364
  timestamp: 1602721536
  timesteps_since_restore: 0
  timesteps_total: 5986304
  training_iteration: 37
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     37 |          986.591 | 5986304 |  239.954 |              289.323 |              74.7778 |            815.838 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3368.2841493102515
    time_step_min: 3043
  date: 2020-10-15_00-26-03
  done: false
  episode_len_mean: 815.0859795479009
  episode_reward_max: 289.3232323232328
  episode_reward_mean: 240.43111551467302
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 164
  episodes_total: 7432
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5638417750597
        entropy_coeff: 0.0005000000000000001
        kl: 0.006015786590675513
        model: {}
        policy_loss: -0.010645508669161549
        total_loss: 7.669543464978536
        vf_explained_var: 0.9834616184234619
        vf_loss: 7.680170098940532
    num_steps_sampled: 6148096
    num_steps_trained: 6148096
  iterations_since_restore: 38
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.97741935483871
    gpu_util_percent0: 0.33161290322580644
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.780645161290322
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1491544466264861
    mean_env_wait_ms: 1.2099421443453784
    mean_inference_ms: 4.457992893614814
    mean_raw_obs_processing_ms: 0.3869466368673488
  time_since_restore: 1013.231281042099
  time_this_iter_s: 26.64019536972046
  time_total_s: 1013.231281042099
  timers:
    learn_throughput: 8231.215
    learn_time_ms: 19655.907
    sample_throughput: 23715.155
    sample_time_ms: 6822.304
    update_time_ms: 28.993
  timestamp: 1602721563
  timesteps_since_restore: 0
  timesteps_total: 6148096
  training_iteration: 38
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     38 |          1013.23 | 6148096 |  240.431 |              289.323 |              74.7778 |            815.086 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3363.321894572025
    time_step_min: 3034
  date: 2020-10-15_00-26-30
  done: false
  episode_len_mean: 813.8701635938717
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 241.1807585069075
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 270
  episodes_total: 7702
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5422514081001282
        entropy_coeff: 0.0005000000000000001
        kl: 0.005400637979619205
        model: {}
        policy_loss: -0.009160482943116222
        total_loss: 9.511011521021524
        vf_explained_var: 0.9845170378684998
        vf_loss: 9.520172437032064
    num_steps_sampled: 6309888
    num_steps_trained: 6309888
  iterations_since_restore: 39
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.63
    gpu_util_percent0: 0.35033333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1490601522814971
    mean_env_wait_ms: 1.2104713610431936
    mean_inference_ms: 4.452226933824835
    mean_raw_obs_processing_ms: 0.38666735244358524
  time_since_restore: 1039.6506071090698
  time_this_iter_s: 26.419326066970825
  time_total_s: 1039.6506071090698
  timers:
    learn_throughput: 8236.033
    learn_time_ms: 19644.408
    sample_throughput: 23716.309
    sample_time_ms: 6821.972
    update_time_ms: 29.004
  timestamp: 1602721590
  timesteps_since_restore: 0
  timesteps_total: 6309888
  training_iteration: 39
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     39 |          1039.65 | 6309888 |  241.181 |              291.141 |              74.7778 |             813.87 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3359.8478371501274
    time_step_min: 3034
  date: 2020-10-15_00-26-56
  done: false
  episode_len_mean: 813.0749556849836
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 241.69890088527714
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 196
  episodes_total: 7898
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5340648392836252
        entropy_coeff: 0.0005000000000000001
        kl: 0.005210299471703668
        model: {}
        policy_loss: -0.0101865164275902
        total_loss: 6.542128682136536
        vf_explained_var: 0.9868605136871338
        vf_loss: 6.552321950594584
    num_steps_sampled: 6471680
    num_steps_trained: 6471680
  iterations_since_restore: 40
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.225806451612907
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14900233007331562
    mean_env_wait_ms: 1.210833401132249
    mean_inference_ms: 4.448398490939118
    mean_raw_obs_processing_ms: 0.38647637258230794
  time_since_restore: 1065.887858390808
  time_this_iter_s: 26.23725128173828
  time_total_s: 1065.887858390808
  timers:
    learn_throughput: 8247.75
    learn_time_ms: 19616.5
    sample_throughput: 23728.371
    sample_time_ms: 6818.504
    update_time_ms: 29.159
  timestamp: 1602721616
  timesteps_since_restore: 0
  timesteps_total: 6471680
  training_iteration: 40
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     40 |          1065.89 | 6471680 |  241.699 |              291.141 |              74.7778 |            813.075 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3356.760064813661
    time_step_min: 3034
  date: 2020-10-15_00-27-23
  done: false
  episode_len_mean: 812.4011909192408
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 242.15439095081814
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 163
  episodes_total: 8061
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5370505700508753
        entropy_coeff: 0.0005000000000000001
        kl: 0.005642464306826393
        model: {}
        policy_loss: -0.011000998706246415
        total_loss: 6.366736014684041
        vf_explained_var: 0.985722005367279
        vf_loss: 6.377723256746928
    num_steps_sampled: 6633472
    num_steps_trained: 6633472
  iterations_since_restore: 41
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.886666666666667
    gpu_util_percent0: 0.3426666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14895158851531043
    mean_env_wait_ms: 1.2111253764704082
    mean_inference_ms: 4.4452213843557145
    mean_raw_obs_processing_ms: 0.38631882117206595
  time_since_restore: 1091.8973212242126
  time_this_iter_s: 26.00946283340454
  time_total_s: 1091.8973212242126
  timers:
    learn_throughput: 8271.603
    learn_time_ms: 19559.932
    sample_throughput: 23724.468
    sample_time_ms: 6819.626
    update_time_ms: 28.72
  timestamp: 1602721643
  timesteps_since_restore: 0
  timesteps_total: 6633472
  training_iteration: 41
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     41 |           1091.9 | 6633472 |  242.154 |              291.141 |              74.7778 |            812.401 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3352.4010158423025
    time_step_min: 3034
  date: 2020-10-15_00-27-49
  done: false
  episode_len_mean: 811.5095702419646
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 242.76242745256823
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 246
  episodes_total: 8307
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.517037163178126
        entropy_coeff: 0.0005000000000000001
        kl: 0.005597573200551172
        model: {}
        policy_loss: -0.010149982525035739
        total_loss: 8.83277416229248
        vf_explained_var: 0.9849687218666077
        vf_loss: 8.842902421951294
    num_steps_sampled: 6795264
    num_steps_trained: 6795264
  iterations_since_restore: 42
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.233333333333334
    gpu_util_percent0: 0.3456666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.77
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14888110112665104
    mean_env_wait_ms: 1.2115752239817805
    mean_inference_ms: 4.440630342286543
    mean_raw_obs_processing_ms: 0.3860942524541397
  time_since_restore: 1118.1019840240479
  time_this_iter_s: 26.204662799835205
  time_total_s: 1118.1019840240479
  timers:
    learn_throughput: 8289.761
    learn_time_ms: 19517.088
    sample_throughput: 23668.022
    sample_time_ms: 6835.89
    update_time_ms: 27.21
  timestamp: 1602721669
  timesteps_since_restore: 0
  timesteps_total: 6795264
  training_iteration: 42
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     42 |           1118.1 | 6795264 |  242.762 |              291.141 |              74.7778 |             811.51 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3348.921554770318
    time_step_min: 3034
  date: 2020-10-15_00-28-16
  done: false
  episode_len_mean: 810.6337945590994
  episode_reward_max: 291.1414141414139
  episode_reward_mean: 243.2807306176208
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 221
  episodes_total: 8528
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4924461245536804
        entropy_coeff: 0.0005000000000000001
        kl: 0.005016234703361988
        model: {}
        policy_loss: -0.009520547231659293
        total_loss: 6.755296031634013
        vf_explained_var: 0.9874246120452881
        vf_loss: 6.764811992645264
    num_steps_sampled: 6957056
    num_steps_trained: 6957056
  iterations_since_restore: 43
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35333333333334
    gpu_util_percent0: 0.30766666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14881798094651985
    mean_env_wait_ms: 1.2119512070930285
    mean_inference_ms: 4.436817563043459
    mean_raw_obs_processing_ms: 0.3859071640012789
  time_since_restore: 1144.4843730926514
  time_this_iter_s: 26.382389068603516
  time_total_s: 1144.4843730926514
  timers:
    learn_throughput: 8299.495
    learn_time_ms: 19494.199
    sample_throughput: 23631.693
    sample_time_ms: 6846.399
    update_time_ms: 26.425
  timestamp: 1602721696
  timesteps_since_restore: 0
  timesteps_total: 6957056
  training_iteration: 43
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     43 |          1144.48 | 6957056 |  243.281 |              291.141 |              74.7778 |            810.634 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3346.3664624985554
    time_step_min: 3034
  date: 2020-10-15_00-28-42
  done: false
  episode_len_mean: 810.0192152801749
  episode_reward_max: 291.5959595959604
  episode_reward_mean: 243.67902706735978
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 163
  episodes_total: 8691
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.5038283864657084
        entropy_coeff: 0.0005000000000000001
        kl: 0.00519260298460722
        model: {}
        policy_loss: -0.012088534994594132
        total_loss: 6.511295437812805
        vf_explained_var: 0.9850991368293762
        vf_loss: 6.52337630589803
    num_steps_sampled: 7118848
    num_steps_trained: 7118848
  iterations_since_restore: 44
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.729032258064517
    gpu_util_percent0: 0.2848387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148773678922714
    mean_env_wait_ms: 1.212219603714408
    mean_inference_ms: 4.4340200571927
    mean_raw_obs_processing_ms: 0.3857703150697465
  time_since_restore: 1170.8589658737183
  time_this_iter_s: 26.374592781066895
  time_total_s: 1170.8589658737183
  timers:
    learn_throughput: 8309.264
    learn_time_ms: 19471.279
    sample_throughput: 23613.601
    sample_time_ms: 6851.645
    update_time_ms: 26.089
  timestamp: 1602721722
  timesteps_since_restore: 0
  timesteps_total: 7118848
  training_iteration: 44
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     44 |          1170.86 | 7118848 |  243.679 |              291.596 |              74.7778 |            810.019 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3342.93476056338
    time_step_min: 3034
  date: 2020-10-15_00-29-09
  done: false
  episode_len_mean: 809.2556939302143
  episode_reward_max: 291.5959595959604
  episode_reward_mean: 244.2004970608134
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 8913
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4844573761026065
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053589244683583575
        model: {}
        policy_loss: -0.011921500651321063
        total_loss: 7.8355962832768755
        vf_explained_var: 0.9858234524726868
        vf_loss: 7.847492138544719
    num_steps_sampled: 7280640
    num_steps_trained: 7280640
  iterations_since_restore: 45
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.616666666666667
    gpu_util_percent0: 0.3773333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1487172231774407
    mean_env_wait_ms: 1.2125976405037528
    mean_inference_ms: 4.43035706939879
    mean_raw_obs_processing_ms: 0.38559490259100077
  time_since_restore: 1197.0701005458832
  time_this_iter_s: 26.211134672164917
  time_total_s: 1197.0701005458832
  timers:
    learn_throughput: 8320.85
    learn_time_ms: 19444.167
    sample_throughput: 23618.418
    sample_time_ms: 6850.247
    update_time_ms: 26.839
  timestamp: 1602721749
  timesteps_since_restore: 0
  timesteps_total: 7280640
  training_iteration: 45
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     45 |          1197.07 | 7280640 |    244.2 |              291.596 |              74.7778 |            809.256 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3339.027192982456
    time_step_min: 3034
  date: 2020-10-15_00-29-35
  done: false
  episode_len_mean: 808.4010701026425
  episode_reward_max: 292.95959595959573
  episode_reward_mean: 244.7556411461194
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 245
  episodes_total: 9158
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.4631484200557073
        entropy_coeff: 0.0005000000000000001
        kl: 0.005080961001416047
        model: {}
        policy_loss: -0.008889351251127664
        total_loss: 7.061241348584493
        vf_explained_var: 0.9872291088104248
        vf_loss: 7.07010833422343
    num_steps_sampled: 7442432
    num_steps_trained: 7442432
  iterations_since_restore: 46
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.89
    gpu_util_percent0: 0.3866666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.766666666666666
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14865433061186187
    mean_env_wait_ms: 1.2129869868168692
    mean_inference_ms: 4.426559486228597
    mean_raw_obs_processing_ms: 0.38540474870404395
  time_since_restore: 1223.0623607635498
  time_this_iter_s: 25.992260217666626
  time_total_s: 1223.0623607635498
  timers:
    learn_throughput: 8346.615
    learn_time_ms: 19384.147
    sample_throughput: 23632.127
    sample_time_ms: 6846.273
    update_time_ms: 26.636
  timestamp: 1602721775
  timesteps_since_restore: 0
  timesteps_total: 7442432
  training_iteration: 46
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     46 |          1223.06 | 7442432 |  244.756 |               292.96 |              74.7778 |            808.401 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3336.404631125471
    time_step_min: 3034
  date: 2020-10-15_00-30-01
  done: false
  episode_len_mean: 807.995387750724
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 245.14376739615392
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 165
  episodes_total: 9323
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.47594211747248966
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052220659563317895
        model: {}
        policy_loss: -0.010468926494165013
        total_loss: 5.399368405342102
        vf_explained_var: 0.9877247214317322
        vf_loss: 5.409814119338989
    num_steps_sampled: 7604224
    num_steps_trained: 7604224
  iterations_since_restore: 47
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.973333333333336
    gpu_util_percent0: 0.38966666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1486141876252997
    mean_env_wait_ms: 1.2132356412595107
    mean_inference_ms: 4.424035886265109
    mean_raw_obs_processing_ms: 0.3852827482321503
  time_since_restore: 1249.1596517562866
  time_this_iter_s: 26.097290992736816
  time_total_s: 1249.1596517562866
  timers:
    learn_throughput: 8375.719
    learn_time_ms: 19316.79
    sample_throughput: 23681.351
    sample_time_ms: 6832.043
    update_time_ms: 24.002
  timestamp: 1602721801
  timesteps_since_restore: 0
  timesteps_total: 7604224
  training_iteration: 47
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     47 |          1249.16 | 7604224 |  245.144 |              296.141 |              74.7778 |            807.995 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3333.4631856540086
    time_step_min: 3023
  date: 2020-10-15_00-30-27
  done: false
  episode_len_mean: 807.5162849338096
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 245.56862383023335
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 9518
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.46213724960883457
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057576741091907024
        model: {}
        policy_loss: -0.00961345701944083
        total_loss: 6.898455818494161
        vf_explained_var: 0.9866754412651062
        vf_loss: 6.9080125490824384
    num_steps_sampled: 7766016
    num_steps_trained: 7766016
  iterations_since_restore: 48
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.13666666666667
    gpu_util_percent0: 0.43200000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14856913653226708
    mean_env_wait_ms: 1.2135400504735177
    mean_inference_ms: 4.421177399704137
    mean_raw_obs_processing_ms: 0.38514458863854917
  time_since_restore: 1275.309280872345
  time_this_iter_s: 26.14962911605835
  time_total_s: 1275.309280872345
  timers:
    learn_throughput: 8401.894
    learn_time_ms: 19256.61
    sample_throughput: 23643.979
    sample_time_ms: 6842.841
    update_time_ms: 23.812
  timestamp: 1602721827
  timesteps_since_restore: 0
  timesteps_total: 7766016
  training_iteration: 48
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     48 |          1275.31 | 7766016 |  245.569 |              296.141 |              74.7778 |            807.516 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3329.772895277207
    time_step_min: 3023
  date: 2020-10-15_00-30-54
  done: false
  episode_len_mean: 806.8021067703007
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 246.11465338597677
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 260
  episodes_total: 9778
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.05000000000000001
        cur_lr: 5.0e-05
        entropy: 0.435211144387722
        entropy_coeff: 0.0005000000000000001
        kl: 0.004585651952462892
        model: {}
        policy_loss: -0.010591738411070159
        total_loss: 6.905542532602946
        vf_explained_var: 0.9884118437767029
        vf_loss: 6.916122396787007
    num_steps_sampled: 7927808
    num_steps_trained: 7927808
  iterations_since_restore: 49
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.509999999999998
    gpu_util_percent0: 0.2866666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7766666666666664
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14851071776282396
    mean_env_wait_ms: 1.2139088830375064
    mean_inference_ms: 4.4175484876836215
    mean_raw_obs_processing_ms: 0.38496561954483666
  time_since_restore: 1301.5257494449615
  time_this_iter_s: 26.216468572616577
  time_total_s: 1301.5257494449615
  timers:
    learn_throughput: 8415.245
    learn_time_ms: 19226.059
    sample_throughput: 23612.013
    sample_time_ms: 6852.105
    update_time_ms: 23.878
  timestamp: 1602721854
  timesteps_since_restore: 0
  timesteps_total: 7927808
  training_iteration: 49
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     49 |          1301.53 | 7927808 |  246.115 |              296.141 |              74.7778 |            806.802 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3327.5307583703107
    time_step_min: 3003
  date: 2020-10-15_00-31-21
  done: false
  episode_len_mean: 806.3794454490657
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 246.46908810832858
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 9954
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4466582238674164
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051932833933581906
        model: {}
        policy_loss: -0.011864040590201816
        total_loss: 5.898092150688171
        vf_explained_var: 0.9870522022247314
        vf_loss: 5.910049915313721
    num_steps_sampled: 8089600
    num_steps_trained: 8089600
  iterations_since_restore: 50
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.65483870967742
    gpu_util_percent0: 0.2848387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7774193548387096
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14847235364884415
    mean_env_wait_ms: 1.2141479566842475
    mean_inference_ms: 4.415176617322526
    mean_raw_obs_processing_ms: 0.38485042003397635
  time_since_restore: 1327.9679563045502
  time_this_iter_s: 26.442206859588623
  time_total_s: 1327.9679563045502
  timers:
    learn_throughput: 8419.381
    learn_time_ms: 19216.614
    sample_throughput: 23513.276
    sample_time_ms: 6880.879
    update_time_ms: 23.57
  timestamp: 1602721881
  timesteps_since_restore: 0
  timesteps_total: 8089600
  training_iteration: 50
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     50 |          1327.97 | 8089600 |  246.469 |              296.141 |              74.7778 |            806.379 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3325.1214193676283
    time_step_min: 3003
  date: 2020-10-15_00-31-47
  done: false
  episode_len_mean: 805.9673150982522
  episode_reward_max: 296.14141414141426
  episode_reward_mean: 246.82856609942615
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 173
  episodes_total: 10127
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.4431249772508939
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053331688201675815
        model: {}
        policy_loss: -0.008290527951127539
        total_loss: 6.406747579574585
        vf_explained_var: 0.9863852858543396
        vf_loss: 6.415126125017802
    num_steps_sampled: 8251392
    num_steps_trained: 8251392
  iterations_since_restore: 51
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.62333333333333
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7733333333333334
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14843757682488176
    mean_env_wait_ms: 1.2143777096958506
    mean_inference_ms: 4.412930706003781
    mean_raw_obs_processing_ms: 0.3847401601830363
  time_since_restore: 1354.3463952541351
  time_this_iter_s: 26.37843894958496
  time_total_s: 1354.3463952541351
  timers:
    learn_throughput: 8407.667
    learn_time_ms: 19243.388
    sample_throughput: 23484.72
    sample_time_ms: 6889.245
    update_time_ms: 24.206
  timestamp: 1602721907
  timesteps_since_restore: 0
  timesteps_total: 8251392
  training_iteration: 51
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     51 |          1354.35 | 8251392 |  246.829 |              296.141 |              74.7778 |            805.967 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3321.5150608225526
    time_step_min: 3003
  date: 2020-10-15_00-32-14
  done: false
  episode_len_mean: 805.3527318199308
  episode_reward_max: 297.0505050505049
  episode_reward_mean: 247.3874499127481
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 269
  episodes_total: 10396
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.025000000000000005
        cur_lr: 5.0e-05
        entropy: 0.415935255587101
        entropy_coeff: 0.0005000000000000001
        kl: 0.00476513112274309
        model: {}
        policy_loss: -0.010463526453046748
        total_loss: 7.400005221366882
        vf_explained_var: 0.987629234790802
        vf_loss: 7.4105576276779175
    num_steps_sampled: 8413184
    num_steps_trained: 8413184
  iterations_since_restore: 52
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.613333333333337
    gpu_util_percent0: 0.3306666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.769999999999999
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14838172598633348
    mean_env_wait_ms: 1.2147392189316857
    mean_inference_ms: 4.409482800876633
    mean_raw_obs_processing_ms: 0.38457361302950954
  time_since_restore: 1380.445829629898
  time_this_iter_s: 26.09943437576294
  time_total_s: 1380.445829629898
  timers:
    learn_throughput: 8406.779
    learn_time_ms: 19245.421
    sample_throughput: 23533.663
    sample_time_ms: 6874.918
    update_time_ms: 25.421
  timestamp: 1602721934
  timesteps_since_restore: 0
  timesteps_total: 8413184
  training_iteration: 52
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     52 |          1380.45 | 8413184 |  247.387 |              297.051 |              74.7778 |            805.353 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3319.2956290888405
    time_step_min: 3003
  date: 2020-10-15_00-32-40
  done: false
  episode_len_mean: 804.9416154936231
  episode_reward_max: 297.0505050505049
  episode_reward_mean: 247.74413955330346
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 10585
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4188198422392209
        entropy_coeff: 0.0005000000000000001
        kl: 0.005146876714813213
        model: {}
        policy_loss: -0.011319775764908021
        total_loss: 5.563521305720012
        vf_explained_var: 0.9883091449737549
        vf_loss: 5.574986298878987
    num_steps_sampled: 8574976
    num_steps_trained: 8574976
  iterations_since_restore: 53
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.109677419354842
    gpu_util_percent0: 0.3809677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.777419354838709
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14834514497423035
    mean_env_wait_ms: 1.214972766514045
    mean_inference_ms: 4.407223842538922
    mean_raw_obs_processing_ms: 0.38446540482522623
  time_since_restore: 1406.6548085212708
  time_this_iter_s: 26.20897889137268
  time_total_s: 1406.6548085212708
  timers:
    learn_throughput: 8409.709
    learn_time_ms: 19238.715
    sample_throughput: 23579.321
    sample_time_ms: 6861.606
    update_time_ms: 27.166
  timestamp: 1602721960
  timesteps_since_restore: 0
  timesteps_total: 8574976
  training_iteration: 53
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     53 |          1406.65 | 8574976 |  247.744 |              297.051 |              74.7778 |            804.942 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3317.2911534154537
    time_step_min: 3003
  date: 2020-10-15_00-33-07
  done: false
  episode_len_mean: 804.5798772549749
  episode_reward_max: 297.0505050505049
  episode_reward_mean: 248.03962255998707
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 169
  episodes_total: 10754
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4299461667736371
        entropy_coeff: 0.0005000000000000001
        kl: 0.0056602720481654005
        model: {}
        policy_loss: -0.009629145381040871
        total_loss: 6.223726034164429
        vf_explained_var: 0.986443817615509
        vf_loss: 6.23349932829539
    num_steps_sampled: 8736768
    num_steps_trained: 8736768
  iterations_since_restore: 54
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.92333333333334
    gpu_util_percent0: 0.354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14831291362747615
    mean_env_wait_ms: 1.2151766473375822
    mean_inference_ms: 4.405211405277482
    mean_raw_obs_processing_ms: 0.3843667376047883
  time_since_restore: 1433.1083633899689
  time_this_iter_s: 26.45355486869812
  time_total_s: 1433.1083633899689
  timers:
    learn_throughput: 8415.992
    learn_time_ms: 19224.352
    sample_throughput: 23510.555
    sample_time_ms: 6881.675
    update_time_ms: 28.64
  timestamp: 1602721987
  timesteps_since_restore: 0
  timesteps_total: 8736768
  training_iteration: 54
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     54 |          1433.11 | 8736768 |   248.04 |              297.051 |              74.7778 |             804.58 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3314.206145149526
    time_step_min: 2974
  date: 2020-10-15_00-33-33
  done: false
  episode_len_mean: 804.1219334908232
  episode_reward_max: 299.7777777777774
  episode_reward_mean: 248.53024888169352
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 252
  episodes_total: 11006
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4075028871496518
        entropy_coeff: 0.0005000000000000001
        kl: 0.005991729791276157
        model: {}
        policy_loss: -0.007751054183851617
        total_loss: 7.360698779424031
        vf_explained_var: 0.9872749447822571
        vf_loss: 7.368578831354777
    num_steps_sampled: 8898560
    num_steps_trained: 8898560
  iterations_since_restore: 55
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.377419354838715
    gpu_util_percent0: 0.3261290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14826334946907543
    mean_env_wait_ms: 1.2154717039431697
    mean_inference_ms: 4.40224748030586
    mean_raw_obs_processing_ms: 0.38422276232285085
  time_since_restore: 1459.513684272766
  time_this_iter_s: 26.40532088279724
  time_total_s: 1459.513684272766
  timers:
    learn_throughput: 8411.716
    learn_time_ms: 19234.126
    sample_throughput: 23488.581
    sample_time_ms: 6888.113
    update_time_ms: 29.106
  timestamp: 1602722013
  timesteps_since_restore: 0
  timesteps_total: 8898560
  training_iteration: 55
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     55 |          1459.51 | 8898560 |   248.53 |              299.778 |              74.7778 |            804.122 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3311.512974230494
    time_step_min: 2974
  date: 2020-10-15_00-34-00
  done: false
  episode_len_mean: 803.8080078473337
  episode_reward_max: 299.7777777777774
  episode_reward_mean: 248.91747599050964
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 11214
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.39224373549222946
        entropy_coeff: 0.0005000000000000001
        kl: 0.005173111334443092
        model: {}
        policy_loss: -0.009124329410648594
        total_loss: 6.454073071479797
        vf_explained_var: 0.9874857068061829
        vf_loss: 6.4633287986119585
    num_steps_sampled: 9060352
    num_steps_trained: 9060352
  iterations_since_restore: 56
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12903225806452
    gpu_util_percent0: 0.3635483870967741
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.767741935483871
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14822936846410298
    mean_env_wait_ms: 1.2157124360005689
    mean_inference_ms: 4.4000429167357
    mean_raw_obs_processing_ms: 0.3841176024377473
  time_since_restore: 1485.8407769203186
  time_this_iter_s: 26.32709264755249
  time_total_s: 1485.8407769203186
  timers:
    learn_throughput: 8395.217
    learn_time_ms: 19271.926
    sample_throughput: 23512.925
    sample_time_ms: 6880.981
    update_time_ms: 31.334
  timestamp: 1602722040
  timesteps_since_restore: 0
  timesteps_total: 9060352
  training_iteration: 56
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     56 |          1485.84 | 9060352 |  248.917 |              299.778 |              74.7778 |            803.808 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3309.548443974257
    time_step_min: 2974
  date: 2020-10-15_00-34-26
  done: false
  episode_len_mean: 803.582461998067
  episode_reward_max: 299.7777777777774
  episode_reward_mean: 249.22837726176624
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 11381
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.4059066101908684
        entropy_coeff: 0.0005000000000000001
        kl: 0.005832684226334095
        model: {}
        policy_loss: -0.007933409598384364
        total_loss: 5.164769212404887
        vf_explained_var: 0.9883384704589844
        vf_loss: 5.172832767168681
    num_steps_sampled: 9222144
    num_steps_trained: 9222144
  iterations_since_restore: 57
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38333333333333
    gpu_util_percent0: 0.4196666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7833333333333328
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.148199928895794
    mean_env_wait_ms: 1.2158854098735377
    mean_inference_ms: 4.3982139724389855
    mean_raw_obs_processing_ms: 0.3840292309482219
  time_since_restore: 1512.0111737251282
  time_this_iter_s: 26.17039680480957
  time_total_s: 1512.0111737251282
  timers:
    learn_throughput: 8394.83
    learn_time_ms: 19272.814
    sample_throughput: 23495.121
    sample_time_ms: 6886.196
    update_time_ms: 31.735
  timestamp: 1602722066
  timesteps_since_restore: 0
  timesteps_total: 9222144
  training_iteration: 57
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     57 |          1512.01 | 9222144 |  249.228 |              299.778 |              74.7778 |            803.582 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3306.7504758608757
    time_step_min: 2949
  date: 2020-10-15_00-34-53
  done: false
  episode_len_mean: 803.2393928941015
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 249.65313971031458
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 215
  episodes_total: 11596
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.012500000000000002
        cur_lr: 5.0e-05
        entropy: 0.3887382075190544
        entropy_coeff: 0.0005000000000000001
        kl: 0.004961162572726607
        model: {}
        policy_loss: -0.00886147262644954
        total_loss: 6.426156918207805
        vf_explained_var: 0.9878730773925781
        vf_loss: 6.435150623321533
    num_steps_sampled: 9383936
    num_steps_trained: 9383936
  iterations_since_restore: 58
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.373333333333335
    gpu_util_percent0: 0.30166666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.78
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1481632304869993
    mean_env_wait_ms: 1.2161178528947956
    mean_inference_ms: 4.395951936363492
    mean_raw_obs_processing_ms: 0.3839216315167905
  time_since_restore: 1538.3567397594452
  time_this_iter_s: 26.345566034317017
  time_total_s: 1538.3567397594452
  timers:
    learn_throughput: 8382.497
    learn_time_ms: 19301.17
    sample_throughput: 23528.986
    sample_time_ms: 6876.284
    update_time_ms: 31.679
  timestamp: 1602722093
  timesteps_since_restore: 0
  timesteps_total: 9383936
  training_iteration: 58
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     58 |          1538.36 | 9383936 |  249.653 |              303.566 |              74.7778 |            803.239 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3303.5622299415404
    time_step_min: 2949
  date: 2020-10-15_00-35-19
  done: false
  episode_len_mean: 802.8824423612871
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 250.1446847497012
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 245
  episodes_total: 11841
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.006250000000000001
        cur_lr: 5.0e-05
        entropy: 0.3674825330575307
        entropy_coeff: 0.0005000000000000001
        kl: 0.004915991177161534
        model: {}
        policy_loss: -0.009662117537421485
        total_loss: 5.518146475156148
        vf_explained_var: 0.9897975921630859
        vf_loss: 5.5279615720113116
    num_steps_sampled: 9545728
    num_steps_trained: 9545728
  iterations_since_restore: 59
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.50967741935484
    gpu_util_percent0: 0.37322580645161296
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.770967741935483
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14812531269724927
    mean_env_wait_ms: 1.2163620614928246
    mean_inference_ms: 4.393507668582146
    mean_raw_obs_processing_ms: 0.3838049781330105
  time_since_restore: 1564.5230536460876
  time_this_iter_s: 26.166313886642456
  time_total_s: 1564.5230536460876
  timers:
    learn_throughput: 8384.327
    learn_time_ms: 19296.957
    sample_throughput: 23537.442
    sample_time_ms: 6873.814
    update_time_ms: 32.642
  timestamp: 1602722119
  timesteps_since_restore: 0
  timesteps_total: 9545728
  training_iteration: 59
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     59 |          1564.52 | 9545728 |  250.145 |              303.566 |              74.7778 |            802.882 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3301.4739348370927
    time_step_min: 2949
  date: 2020-10-15_00-35-46
  done: false
  episode_len_mean: 802.6105929380413
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 250.44571632379754
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 12008
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.37949976076682407
        entropy_coeff: 0.0005000000000000001
        kl: 0.005423328140750527
        model: {}
        policy_loss: -0.011651926053067049
        total_loss: 5.225569645563762
        vf_explained_var: 0.9881870150566101
        vf_loss: 5.237394571304321
    num_steps_sampled: 9707520
    num_steps_trained: 9707520
  iterations_since_restore: 60
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.7
    gpu_util_percent0: 0.36766666666666675
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.786666666666667
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1480982429984127
    mean_env_wait_ms: 1.2165184703126892
    mean_inference_ms: 4.391848758106165
    mean_raw_obs_processing_ms: 0.383727905699216
  time_since_restore: 1590.7159621715546
  time_this_iter_s: 26.19290852546692
  time_total_s: 1590.7159621715546
  timers:
    learn_throughput: 8382.587
    learn_time_ms: 19300.962
    sample_throughput: 23637.752
    sample_time_ms: 6844.644
    update_time_ms: 32.373
  timestamp: 1602722146
  timesteps_since_restore: 0
  timesteps_total: 9707520
  training_iteration: 60
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     60 |          1590.72 | 9707520 |  250.446 |              303.566 |              74.7778 |            802.611 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3299.2453745580133
    time_step_min: 2949
  date: 2020-10-15_00-36-12
  done: false
  episode_len_mean: 802.3424051151734
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 250.7686563147666
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 12199
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3746684566140175
        entropy_coeff: 0.0005000000000000001
        kl: 0.005291714294192691
        model: {}
        policy_loss: -0.009012324328068644
        total_loss: 6.054218252499898
        vf_explained_var: 0.9877989888191223
        vf_loss: 6.0634013414382935
    num_steps_sampled: 9869312
    num_steps_trained: 9869312
  iterations_since_restore: 61
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.253333333333337
    gpu_util_percent0: 0.33766666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14806942863103378
    mean_env_wait_ms: 1.2166929851417416
    mean_inference_ms: 4.3899968037316635
    mean_raw_obs_processing_ms: 0.38364115864570236
  time_since_restore: 1616.9134185314178
  time_this_iter_s: 26.19745635986328
  time_total_s: 1616.9134185314178
  timers:
    learn_throughput: 8387.472
    learn_time_ms: 19289.722
    sample_throughput: 23669.976
    sample_time_ms: 6835.326
    update_time_ms: 34.516
  timestamp: 1602722172
  timesteps_since_restore: 0
  timesteps_total: 9869312
  training_iteration: 61
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     61 |          1616.91 | 9869312 |  250.769 |              303.566 |              74.7778 |            802.342 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3296.2546903937514
    time_step_min: 2949
  date: 2020-10-15_00-36-39
  done: false
  episode_len_mean: 802.021835112788
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 251.2066081056207
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 258
  episodes_total: 12457
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.35425874094168347
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057503182130555315
        model: {}
        policy_loss: -0.009201771514199208
        total_loss: 5.672591805458069
        vf_explained_var: 0.990231990814209
        vf_loss: 5.681952754656474
    num_steps_sampled: 10031104
    num_steps_trained: 10031104
  iterations_since_restore: 62
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.296774193548387
    gpu_util_percent0: 0.36677419354838714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14803065457785458
    mean_env_wait_ms: 1.2169226799881092
    mean_inference_ms: 4.38762350112043
    mean_raw_obs_processing_ms: 0.38352727568529205
  time_since_restore: 1643.2332785129547
  time_this_iter_s: 26.319859981536865
  time_total_s: 1643.2332785129547
  timers:
    learn_throughput: 8388.531
    learn_time_ms: 19287.286
    sample_throughput: 23590.677
    sample_time_ms: 6858.303
    update_time_ms: 35.306
  timestamp: 1602722199
  timesteps_since_restore: 0
  timesteps_total: 10031104
  training_iteration: 62
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     62 |          1643.23 | 10031104 |  251.207 |              303.566 |              74.7778 |            802.022 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3294.1152289500833
    time_step_min: 2949
  date: 2020-10-15_00-37-05
  done: false
  episode_len_mean: 801.7741909961231
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 251.53180031983732
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 12639
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3602635934948921
        entropy_coeff: 0.0005000000000000001
        kl: 0.005350144618811707
        model: {}
        policy_loss: -0.009856152654416897
        total_loss: 4.978982488314311
        vf_explained_var: 0.989046573638916
        vf_loss: 4.989002068837483
    num_steps_sampled: 10192896
    num_steps_trained: 10192896
  iterations_since_restore: 63
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 24.063333333333336
    gpu_util_percent0: 0.2786666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.783333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14800438389461767
    mean_env_wait_ms: 1.217081547537412
    mean_inference_ms: 4.385986153742188
    mean_raw_obs_processing_ms: 0.3834511979471789
  time_since_restore: 1669.3565289974213
  time_this_iter_s: 26.123250484466553
  time_total_s: 1669.3565289974213
  timers:
    learn_throughput: 8389.332
    learn_time_ms: 19285.445
    sample_throughput: 23615.579
    sample_time_ms: 6851.071
    update_time_ms: 35.686
  timestamp: 1602722225
  timesteps_since_restore: 0
  timesteps_total: 10192896
  training_iteration: 63
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     63 |          1669.36 | 10192896 |  251.532 |              303.566 |              74.7778 |            801.774 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3292.2328584846587
    time_step_min: 2949
  date: 2020-10-15_00-37-32
  done: false
  episode_len_mean: 801.5559544248479
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 251.81233042142983
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 175
  episodes_total: 12814
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3632343610127767
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050664207665249705
        model: {}
        policy_loss: -0.00965268024689673
        total_loss: 4.904102603594462
        vf_explained_var: 0.9893550872802734
        vf_loss: 4.913921038309733
    num_steps_sampled: 10354688
    num_steps_trained: 10354688
  iterations_since_restore: 64
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.364516129032257
    gpu_util_percent0: 0.3006451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7806451612903222
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14797966914151267
    mean_env_wait_ms: 1.217221249403615
    mean_inference_ms: 4.384416612194577
    mean_raw_obs_processing_ms: 0.383374626735174
  time_since_restore: 1695.4413301944733
  time_this_iter_s: 26.084801197052002
  time_total_s: 1695.4413301944733
  timers:
    learn_throughput: 8398.914
    learn_time_ms: 19263.443
    sample_throughput: 23663.876
    sample_time_ms: 6837.088
    update_time_ms: 33.83
  timestamp: 1602722252
  timesteps_since_restore: 0
  timesteps_total: 10354688
  training_iteration: 64
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     64 |          1695.44 | 10354688 |  251.812 |              303.566 |              74.7778 |            801.556 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3289.4561430435115
    time_step_min: 2949
  date: 2020-10-15_00-37-58
  done: false
  episode_len_mean: 801.235442650547
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 252.2313053250386
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 255
  episodes_total: 13069
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.34738215804100037
        entropy_coeff: 0.0005000000000000001
        kl: 0.0055323335497329635
        model: {}
        policy_loss: -0.008751247672383519
        total_loss: 7.331027666727702
        vf_explained_var: 0.9872035384178162
        vf_loss: 7.339935262997945
    num_steps_sampled: 10516480
    num_steps_trained: 10516480
  iterations_since_restore: 65
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.41
    gpu_util_percent0: 0.37
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.773333333333333
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1479436375671774
    mean_env_wait_ms: 1.2174300023629032
    mean_inference_ms: 4.382247940200406
    mean_raw_obs_processing_ms: 0.3832743600172067
  time_since_restore: 1721.6946771144867
  time_this_iter_s: 26.253346920013428
  time_total_s: 1721.6946771144867
  timers:
    learn_throughput: 8401.805
    learn_time_ms: 19256.815
    sample_throughput: 23682.916
    sample_time_ms: 6831.591
    update_time_ms: 31.816
  timestamp: 1602722278
  timesteps_since_restore: 0
  timesteps_total: 10516480
  training_iteration: 65
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     65 |          1721.69 | 10516480 |  252.231 |              303.566 |              74.7778 |            801.235 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3287.3278663744236
    time_step_min: 2949
  date: 2020-10-15_00-38-25
  done: false
  episode_len_mean: 801.0252468158867
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 252.56578673919842
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 200
  episodes_total: 13269
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0031250000000000006
        cur_lr: 5.0e-05
        entropy: 0.3430374910434087
        entropy_coeff: 0.0005000000000000001
        kl: 0.004587248161745568
        model: {}
        policy_loss: -0.010506312828511
        total_loss: 5.565648555755615
        vf_explained_var: 0.9885144233703613
        vf_loss: 5.576312144597371
    num_steps_sampled: 10678272
    num_steps_trained: 10678272
  iterations_since_restore: 66
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.243333333333336
    gpu_util_percent0: 0.30566666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14791817090834847
    mean_env_wait_ms: 1.2175840912643867
    mean_inference_ms: 4.380612821109172
    mean_raw_obs_processing_ms: 0.3831981057380581
  time_since_restore: 1747.9119138717651
  time_this_iter_s: 26.217236757278442
  time_total_s: 1747.9119138717651
  timers:
    learn_throughput: 8409.083
    learn_time_ms: 19240.147
    sample_throughput: 23660.108
    sample_time_ms: 6838.177
    update_time_ms: 30.608
  timestamp: 1602722305
  timesteps_since_restore: 0
  timesteps_total: 10678272
  training_iteration: 66
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     66 |          1747.91 | 10678272 |  252.566 |              303.566 |              74.7778 |            801.025 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3285.5690079868627
    time_step_min: 2949
  date: 2020-10-15_00-38-51
  done: false
  episode_len_mean: 800.8935615928544
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 252.8280084056042
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 166
  episodes_total: 13435
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.35400168349345523
        entropy_coeff: 0.0005000000000000001
        kl: 0.005854726885445416
        model: {}
        policy_loss: -0.01025951478125838
        total_loss: 5.445642948150635
        vf_explained_var: 0.9879546165466309
        vf_loss: 5.456070303916931
    num_steps_sampled: 10840064
    num_steps_trained: 10840064
  iterations_since_restore: 67
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.676666666666673
    gpu_util_percent0: 0.29500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7899999999999996
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14789659666743066
    mean_env_wait_ms: 1.2177007458918496
    mean_inference_ms: 4.379252124032221
    mean_raw_obs_processing_ms: 0.3831330884229987
  time_since_restore: 1773.9569799900055
  time_this_iter_s: 26.045066118240356
  time_total_s: 1773.9569799900055
  timers:
    learn_throughput: 8414.514
    learn_time_ms: 19227.73
    sample_throughput: 23663.696
    sample_time_ms: 6837.14
    update_time_ms: 30.173
  timestamp: 1602722331
  timesteps_since_restore: 0
  timesteps_total: 10840064
  training_iteration: 67
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     67 |          1773.96 | 10840064 |  252.828 |              303.566 |              74.7778 |            800.894 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3283.3482634554666
    time_step_min: 2949
  date: 2020-10-15_00-39-18
  done: false
  episode_len_mean: 800.7516292011422
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 253.14689029860736
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 13657
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0015625000000000003
        cur_lr: 5.0e-05
        entropy: 0.3397717500726382
        entropy_coeff: 0.0005000000000000001
        kl: 0.004544714892593523
        model: {}
        policy_loss: -0.009270166881227246
        total_loss: 6.19477625687917
        vf_explained_var: 0.9888274669647217
        vf_loss: 6.204209327697754
    num_steps_sampled: 11001856
    num_steps_trained: 11001856
  iterations_since_restore: 68
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.348387096774196
    gpu_util_percent0: 0.37580645161290327
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7741935483870965
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478680830642417
    mean_env_wait_ms: 1.2178561354109743
    mean_inference_ms: 4.377496088599825
    mean_raw_obs_processing_ms: 0.3830507585263021
  time_since_restore: 1800.2702901363373
  time_this_iter_s: 26.313310146331787
  time_total_s: 1800.2702901363373
  timers:
    learn_throughput: 8421.743
    learn_time_ms: 19211.224
    sample_throughput: 23624.612
    sample_time_ms: 6848.451
    update_time_ms: 31.253
  timestamp: 1602722358
  timesteps_since_restore: 0
  timesteps_total: 11001856
  training_iteration: 68
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     68 |          1800.27 | 11001856 |  253.147 |              303.566 |              74.7778 |            800.752 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3281.107333622059
    time_step_min: 2949
  date: 2020-10-15_00-39-45
  done: false
  episode_len_mean: 800.5938669737978
  episode_reward_max: 303.5656565656565
  episode_reward_mean: 253.4927019983887
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 13892
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0007812500000000002
        cur_lr: 5.0e-05
        entropy: 0.32775454968214035
        entropy_coeff: 0.0005000000000000001
        kl: 0.004657920799218118
        model: {}
        policy_loss: -0.01063937455182895
        total_loss: 7.272084633509318
        vf_explained_var: 0.9869036674499512
        vf_loss: 7.282884279886882
    num_steps_sampled: 11163648
    num_steps_trained: 11163648
  iterations_since_restore: 69
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.20967741935484
    gpu_util_percent0: 0.30870967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7709677419354835
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1478401899059538
    mean_env_wait_ms: 1.218010060239364
    mean_inference_ms: 4.37572243002654
    mean_raw_obs_processing_ms: 0.382965688441193
  time_since_restore: 1826.6884536743164
  time_this_iter_s: 26.418163537979126
  time_total_s: 1826.6884536743164
  timers:
    learn_throughput: 8409.811
    learn_time_ms: 19238.481
    sample_throughput: 23637.689
    sample_time_ms: 6844.662
    update_time_ms: 31.683
  timestamp: 1602722385
  timesteps_since_restore: 0
  timesteps_total: 11163648
  training_iteration: 69
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     69 |          1826.69 | 11163648 |  253.493 |              303.566 |              74.7778 |            800.594 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3279.43696520251
    time_step_min: 2939
  date: 2020-10-15_00-40-11
  done: false
  episode_len_mean: 800.446664770303
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 253.7351225237728
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 170
  episodes_total: 14062
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.0003906250000000001
        cur_lr: 5.0e-05
        entropy: 0.3373778189222018
        entropy_coeff: 0.0005000000000000001
        kl: 0.004869511739040415
        model: {}
        policy_loss: -0.011104502812183151
        total_loss: 5.528688748677571
        vf_explained_var: 0.9876391291618347
        vf_loss: 5.539959907531738
    num_steps_sampled: 11325440
    num_steps_trained: 11325440
  iterations_since_restore: 70
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95
    gpu_util_percent0: 0.2883333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8199999999999994
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14782031797259526
    mean_env_wait_ms: 1.2181166950740596
    mean_inference_ms: 4.374468438675191
    mean_raw_obs_processing_ms: 0.38290715668603775
  time_since_restore: 1853.1110713481903
  time_this_iter_s: 26.4226176738739
  time_total_s: 1853.1110713481903
  timers:
    learn_throughput: 8403.244
    learn_time_ms: 19253.517
    sample_throughput: 23613.524
    sample_time_ms: 6851.667
    update_time_ms: 31.697
  timestamp: 1602722411
  timesteps_since_restore: 0
  timesteps_total: 11325440
  training_iteration: 70
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     70 |          1853.11 | 11325440 |  253.735 |              305.081 |              74.7778 |            800.447 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3277.488110313775
    time_step_min: 2939
  date: 2020-10-15_00-40-38
  done: false
  episode_len_mean: 800.3270418186921
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.0209462007104
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 190
  episodes_total: 14252
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.3390084778269132
        entropy_coeff: 0.0005000000000000001
        kl: 0.005157003877684474
        model: {}
        policy_loss: -0.010564725331884498
        total_loss: 5.454565564791362
        vf_explained_var: 0.9887983202934265
        vf_loss: 5.465298732121785
    num_steps_sampled: 11487232
    num_steps_trained: 11487232
  iterations_since_restore: 71
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.54666666666667
    gpu_util_percent0: 0.41566666666666674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8633333333333346
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14779861954259496
    mean_env_wait_ms: 1.2182329059230737
    mean_inference_ms: 4.373085028614499
    mean_raw_obs_processing_ms: 0.3828409937415583
  time_since_restore: 1879.122456073761
  time_this_iter_s: 26.01138472557068
  time_total_s: 1879.122456073761
  timers:
    learn_throughput: 8412.825
    learn_time_ms: 19231.589
    sample_throughput: 23600.486
    sample_time_ms: 6855.452
    update_time_ms: 29.413
  timestamp: 1602722438
  timesteps_since_restore: 0
  timesteps_total: 11487232
  training_iteration: 71
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     71 |          1879.12 | 11487232 |  254.021 |              305.081 |              74.7778 |            800.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3274.946851890248
    time_step_min: 2939
  date: 2020-10-15_00-41-04
  done: false
  episode_len_mean: 800.1033983594127
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.41545530440547
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 255
  episodes_total: 14507
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.00019531250000000004
        cur_lr: 5.0e-05
        entropy: 0.32005507747332257
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044149528645599885
        model: {}
        policy_loss: -0.008633171014177302
        total_loss: 5.359514395395915
        vf_explained_var: 0.9904983639717102
        vf_loss: 5.368306756019592
    num_steps_sampled: 11649024
    num_steps_trained: 11649024
  iterations_since_restore: 72
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.8
    gpu_util_percent0: 0.2886666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8633333333333346
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14777089880595062
    mean_env_wait_ms: 1.218382686673371
    mean_inference_ms: 4.371337535837285
    mean_raw_obs_processing_ms: 0.38275498282922416
  time_since_restore: 1905.3212649822235
  time_this_iter_s: 26.198808908462524
  time_total_s: 1905.3212649822235
  timers:
    learn_throughput: 8410.771
    learn_time_ms: 19236.286
    sample_throughput: 23658.429
    sample_time_ms: 6838.662
    update_time_ms: 27.774
  timestamp: 1602722464
  timesteps_since_restore: 0
  timesteps_total: 11649024
  training_iteration: 72
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     72 |          1905.32 | 11649024 |  254.415 |              305.081 |              74.7778 |            800.103 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3273.1265097236437
    time_step_min: 2939
  date: 2020-10-15_00-41-31
  done: false
  episode_len_mean: 799.99101613013
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.6782972995455
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 14693
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.765625000000002e-05
        cur_lr: 5.0e-05
        entropy: 0.32103421290715534
        entropy_coeff: 0.0005000000000000001
        kl: 0.004831296120149394
        model: {}
        policy_loss: -0.010107869062873457
        total_loss: 4.698035915692647
        vf_explained_var: 0.9900762438774109
        vf_loss: 4.708303809165955
    num_steps_sampled: 11810816
    num_steps_trained: 11810816
  iterations_since_restore: 73
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.62258064516129
    gpu_util_percent0: 0.32677419354838705
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477500765161918
    mean_env_wait_ms: 1.2184824716084652
    mean_inference_ms: 4.3700563955565155
    mean_raw_obs_processing_ms: 0.3826926861240504
  time_since_restore: 1931.9445838928223
  time_this_iter_s: 26.623318910598755
  time_total_s: 1931.9445838928223
  timers:
    learn_throughput: 8400.431
    learn_time_ms: 19259.964
    sample_throughput: 23572.827
    sample_time_ms: 6863.496
    update_time_ms: 27.257
  timestamp: 1602722491
  timesteps_since_restore: 0
  timesteps_total: 11810816
  training_iteration: 73
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     73 |          1931.94 | 11810816 |  254.678 |              305.081 |              74.7778 |            799.991 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3271.532991499123
    time_step_min: 2939
  date: 2020-10-15_00-41-58
  done: false
  episode_len_mean: 799.9195154777927
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 254.91845439591063
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 14860
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.882812500000001e-05
        cur_lr: 5.0e-05
        entropy: 0.3286144882440567
        entropy_coeff: 0.0005000000000000001
        kl: 0.004885840035664539
        model: {}
        policy_loss: -0.010596770531265065
        total_loss: 4.796510457992554
        vf_explained_var: 0.9895806312561035
        vf_loss: 4.807271321614583
    num_steps_sampled: 11972608
    num_steps_trained: 11972608
  iterations_since_restore: 74
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.393333333333334
    gpu_util_percent0: 0.3706666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1477315004407032
    mean_env_wait_ms: 1.2185650663414558
    mean_inference_ms: 4.368916593717938
    mean_raw_obs_processing_ms: 0.38263458752519175
  time_since_restore: 1958.1638488769531
  time_this_iter_s: 26.21926498413086
  time_total_s: 1958.1638488769531
  timers:
    learn_throughput: 8395.791
    learn_time_ms: 19270.608
    sample_throughput: 23568.577
    sample_time_ms: 6864.734
    update_time_ms: 27.771
  timestamp: 1602722518
  timesteps_since_restore: 0
  timesteps_total: 11972608
  training_iteration: 74
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     74 |          1958.16 | 11972608 |  254.918 |              305.081 |              74.7778 |             799.92 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3269.2735780181856
    time_step_min: 2939
  date: 2020-10-15_00-42-24
  done: false
  episode_len_mean: 799.7875537901357
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 255.24874364298398
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 245
  episodes_total: 15105
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.3112246294816335
        entropy_coeff: 0.0005000000000000001
        kl: 0.005289243262571593
        model: {}
        policy_loss: -0.008476760820485651
        total_loss: 5.7805165847142534
        vf_explained_var: 0.9898079037666321
        vf_loss: 5.789149085680644
    num_steps_sampled: 12134400
    num_steps_trained: 12134400
  iterations_since_restore: 75
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.10645161290323
    gpu_util_percent0: 0.32225806451612904
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14770697893048435
    mean_env_wait_ms: 1.2186898769433603
    mean_inference_ms: 4.367364464891267
    mean_raw_obs_processing_ms: 0.38255721345610766
  time_since_restore: 1984.6843230724335
  time_this_iter_s: 26.520474195480347
  time_total_s: 1984.6843230724335
  timers:
    learn_throughput: 8395.339
    learn_time_ms: 19271.646
    sample_throughput: 23483.532
    sample_time_ms: 6889.594
    update_time_ms: 27.969
  timestamp: 1602722544
  timesteps_since_restore: 0
  timesteps_total: 12134400
  training_iteration: 75
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     75 |          1984.68 | 12134400 |  255.249 |              305.081 |              74.7778 |            799.788 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3267.3525292847326
    time_step_min: 2939
  date: 2020-10-15_00-42-51
  done: false
  episode_len_mean: 799.6541549709511
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 255.5499521621331
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 214
  episodes_total: 15319
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4414062500000005e-05
        cur_lr: 5.0e-05
        entropy: 0.30035021156072617
        entropy_coeff: 0.0005000000000000001
        kl: 0.004706569171200196
        model: {}
        policy_loss: -0.01010256118994827
        total_loss: 5.084058960278829
        vf_explained_var: 0.9900557398796082
        vf_loss: 5.094311555226644
    num_steps_sampled: 12296192
    num_steps_trained: 12296192
  iterations_since_restore: 76
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.500000000000004
    gpu_util_percent0: 0.36033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14768508475789646
    mean_env_wait_ms: 1.2187942932579519
    mean_inference_ms: 4.366028802886827
    mean_raw_obs_processing_ms: 0.38249047596304037
  time_since_restore: 2010.9464180469513
  time_this_iter_s: 26.262094974517822
  time_total_s: 2010.9464180469513
  timers:
    learn_throughput: 8392.003
    learn_time_ms: 19279.306
    sample_throughput: 23498.243
    sample_time_ms: 6885.281
    update_time_ms: 27.119
  timestamp: 1602722571
  timesteps_since_restore: 0
  timesteps_total: 12296192
  training_iteration: 76
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     76 |          2010.95 | 12296192 |   255.55 |              305.081 |              74.7778 |            799.654 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3265.9380462225677
    time_step_min: 2939
  date: 2020-10-15_00-43-18
  done: false
  episode_len_mean: 799.5577655795931
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 255.77696565265177
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 166
  episodes_total: 15485
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.3129647398988406
        entropy_coeff: 0.0005000000000000001
        kl: 0.005494995159097016
        model: {}
        policy_loss: -0.009574915166012943
        total_loss: 4.7394436200459795
        vf_explained_var: 0.9893284440040588
        vf_loss: 4.749174912770589
    num_steps_sampled: 12457984
    num_steps_trained: 12457984
  iterations_since_restore: 77
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.39666666666667
    gpu_util_percent0: 0.3346666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476681665938377
    mean_env_wait_ms: 1.218866829343962
    mean_inference_ms: 4.364991343576103
    mean_raw_obs_processing_ms: 0.38243882644326627
  time_since_restore: 2037.2566800117493
  time_this_iter_s: 26.310261964797974
  time_total_s: 2037.2566800117493
  timers:
    learn_throughput: 8378.614
    learn_time_ms: 19310.116
    sample_throughput: 23517.08
    sample_time_ms: 6879.766
    update_time_ms: 28.409
  timestamp: 1602722598
  timesteps_since_restore: 0
  timesteps_total: 12457984
  training_iteration: 77
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     77 |          2037.26 | 12457984 |  255.777 |              305.081 |              74.7778 |            799.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3264.17567394915
    time_step_min: 2939
  date: 2020-10-15_00-43-44
  done: false
  episode_len_mean: 799.4864262044354
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 256.0576810676224
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 15692
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2207031250000002e-05
        cur_lr: 5.0e-05
        entropy: 0.30396267026662827
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044093645410612226
        model: {}
        policy_loss: -0.010947145744770145
        total_loss: 5.748634099960327
        vf_explained_var: 0.9888679385185242
        vf_loss: 5.7597330411275225
    num_steps_sampled: 12619776
    num_steps_trained: 12619776
  iterations_since_restore: 78
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55161290322581
    gpu_util_percent0: 0.32612903225806456
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1476488543417303
    mean_env_wait_ms: 1.2189526990010917
    mean_inference_ms: 4.363701708075817
    mean_raw_obs_processing_ms: 0.38237322759919357
  time_since_restore: 2063.717297554016
  time_this_iter_s: 26.460617542266846
  time_total_s: 2063.717297554016
  timers:
    learn_throughput: 8367.012
    learn_time_ms: 19336.891
    sample_throughput: 23555.644
    sample_time_ms: 6868.503
    update_time_ms: 26.239
  timestamp: 1602722624
  timesteps_since_restore: 0
  timesteps_total: 12619776
  training_iteration: 78
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     78 |          2063.72 | 12619776 |  256.058 |              305.081 |              74.7778 |            799.486 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3262.04574628744
    time_step_min: 2939
  date: 2020-10-15_00-44-11
  done: false
  episode_len_mean: 799.4197740112994
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 256.3767049021286
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 15930
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.103515625000001e-06
        cur_lr: 5.0e-05
        entropy: 0.2848590662082036
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044003649381920695
        model: {}
        policy_loss: -0.008621891727671027
        total_loss: 4.8070762157440186
        vf_explained_var: 0.9914414286613464
        vf_loss: 4.815840562184651
    num_steps_sampled: 12781568
    num_steps_trained: 12781568
  iterations_since_restore: 79
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.087096774193554
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14762569253163926
    mean_env_wait_ms: 1.2190532777003995
    mean_inference_ms: 4.3623636627706786
    mean_raw_obs_processing_ms: 0.38230394904805914
  time_since_restore: 2090.133356809616
  time_this_iter_s: 26.416059255599976
  time_total_s: 2090.133356809616
  timers:
    learn_throughput: 8362.136
    learn_time_ms: 19348.166
    sample_throughput: 23595.166
    sample_time_ms: 6856.998
    update_time_ms: 24.905
  timestamp: 1602722651
  timesteps_since_restore: 0
  timesteps_total: 12781568
  training_iteration: 79
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     79 |          2090.13 | 12781568 |  256.377 |              305.081 |              74.7778 |             799.42 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3260.5543944765814
    time_step_min: 2939
  date: 2020-10-15_00-44-38
  done: false
  episode_len_mean: 799.3571827489916
  episode_reward_max: 305.0808080808083
  episode_reward_mean: 256.6164562158977
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 16115
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0517578125000006e-06
        cur_lr: 5.0e-05
        entropy: 0.2888157243529956
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049531990274166065
        model: {}
        policy_loss: -0.010214789110856751
        total_loss: 4.23901351292928
        vf_explained_var: 0.9907038807868958
        vf_loss: 4.249372760454814
    num_steps_sampled: 12943360
    num_steps_trained: 12943360
  iterations_since_restore: 80
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.55666666666666
    gpu_util_percent0: 0.37500000000000006
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14760843859851747
    mean_env_wait_ms: 1.2191236463499633
    mean_inference_ms: 4.361280665404032
    mean_raw_obs_processing_ms: 0.3822500408958688
  time_since_restore: 2116.4499378204346
  time_this_iter_s: 26.31658101081848
  time_total_s: 2116.4499378204346
  timers:
    learn_throughput: 8362.921
    learn_time_ms: 19346.349
    sample_throughput: 23634.525
    sample_time_ms: 6845.579
    update_time_ms: 26.316
  timestamp: 1602722678
  timesteps_since_restore: 0
  timesteps_total: 12943360
  training_iteration: 80
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     80 |          2116.45 | 12943360 |  256.616 |              305.081 |              74.7778 |            799.357 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3259.040231299213
    time_step_min: 2939
  date: 2020-10-15_00-45-05
  done: false
  episode_len_mean: 799.2862403338652
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 256.84367115366257
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 16294
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.2900454079111417
        entropy_coeff: 0.0005000000000000001
        kl: 0.0051606989388043685
        model: {}
        policy_loss: -0.008904293567563096
        total_loss: 5.0030977328618365
        vf_explained_var: 0.9894735217094421
        vf_loss: 5.012146910031636
    num_steps_sampled: 13105152
    num_steps_trained: 13105152
  iterations_since_restore: 81
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.640000000000004
    gpu_util_percent0: 0.2723333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14759191031358185
    mean_env_wait_ms: 1.2191845476293657
    mean_inference_ms: 4.3602433067336115
    mean_raw_obs_processing_ms: 0.3821966502208095
  time_since_restore: 2142.7266249656677
  time_this_iter_s: 26.276687145233154
  time_total_s: 2142.7266249656677
  timers:
    learn_throughput: 8350.021
    learn_time_ms: 19376.239
    sample_throughput: 23646.294
    sample_time_ms: 6842.171
    update_time_ms: 26.539
  timestamp: 1602722705
  timesteps_since_restore: 0
  timesteps_total: 13105152
  training_iteration: 81
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     81 |          2142.73 | 13105152 |  256.844 |              305.687 |              74.7778 |            799.286 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3256.941957787482
    time_step_min: 2939
  date: 2020-10-15_00-45-31
  done: false
  episode_len_mean: 799.224313203437
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.16085458237217
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 232
  episodes_total: 16526
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5258789062500003e-06
        cur_lr: 5.0e-05
        entropy: 0.276415911813577
        entropy_coeff: 0.0005000000000000001
        kl: 0.004555096966214478
        model: {}
        policy_loss: -0.009362990567751694
        total_loss: 4.814544200897217
        vf_explained_var: 0.9913424849510193
        vf_loss: 4.824045300483704
    num_steps_sampled: 13266944
    num_steps_trained: 13266944
  iterations_since_restore: 82
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.116129032258065
    gpu_util_percent0: 0.3441935483870967
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14757199889052425
    mean_env_wait_ms: 1.2192625671640596
    mean_inference_ms: 4.358974645509754
    mean_raw_obs_processing_ms: 0.38213069125711663
  time_since_restore: 2169.1431901454926
  time_this_iter_s: 26.41656517982483
  time_total_s: 2169.1431901454926
  timers:
    learn_throughput: 8347.6
    learn_time_ms: 19381.858
    sample_throughput: 23594.675
    sample_time_ms: 6857.14
    update_time_ms: 26.622
  timestamp: 1602722731
  timesteps_since_restore: 0
  timesteps_total: 13266944
  training_iteration: 82
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     82 |          2169.14 | 13266944 |  257.161 |              305.687 |              74.7778 |            799.224 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3255.099185921226
    time_step_min: 2939
  date: 2020-10-15_00-45-58
  done: false
  episode_len_mean: 799.1691949354993
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.4201444690575
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 16744
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.629394531250001e-07
        cur_lr: 5.0e-05
        entropy: 0.27031079679727554
        entropy_coeff: 0.0005000000000000001
        kl: 0.004333762840057413
        model: {}
        policy_loss: -0.008990503553529075
        total_loss: 4.899519721666972
        vf_explained_var: 0.9904859662055969
        vf_loss: 4.908645232518514
    num_steps_sampled: 13428736
    num_steps_trained: 13428736
  iterations_since_restore: 83
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.98709677419355
    gpu_util_percent0: 0.32419354838709674
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1475524009467584
    mean_env_wait_ms: 1.2193372901442394
    mean_inference_ms: 4.3578306038097345
    mean_raw_obs_processing_ms: 0.38207117130360163
  time_since_restore: 2195.5109882354736
  time_this_iter_s: 26.36779808998108
  time_total_s: 2195.5109882354736
  timers:
    learn_throughput: 8361.725
    learn_time_ms: 19349.117
    sample_throughput: 23592.402
    sample_time_ms: 6857.801
    update_time_ms: 32.392
  timestamp: 1602722758
  timesteps_since_restore: 0
  timesteps_total: 13428736
  training_iteration: 83
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     83 |          2195.51 | 13428736 |   257.42 |              305.687 |              74.7778 |            799.169 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3253.8381437800035
    time_step_min: 2939
  date: 2020-10-15_00-46-25
  done: false
  episode_len_mean: 799.1290284430253
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.62739332297605
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 167
  episodes_total: 16911
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.814697265625001e-07
        cur_lr: 5.0e-05
        entropy: 0.2760617832342784
        entropy_coeff: 0.0005000000000000001
        kl: 0.004863429193695386
        model: {}
        policy_loss: -0.008182110167884579
        total_loss: 4.305712858835856
        vf_explained_var: 0.9904346466064453
        vf_loss: 4.314032951990764
    num_steps_sampled: 13590528
    num_steps_trained: 13590528
  iterations_since_restore: 84
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.54333333333334
    gpu_util_percent0: 0.33266666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14753798886467065
    mean_env_wait_ms: 1.2193821291375297
    mean_inference_ms: 4.356932180809821
    mean_raw_obs_processing_ms: 0.38202465248355233
  time_since_restore: 2221.7399265766144
  time_this_iter_s: 26.228938341140747
  time_total_s: 2221.7399265766144
  timers:
    learn_throughput: 8357.398
    learn_time_ms: 19359.135
    sample_throughput: 23630.525
    sample_time_ms: 6846.738
    update_time_ms: 33.653
  timestamp: 1602722785
  timesteps_since_restore: 0
  timesteps_total: 13590528
  training_iteration: 84
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     84 |          2221.74 | 13590528 |  257.627 |              305.687 |              74.7778 |            799.129 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3251.9842607220176
    time_step_min: 2939
  date: 2020-10-15_00-46-51
  done: false
  episode_len_mean: 799.043143207426
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 257.90023358106725
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 17129
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9073486328125004e-07
        cur_lr: 5.0e-05
        entropy: 0.26749257495005924
        entropy_coeff: 0.0005000000000000001
        kl: 0.004825605894438922
        model: {}
        policy_loss: -0.00840260067464745
        total_loss: 4.5115755796432495
        vf_explained_var: 0.9911289811134338
        vf_loss: 4.520111918449402
    num_steps_sampled: 13752320
    num_steps_trained: 13752320
  iterations_since_restore: 85
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.8
    gpu_util_percent0: 0.32533333333333336
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14751988086753606
    mean_env_wait_ms: 1.2194372170802878
    mean_inference_ms: 4.355779141989556
    mean_raw_obs_processing_ms: 0.38196566251979963
  time_since_restore: 2248.00270152092
  time_this_iter_s: 26.26277494430542
  time_total_s: 2248.00270152092
  timers:
    learn_throughput: 8355.105
    learn_time_ms: 19364.448
    sample_throughput: 23742.498
    sample_time_ms: 6814.447
    update_time_ms: 34.137
  timestamp: 1602722811
  timesteps_since_restore: 0
  timesteps_total: 13752320
  training_iteration: 85
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     85 |             2248 | 13752320 |    257.9 |              305.687 |              74.7778 |            799.043 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3249.9984991052356
    time_step_min: 2939
  date: 2020-10-15_00-47-18
  done: false
  episode_len_mean: 798.9421692298831
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.1904774372374
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 232
  episodes_total: 17361
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.536743164062502e-08
        cur_lr: 5.0e-05
        entropy: 0.25233036528031033
        entropy_coeff: 0.0005000000000000001
        kl: 0.0048353523792078095
        model: {}
        policy_loss: -0.0055988599779084325
        total_loss: 5.573948303858439
        vf_explained_var: 0.9899200797080994
        vf_loss: 5.579673250516255
    num_steps_sampled: 13914112
    num_steps_trained: 13914112
  iterations_since_restore: 86
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.312903225806455
    gpu_util_percent0: 0.32967741935483874
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14750103266742196
    mean_env_wait_ms: 1.2195064346551685
    mean_inference_ms: 4.354650672150964
    mean_raw_obs_processing_ms: 0.38190561664192835
  time_since_restore: 2274.247854948044
  time_this_iter_s: 26.245153427124023
  time_total_s: 2274.247854948044
  timers:
    learn_throughput: 8358.026
    learn_time_ms: 19357.682
    sample_throughput: 23729.27
    sample_time_ms: 6818.246
    update_time_ms: 35.441
  timestamp: 1602722838
  timesteps_since_restore: 0
  timesteps_total: 13914112
  training_iteration: 86
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     86 |          2274.25 | 13914112 |   258.19 |              305.687 |              74.7778 |            798.942 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3248.472141265215
    time_step_min: 2939
  date: 2020-10-15_00-47-45
  done: false
  episode_len_mean: 798.8470662028853
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.4183662478695
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 17537
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.768371582031251e-08
        cur_lr: 5.0e-05
        entropy: 0.2585073684652646
        entropy_coeff: 0.0005000000000000001
        kl: 0.00489986757747829
        model: {}
        policy_loss: -0.008975959771002332
        total_loss: 3.498164494832357
        vf_explained_var: 0.99190354347229
        vf_loss: 3.5072697401046753
    num_steps_sampled: 14075904
    num_steps_trained: 14075904
  iterations_since_restore: 87
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.703333333333333
    gpu_util_percent0: 0.2973333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14748657876440444
    mean_env_wait_ms: 1.2195520540887677
    mean_inference_ms: 4.353783836920002
    mean_raw_obs_processing_ms: 0.3818610307026379
  time_since_restore: 2300.645466566086
  time_this_iter_s: 26.397611618041992
  time_total_s: 2300.645466566086
  timers:
    learn_throughput: 8354.079
    learn_time_ms: 19366.827
    sample_throughput: 23731.876
    sample_time_ms: 6817.497
    update_time_ms: 34.165
  timestamp: 1602722865
  timesteps_since_restore: 0
  timesteps_total: 14075904
  training_iteration: 87
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     87 |          2300.65 | 14075904 |  258.418 |              305.687 |              74.7778 |            798.847 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3246.9174674957603
    time_step_min: 2939
  date: 2020-10-15_00-48-11
  done: false
  episode_len_mean: 798.7289034296028
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.6413201281771
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 17728
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3841857910156255e-08
        cur_lr: 5.0e-05
        entropy: 0.26109440128008526
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047555947676301
        model: {}
        policy_loss: -0.008867673760202402
        total_loss: 5.586236993471782
        vf_explained_var: 0.98823481798172
        vf_loss: 5.595235109329224
    num_steps_sampled: 14237696
    num_steps_trained: 14237696
  iterations_since_restore: 88
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.858064516129033
    gpu_util_percent0: 0.41032258064516125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14747127216146053
    mean_env_wait_ms: 1.2195957021521555
    mean_inference_ms: 4.352816484281495
    mean_raw_obs_processing_ms: 0.38181099460452894
  time_since_restore: 2327.0497114658356
  time_this_iter_s: 26.404244899749756
  time_total_s: 2327.0497114658356
  timers:
    learn_throughput: 8363.059
    learn_time_ms: 19346.032
    sample_throughput: 23680.963
    sample_time_ms: 6832.155
    update_time_ms: 33.858
  timestamp: 1602722891
  timesteps_since_restore: 0
  timesteps_total: 14237696
  training_iteration: 88
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     88 |          2327.05 | 14237696 |  258.641 |              305.687 |              74.7778 |            798.729 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3245.0426125271906
    time_step_min: 2939
  date: 2020-10-15_00-48-38
  done: false
  episode_len_mean: 798.6408415428285
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 258.92065082280476
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 239
  episodes_total: 17967
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1920928955078127e-08
        cur_lr: 5.0e-05
        entropy: 0.24719692642490068
        entropy_coeff: 0.0005000000000000001
        kl: 0.004926367856872578
        model: {}
        policy_loss: -0.00856569221165652
        total_loss: 5.425957957903544
        vf_explained_var: 0.990248441696167
        vf_loss: 5.434647123018901
    num_steps_sampled: 14399488
    num_steps_trained: 14399488
  iterations_since_restore: 89
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69677419354839
    gpu_util_percent0: 0.31935483870967746
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14745340563079412
    mean_env_wait_ms: 1.2196503684213298
    mean_inference_ms: 4.351722470255424
    mean_raw_obs_processing_ms: 0.38175213419297654
  time_since_restore: 2353.552812099457
  time_this_iter_s: 26.503100633621216
  time_total_s: 2353.552812099457
  timers:
    learn_throughput: 8363.738
    learn_time_ms: 19344.46
    sample_throughput: 23677.668
    sample_time_ms: 6833.105
    update_time_ms: 42.04
  timestamp: 1602722918
  timesteps_since_restore: 0
  timesteps_total: 14399488
  training_iteration: 89
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     89 |          2353.55 | 14399488 |  258.921 |              305.687 |              74.7778 |            798.641 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3243.501599735216
    time_step_min: 2939
  date: 2020-10-15_00-49-05
  done: false
  episode_len_mean: 798.5295056699329
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.15542800013793
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 18166
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.960464477539064e-09
        cur_lr: 5.0e-05
        entropy: 0.24320283780495325
        entropy_coeff: 0.0005000000000000001
        kl: 0.004537505408128102
        model: {}
        policy_loss: -0.009045854256934641
        total_loss: 3.8153651356697083
        vf_explained_var: 0.992069661617279
        vf_loss: 3.8245325485865274
    num_steps_sampled: 14561280
    num_steps_trained: 14561280
  iterations_since_restore: 90
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.57666666666667
    gpu_util_percent0: 0.3346666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14743801222943853
    mean_env_wait_ms: 1.2196942148417018
    mean_inference_ms: 4.350806642355259
    mean_raw_obs_processing_ms: 0.38170437011442343
  time_since_restore: 2379.627952814102
  time_this_iter_s: 26.075140714645386
  time_total_s: 2379.627952814102
  timers:
    learn_throughput: 8373.041
    learn_time_ms: 19322.968
    sample_throughput: 23690.333
    sample_time_ms: 6829.452
    update_time_ms: 42.092
  timestamp: 1602722945
  timesteps_since_restore: 0
  timesteps_total: 14561280
  training_iteration: 90
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     90 |          2379.63 | 14561280 |  259.155 |              305.687 |              74.7778 |             798.53 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3242.1898388418463
    time_step_min: 2939
  date: 2020-10-15_00-49-32
  done: false
  episode_len_mean: 798.4458921659489
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.3590977099127
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 177
  episodes_total: 18343
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.980232238769532e-09
        cur_lr: 5.0e-05
        entropy: 0.2525920110444228
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049233135068789124
        model: {}
        policy_loss: -0.011446780166200673
        total_loss: 4.483220895131429
        vf_explained_var: 0.9901518225669861
        vf_loss: 4.494793891906738
    num_steps_sampled: 14723072
    num_steps_trained: 14723072
  iterations_since_restore: 91
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.129032258064516
    gpu_util_percent0: 0.27451612903225814
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1474252253822833
    mean_env_wait_ms: 1.2197238946284992
    mean_inference_ms: 4.349989866790658
    mean_raw_obs_processing_ms: 0.38166169741149586
  time_since_restore: 2406.0972232818604
  time_this_iter_s: 26.46927046775818
  time_total_s: 2406.0972232818604
  timers:
    learn_throughput: 8368.76
    learn_time_ms: 19332.852
    sample_throughput: 23691.68
    sample_time_ms: 6829.064
    update_time_ms: 49.752
  timestamp: 1602722972
  timesteps_since_restore: 0
  timesteps_total: 14723072
  training_iteration: 91
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     91 |           2406.1 | 14723072 |  259.359 |              305.687 |              74.7778 |            798.446 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3240.496034529269
    time_step_min: 2939
  date: 2020-10-15_00-49-58
  done: false
  episode_len_mean: 798.3073816830884
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.6141237932548
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 230
  episodes_total: 18573
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.490116119384766e-09
        cur_lr: 5.0e-05
        entropy: 0.24378551791111627
        entropy_coeff: 0.0005000000000000001
        kl: 0.004815731663256884
        model: {}
        policy_loss: -0.011370247438511191
        total_loss: 5.186391592025757
        vf_explained_var: 0.990380585193634
        vf_loss: 5.19788384437561
    num_steps_sampled: 14884864
    num_steps_trained: 14884864
  iterations_since_restore: 92
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.453333333333333
    gpu_util_percent0: 0.368
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14740928786154991
    mean_env_wait_ms: 1.2197704575575847
    mean_inference_ms: 4.348955805607367
    mean_raw_obs_processing_ms: 0.3816078081564477
  time_since_restore: 2432.3650665283203
  time_this_iter_s: 26.26784324645996
  time_total_s: 2432.3650665283203
  timers:
    learn_throughput: 8367.541
    learn_time_ms: 19335.668
    sample_throughput: 23756.147
    sample_time_ms: 6810.532
    update_time_ms: 49.701
  timestamp: 1602722998
  timesteps_since_restore: 0
  timesteps_total: 14884864
  training_iteration: 92
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     92 |          2432.37 | 14884864 |  259.614 |              305.687 |              74.7778 |            798.307 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3238.8874553500027
    time_step_min: 2938
  date: 2020-10-15_00-50-25
  done: false
  episode_len_mean: 798.1329076882149
  episode_reward_max: 305.68686868686865
  episode_reward_mean: 259.86059316226914
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 18795
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.45058059692383e-10
        cur_lr: 5.0e-05
        entropy: 0.2296330196162065
        entropy_coeff: 0.0005000000000000001
        kl: 0.004444085760042071
        model: {}
        policy_loss: -0.008004582765958427
        total_loss: 5.105821490287781
        vf_explained_var: 0.990095853805542
        vf_loss: 5.113940874735515
    num_steps_sampled: 15046656
    num_steps_trained: 15046656
  iterations_since_restore: 93
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.522580645161295
    gpu_util_percent0: 0.33225806451612905
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473923776469285
    mean_env_wait_ms: 1.2198130754878775
    mean_inference_ms: 4.348001771785144
    mean_raw_obs_processing_ms: 0.3815566185215916
  time_since_restore: 2458.857023000717
  time_this_iter_s: 26.49195647239685
  time_total_s: 2458.857023000717
  timers:
    learn_throughput: 8354.715
    learn_time_ms: 19365.351
    sample_throughput: 23822.868
    sample_time_ms: 6791.458
    update_time_ms: 50.727
  timestamp: 1602723025
  timesteps_since_restore: 0
  timesteps_total: 15046656
  training_iteration: 93
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     93 |          2458.86 | 15046656 |  259.861 |              305.687 |              74.7778 |            798.133 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3237.6421325161155
    time_step_min: 2929
  date: 2020-10-15_00-50-52
  done: false
  episode_len_mean: 797.9981543978064
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.0608020726139
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 169
  episodes_total: 18964
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.2407698097328345
        entropy_coeff: 0.0005000000000000001
        kl: 0.005226831339920561
        model: {}
        policy_loss: -0.0071348769997712225
        total_loss: 3.945502817630768
        vf_explained_var: 0.9911749958992004
        vf_loss: 3.9527581135431924
    num_steps_sampled: 15208448
    num_steps_trained: 15208448
  iterations_since_restore: 94
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.12666666666667
    gpu_util_percent0: 0.3586666666666665
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14738039178601559
    mean_env_wait_ms: 1.2198401497139153
    mean_inference_ms: 4.347270009079576
    mean_raw_obs_processing_ms: 0.38151867759388863
  time_since_restore: 2485.075926065445
  time_this_iter_s: 26.218903064727783
  time_total_s: 2485.075926065445
  timers:
    learn_throughput: 8356.735
    learn_time_ms: 19360.671
    sample_throughput: 23808.396
    sample_time_ms: 6795.586
    update_time_ms: 49.416
  timestamp: 1602723052
  timesteps_since_restore: 0
  timesteps_total: 15208448
  training_iteration: 94
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     94 |          2485.08 | 15208448 |  260.061 |              306.596 |              74.7778 |            797.998 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3235.9981716554353
    time_step_min: 2929
  date: 2020-10-15_00-51-18
  done: false
  episode_len_mean: 797.8404671289296
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.31414346794145
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 217
  episodes_total: 19181
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.725290298461915e-10
        cur_lr: 5.0e-05
        entropy: 0.240451380610466
        entropy_coeff: 0.0005000000000000001
        kl: 0.0044286402796084685
        model: {}
        policy_loss: -0.008429673228723308
        total_loss: 4.66086208820343
        vf_explained_var: 0.9907262325286865
        vf_loss: 4.669412096341451
    num_steps_sampled: 15370240
    num_steps_trained: 15370240
  iterations_since_restore: 95
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.080000000000002
    gpu_util_percent0: 0.36533333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14736561675190843
    mean_env_wait_ms: 1.2198765352605683
    mean_inference_ms: 4.346320209348884
    mean_raw_obs_processing_ms: 0.38147031234727463
  time_since_restore: 2511.2759749889374
  time_this_iter_s: 26.20004892349243
  time_total_s: 2511.2759749889374
  timers:
    learn_throughput: 8360.557
    learn_time_ms: 19351.822
    sample_throughput: 23806.537
    sample_time_ms: 6796.117
    update_time_ms: 49.293
  timestamp: 1602723078
  timesteps_since_restore: 0
  timesteps_total: 15370240
  training_iteration: 95
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     95 |          2511.28 | 15370240 |  260.314 |              306.596 |              74.7778 |             797.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3234.4301785529983
    time_step_min: 2929
  date: 2020-10-15_00-51-45
  done: false
  episode_len_mean: 797.6979295426453
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.57312307250504
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 19416
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8626451492309574e-10
        cur_lr: 5.0e-05
        entropy: 0.22621110330025354
        entropy_coeff: 0.0005000000000000001
        kl: 0.004403940790022413
        model: {}
        policy_loss: -0.01013193831507427
        total_loss: 4.560421784718831
        vf_explained_var: 0.9914420247077942
        vf_loss: 4.570666670799255
    num_steps_sampled: 15532032
    num_steps_trained: 15532032
  iterations_since_restore: 96
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33548387096775
    gpu_util_percent0: 0.31000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14734946601295196
    mean_env_wait_ms: 1.2199126257005277
    mean_inference_ms: 4.345379047546388
    mean_raw_obs_processing_ms: 0.3814182085236884
  time_since_restore: 2537.6443259716034
  time_this_iter_s: 26.368350982666016
  time_total_s: 2537.6443259716034
  timers:
    learn_throughput: 8355.356
    learn_time_ms: 19363.867
    sample_throughput: 23806.787
    sample_time_ms: 6796.045
    update_time_ms: 48.035
  timestamp: 1602723105
  timesteps_since_restore: 0
  timesteps_total: 15532032
  training_iteration: 96
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     96 |          2537.64 | 15532032 |  260.573 |              306.596 |              74.7778 |            797.698 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3233.11138386008
    time_step_min: 2929
  date: 2020-10-15_00-52-12
  done: false
  episode_len_mean: 797.6013168640261
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.7733139892184
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 19592
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.22750935579339662
        entropy_coeff: 0.0005000000000000001
        kl: 0.005514377650494377
        model: {}
        policy_loss: -0.009730751354557773
        total_loss: 3.740702509880066
        vf_explained_var: 0.9914469122886658
        vf_loss: 3.750546932220459
    num_steps_sampled: 15693824
    num_steps_trained: 15693824
  iterations_since_restore: 97
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993333333333336
    gpu_util_percent0: 0.2876666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14733761474907572
    mean_env_wait_ms: 1.2199413029950172
    mean_inference_ms: 4.344663304389056
    mean_raw_obs_processing_ms: 0.3813824689745814
  time_since_restore: 2563.986717224121
  time_this_iter_s: 26.3423912525177
  time_total_s: 2563.986717224121
  timers:
    learn_throughput: 8356.556
    learn_time_ms: 19361.086
    sample_throughput: 23818.548
    sample_time_ms: 6792.689
    update_time_ms: 48.114
  timestamp: 1602723132
  timesteps_since_restore: 0
  timesteps_total: 15693824
  training_iteration: 97
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     97 |          2563.99 | 15693824 |  260.773 |              306.596 |              74.7778 |            797.601 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3231.7170346364187
    time_step_min: 2929
  date: 2020-10-15_00-52-38
  done: false
  episode_len_mean: 797.479025573638
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 260.9903737669835
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 194
  episodes_total: 19786
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.313225746154787e-11
        cur_lr: 5.0e-05
        entropy: 0.2317101942996184
        entropy_coeff: 0.0005000000000000001
        kl: 0.004682655485036473
        model: {}
        policy_loss: -0.009470400982536376
        total_loss: 4.304901401201884
        vf_explained_var: 0.9910011887550354
        vf_loss: 4.31448769569397
    num_steps_sampled: 15855616
    num_steps_trained: 15855616
  iterations_since_restore: 98
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.612903225806452
    gpu_util_percent0: 0.27677419354838706
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1473257880846485
    mean_env_wait_ms: 1.2199662576948171
    mean_inference_ms: 4.343882928601813
    mean_raw_obs_processing_ms: 0.38134220559861987
  time_since_restore: 2590.0248622894287
  time_this_iter_s: 26.038145065307617
  time_total_s: 2590.0248622894287
  timers:
    learn_throughput: 8368.59
    learn_time_ms: 19333.244
    sample_throughput: 23852.014
    sample_time_ms: 6783.159
    update_time_ms: 48.401
  timestamp: 1602723158
  timesteps_since_restore: 0
  timesteps_total: 15855616
  training_iteration: 98
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     98 |          2590.02 | 15855616 |   260.99 |              306.596 |              74.7778 |            797.479 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3230.1126401120896
    time_step_min: 2929
  date: 2020-10-15_00-53-05
  done: false
  episode_len_mean: 797.3846269103985
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.2417129036847
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 236
  episodes_total: 20022
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.6566128730773935e-11
        cur_lr: 5.0e-05
        entropy: 0.2173848090072473
        entropy_coeff: 0.0005000000000000001
        kl: 0.004540561775987347
        model: {}
        policy_loss: -0.00959299021148278
        total_loss: 4.721267620722453
        vf_explained_var: 0.9914255142211914
        vf_loss: 4.730969150861104
    num_steps_sampled: 16017408
    num_steps_trained: 16017408
  iterations_since_restore: 99
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.78333333333334
    gpu_util_percent0: 0.3446666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14731012577429348
    mean_env_wait_ms: 1.2199962205234811
    mean_inference_ms: 4.342962367983941
    mean_raw_obs_processing_ms: 0.3812928507426948
  time_since_restore: 2616.3691856861115
  time_this_iter_s: 26.34432339668274
  time_total_s: 2616.3691856861115
  timers:
    learn_throughput: 8372.019
    learn_time_ms: 19325.327
    sample_throughput: 23857.513
    sample_time_ms: 6781.595
    update_time_ms: 42.35
  timestamp: 1602723185
  timesteps_since_restore: 0
  timesteps_total: 16017408
  training_iteration: 99
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |     99 |          2616.37 | 16017408 |  261.242 |              306.596 |              74.7778 |            797.385 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3228.6634296189864
    time_step_min: 2929
  date: 2020-10-15_00-53-32
  done: false
  episode_len_mean: 797.3345531872806
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.458628618413
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 20221
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3283064365386967e-11
        cur_lr: 5.0e-05
        entropy: 0.20883943513035774
        entropy_coeff: 0.0005000000000000001
        kl: 0.004350139565455417
        model: {}
        policy_loss: -0.008186976653936048
        total_loss: 3.8756356040636697
        vf_explained_var: 0.9918270111083984
        vf_loss: 3.8839269479115806
    num_steps_sampled: 16179200
    num_steps_trained: 16179200
  iterations_since_restore: 100
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.506451612903227
    gpu_util_percent0: 0.37354838709677424
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14729752168527444
    mean_env_wait_ms: 1.220024695843724
    mean_inference_ms: 4.3422028290381345
    mean_raw_obs_processing_ms: 0.38125328043056234
  time_since_restore: 2642.8386101722717
  time_this_iter_s: 26.46942448616028
  time_total_s: 2642.8386101722717
  timers:
    learn_throughput: 8359.163
    learn_time_ms: 19355.048
    sample_throughput: 23827.949
    sample_time_ms: 6790.009
    update_time_ms: 43.177
  timestamp: 1602723212
  timesteps_since_restore: 0
  timesteps_total: 16179200
  training_iteration: 100
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    100 |          2642.84 | 16179200 |  261.459 |              306.596 |              74.7778 |            797.335 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3227.3365422396855
    time_step_min: 2929
  date: 2020-10-15_00-53-59
  done: false
  episode_len_mean: 797.2962055103442
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.6589485402114
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 177
  episodes_total: 20398
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.22018000856041908
        entropy_coeff: 0.0005000000000000001
        kl: 0.005238248811413844
        model: {}
        policy_loss: -0.011148986018573245
        total_loss: 3.3956047097841897
        vf_explained_var: 0.9924342632293701
        vf_loss: 3.4068637688954673
    num_steps_sampled: 16340992
    num_steps_trained: 16340992
  iterations_since_restore: 101
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.145161290322587
    gpu_util_percent0: 0.2574193548387097
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14728695646013407
    mean_env_wait_ms: 1.2200403442819718
    mean_inference_ms: 4.341524011261373
    mean_raw_obs_processing_ms: 0.38121884548839197
  time_since_restore: 2669.3566858768463
  time_this_iter_s: 26.518075704574585
  time_total_s: 2669.3566858768463
  timers:
    learn_throughput: 8361.573
    learn_time_ms: 19349.47
    sample_throughput: 23766.05
    sample_time_ms: 6807.694
    update_time_ms: 36.02
  timestamp: 1602723239
  timesteps_since_restore: 0
  timesteps_total: 16340992
  training_iteration: 101
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    101 |          2669.36 | 16340992 |  261.659 |              306.596 |              74.7778 |            797.296 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3225.6533547101976
    time_step_min: 2929
  date: 2020-10-15_00-54-26
  done: false
  episode_len_mean: 797.2596867271228
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 261.91837290513394
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 20621
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1641532182693484e-11
        cur_lr: 5.0e-05
        entropy: 0.21474727243185043
        entropy_coeff: 0.0005000000000000001
        kl: 0.004982666151287655
        model: {}
        policy_loss: -0.008769697775278473
        total_loss: 3.3455925782521567
        vf_explained_var: 0.9934398531913757
        vf_loss: 3.354469656944275
    num_steps_sampled: 16502784
    num_steps_trained: 16502784
  iterations_since_restore: 102
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.883870967741935
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14727353643378674
    mean_env_wait_ms: 1.220063400895031
    mean_inference_ms: 4.340684386794711
    mean_raw_obs_processing_ms: 0.38117540478495576
  time_since_restore: 2695.822493314743
  time_this_iter_s: 26.46580743789673
  time_total_s: 2695.822493314743
  timers:
    learn_throughput: 8353.751
    learn_time_ms: 19367.588
    sample_throughput: 23766.401
    sample_time_ms: 6807.594
    update_time_ms: 37.158
  timestamp: 1602723266
  timesteps_since_restore: 0
  timesteps_total: 16502784
  training_iteration: 102
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    102 |          2695.82 | 16502784 |  261.918 |              306.596 |              74.7778 |             797.26 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3223.9988947621337
    time_step_min: 2929
  date: 2020-10-15_00-54-53
  done: false
  episode_len_mean: 797.2032329240214
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 262.16717927548706
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 20848
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.820766091346742e-12
        cur_lr: 5.0e-05
        entropy: 0.20586828514933586
        entropy_coeff: 0.0005000000000000001
        kl: 0.004019270612237354
        model: {}
        policy_loss: -0.007916943733713802
        total_loss: 3.878064215183258
        vf_explained_var: 0.9924182295799255
        vf_loss: 3.8860840598742166
    num_steps_sampled: 16664576
    num_steps_trained: 16664576
  iterations_since_restore: 103
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.04193548387097
    gpu_util_percent0: 0.39000000000000007
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472598868428203
    mean_env_wait_ms: 1.2200848698089557
    mean_inference_ms: 4.339882677590588
    mean_raw_obs_processing_ms: 0.38113234540528035
  time_since_restore: 2722.3208527565002
  time_this_iter_s: 26.498359441757202
  time_total_s: 2722.3208527565002
  timers:
    learn_throughput: 8353.295
    learn_time_ms: 19368.643
    sample_throughput: 23770.454
    sample_time_ms: 6806.433
    update_time_ms: 29.054
  timestamp: 1602723293
  timesteps_since_restore: 0
  timesteps_total: 16664576
  training_iteration: 103
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    103 |          2722.32 | 16664576 |  262.167 |              306.596 |              74.7778 |            797.203 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3222.746496329488
    time_step_min: 2929
  date: 2020-10-15_00-55-20
  done: false
  episode_len_mean: 797.1631614008375
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 262.3537660579914
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 168
  episodes_total: 21016
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.910383045673371e-12
        cur_lr: 5.0e-05
        entropy: 0.21039297183354697
        entropy_coeff: 0.0005000000000000001
        kl: 0.004545592547704776
        model: {}
        policy_loss: -0.01044849571674907
        total_loss: 2.9206919272740683
        vf_explained_var: 0.9932246208190918
        vf_loss: 2.9312456051508584
    num_steps_sampled: 16826368
    num_steps_trained: 16826368
  iterations_since_restore: 104
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.326666666666664
    gpu_util_percent0: 0.2946666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14724936982026135
    mean_env_wait_ms: 1.2200948934501448
    mean_inference_ms: 4.339269339884308
    mean_raw_obs_processing_ms: 0.38109956417216634
  time_since_restore: 2748.751081466675
  time_this_iter_s: 26.43022871017456
  time_total_s: 2748.751081466675
  timers:
    learn_throughput: 8342.782
    learn_time_ms: 19393.052
    sample_throughput: 23783.538
    sample_time_ms: 6802.689
    update_time_ms: 28.734
  timestamp: 1602723320
  timesteps_since_restore: 0
  timesteps_total: 16826368
  training_iteration: 104
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    104 |          2748.75 | 16826368 |  262.354 |              306.596 |              74.7778 |            797.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3221.236968838527
    time_step_min: 2929
  date: 2020-10-15_00-55-46
  done: false
  episode_len_mean: 797.1100952021868
  episode_reward_max: 306.5959595959589
  episode_reward_mean: 262.5738671472953
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 21218
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4551915228366855e-12
        cur_lr: 5.0e-05
        entropy: 0.21486473580201468
        entropy_coeff: 0.0005000000000000001
        kl: 0.00453416813009729
        model: {}
        policy_loss: -0.009656874948025992
        total_loss: 3.907158533732096
        vf_explained_var: 0.9920961260795593
        vf_loss: 3.916922847429911
    num_steps_sampled: 16988160
    num_steps_trained: 16988160
  iterations_since_restore: 105
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.332258064516132
    gpu_util_percent0: 0.2787096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1472385316304918
    mean_env_wait_ms: 1.2201081466403763
    mean_inference_ms: 4.338543021554887
    mean_raw_obs_processing_ms: 0.3810631618752973
  time_since_restore: 2774.9259095191956
  time_this_iter_s: 26.174828052520752
  time_total_s: 2774.9259095191956
  timers:
    learn_throughput: 8344.609
    learn_time_ms: 19388.805
    sample_throughput: 23776.51
    sample_time_ms: 6804.699
    update_time_ms: 28.518
  timestamp: 1602723346
  timesteps_since_restore: 0
  timesteps_total: 16988160
  training_iteration: 105
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    105 |          2774.93 | 16988160 |  262.574 |              306.596 |              74.7778 |             797.11 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3219.373937809319
    time_step_min: 2924
  date: 2020-10-15_00-56-13
  done: false
  episode_len_mean: 797.0404548844147
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 262.84718832621513
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 21456
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.275957614183427e-13
        cur_lr: 5.0e-05
        entropy: 0.19974111268917719
        entropy_coeff: 0.0005000000000000001
        kl: 0.0049743565420309705
        model: {}
        policy_loss: -0.009263245214242488
        total_loss: 3.4402349392573037
        vf_explained_var: 0.99342280626297
        vf_loss: 3.4495980739593506
    num_steps_sampled: 17149952
    num_steps_trained: 17149952
  iterations_since_restore: 106
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.370967741935488
    gpu_util_percent0: 0.33096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14722512292106368
    mean_env_wait_ms: 1.2201197326020536
    mean_inference_ms: 4.337731934649686
    mean_raw_obs_processing_ms: 0.3810195570481468
  time_since_restore: 2801.0398433208466
  time_this_iter_s: 26.113933801651
  time_total_s: 2801.0398433208466
  timers:
    learn_throughput: 8356.47
    learn_time_ms: 19361.284
    sample_throughput: 23770.403
    sample_time_ms: 6806.448
    update_time_ms: 28.126
  timestamp: 1602723373
  timesteps_since_restore: 0
  timesteps_total: 17149952
  training_iteration: 106
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    106 |          2801.04 | 17149952 |  262.847 |              307.354 |              74.7778 |             797.04 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3217.991576413959
    time_step_min: 2924
  date: 2020-10-15_00-56-40
  done: false
  episode_len_mean: 797.0049898355203
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.0521543283509
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 21644
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6379788070917137e-13
        cur_lr: 5.0e-05
        entropy: 0.195981714874506
        entropy_coeff: 0.0005000000000000001
        kl: 0.00416200408168758
        model: {}
        policy_loss: -0.01051843585203945
        total_loss: 3.1847651402155557
        vf_explained_var: 0.9928480982780457
        vf_loss: 3.1953815619150796
    num_steps_sampled: 17311744
    num_steps_trained: 17311744
  iterations_since_restore: 107
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.380000000000003
    gpu_util_percent0: 0.32966666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14721435456081203
    mean_env_wait_ms: 1.2201339825415682
    mean_inference_ms: 4.3371072947038805
    mean_raw_obs_processing_ms: 0.3809868747302804
  time_since_restore: 2827.460910797119
  time_this_iter_s: 26.421067476272583
  time_total_s: 2827.460910797119
  timers:
    learn_throughput: 8354.652
    learn_time_ms: 19365.498
    sample_throughput: 23763.54
    sample_time_ms: 6808.413
    update_time_ms: 29.068
  timestamp: 1602723400
  timesteps_since_restore: 0
  timesteps_total: 17311744
  training_iteration: 107
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    107 |          2827.46 | 17311744 |  263.052 |              307.354 |              74.7778 |            797.005 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3216.5273953744495
    time_step_min: 2924
  date: 2020-10-15_00-57-07
  done: false
  episode_len_mean: 796.9402198808979
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.270982847254
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 21830
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8189894035458568e-13
        cur_lr: 5.0e-05
        entropy: 0.20648310085137686
        entropy_coeff: 0.0005000000000000001
        kl: 0.004946530524951716
        model: {}
        policy_loss: -0.00904949654553396
        total_loss: 2.99210395415624
        vf_explained_var: 0.9933214783668518
        vf_loss: 3.0012566447257996
    num_steps_sampled: 17473536
    num_steps_trained: 17473536
  iterations_since_restore: 108
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.551612903225802
    gpu_util_percent0: 0.22645161290322582
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14720454436194289
    mean_env_wait_ms: 1.2201375000169885
    mean_inference_ms: 4.336465021712899
    mean_raw_obs_processing_ms: 0.38095337956326386
  time_since_restore: 2854.0705082416534
  time_this_iter_s: 26.6095974445343
  time_total_s: 2854.0705082416534
  timers:
    learn_throughput: 8331.317
    learn_time_ms: 19419.739
    sample_throughput: 23759.089
    sample_time_ms: 6809.689
    update_time_ms: 28.949
  timestamp: 1602723427
  timesteps_since_restore: 0
  timesteps_total: 17473536
  training_iteration: 108
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    108 |          2854.07 | 17473536 |  263.271 |              307.354 |              74.7778 |             796.94 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3214.659736722651
    time_step_min: 2924
  date: 2020-10-15_00-57-33
  done: false
  episode_len_mean: 796.834556824361
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.5297620028452
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 22068
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.094947017729284e-14
        cur_lr: 5.0e-05
        entropy: 0.19852573176225027
        entropy_coeff: 0.0005000000000000001
        kl: 0.004543304055308302
        model: {}
        policy_loss: -0.009153506873796383
        total_loss: 3.8071552515029907
        vf_explained_var: 0.9927506446838379
        vf_loss: 3.816408097743988
    num_steps_sampled: 17635328
    num_steps_trained: 17635328
  iterations_since_restore: 109
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.606451612903225
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14719230758794458
    mean_env_wait_ms: 1.2201456373933117
    mean_inference_ms: 4.33570163146042
    mean_raw_obs_processing_ms: 0.38091306322909047
  time_since_restore: 2880.209086418152
  time_this_iter_s: 26.138578176498413
  time_total_s: 2880.209086418152
  timers:
    learn_throughput: 8344.542
    learn_time_ms: 19388.961
    sample_throughput: 23721.174
    sample_time_ms: 6820.573
    update_time_ms: 26.74
  timestamp: 1602723453
  timesteps_since_restore: 0
  timesteps_total: 17635328
  training_iteration: 109
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    109 |          2880.21 | 17635328 |   263.53 |              307.354 |              74.7778 |            796.835 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3213.098979178846
    time_step_min: 2924
  date: 2020-10-15_00-58-00
  done: false
  episode_len_mean: 796.7334231200898
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.7613168724279
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 22275
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.547473508864642e-14
        cur_lr: 5.0e-05
        entropy: 0.18953226755062738
        entropy_coeff: 0.0005000000000000001
        kl: 0.00424206592530633
        model: {}
        policy_loss: -0.008345151819109256
        total_loss: 3.069048523902893
        vf_explained_var: 0.993450939655304
        vf_loss: 3.077488442262014
    num_steps_sampled: 17797120
    num_steps_trained: 17797120
  iterations_since_restore: 110
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.026666666666674
    gpu_util_percent0: 0.3016666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471807263023131
    mean_env_wait_ms: 1.2201555863755627
    mean_inference_ms: 4.335036127813343
    mean_raw_obs_processing_ms: 0.3808773941386155
  time_since_restore: 2906.6532020568848
  time_this_iter_s: 26.44411563873291
  time_total_s: 2906.6532020568848
  timers:
    learn_throughput: 8344.038
    learn_time_ms: 19390.131
    sample_throughput: 23731.585
    sample_time_ms: 6817.581
    update_time_ms: 25.194
  timestamp: 1602723480
  timesteps_since_restore: 0
  timesteps_total: 17797120
  training_iteration: 110
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    110 |          2906.65 | 17797120 |  263.761 |              307.354 |              74.7778 |            796.733 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3211.845934124788
    time_step_min: 2924
  date: 2020-10-15_00-58-27
  done: false
  episode_len_mean: 796.6694885047228
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 263.9520728583284
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 169
  episodes_total: 22444
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.273736754432321e-14
        cur_lr: 5.0e-05
        entropy: 0.19718386108676592
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038958047516644
        model: {}
        policy_loss: -0.008478434955274375
        total_loss: 2.9254135886828103
        vf_explained_var: 0.9932602047920227
        vf_loss: 2.9339906175931296
    num_steps_sampled: 17958912
    num_steps_trained: 17958912
  iterations_since_restore: 111
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.538709677419348
    gpu_util_percent0: 0.33677419354838706
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14717185769045674
    mean_env_wait_ms: 1.2201576610722438
    mean_inference_ms: 4.3344962718047
    mean_raw_obs_processing_ms: 0.3808491709326583
  time_since_restore: 2933.1834647655487
  time_this_iter_s: 26.53026270866394
  time_total_s: 2933.1834647655487
  timers:
    learn_throughput: 8335.765
    learn_time_ms: 19409.377
    sample_throughput: 23795.999
    sample_time_ms: 6799.126
    update_time_ms: 24.592
  timestamp: 1602723507
  timesteps_since_restore: 0
  timesteps_total: 17958912
  training_iteration: 111
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    111 |          2933.18 | 17958912 |  263.952 |              307.354 |              74.7778 |            796.669 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3210.136917911107
    time_step_min: 2924
  date: 2020-10-15_00-58-54
  done: false
  episode_len_mean: 796.5705275229358
  episode_reward_max: 307.3535353535351
  episode_reward_mean: 264.21281044388843
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 228
  episodes_total: 22672
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1368683772161605e-14
        cur_lr: 5.0e-05
        entropy: 0.1978266810377439
        entropy_coeff: 0.0005000000000000001
        kl: 0.00442284275777638
        model: {}
        policy_loss: -0.01004474914225284
        total_loss: 3.4646064043045044
        vf_explained_var: 0.9931136965751648
        vf_loss: 3.474750022093455
    num_steps_sampled: 18120704
    num_steps_trained: 18120704
  iterations_since_restore: 112
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706451612903226
    gpu_util_percent0: 0.28161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14716107828996922
    mean_env_wait_ms: 1.2201589832278648
    mean_inference_ms: 4.3337726548354345
    mean_raw_obs_processing_ms: 0.38081162984119676
  time_since_restore: 2959.739729642868
  time_this_iter_s: 26.556264877319336
  time_total_s: 2959.739729642868
  timers:
    learn_throughput: 8330.758
    learn_time_ms: 19421.043
    sample_throughput: 23808.707
    sample_time_ms: 6795.497
    update_time_ms: 24.926
  timestamp: 1602723534
  timesteps_since_restore: 0
  timesteps_total: 18120704
  training_iteration: 112
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    112 |          2959.74 | 18120704 |  264.213 |              307.354 |              74.7778 |            796.571 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3208.55666856218
    time_step_min: 2924
  date: 2020-10-15_00-59-21
  done: false
  episode_len_mean: 796.4563518057557
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 264.44784409005547
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 22899
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6843418860808026e-15
        cur_lr: 5.0e-05
        entropy: 0.18136985103289285
        entropy_coeff: 0.0005000000000000001
        kl: 0.004298216934936742
        model: {}
        policy_loss: -0.006906143156811595
        total_loss: 3.3213775555292764
        vf_explained_var: 0.9935495853424072
        vf_loss: 3.3283743262290955
    num_steps_sampled: 18282496
    num_steps_trained: 18282496
  iterations_since_restore: 113
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.69
    gpu_util_percent0: 0.37399999999999994
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14714929086435216
    mean_env_wait_ms: 1.2201611649172697
    mean_inference_ms: 4.3330987184970375
    mean_raw_obs_processing_ms: 0.3807736870903944
  time_since_restore: 2985.9173498153687
  time_this_iter_s: 26.17762017250061
  time_total_s: 2985.9173498153687
  timers:
    learn_throughput: 8336.564
    learn_time_ms: 19407.517
    sample_throughput: 23848.798
    sample_time_ms: 6784.074
    update_time_ms: 25.738
  timestamp: 1602723561
  timesteps_since_restore: 0
  timesteps_total: 18282496
  training_iteration: 113
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    113 |          2985.92 | 18282496 |  264.448 |              307.808 |              74.7778 |            796.456 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3207.35630237506
    time_step_min: 2924
  date: 2020-10-15_00-59-48
  done: false
  episode_len_mean: 796.37795309723
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 264.6342360708826
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 170
  episodes_total: 23069
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.8421709430404013e-15
        cur_lr: 5.0e-05
        entropy: 0.18563113113244376
        entropy_coeff: 0.0005000000000000001
        kl: 0.004801140322039525
        model: {}
        policy_loss: -0.010135711966237674
        total_loss: 2.932145377000173
        vf_explained_var: 0.9931931495666504
        vf_loss: 2.9423738916714988
    num_steps_sampled: 18444288
    num_steps_trained: 18444288
  iterations_since_restore: 114
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925806451612907
    gpu_util_percent0: 0.3348387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471403589060176
    mean_env_wait_ms: 1.2201624914341522
    mean_inference_ms: 4.332576634482926
    mean_raw_obs_processing_ms: 0.38074599091118483
  time_since_restore: 3012.3212583065033
  time_this_iter_s: 26.403908491134644
  time_total_s: 3012.3212583065033
  timers:
    learn_throughput: 8349.614
    learn_time_ms: 19377.184
    sample_throughput: 23783.215
    sample_time_ms: 6802.781
    update_time_ms: 25.658
  timestamp: 1602723588
  timesteps_since_restore: 0
  timesteps_total: 18444288
  training_iteration: 114
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    114 |          3012.32 | 18444288 |  264.634 |              307.808 |              74.7778 |            796.378 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3205.8207246751012
    time_step_min: 2924
  date: 2020-10-15_01-00-15
  done: false
  episode_len_mean: 796.2741020793951
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 264.86940551762683
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 23276
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.4210854715202006e-15
        cur_lr: 5.0e-05
        entropy: 0.19106416776776314
        entropy_coeff: 0.0005000000000000001
        kl: 0.004714735162754853
        model: {}
        policy_loss: -0.008473456546198577
        total_loss: 3.303392847379049
        vf_explained_var: 0.9929433465003967
        vf_loss: 3.3119618693987527
    num_steps_sampled: 18606080
    num_steps_trained: 18606080
  iterations_since_restore: 115
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.40666666666667
    gpu_util_percent0: 0.36966666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471309222772218
    mean_env_wait_ms: 1.2201627788624287
    mean_inference_ms: 4.331946080624285
    mean_raw_obs_processing_ms: 0.38071438960090026
  time_since_restore: 3038.632426261902
  time_this_iter_s: 26.31116795539856
  time_total_s: 3038.632426261902
  timers:
    learn_throughput: 8343.482
    learn_time_ms: 19391.424
    sample_throughput: 23788.922
    sample_time_ms: 6801.149
    update_time_ms: 25.405
  timestamp: 1602723615
  timesteps_since_restore: 0
  timesteps_total: 18606080
  training_iteration: 115
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    115 |          3038.63 | 18606080 |  264.869 |              307.808 |              74.7778 |            796.274 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3204.0219798943604
    time_step_min: 2924
  date: 2020-10-15_01-00-41
  done: false
  episode_len_mean: 796.1630092710725
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.1257200739211
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 238
  episodes_total: 23514
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.105427357601003e-16
        cur_lr: 5.0e-05
        entropy: 0.18019420405228934
        entropy_coeff: 0.0005000000000000001
        kl: 0.004146733049613734
        model: {}
        policy_loss: -0.008227977940502266
        total_loss: 3.778904696305593
        vf_explained_var: 0.9928925037384033
        vf_loss: 3.7872227827707925
    num_steps_sampled: 18767872
    num_steps_trained: 18767872
  iterations_since_restore: 116
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.958064516129035
    gpu_util_percent0: 0.29612903225806453
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14711947349996982
    mean_env_wait_ms: 1.2201587163834973
    mean_inference_ms: 4.331268195082155
    mean_raw_obs_processing_ms: 0.38067558767929766
  time_since_restore: 3064.803185939789
  time_this_iter_s: 26.170759677886963
  time_total_s: 3064.803185939789
  timers:
    learn_throughput: 8340.958
    learn_time_ms: 19397.292
    sample_throughput: 23793.717
    sample_time_ms: 6799.778
    update_time_ms: 25.513
  timestamp: 1602723641
  timesteps_since_restore: 0
  timesteps_total: 18767872
  training_iteration: 116
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    116 |           3064.8 | 18767872 |  265.126 |              307.808 |              74.7778 |            796.163 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3202.6726120033813
    time_step_min: 2924
  date: 2020-10-15_01-01-08
  done: false
  episode_len_mean: 796.1047767744113
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.31463423158925
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 23698
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.5527136788005016e-16
        cur_lr: 5.0e-05
        entropy: 0.1740105872352918
        entropy_coeff: 0.0005000000000000001
        kl: 0.003510866896249354
        model: {}
        policy_loss: -0.007805104488700938
        total_loss: 2.7357622186342874
        vf_explained_var: 0.9937769770622253
        vf_loss: 2.7436543107032776
    num_steps_sampled: 18929664
    num_steps_trained: 18929664
  iterations_since_restore: 117
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.741935483870968
    gpu_util_percent0: 0.3970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1471104930724593
    mean_env_wait_ms: 1.2201581419640342
    mean_inference_ms: 4.330744112933679
    mean_raw_obs_processing_ms: 0.38064840808249284
  time_since_restore: 3091.0532648563385
  time_this_iter_s: 26.250078916549683
  time_total_s: 3091.0532648563385
  timers:
    learn_throughput: 8348.996
    learn_time_ms: 19378.619
    sample_throughput: 23789.536
    sample_time_ms: 6800.973
    update_time_ms: 24.918
  timestamp: 1602723668
  timesteps_since_restore: 0
  timesteps_total: 18929664
  training_iteration: 117
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    117 |          3091.05 | 18929664 |  265.315 |              307.808 |              74.7778 |            796.105 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3201.392276081852
    time_step_min: 2924
  date: 2020-10-15_01-01-35
  done: false
  episode_len_mean: 796.054718244997
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.51212451061735
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 23886
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7763568394002508e-16
        cur_lr: 5.0e-05
        entropy: 0.18454369033376375
        entropy_coeff: 0.0005000000000000001
        kl: 0.004071201061985145
        model: {}
        policy_loss: -0.01007609095237664
        total_loss: 2.79981122414271
        vf_explained_var: 0.9939806461334229
        vf_loss: 2.809979736804962
    num_steps_sampled: 19091456
    num_steps_trained: 19091456
  iterations_since_restore: 118
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.413333333333338
    gpu_util_percent0: 0.337
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14710273196172555
    mean_env_wait_ms: 1.2201518760777226
    mean_inference_ms: 4.330210455794253
    mean_raw_obs_processing_ms: 0.38062030674050695
  time_since_restore: 3117.1768560409546
  time_this_iter_s: 26.12359118461609
  time_total_s: 3117.1768560409546
  timers:
    learn_throughput: 8365.21
    learn_time_ms: 19341.057
    sample_throughput: 23829.714
    sample_time_ms: 6789.507
    update_time_ms: 25.147
  timestamp: 1602723695
  timesteps_since_restore: 0
  timesteps_total: 19091456
  training_iteration: 118
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    118 |          3117.18 | 19091456 |  265.512 |              307.808 |              74.7778 |            796.055 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3199.693227587925
    time_step_min: 2924
  date: 2020-10-15_01-02-01
  done: false
  episode_len_mean: 795.9970150491273
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.7624786482628
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 24121
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.881784197001254e-17
        cur_lr: 5.0e-05
        entropy: 0.17652543634176254
        entropy_coeff: 0.0005000000000000001
        kl: 0.004322420184810956
        model: {}
        policy_loss: -0.008132235573915144
        total_loss: 2.433333079020182
        vf_explained_var: 0.9953238368034363
        vf_loss: 2.4415535926818848
    num_steps_sampled: 19253248
    num_steps_trained: 19253248
  iterations_since_restore: 119
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.413333333333338
    gpu_util_percent0: 0.27166666666666667
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14709159837541128
    mean_env_wait_ms: 1.2201447291539407
    mean_inference_ms: 4.329557053997787
    mean_raw_obs_processing_ms: 0.38058481514589376
  time_since_restore: 3143.2884216308594
  time_this_iter_s: 26.111565589904785
  time_total_s: 3143.2884216308594
  timers:
    learn_throughput: 8368.151
    learn_time_ms: 19334.258
    sample_throughput: 23818.189
    sample_time_ms: 6792.792
    update_time_ms: 25.625
  timestamp: 1602723721
  timesteps_since_restore: 0
  timesteps_total: 19253248
  training_iteration: 119
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    119 |          3143.29 | 19253248 |  265.762 |              307.808 |              74.7778 |            795.997 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3198.193832599119
    time_step_min: 2924
  date: 2020-10-15_01-02-28
  done: false
  episode_len_mean: 795.9422863485016
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 265.9848104923946
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 206
  episodes_total: 24327
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.440892098500627e-17
        cur_lr: 5.0e-05
        entropy: 0.1611424870789051
        entropy_coeff: 0.0005000000000000001
        kl: 0.0031780594727024436
        model: {}
        policy_loss: -0.008321759795459608
        total_loss: 2.27426278591156
        vf_explained_var: 0.9950885772705078
        vf_loss: 2.2826651334762573
    num_steps_sampled: 19415040
    num_steps_trained: 19415040
  iterations_since_restore: 120
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.493548387096777
    gpu_util_percent0: 0.37225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470822255010377
    mean_env_wait_ms: 1.2201386311514124
    mean_inference_ms: 4.328992811704753
    mean_raw_obs_processing_ms: 0.38055405727804315
  time_since_restore: 3169.841106414795
  time_this_iter_s: 26.552684783935547
  time_total_s: 3169.841106414795
  timers:
    learn_throughput: 8366.927
    learn_time_ms: 19337.087
    sample_throughput: 23798.724
    sample_time_ms: 6798.348
    update_time_ms: 26.501
  timestamp: 1602723748
  timesteps_since_restore: 0
  timesteps_total: 19415040
  training_iteration: 120
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    120 |          3169.84 | 19415040 |  265.985 |              307.808 |              74.7778 |            795.942 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3197.0685989943177
    time_step_min: 2924
  date: 2020-10-15_01-02-55
  done: false
  episode_len_mean: 795.9045267153762
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.1595719635639
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 172
  episodes_total: 24499
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2204460492503135e-17
        cur_lr: 5.0e-05
        entropy: 0.16785553097724915
        entropy_coeff: 0.0005000000000000001
        kl: 0.003992234162675838
        model: {}
        policy_loss: -0.009055366480121544
        total_loss: 2.390176753203074
        vf_explained_var: 0.9945241808891296
        vf_loss: 2.399316112200419
    num_steps_sampled: 19576832
    num_steps_trained: 19576832
  iterations_since_restore: 121
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.945161290322584
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14707448355487387
    mean_env_wait_ms: 1.2201293624957161
    mean_inference_ms: 4.3285214037896145
    mean_raw_obs_processing_ms: 0.3805298669935998
  time_since_restore: 3196.3467247486115
  time_this_iter_s: 26.50561833381653
  time_total_s: 3196.3467247486115
  timers:
    learn_throughput: 8371.905
    learn_time_ms: 19325.59
    sample_throughput: 23772.273
    sample_time_ms: 6805.912
    update_time_ms: 27.222
  timestamp: 1602723775
  timesteps_since_restore: 0
  timesteps_total: 19576832
  training_iteration: 121
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    121 |          3196.35 | 19576832 |   266.16 |              307.808 |              74.7778 |            795.905 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3195.478954830869
    time_step_min: 2924
  date: 2020-10-15_01-03-22
  done: false
  episode_len_mean: 795.8419690167051
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.3957534328848
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 224
  episodes_total: 24723
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1102230246251568e-17
        cur_lr: 5.0e-05
        entropy: 0.16944307213028273
        entropy_coeff: 0.0005000000000000001
        kl: 0.004014179537383218
        model: {}
        policy_loss: -0.007565232701987649
        total_loss: 2.3067109187444053
        vf_explained_var: 0.9953827261924744
        vf_loss: 2.3143608371416726
    num_steps_sampled: 19738624
    num_steps_trained: 19738624
  iterations_since_restore: 122
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.822580645161292
    gpu_util_percent0: 0.34225806451612895
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14706468650995777
    mean_env_wait_ms: 1.220116194423148
    mean_inference_ms: 4.327906200007397
    mean_raw_obs_processing_ms: 0.3804965606878631
  time_since_restore: 3222.6861538887024
  time_this_iter_s: 26.339429140090942
  time_total_s: 3222.6861538887024
  timers:
    learn_throughput: 8386.739
    learn_time_ms: 19291.408
    sample_throughput: 23732.174
    sample_time_ms: 6817.412
    update_time_ms: 26.743
  timestamp: 1602723802
  timesteps_since_restore: 0
  timesteps_total: 19738624
  training_iteration: 122
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    122 |          3222.69 | 19738624 |  266.396 |              307.808 |              74.7778 |            795.842 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3193.8962665596146
    time_step_min: 2924
  date: 2020-10-15_01-03-49
  done: false
  episode_len_mean: 795.8021083854417
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.635059104756
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 225
  episodes_total: 24948
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.551115123125784e-18
        cur_lr: 5.0e-05
        entropy: 0.15401445453365645
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046234835948174196
        model: {}
        policy_loss: -0.007929744202556321
        total_loss: 2.3736257354418435
        vf_explained_var: 0.9951786994934082
        vf_loss: 2.381632447242737
    num_steps_sampled: 19900416
    num_steps_trained: 19900416
  iterations_since_restore: 123
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.35
    gpu_util_percent0: 0.28366666666666673
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470552395109615
    mean_env_wait_ms: 1.220105060475992
    mean_inference_ms: 4.32734118630413
    mean_raw_obs_processing_ms: 0.38046532678431694
  time_since_restore: 3248.921319961548
  time_this_iter_s: 26.23516607284546
  time_total_s: 3248.921319961548
  timers:
    learn_throughput: 8387.471
    learn_time_ms: 19289.725
    sample_throughput: 23707.234
    sample_time_ms: 6824.584
    update_time_ms: 26.152
  timestamp: 1602723829
  timesteps_since_restore: 0
  timesteps_total: 19900416
  training_iteration: 123
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    123 |          3248.92 | 19900416 |  266.635 |              307.808 |              74.7778 |            795.802 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3192.713721894435
    time_step_min: 2924
  date: 2020-10-15_01-04-16
  done: false
  episode_len_mean: 795.7593742536422
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 266.8187813168706
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 174
  episodes_total: 25122
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.775557561562892e-18
        cur_lr: 5.0e-05
        entropy: 0.15736312294999757
        entropy_coeff: 0.0005000000000000001
        kl: 0.004359201528131962
        model: {}
        policy_loss: -0.009375940154617032
        total_loss: 1.9298242429892223
        vf_explained_var: 0.9953746795654297
        vf_loss: 1.9392788012822468
    num_steps_sampled: 20062208
    num_steps_trained: 20062208
  iterations_since_restore: 124
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.232258064516135
    gpu_util_percent0: 0.302258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470476731187004
    mean_env_wait_ms: 1.2200954059685016
    mean_inference_ms: 4.326885252631421
    mean_raw_obs_processing_ms: 0.38044107105461666
  time_since_restore: 3275.423604249954
  time_this_iter_s: 26.502284288406372
  time_total_s: 3275.423604249954
  timers:
    learn_throughput: 8371.854
    learn_time_ms: 19325.708
    sample_throughput: 23773.599
    sample_time_ms: 6805.532
    update_time_ms: 26.028
  timestamp: 1602723856
  timesteps_since_restore: 0
  timesteps_total: 20062208
  training_iteration: 124
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    124 |          3275.42 | 20062208 |  266.819 |              307.808 |              74.7778 |            795.759 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3191.3044406659023
    time_step_min: 2924
  date: 2020-10-15_01-04-42
  done: false
  episode_len_mean: 795.6953054052987
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 267.02972832522323
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 205
  episodes_total: 25327
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.387778780781446e-18
        cur_lr: 5.0e-05
        entropy: 0.17259028926491737
        entropy_coeff: 0.0005000000000000001
        kl: 0.004520415289637943
        model: {}
        policy_loss: -0.0073667444521561265
        total_loss: 2.7579530278841653
        vf_explained_var: 0.9940579533576965
        vf_loss: 2.7654060324033103
    num_steps_sampled: 20224000
    num_steps_trained: 20224000
  iterations_since_restore: 125
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.45806451612904
    gpu_util_percent0: 0.31129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.147039957182213
    mean_env_wait_ms: 1.2200810371972421
    mean_inference_ms: 4.326370066284135
    mean_raw_obs_processing_ms: 0.380413246188195
  time_since_restore: 3301.6202623844147
  time_this_iter_s: 26.19665813446045
  time_total_s: 3301.6202623844147
  timers:
    learn_throughput: 8381.382
    learn_time_ms: 19303.739
    sample_throughput: 23743.431
    sample_time_ms: 6814.179
    update_time_ms: 26.397
  timestamp: 1602723882
  timesteps_since_restore: 0
  timesteps_total: 20224000
  training_iteration: 125
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    125 |          3301.62 | 20224000 |   267.03 |              307.808 |              74.7778 |            795.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3189.7585801598498
    time_step_min: 2924
  date: 2020-10-15_01-05-10
  done: false
  episode_len_mean: 795.6599248885063
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 267.26050545356543
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 235
  episodes_total: 25562
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.93889390390723e-19
        cur_lr: 5.0e-05
        entropy: 0.1551519309480985
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035465058172121644
        model: {}
        policy_loss: -0.008046312674802417
        total_loss: 2.974668542544047
        vf_explained_var: 0.9942945837974548
        vf_loss: 2.9827925165494285
    num_steps_sampled: 20385792
    num_steps_trained: 20385792
  iterations_since_restore: 126
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.19677419354839
    gpu_util_percent0: 0.2964516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470299572286485
    mean_env_wait_ms: 1.2200595918741914
    mean_inference_ms: 4.325768963986008
    mean_raw_obs_processing_ms: 0.38037979595204147
  time_since_restore: 3328.1524856090546
  time_this_iter_s: 26.532223224639893
  time_total_s: 3328.1524856090546
  timers:
    learn_throughput: 8368.235
    learn_time_ms: 19334.064
    sample_throughput: 23759.118
    sample_time_ms: 6809.68
    update_time_ms: 27.335
  timestamp: 1602723910
  timesteps_since_restore: 0
  timesteps_total: 20385792
  training_iteration: 126
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    126 |          3328.15 | 20385792 |  267.261 |              307.808 |              74.7778 |             795.66 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3188.546455100533
    time_step_min: 2924
  date: 2020-10-15_01-05-37
  done: false
  episode_len_mean: 795.6390431439555
  episode_reward_max: 307.80808080808106
  episode_reward_mean: 267.44860668351015
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 25751
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.469446951953615e-19
        cur_lr: 5.0e-05
        entropy: 0.143906868994236
        entropy_coeff: 0.0005000000000000001
        kl: 0.004522902697014312
        model: {}
        policy_loss: -0.00920199352549389
        total_loss: 1.7865516046682994
        vf_explained_var: 0.9959679245948792
        vf_loss: 1.795825570821762
    num_steps_sampled: 20547584
    num_steps_trained: 20547584
  iterations_since_restore: 127
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4741935483871
    gpu_util_percent0: 0.41354838709677433
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14702207201346618
    mean_env_wait_ms: 1.2200467671687674
    mean_inference_ms: 4.32530949723128
    mean_raw_obs_processing_ms: 0.3803551354060116
  time_since_restore: 3354.6474463939667
  time_this_iter_s: 26.49496078491211
  time_total_s: 3354.6474463939667
  timers:
    learn_throughput: 8359.401
    learn_time_ms: 19354.498
    sample_throughput: 23750.212
    sample_time_ms: 6812.234
    update_time_ms: 26.85
  timestamp: 1602723937
  timesteps_since_restore: 0
  timesteps_total: 20547584
  training_iteration: 127
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    127 |          3354.65 | 20547584 |  267.449 |              307.808 |              74.7778 |            795.639 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3187.378543291882
    time_step_min: 2924
  date: 2020-10-15_01-06-04
  done: false
  episode_len_mean: 795.6139904365263
  episode_reward_max: 307.959595959596
  episode_reward_mean: 267.63031401474245
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 181
  episodes_total: 25932
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7347234759768074e-19
        cur_lr: 5.0e-05
        entropy: 0.1585683412849903
        entropy_coeff: 0.0005000000000000001
        kl: 0.004497177433222532
        model: {}
        policy_loss: -0.010205297032371163
        total_loss: 1.8996409873167674
        vf_explained_var: 0.9956421852111816
        vf_loss: 1.9099255204200745
    num_steps_sampled: 20709376
    num_steps_trained: 20709376
  iterations_since_restore: 128
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.16
    gpu_util_percent0: 0.30366666666666664
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14701521089746614
    mean_env_wait_ms: 1.2200303034706055
    mean_inference_ms: 4.324867290538939
    mean_raw_obs_processing_ms: 0.38033264707022524
  time_since_restore: 3380.779067993164
  time_this_iter_s: 26.131621599197388
  time_total_s: 3380.779067993164
  timers:
    learn_throughput: 8360.554
    learn_time_ms: 19351.827
    sample_throughput: 23740.786
    sample_time_ms: 6814.939
    update_time_ms: 26.51
  timestamp: 1602723964
  timesteps_since_restore: 0
  timesteps_total: 20709376
  training_iteration: 128
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    128 |          3380.78 | 20709376 |   267.63 |               307.96 |              74.7778 |            795.614 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3185.919923444976
    time_step_min: 2924
  date: 2020-10-15_01-06-31
  done: false
  episode_len_mean: 795.5657608072469
  episode_reward_max: 307.959595959596
  episode_reward_mean: 267.8478605571829
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 231
  episodes_total: 26163
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.673617379884037e-20
        cur_lr: 5.0e-05
        entropy: 0.15509062260389328
        entropy_coeff: 0.0005000000000000001
        kl: 0.004219735545727114
        model: {}
        policy_loss: -0.009656242385972291
        total_loss: 2.255702634652456
        vf_explained_var: 0.995591938495636
        vf_loss: 2.2654364506403604
    num_steps_sampled: 20871168
    num_steps_trained: 20871168
  iterations_since_restore: 129
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.841935483870966
    gpu_util_percent0: 0.29935483870967733
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1470060198702932
    mean_env_wait_ms: 1.2200048226329967
    mean_inference_ms: 4.324302289591288
    mean_raw_obs_processing_ms: 0.3803006526226467
  time_since_restore: 3407.386449575424
  time_this_iter_s: 26.607381582260132
  time_total_s: 3407.386449575424
  timers:
    learn_throughput: 8341.132
    learn_time_ms: 19396.887
    sample_throughput: 23732.594
    sample_time_ms: 6817.291
    update_time_ms: 27.627
  timestamp: 1602723991
  timesteps_since_restore: 0
  timesteps_total: 20871168
  training_iteration: 129
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    129 |          3407.39 | 20871168 |  267.848 |               307.96 |              74.7778 |            795.566 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3184.5350823904623
    time_step_min: 2924
  date: 2020-10-15_01-06-58
  done: false
  episode_len_mean: 795.5212693357598
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.05721110100086
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 213
  episodes_total: 26376
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3368086899420186e-20
        cur_lr: 5.0e-05
        entropy: 0.14217245702942213
        entropy_coeff: 0.0005000000000000001
        kl: 0.003843813940572242
        model: {}
        policy_loss: -0.00743472120181347
        total_loss: 2.0694449047247567
        vf_explained_var: 0.9955958724021912
        vf_loss: 2.0769506990909576
    num_steps_sampled: 21032960
    num_steps_trained: 21032960
  iterations_since_restore: 130
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.254838709677426
    gpu_util_percent0: 0.2780645161290322
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469974520180134
    mean_env_wait_ms: 1.219986473644227
    mean_inference_ms: 4.323818259924393
    mean_raw_obs_processing_ms: 0.3802738442763722
  time_since_restore: 3433.8399794101715
  time_this_iter_s: 26.453529834747314
  time_total_s: 3433.8399794101715
  timers:
    learn_throughput: 8342.268
    learn_time_ms: 19394.247
    sample_throughput: 23753.084
    sample_time_ms: 6811.41
    update_time_ms: 26.104
  timestamp: 1602724018
  timesteps_since_restore: 0
  timesteps_total: 21032960
  training_iteration: 130
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    130 |          3433.84 | 21032960 |  268.057 |               307.96 |              74.7778 |            795.521 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3183.454449432268
    time_step_min: 2924
  date: 2020-10-15_01-07-25
  done: false
  episode_len_mean: 795.4806192790146
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.2236330989862
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 171
  episodes_total: 26547
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1684043449710093e-20
        cur_lr: 5.0e-05
        entropy: 0.1456296518445015
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036228040892941258
        model: {}
        policy_loss: -0.007293626559354986
        total_loss: 1.6113335688908894
        vf_explained_var: 0.9961920380592346
        vf_loss: 1.6187000672022502
    num_steps_sampled: 21194752
    num_steps_trained: 21194752
  iterations_since_restore: 131
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.08709677419355
    gpu_util_percent0: 0.3029032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14699119246597495
    mean_env_wait_ms: 1.2199699036803868
    mean_inference_ms: 4.323419119479828
    mean_raw_obs_processing_ms: 0.38025319390566004
  time_since_restore: 3460.279097557068
  time_this_iter_s: 26.439118146896362
  time_total_s: 3460.279097557068
  timers:
    learn_throughput: 8345.366
    learn_time_ms: 19387.046
    sample_throughput: 23780.328
    sample_time_ms: 6803.607
    update_time_ms: 25.137
  timestamp: 1602724045
  timesteps_since_restore: 0
  timesteps_total: 21194752
  training_iteration: 131
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    131 |          3460.28 | 21194752 |  268.224 |               307.96 |              74.7778 |            795.481 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3182.0175819242854
    time_step_min: 2924
  date: 2020-10-15_01-07-51
  done: false
  episode_len_mean: 795.4318266716474
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.4376129618938
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 26770
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0842021724855046e-20
        cur_lr: 5.0e-05
        entropy: 0.1535994050403436
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037392685771919787
        model: {}
        policy_loss: -0.008139425100428829
        total_loss: 2.259928127129873
        vf_explained_var: 0.9953439235687256
        vf_loss: 2.268144349257151
    num_steps_sampled: 21356544
    num_steps_trained: 21356544
  iterations_since_restore: 132
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.836666666666666
    gpu_util_percent0: 0.3633333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14698229665690513
    mean_env_wait_ms: 1.2199427800616798
    mean_inference_ms: 4.3228843684367275
    mean_raw_obs_processing_ms: 0.3802240842677249
  time_since_restore: 3486.217154979706
  time_this_iter_s: 25.93805742263794
  time_total_s: 3486.217154979706
  timers:
    learn_throughput: 8357.109
    learn_time_ms: 19359.804
    sample_throughput: 23833.591
    sample_time_ms: 6788.402
    update_time_ms: 25.832
  timestamp: 1602724071
  timesteps_since_restore: 0
  timesteps_total: 21356544
  training_iteration: 132
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    132 |          3486.22 | 21356544 |  268.438 |               307.96 |              74.7778 |            795.432 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3180.550037091988
    time_step_min: 2924
  date: 2020-10-15_01-08-18
  done: false
  episode_len_mean: 795.3863989925179
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.65365971740516
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 228
  episodes_total: 26998
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.421010862427523e-21
        cur_lr: 5.0e-05
        entropy: 0.13972046971321106
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035111960411692658
        model: {}
        policy_loss: -0.006567574411747046
        total_loss: 2.4955084125200906
        vf_explained_var: 0.9950327277183533
        vf_loss: 2.5021459460258484
    num_steps_sampled: 21518336
    num_steps_trained: 21518336
  iterations_since_restore: 133
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.577419354838714
    gpu_util_percent0: 0.3841935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14697430084894136
    mean_env_wait_ms: 1.2199193885251463
    mean_inference_ms: 4.32239437512218
    mean_raw_obs_processing_ms: 0.38019655658857915
  time_since_restore: 3512.8101868629456
  time_this_iter_s: 26.593031883239746
  time_total_s: 3512.8101868629456
  timers:
    learn_throughput: 8347.605
    learn_time_ms: 19381.847
    sample_throughput: 23824.474
    sample_time_ms: 6791.0
    update_time_ms: 27.614
  timestamp: 1602724098
  timesteps_since_restore: 0
  timesteps_total: 21518336
  training_iteration: 133
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    133 |          3512.81 | 21518336 |  268.654 |               307.96 |              74.7778 |            795.386 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3179.419958726415
    time_step_min: 2924
  date: 2020-10-15_01-08-45
  done: false
  episode_len_mean: 795.370170015456
  episode_reward_max: 307.959595959596
  episode_reward_mean: 268.8188230282511
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 176
  episodes_total: 27174
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.7105054312137616e-21
        cur_lr: 5.0e-05
        entropy: 0.13431883975863457
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035796826511311033
        model: {}
        policy_loss: -0.010279505474803349
        total_loss: 1.877610703309377
        vf_explained_var: 0.995654821395874
        vf_loss: 1.887957404057185
    num_steps_sampled: 21680128
    num_steps_trained: 21680128
  iterations_since_restore: 134
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.490000000000002
    gpu_util_percent0: 0.2763333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696765323013178
    mean_env_wait_ms: 1.219899056729687
    mean_inference_ms: 4.322002386504381
    mean_raw_obs_processing_ms: 0.38017571614817375
  time_since_restore: 3539.0433633327484
  time_this_iter_s: 26.233176469802856
  time_total_s: 3539.0433633327484
  timers:
    learn_throughput: 8358.48
    learn_time_ms: 19356.63
    sample_throughput: 23831.922
    sample_time_ms: 6788.877
    update_time_ms: 27.553
  timestamp: 1602724125
  timesteps_since_restore: 0
  timesteps_total: 21680128
  training_iteration: 134
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    134 |          3539.04 | 21680128 |  268.819 |               307.96 |              74.7778 |             795.37 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3178.2238363583137
    time_step_min: 2924
  date: 2020-10-15_01-09-12
  done: false
  episode_len_mean: 795.3360739603888
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.0020529788124
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 27366
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3552527156068808e-21
        cur_lr: 5.0e-05
        entropy: 0.14118772372603416
        entropy_coeff: 0.0005000000000000001
        kl: 0.003906580755331864
        model: {}
        policy_loss: -0.008181799203157425
        total_loss: 2.2890235682328544
        vf_explained_var: 0.9951767921447754
        vf_loss: 2.297275980313619
    num_steps_sampled: 21841920
    num_steps_trained: 21841920
  iterations_since_restore: 135
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.67419354838709
    gpu_util_percent0: 0.2970967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14696173958878198
    mean_env_wait_ms: 1.2198749014835102
    mean_inference_ms: 4.321594580138483
    mean_raw_obs_processing_ms: 0.38015387987104277
  time_since_restore: 3565.2663657665253
  time_this_iter_s: 26.223002433776855
  time_total_s: 3565.2663657665253
  timers:
    learn_throughput: 8356.656
    learn_time_ms: 19360.855
    sample_throughput: 23836.513
    sample_time_ms: 6787.57
    update_time_ms: 27.127
  timestamp: 1602724152
  timesteps_since_restore: 0
  timesteps_total: 21841920
  training_iteration: 135
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    135 |          3565.27 | 21841920 |  269.002 |              308.414 |              74.7778 |            795.336 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3176.7530658152527
    time_step_min: 2924
  date: 2020-10-15_01-09-38
  done: false
  episode_len_mean: 795.2995289855072
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.22945212999554
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 234
  episodes_total: 27600
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.776263578034404e-22
        cur_lr: 5.0e-05
        entropy: 0.13460138936837515
        entropy_coeff: 0.0005000000000000001
        kl: 0.003422746706443528
        model: {}
        policy_loss: -0.00847506593951645
        total_loss: 2.073725998401642
        vf_explained_var: 0.9959105849266052
        vf_loss: 2.0822683572769165
    num_steps_sampled: 22003712
    num_steps_trained: 22003712
  iterations_since_restore: 136
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.436666666666667
    gpu_util_percent0: 0.2863333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469531450424056
    mean_env_wait_ms: 1.2198430379908731
    mean_inference_ms: 4.321078116276218
    mean_raw_obs_processing_ms: 0.3801251315974917
  time_since_restore: 3591.3468816280365
  time_this_iter_s: 26.08051586151123
  time_total_s: 3591.3468816280365
  timers:
    learn_throughput: 8374.403
    learn_time_ms: 19319.826
    sample_throughput: 23818.962
    sample_time_ms: 6792.571
    update_time_ms: 26.216
  timestamp: 1602724178
  timesteps_since_restore: 0
  timesteps_total: 22003712
  training_iteration: 136
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    136 |          3591.35 | 22003712 |  269.229 |              308.414 |              74.7778 |              795.3 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3175.475183691111
    time_step_min: 2924
  date: 2020-10-15_01-10-05
  done: false
  episode_len_mean: 795.2709517300914
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.41611314933374
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 27802
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.388131789017202e-22
        cur_lr: 5.0e-05
        entropy: 0.1251864731311798
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037129991105757654
        model: {}
        policy_loss: -0.007405182608636096
        total_loss: 2.101415902376175
        vf_explained_var: 0.9954603314399719
        vf_loss: 2.1088836193084717
    num_steps_sampled: 22165504
    num_steps_trained: 22165504
  iterations_since_restore: 137
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.43870967741935
    gpu_util_percent0: 0.3677419354838709
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14694525629636757
    mean_env_wait_ms: 1.2198166635719816
    mean_inference_ms: 4.320644002466286
    mean_raw_obs_processing_ms: 0.38010111042373257
  time_since_restore: 3617.6634364128113
  time_this_iter_s: 26.31655478477478
  time_total_s: 3617.6634364128113
  timers:
    learn_throughput: 8384.641
    learn_time_ms: 19296.234
    sample_throughput: 23798.37
    sample_time_ms: 6798.449
    update_time_ms: 26.07
  timestamp: 1602724205
  timesteps_since_restore: 0
  timesteps_total: 22165504
  training_iteration: 137
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    137 |          3617.66 | 22165504 |  269.416 |              308.414 |              74.7778 |            795.271 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3174.3302555293108
    time_step_min: 2924
  date: 2020-10-15_01-10-32
  done: false
  episode_len_mean: 795.2460686204432
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.5890444834333
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 178
  episodes_total: 27980
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.694065894508601e-22
        cur_lr: 5.0e-05
        entropy: 0.130267063776652
        entropy_coeff: 0.0005000000000000001
        kl: 0.004813761139909427
        model: {}
        policy_loss: -0.008824487410796186
        total_loss: 1.7493476569652557
        vf_explained_var: 0.9959143996238708
        vf_loss: 1.7582372725009918
    num_steps_sampled: 22327296
    num_steps_trained: 22327296
  iterations_since_restore: 138
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.403225806451612
    gpu_util_percent0: 0.3167741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14693927220181235
    mean_env_wait_ms: 1.2197900316077772
    mean_inference_ms: 4.320268555095389
    mean_raw_obs_processing_ms: 0.3800811823157749
  time_since_restore: 3643.862667798996
  time_this_iter_s: 26.199231386184692
  time_total_s: 3643.862667798996
  timers:
    learn_throughput: 8381.478
    learn_time_ms: 19303.517
    sample_throughput: 23805.927
    sample_time_ms: 6796.291
    update_time_ms: 25.925
  timestamp: 1602724232
  timesteps_since_restore: 0
  timesteps_total: 22327296
  training_iteration: 138
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    138 |          3643.86 | 22327296 |  269.589 |              308.414 |              74.7778 |            795.246 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3172.8971209485603
    time_step_min: 2920
  date: 2020-10-15_01-10-59
  done: false
  episode_len_mean: 795.2088843194952
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.8047536735096
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 28207
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.470329472543005e-23
        cur_lr: 5.0e-05
        entropy: 0.13515049094955126
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039993652802271145
        model: {}
        policy_loss: -0.009931408383029824
        total_loss: 2.006042162577311
        vf_explained_var: 0.995943546295166
        vf_loss: 2.016041169563929
    num_steps_sampled: 22489088
    num_steps_trained: 22489088
  iterations_since_restore: 139
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.28666666666667
    gpu_util_percent0: 0.3506666666666666
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1469311048119625
    mean_env_wait_ms: 1.2197538239888344
    mean_inference_ms: 4.319781732349833
    mean_raw_obs_processing_ms: 0.38005393060325715
  time_since_restore: 3670.082041501999
  time_this_iter_s: 26.21937370300293
  time_total_s: 3670.082041501999
  timers:
    learn_throughput: 8393.029
    learn_time_ms: 19276.951
    sample_throughput: 23847.251
    sample_time_ms: 6784.514
    update_time_ms: 24.427
  timestamp: 1602724259
  timesteps_since_restore: 0
  timesteps_total: 22489088
  training_iteration: 139
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    139 |          3670.08 | 22489088 |  269.805 |              308.414 |              74.7778 |            795.209 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3171.6120838471024
    time_step_min: 2920
  date: 2020-10-15_01-11-26
  done: false
  episode_len_mean: 795.1755620448229
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 269.9990187915107
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 216
  episodes_total: 28423
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2351647362715025e-23
        cur_lr: 5.0e-05
        entropy: 0.12379609669248264
        entropy_coeff: 0.0005000000000000001
        kl: 0.003206410755713781
        model: {}
        policy_loss: -0.01025933045699882
        total_loss: 2.1898199915885925
        vf_explained_var: 0.9955307841300964
        vf_loss: 2.2001412510871887
    num_steps_sampled: 22650880
    num_steps_trained: 22650880
  iterations_since_restore: 140
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.461290322580652
    gpu_util_percent0: 0.39580645161290323
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14692403062524437
    mean_env_wait_ms: 1.2197239863988
    mean_inference_ms: 4.319361957376541
    mean_raw_obs_processing_ms: 0.38003013121949714
  time_since_restore: 3696.4571690559387
  time_this_iter_s: 26.37512755393982
  time_total_s: 3696.4571690559387
  timers:
    learn_throughput: 8399.082
    learn_time_ms: 19263.058
    sample_throughput: 23836.441
    sample_time_ms: 6787.591
    update_time_ms: 26.026
  timestamp: 1602724286
  timesteps_since_restore: 0
  timesteps_total: 22650880
  training_iteration: 140
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    140 |          3696.46 | 22650880 |  269.999 |              308.414 |              74.7778 |            795.176 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3170.5573139135913
    time_step_min: 2920
  date: 2020-10-15_01-11-53
  done: false
  episode_len_mean: 795.1547552447553
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 270.15825033552306
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 177
  episodes_total: 28600
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1175823681357513e-23
        cur_lr: 5.0e-05
        entropy: 0.12362613094349702
        entropy_coeff: 0.0005000000000000001
        kl: 0.003781976760365069
        model: {}
        policy_loss: -0.009440499998163432
        total_loss: 1.7644678850968678
        vf_explained_var: 0.9959544539451599
        vf_loss: 1.7739701569080353
    num_steps_sampled: 22812672
    num_steps_trained: 22812672
  iterations_since_restore: 141
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.822580645161292
    gpu_util_percent0: 0.29354838709677417
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146918119022963
    mean_env_wait_ms: 1.2196983719444594
    mean_inference_ms: 4.3190073386170535
    mean_raw_obs_processing_ms: 0.38001227980030766
  time_since_restore: 3723.1388731002808
  time_this_iter_s: 26.68170404434204
  time_total_s: 3723.1388731002808
  timers:
    learn_throughput: 8395.586
    learn_time_ms: 19271.08
    sample_throughput: 23759.032
    sample_time_ms: 6809.705
    update_time_ms: 27.296
  timestamp: 1602724313
  timesteps_since_restore: 0
  timesteps_total: 22812672
  training_iteration: 141
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    141 |          3723.14 | 22812672 |  270.158 |              308.414 |              74.7778 |            795.155 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3169.321666898373
    time_step_min: 2920
  date: 2020-10-15_01-12-20
  done: false
  episode_len_mean: 795.1204790003471
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 270.3446895192816
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 210
  episodes_total: 28810
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0587911840678756e-23
        cur_lr: 5.0e-05
        entropy: 0.13305428127447763
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035048399198179445
        model: {}
        policy_loss: -0.007412093866150826
        total_loss: 1.9814454317092896
        vf_explained_var: 0.9958277344703674
        vf_loss: 1.9889240463574727
    num_steps_sampled: 22974464
    num_steps_trained: 22974464
  iterations_since_restore: 142
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.76129032258064
    gpu_util_percent0: 0.3283870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14691162146243414
    mean_env_wait_ms: 1.2196634246842097
    mean_inference_ms: 4.318582075909425
    mean_raw_obs_processing_ms: 0.37998912757646375
  time_since_restore: 3749.1641359329224
  time_this_iter_s: 26.0252628326416
  time_total_s: 3749.1641359329224
  timers:
    learn_throughput: 8392.42
    learn_time_ms: 19278.349
    sample_throughput: 23746.536
    sample_time_ms: 6813.288
    update_time_ms: 25.564
  timestamp: 1602724340
  timesteps_since_restore: 0
  timesteps_total: 22974464
  training_iteration: 142
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    142 |          3749.16 | 22974464 |  270.345 |              308.414 |              74.7778 |             795.12 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3167.9180056547825
    time_step_min: 2920
  date: 2020-10-15_01-12-47
  done: false
  episode_len_mean: 795.0799931129477
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 270.5590356039736
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 230
  episodes_total: 29040
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.293955920339378e-24
        cur_lr: 5.0e-05
        entropy: 0.12667775650819144
        entropy_coeff: 0.0005000000000000001
        kl: 0.00492284051142633
        model: {}
        policy_loss: -0.007317290791737226
        total_loss: 1.7784966826438904
        vf_explained_var: 0.9964411854743958
        vf_loss: 1.7858772973219554
    num_steps_sampled: 23136256
    num_steps_trained: 23136256
  iterations_since_restore: 143
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.290000000000006
    gpu_util_percent0: 0.2693333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14690381183832063
    mean_env_wait_ms: 1.2196269598807603
    mean_inference_ms: 4.31813666595777
    mean_raw_obs_processing_ms: 0.379963643693886
  time_since_restore: 3775.483208656311
  time_this_iter_s: 26.319072723388672
  time_total_s: 3775.483208656311
  timers:
    learn_throughput: 8402.158
    learn_time_ms: 19256.005
    sample_throughput: 23730.829
    sample_time_ms: 6817.798
    update_time_ms: 23.176
  timestamp: 1602724367
  timesteps_since_restore: 0
  timesteps_total: 23136256
  training_iteration: 143
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    143 |          3775.48 | 23136256 |  270.559 |              308.414 |              74.7778 |             795.08 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3166.7544364508394
    time_step_min: 2920
  date: 2020-10-15_01-13-14
  done: false
  episode_len_mean: 795.0319556589571
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 270.7306104703805
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 29228
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.646977960169689e-24
        cur_lr: 5.0e-05
        entropy: 0.12087202134231727
        entropy_coeff: 0.0005000000000000001
        kl: 0.003885650832671672
        model: {}
        policy_loss: -0.008519552007783204
        total_loss: 1.4720495243867238
        vf_explained_var: 0.996600329875946
        vf_loss: 1.4806294739246368
    num_steps_sampled: 23298048
    num_steps_trained: 23298048
  iterations_since_restore: 144
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03870967741936
    gpu_util_percent0: 0.4258064516129032
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689752368737785
    mean_env_wait_ms: 1.2195967978768594
    mean_inference_ms: 4.317779015920907
    mean_raw_obs_processing_ms: 0.3799444729778644
  time_since_restore: 3801.9175317287445
  time_this_iter_s: 26.43432307243347
  time_total_s: 3801.9175317287445
  timers:
    learn_throughput: 8395.93
    learn_time_ms: 19270.289
    sample_throughput: 23711.618
    sample_time_ms: 6823.322
    update_time_ms: 22.767
  timestamp: 1602724394
  timesteps_since_restore: 0
  timesteps_total: 23298048
  training_iteration: 144
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    144 |          3801.92 | 23298048 |  270.731 |              308.414 |              74.7778 |            795.032 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3165.679773942056
    time_step_min: 2920
  date: 2020-10-15_01-13-41
  done: false
  episode_len_mean: 794.9848696066098
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 270.89087914265565
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 183
  episodes_total: 29411
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3234889800848445e-24
        cur_lr: 5.0e-05
        entropy: 0.12746182146171728
        entropy_coeff: 0.0005000000000000001
        kl: 0.004253778761873643
        model: {}
        policy_loss: -0.010120474487242367
        total_loss: 1.6703322331110637
        vf_explained_var: 0.9962541460990906
        vf_loss: 1.680516431728999
    num_steps_sampled: 23459840
    num_steps_trained: 23459840
  iterations_since_restore: 145
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.306451612903228
    gpu_util_percent0: 0.3467741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14689238972511373
    mean_env_wait_ms: 1.2195665604144816
    mean_inference_ms: 4.317434690851999
    mean_raw_obs_processing_ms: 0.37992608603248695
  time_since_restore: 3828.268129825592
  time_this_iter_s: 26.350598096847534
  time_total_s: 3828.268129825592
  timers:
    learn_throughput: 8390.578
    learn_time_ms: 19282.58
    sample_throughput: 23722.105
    sample_time_ms: 6820.305
    update_time_ms: 23.894
  timestamp: 1602724421
  timesteps_since_restore: 0
  timesteps_total: 23459840
  training_iteration: 145
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    145 |          3828.27 | 23459840 |  270.891 |              308.414 |              74.7778 |            794.985 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3164.3092278592176
    time_step_min: 2920
  date: 2020-10-15_01-14-08
  done: false
  episode_len_mean: 794.9359060855485
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 271.0963490661574
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 233
  episodes_total: 29644
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.617444900424223e-25
        cur_lr: 5.0e-05
        entropy: 0.12905923339227834
        entropy_coeff: 0.0005000000000000001
        kl: 0.004093328180412452
        model: {}
        policy_loss: -0.008366586601672074
        total_loss: 1.9019785424073536
        vf_explained_var: 0.9962279200553894
        vf_loss: 1.9104096392790477
    num_steps_sampled: 23621632
    num_steps_trained: 23621632
  iterations_since_restore: 146
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.593548387096778
    gpu_util_percent0: 0.3319354838709678
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14688453337519586
    mean_env_wait_ms: 1.2195221696788672
    mean_inference_ms: 4.316977088389289
    mean_raw_obs_processing_ms: 0.37989988507828676
  time_since_restore: 3854.771952867508
  time_this_iter_s: 26.503823041915894
  time_total_s: 3854.771952867508
  timers:
    learn_throughput: 8373.589
    learn_time_ms: 19321.704
    sample_throughput: 23718.667
    sample_time_ms: 6821.294
    update_time_ms: 24.33
  timestamp: 1602724448
  timesteps_since_restore: 0
  timesteps_total: 23621632
  training_iteration: 146
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    146 |          3854.77 | 23621632 |  271.096 |              308.414 |              74.7778 |            794.936 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3163.0818297672545
    time_step_min: 2920
  date: 2020-10-15_01-14-35
  done: false
  episode_len_mean: 794.893723204716
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 271.283261676248
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 212
  episodes_total: 29856
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3087224502121113e-25
        cur_lr: 5.0e-05
        entropy: 0.11751252847413222
        entropy_coeff: 0.0005000000000000001
        kl: 0.004421637277118862
        model: {}
        policy_loss: -0.008145116701295288
        total_loss: 1.4339002867539723
        vf_explained_var: 0.9968571066856384
        vf_loss: 1.442104160785675
    num_steps_sampled: 23783424
    num_steps_trained: 23783424
  iterations_since_restore: 147
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.641935483870967
    gpu_util_percent0: 0.32096774193548383
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687773229653892
    mean_env_wait_ms: 1.2194881744585206
    mean_inference_ms: 4.316595859455678
    mean_raw_obs_processing_ms: 0.3798792474019257
  time_since_restore: 3881.239435195923
  time_this_iter_s: 26.467482328414917
  time_total_s: 3881.239435195923
  timers:
    learn_throughput: 8365.128
    learn_time_ms: 19341.247
    sample_throughput: 23747.703
    sample_time_ms: 6812.954
    update_time_ms: 26.651
  timestamp: 1602724475
  timesteps_since_restore: 0
  timesteps_total: 23783424
  training_iteration: 147
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    147 |          3881.24 | 23783424 |  271.283 |              308.414 |              74.7778 |            794.894 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3162.068251533742
    time_step_min: 2920
  date: 2020-10-15_01-15-02
  done: false
  episode_len_mean: 794.8574425574426
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 271.43592770865496
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 174
  episodes_total: 30030
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6543612251060557e-25
        cur_lr: 5.0e-05
        entropy: 0.11779219160477321
        entropy_coeff: 0.0005000000000000001
        kl: 0.0036287559778429568
        model: {}
        policy_loss: -0.010393784764649657
        total_loss: 1.5449826618035634
        vf_explained_var: 0.9963787198066711
        vf_loss: 1.5554353892803192
    num_steps_sampled: 23945216
    num_steps_trained: 23945216
  iterations_since_restore: 148
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.59677419354839
    gpu_util_percent0: 0.31032258064516133
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14687250995075432
    mean_env_wait_ms: 1.2194554492612943
    mean_inference_ms: 4.3162753506236236
    mean_raw_obs_processing_ms: 0.3798623117071707
  time_since_restore: 3907.5579385757446
  time_this_iter_s: 26.318503379821777
  time_total_s: 3907.5579385757446
  timers:
    learn_throughput: 8368.515
    learn_time_ms: 19333.418
    sample_throughput: 23711.067
    sample_time_ms: 6823.48
    update_time_ms: 26.998
  timestamp: 1602724502
  timesteps_since_restore: 0
  timesteps_total: 23945216
  training_iteration: 148
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    148 |          3907.56 | 23945216 |  271.436 |              308.414 |              74.7778 |            794.857 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3160.784804262219
    time_step_min: 2920
  date: 2020-10-15_01-15-29
  done: false
  episode_len_mean: 794.8107875863436
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 271.63192422623297
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 30257
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.271806125530278e-26
        cur_lr: 5.0e-05
        entropy: 0.1219485489030679
        entropy_coeff: 0.0005000000000000001
        kl: 0.003947349478645871
        model: {}
        policy_loss: -0.006697359213527913
        total_loss: 2.1094170014063516
        vf_explained_var: 0.9958721995353699
        vf_loss: 2.116175333658854
    num_steps_sampled: 24107008
    num_steps_trained: 24107008
  iterations_since_restore: 149
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.38333333333334
    gpu_util_percent0: 0.3653333333333333
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14686563029904123
    mean_env_wait_ms: 1.2194133377052836
    mean_inference_ms: 4.315842070900498
    mean_raw_obs_processing_ms: 0.3798382182072819
  time_since_restore: 3933.8278274536133
  time_this_iter_s: 26.269888877868652
  time_total_s: 3933.8278274536133
  timers:
    learn_throughput: 8366.105
    learn_time_ms: 19338.989
    sample_throughput: 23719.373
    sample_time_ms: 6821.091
    update_time_ms: 28.239
  timestamp: 1602724529
  timesteps_since_restore: 0
  timesteps_total: 24107008
  training_iteration: 149
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    149 |          3933.83 | 24107008 |  271.632 |              308.414 |              74.7778 |            794.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3159.5185623234115
    time_step_min: 2920
  date: 2020-10-15_01-15-56
  done: false
  episode_len_mean: 794.7544625278908
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 271.8177287376985
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 30476
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.135903062765139e-26
        cur_lr: 5.0e-05
        entropy: 0.11414973624050617
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037027442983041206
        model: {}
        policy_loss: -0.00700876828826343
        total_loss: 1.984802395105362
        vf_explained_var: 0.9960281848907471
        vf_loss: 1.9918682277202606
    num_steps_sampled: 24268800
    num_steps_trained: 24268800
  iterations_since_restore: 150
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.451612903225808
    gpu_util_percent0: 0.40161290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468593757547107
    mean_env_wait_ms: 1.2193722712299069
    mean_inference_ms: 4.31547478255403
    mean_raw_obs_processing_ms: 0.37981807259978223
  time_since_restore: 3960.20219540596
  time_this_iter_s: 26.3743679523468
  time_total_s: 3960.20219540596
  timers:
    learn_throughput: 8365.285
    learn_time_ms: 19340.883
    sample_throughput: 23724.262
    sample_time_ms: 6819.685
    update_time_ms: 26.39
  timestamp: 1602724556
  timesteps_since_restore: 0
  timesteps_total: 24268800
  training_iteration: 150
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    150 |           3960.2 | 24268800 |  271.818 |              308.414 |              74.7778 |            794.754 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3158.5361225422953
    time_step_min: 2920
  date: 2020-10-15_01-16-23
  done: false
  episode_len_mean: 794.7126174321503
  episode_reward_max: 308.41414141414117
  episode_reward_mean: 271.9692366646633
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 180
  episodes_total: 30656
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0679515313825696e-26
        cur_lr: 5.0e-05
        entropy: 0.10889130147794883
        entropy_coeff: 0.0005000000000000001
        kl: 0.003036506454615543
        model: {}
        policy_loss: -0.00805585429770872
        total_loss: 1.2627278467019398
        vf_explained_var: 0.9970510601997375
        vf_loss: 1.2708381116390228
    num_steps_sampled: 24430592
    num_steps_trained: 24430592
  iterations_since_restore: 151
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.26774193548387
    gpu_util_percent0: 0.3229032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14685388533954677
    mean_env_wait_ms: 1.2193393928122125
    mean_inference_ms: 4.315158161755317
    mean_raw_obs_processing_ms: 0.3798016098990228
  time_since_restore: 3986.5369729995728
  time_this_iter_s: 26.33477759361267
  time_total_s: 3986.5369729995728
  timers:
    learn_throughput: 8377.754
    learn_time_ms: 19312.096
    sample_throughput: 23774.213
    sample_time_ms: 6805.357
    update_time_ms: 25.323
  timestamp: 1602724583
  timesteps_since_restore: 0
  timesteps_total: 24430592
  training_iteration: 151
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    151 |          3986.54 | 24430592 |  271.969 |              308.414 |              74.7778 |            794.713 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3157.3362211622702
    time_step_min: 2920
  date: 2020-10-15_01-16-50
  done: false
  episode_len_mean: 794.6673040152964
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 272.14793395274313
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 30857
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0339757656912848e-26
        cur_lr: 5.0e-05
        entropy: 0.12048157428701718
        entropy_coeff: 0.0005000000000000001
        kl: 0.004222436905062447
        model: {}
        policy_loss: -0.007020138019773488
        total_loss: 1.5216034054756165
        vf_explained_var: 0.9967145919799805
        vf_loss: 1.5286837716897328
    num_steps_sampled: 24592384
    num_steps_trained: 24592384
  iterations_since_restore: 152
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.83870967741936
    gpu_util_percent0: 0.3125806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468484844024485
    mean_env_wait_ms: 1.2193000329417008
    mean_inference_ms: 4.314795238881555
    mean_raw_obs_processing_ms: 0.3797823414685656
  time_since_restore: 4012.986393213272
  time_this_iter_s: 26.44942021369934
  time_total_s: 4012.986393213272
  timers:
    learn_throughput: 8362.711
    learn_time_ms: 19346.836
    sample_throughput: 23778.337
    sample_time_ms: 6804.176
    update_time_ms: 25.343
  timestamp: 1602724610
  timesteps_since_restore: 0
  timesteps_total: 24592384
  training_iteration: 152
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    152 |          4012.99 | 24592384 |  272.148 |              309.323 |              74.7778 |            794.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3155.9849948480164
    time_step_min: 2920
  date: 2020-10-15_01-17-17
  done: false
  episode_len_mean: 794.6066443686885
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 272.3524097344448
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 237
  episodes_total: 31094
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.169878828456424e-27
        cur_lr: 5.0e-05
        entropy: 0.11523137365778287
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038115935555348792
        model: {}
        policy_loss: -0.00927500263787806
        total_loss: 1.3129855493704479
        vf_explained_var: 0.9973419308662415
        vf_loss: 1.3223181267579396
    num_steps_sampled: 24754176
    num_steps_trained: 24754176
  iterations_since_restore: 153
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290327
    gpu_util_percent0: 0.2938709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468409827479006
    mean_env_wait_ms: 1.2192538363133272
    mean_inference_ms: 4.314393199402007
    mean_raw_obs_processing_ms: 0.379759230763388
  time_since_restore: 4039.358179092407
  time_this_iter_s: 26.371785879135132
  time_total_s: 4039.358179092407
  timers:
    learn_throughput: 8357.702
    learn_time_ms: 19358.43
    sample_throughput: 23804.146
    sample_time_ms: 6796.799
    update_time_ms: 25.882
  timestamp: 1602724637
  timesteps_since_restore: 0
  timesteps_total: 24754176
  training_iteration: 153
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    153 |          4039.36 | 24754176 |  272.352 |              309.323 |              74.7778 |            794.607 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3154.912230722448
    time_step_min: 2920
  date: 2020-10-15_01-17-44
  done: false
  episode_len_mean: 794.5621663096646
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 272.51509693953494
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 31279
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.584939414228212e-27
        cur_lr: 5.0e-05
        entropy: 0.10415689460933208
        entropy_coeff: 0.0005000000000000001
        kl: 0.017936949074889224
        model: {}
        policy_loss: -0.006763588171452284
        total_loss: 1.1289669672648113
        vf_explained_var: 0.9973803162574768
        vf_loss: 1.13578262925148
    num_steps_sampled: 24915968
    num_steps_trained: 24915968
  iterations_since_restore: 154
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.763333333333335
    gpu_util_percent0: 0.255
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468358059348251
    mean_env_wait_ms: 1.2192209236203355
    mean_inference_ms: 4.314085454025455
    mean_raw_obs_processing_ms: 0.3797427544933111
  time_since_restore: 4065.4874365329742
  time_this_iter_s: 26.129257440567017
  time_total_s: 4065.4874365329742
  timers:
    learn_throughput: 8372.009
    learn_time_ms: 19325.351
    sample_throughput: 23802.367
    sample_time_ms: 6797.307
    update_time_ms: 26.625
  timestamp: 1602724664
  timesteps_since_restore: 0
  timesteps_total: 24915968
  training_iteration: 154
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    154 |          4065.49 | 24915968 |  272.515 |              309.323 |              74.7778 |            794.562 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3153.868482168422
    time_step_min: 2920
  date: 2020-10-15_01-18-11
  done: false
  episode_len_mean: 794.4935655047504
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 272.6669648408074
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 31471
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.584939414228212e-27
        cur_lr: 5.0e-05
        entropy: 0.15856528530518213
        entropy_coeff: 0.0005000000000000001
        kl: 0.01051321936150392
        model: {}
        policy_loss: -0.01032400131225586
        total_loss: 2.1709858775138855
        vf_explained_var: 0.9948298931121826
        vf_loss: 2.1813891331354776
    num_steps_sampled: 25077760
    num_steps_trained: 25077760
  iterations_since_restore: 155
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.67741935483871
    gpu_util_percent0: 0.3783870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468314562407502
    mean_env_wait_ms: 1.2191848762979542
    mean_inference_ms: 4.313779617524805
    mean_raw_obs_processing_ms: 0.3797276103576116
  time_since_restore: 4092.0545992851257
  time_this_iter_s: 26.56716275215149
  time_total_s: 4092.0545992851257
  timers:
    learn_throughput: 8366.574
    learn_time_ms: 19337.904
    sample_throughput: 23765.005
    sample_time_ms: 6807.994
    update_time_ms: 25.885
  timestamp: 1602724691
  timesteps_since_restore: 0
  timesteps_total: 25077760
  training_iteration: 155
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    155 |          4092.05 | 25077760 |  272.667 |              309.323 |              74.7778 |            794.494 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3152.9948847489736
    time_step_min: 2920
  date: 2020-10-15_01-18-38
  done: false
  episode_len_mean: 794.3446764223539
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 272.8020742940952
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 237
  episodes_total: 31708
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.584939414228212e-27
        cur_lr: 5.0e-05
        entropy: 0.13446767007311186
        entropy_coeff: 0.0005000000000000001
        kl: 0.004500673462947209
        model: {}
        policy_loss: -0.01042989460984245
        total_loss: 3.477419137954712
        vf_explained_var: 0.9932544827461243
        vf_loss: 3.4879161516825357
    num_steps_sampled: 25239552
    num_steps_trained: 25239552
  iterations_since_restore: 156
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.580645161290324
    gpu_util_percent0: 0.3712903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1468240024397264
    mean_env_wait_ms: 1.219135834037974
    mean_inference_ms: 4.313362856381608
    mean_raw_obs_processing_ms: 0.3797036786719301
  time_since_restore: 4118.363482952118
  time_this_iter_s: 26.308883666992188
  time_total_s: 4118.363482952118
  timers:
    learn_throughput: 8378.638
    learn_time_ms: 19310.061
    sample_throughput: 23736.965
    sample_time_ms: 6816.036
    update_time_ms: 25.444
  timestamp: 1602724718
  timesteps_since_restore: 0
  timesteps_total: 25239552
  training_iteration: 156
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    156 |          4118.36 | 25239552 |  272.802 |              309.323 |              74.7778 |            794.345 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3151.9174170876345
    time_step_min: 2920
  date: 2020-10-15_01-19-05
  done: false
  episode_len_mean: 794.3076561471685
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 272.9655978760306
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 31909
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.292469707114106e-27
        cur_lr: 5.0e-05
        entropy: 0.10840830393135548
        entropy_coeff: 0.0005000000000000001
        kl: 0.003853196142396579
        model: {}
        policy_loss: -0.008345854648117287
        total_loss: 1.67030734817187
        vf_explained_var: 0.9962877631187439
        vf_loss: 1.6787073810895283
    num_steps_sampled: 25401344
    num_steps_trained: 25401344
  iterations_since_restore: 157
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.780645161290323
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681838644360098
    mean_env_wait_ms: 1.2190998151457826
    mean_inference_ms: 4.313039907942146
    mean_raw_obs_processing_ms: 0.37968650177272706
  time_since_restore: 4144.955840826035
  time_this_iter_s: 26.592357873916626
  time_total_s: 4144.955840826035
  timers:
    learn_throughput: 8376.357
    learn_time_ms: 19315.317
    sample_throughput: 23711.039
    sample_time_ms: 6823.488
    update_time_ms: 23.836
  timestamp: 1602724745
  timesteps_since_restore: 0
  timesteps_total: 25401344
  training_iteration: 157
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    157 |          4144.96 | 25401344 |  272.966 |              309.323 |              74.7778 |            794.308 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3150.95507160463
    time_step_min: 2920
  date: 2020-10-15_01-19-32
  done: false
  episode_len_mean: 794.2742372775717
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 273.1109080773139
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 180
  episodes_total: 32089
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.46234853557053e-28
        cur_lr: 5.0e-05
        entropy: 0.1075210701674223
        entropy_coeff: 0.0005000000000000001
        kl: 0.003212205076124519
        model: {}
        policy_loss: -0.008807918425494185
        total_loss: 1.1746951738993328
        vf_explained_var: 0.99723219871521
        vf_loss: 1.1835568447907765
    num_steps_sampled: 25563136
    num_steps_trained: 25563136
  iterations_since_restore: 158
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.161290322580637
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14681394050956756
    mean_env_wait_ms: 1.2190654700993075
    mean_inference_ms: 4.312757107614466
    mean_raw_obs_processing_ms: 0.3796723562613562
  time_since_restore: 4171.138345479965
  time_this_iter_s: 26.182504653930664
  time_total_s: 4171.138345479965
  timers:
    learn_throughput: 8381.663
    learn_time_ms: 19303.09
    sample_throughput: 23717.546
    sample_time_ms: 6821.616
    update_time_ms: 23.891
  timestamp: 1602724772
  timesteps_since_restore: 0
  timesteps_total: 25563136
  training_iteration: 158
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    158 |          4171.14 | 25563136 |  273.111 |              309.323 |              74.7778 |            794.274 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3149.6940640684056
    time_step_min: 2920
  date: 2020-10-15_01-19-59
  done: false
  episode_len_mean: 794.2329805669018
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 273.30291496472324
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 32316
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.231174267785265e-28
        cur_lr: 5.0e-05
        entropy: 0.11281876266002655
        entropy_coeff: 0.0005000000000000001
        kl: 0.004022199214280893
        model: {}
        policy_loss: -0.009614041773602366
        total_loss: 1.4290498395760853
        vf_explained_var: 0.9971585273742676
        vf_loss: 1.438720315694809
    num_steps_sampled: 25724928
    num_steps_trained: 25724928
  iterations_since_restore: 159
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15483870967742
    gpu_util_percent0: 0.3125806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680713576397506
    mean_env_wait_ms: 1.2190161236466863
    mean_inference_ms: 4.312360402385558
    mean_raw_obs_processing_ms: 0.3796504169450089
  time_since_restore: 4197.693056344986
  time_this_iter_s: 26.554710865020752
  time_total_s: 4197.693056344986
  timers:
    learn_throughput: 8375.199
    learn_time_ms: 19317.989
    sample_throughput: 23674.439
    sample_time_ms: 6834.037
    update_time_ms: 24.093
  timestamp: 1602724799
  timesteps_since_restore: 0
  timesteps_total: 25724928
  training_iteration: 159
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    159 |          4197.69 | 25724928 |  273.303 |              309.323 |              74.7778 |            794.233 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3148.5034005231573
    time_step_min: 2920
  date: 2020-10-15_01-20-26
  done: false
  episode_len_mean: 794.1902068668736
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 273.4807218280614
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 217
  episodes_total: 32533
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6155871338926325e-28
        cur_lr: 5.0e-05
        entropy: 0.10280139992634456
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032789016452928386
        model: {}
        policy_loss: -0.007331955207822223
        total_loss: 1.2468681335449219
        vf_explained_var: 0.9973790049552917
        vf_loss: 1.2542515297730763
    num_steps_sampled: 25886720
    num_steps_trained: 25886720
  iterations_since_restore: 160
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.938709677419357
    gpu_util_percent0: 0.42483870967741927
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14680184205442814
    mean_env_wait_ms: 1.2189723816491225
    mean_inference_ms: 4.312035250631746
    mean_raw_obs_processing_ms: 0.3796328437509021
  time_since_restore: 4223.989552736282
  time_this_iter_s: 26.296496391296387
  time_total_s: 4223.989552736282
  timers:
    learn_throughput: 8379.597
    learn_time_ms: 19307.85
    sample_throughput: 23668.899
    sample_time_ms: 6835.637
    update_time_ms: 24.539
  timestamp: 1602724826
  timesteps_since_restore: 0
  timesteps_total: 25886720
  training_iteration: 160
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    160 |          4223.99 | 25886720 |  273.481 |              309.323 |              74.7778 |             794.19 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3147.4770021727822
    time_step_min: 2920
  date: 2020-10-15_01-20-53
  done: false
  episode_len_mean: 794.137857252025
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 273.63525673979586
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 32715
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.077935669463162e-29
        cur_lr: 5.0e-05
        entropy: 0.09823265112936497
        entropy_coeff: 0.0005000000000000001
        kl: 0.003025854976537327
        model: {}
        policy_loss: -0.008268413735398402
        total_loss: 0.985111340880394
        vf_explained_var: 0.9976353049278259
        vf_loss: 0.9934288511673609
    num_steps_sampled: 26048512
    num_steps_trained: 26048512
  iterations_since_restore: 161
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.222580645161294
    gpu_util_percent0: 0.2829032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467970415885248
    mean_env_wait_ms: 1.218938892651081
    mean_inference_ms: 4.311761829926031
    mean_raw_obs_processing_ms: 0.37961919271249434
  time_since_restore: 4250.560168266296
  time_this_iter_s: 26.570615530014038
  time_total_s: 4250.560168266296
  timers:
    learn_throughput: 8366.794
    learn_time_ms: 19337.395
    sample_throughput: 23661.259
    sample_time_ms: 6837.844
    update_time_ms: 24.15
  timestamp: 1602724853
  timesteps_since_restore: 0
  timesteps_total: 26048512
  training_iteration: 161
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    161 |          4250.56 | 26048512 |  273.635 |              309.323 |              74.7778 |            794.138 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3146.278794697154
    time_step_min: 2920
  date: 2020-10-15_01-21-20
  done: false
  episode_len_mean: 794.0783575290044
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 273.81437775679404
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 211
  episodes_total: 32926
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.038967834731581e-29
        cur_lr: 5.0e-05
        entropy: 0.1086427370707194
        entropy_coeff: 0.0005000000000000001
        kl: 0.004374288798620303
        model: {}
        policy_loss: -0.00683564854504463
        total_loss: 1.0502602408329647
        vf_explained_var: 0.997721254825592
        vf_loss: 1.0571501900752385
    num_steps_sampled: 26210304
    num_steps_trained: 26210304
  iterations_since_restore: 162
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.993548387096777
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14679167135681626
    mean_env_wait_ms: 1.2188942280088837
    mean_inference_ms: 4.311420882300259
    mean_raw_obs_processing_ms: 0.37960043966766804
  time_since_restore: 4276.910814285278
  time_this_iter_s: 26.350646018981934
  time_total_s: 4276.910814285278
  timers:
    learn_throughput: 8371.619
    learn_time_ms: 19326.249
    sample_throughput: 23633.585
    sample_time_ms: 6845.851
    update_time_ms: 24.075
  timestamp: 1602724880
  timesteps_since_restore: 0
  timesteps_total: 26210304
  training_iteration: 162
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    162 |          4276.91 | 26210304 |  273.814 |              309.323 |              74.7778 |            794.078 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3145.0657505285412
    time_step_min: 2920
  date: 2020-10-15_01-21-47
  done: false
  episode_len_mean: 794.0250392180524
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 273.9959489915445
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 33148
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0194839173657906e-29
        cur_lr: 5.0e-05
        entropy: 0.10576459144552548
        entropy_coeff: 0.0005000000000000001
        kl: 0.003480523960509648
        model: {}
        policy_loss: -0.008576788502978161
        total_loss: 1.147697736819585
        vf_explained_var: 0.9976896643638611
        vf_loss: 1.1563273966312408
    num_steps_sampled: 26372096
    num_steps_trained: 26372096
  iterations_since_restore: 163
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34193548387097
    gpu_util_percent0: 0.3167741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146785941312237
    mean_env_wait_ms: 1.2188477414456929
    mean_inference_ms: 4.311090375871341
    mean_raw_obs_processing_ms: 0.37958286502981825
  time_since_restore: 4303.477027893066
  time_this_iter_s: 26.566213607788086
  time_total_s: 4303.477027893066
  timers:
    learn_throughput: 8365.487
    learn_time_ms: 19340.416
    sample_throughput: 23618.162
    sample_time_ms: 6850.321
    update_time_ms: 23.786
  timestamp: 1602724907
  timesteps_since_restore: 0
  timesteps_total: 26372096
  training_iteration: 163
  trial_id: 9f737_00000
  
2020-10-15 01:21:48,647	WARNING util.py:136 -- The `process_trial` operation took 0.5113904476165771 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    163 |          4303.48 | 26372096 |  273.996 |              309.323 |              74.7778 |            794.025 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3144.02015015015
    time_step_min: 2920
  date: 2020-10-15_01-22-15
  done: false
  episode_len_mean: 793.9811626372308
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 274.1529607067131
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 190
  episodes_total: 33338
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0097419586828953e-29
        cur_lr: 5.0e-05
        entropy: 0.0981574176500241
        entropy_coeff: 0.0005000000000000001
        kl: 0.00488307629711926
        model: {}
        policy_loss: -0.00976846702299857
        total_loss: 0.641849527756373
        vf_explained_var: 0.9984607100486755
        vf_loss: 0.6516670932372411
    num_steps_sampled: 26533888
    num_steps_trained: 26533888
  iterations_since_restore: 164
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.28125
    gpu_util_percent0: 0.27875000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14678116846977068
    mean_env_wait_ms: 1.2188113643107263
    mean_inference_ms: 4.3108141495089125
    mean_raw_obs_processing_ms: 0.37956873975016553
  time_since_restore: 4329.973841905594
  time_this_iter_s: 26.496814012527466
  time_total_s: 4329.973841905594
  timers:
    learn_throughput: 8350.234
    learn_time_ms: 19375.745
    sample_throughput: 23626.98
    sample_time_ms: 6847.765
    update_time_ms: 25.491
  timestamp: 1602724935
  timesteps_since_restore: 0
  timesteps_total: 26533888
  training_iteration: 164
  trial_id: 9f737_00000
  
2020-10-15 01:22:15,970	WARNING util.py:136 -- The `process_trial` operation took 0.5076007843017578 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    164 |          4329.97 | 26533888 |  274.153 |              309.323 |              74.7778 |            793.981 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3142.9975811508943
    time_step_min: 2920
  date: 2020-10-15_01-22-42
  done: false
  episode_len_mean: 793.9244444444445
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 274.3077893024201
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 187
  episodes_total: 33525
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.0487097934144765e-30
        cur_lr: 5.0e-05
        entropy: 0.10565456313391526
        entropy_coeff: 0.0005000000000000001
        kl: 0.004053384026822944
        model: {}
        policy_loss: -0.009791523683816195
        total_loss: 0.8054841756820679
        vf_explained_var: 0.998250424861908
        vf_loss: 0.8153285433848699
    num_steps_sampled: 26695680
    num_steps_trained: 26695680
  iterations_since_restore: 165
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.17741935483871
    gpu_util_percent0: 0.30741935483870964
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467772070510529
    mean_env_wait_ms: 1.218773463573585
    mean_inference_ms: 4.31054515823255
    mean_raw_obs_processing_ms: 0.37955540885478534
  time_since_restore: 4356.53874540329
  time_this_iter_s: 26.564903497695923
  time_total_s: 4356.53874540329
  timers:
    learn_throughput: 8350.539
    learn_time_ms: 19375.036
    sample_throughput: 23628.56
    sample_time_ms: 6847.307
    update_time_ms: 25.234
  timestamp: 1602724962
  timesteps_since_restore: 0
  timesteps_total: 26695680
  training_iteration: 165
  trial_id: 9f737_00000
  
2020-10-15 01:22:43,198	WARNING util.py:136 -- The `process_trial` operation took 0.5034017562866211 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    165 |          4356.54 | 26695680 |  274.308 |              309.323 |              74.7778 |            793.924 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3141.728342721613
    time_step_min: 2920
  date: 2020-10-15_01-23-09
  done: false
  episode_len_mean: 793.8560471452263
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 274.50274152326386
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 243
  episodes_total: 33768
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5243548967072383e-30
        cur_lr: 5.0e-05
        entropy: 0.10574959715207417
        entropy_coeff: 0.0005000000000000001
        kl: 0.012938918002570668
        model: {}
        policy_loss: -0.007332586057600565
        total_loss: 0.8515675912300745
        vf_explained_var: 0.9982609152793884
        vf_loss: 0.8589530338843664
    num_steps_sampled: 26857472
    num_steps_trained: 26857472
  iterations_since_restore: 166
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03870967741936
    gpu_util_percent0: 0.3141935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14677071200203545
    mean_env_wait_ms: 1.2187207999900038
    mean_inference_ms: 4.3101819401853305
    mean_raw_obs_processing_ms: 0.3795353005337391
  time_since_restore: 4383.041789531708
  time_this_iter_s: 26.50304412841797
  time_total_s: 4383.041789531708
  timers:
    learn_throughput: 8340.124
    learn_time_ms: 19399.231
    sample_throughput: 23660.005
    sample_time_ms: 6838.207
    update_time_ms: 27.733
  timestamp: 1602724989
  timesteps_since_restore: 0
  timesteps_total: 26857472
  training_iteration: 166
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    166 |          4383.04 | 26857472 |  274.503 |              309.323 |              74.7778 |            793.856 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3140.6850498143017
    time_step_min: 2920
  date: 2020-10-15_01-23-36
  done: false
  episode_len_mean: 793.8061476857849
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 274.6568074455543
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 196
  episodes_total: 33964
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5243548967072383e-30
        cur_lr: 5.0e-05
        entropy: 0.10177156639595826
        entropy_coeff: 0.0005000000000000001
        kl: 0.004962601970570783
        model: {}
        policy_loss: -0.009282160327226544
        total_loss: 0.7751749058564504
        vf_explained_var: 0.9982352256774902
        vf_loss: 0.7845079600811005
    num_steps_sampled: 27019264
    num_steps_trained: 27019264
  iterations_since_restore: 167
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.670967741935485
    gpu_util_percent0: 0.30483870967741933
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676583717893693
    mean_env_wait_ms: 1.218679393416266
    mean_inference_ms: 4.3098910042081355
    mean_raw_obs_processing_ms: 0.3795200575258945
  time_since_restore: 4409.60968708992
  time_this_iter_s: 26.56789755821228
  time_total_s: 4409.60968708992
  timers:
    learn_throughput: 8342.837
    learn_time_ms: 19392.924
    sample_throughput: 23651.357
    sample_time_ms: 6840.707
    update_time_ms: 28.861
  timestamp: 1602725016
  timesteps_since_restore: 0
  timesteps_total: 27019264
  training_iteration: 167
  trial_id: 9f737_00000
  
2020-10-15 01:23:37,593	WARNING util.py:136 -- The `process_trial` operation took 0.5111241340637207 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    167 |          4409.61 | 27019264 |  274.657 |              309.323 |              74.7778 |            793.806 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3139.8023572873603
    time_step_min: 2920
  date: 2020-10-15_01-24-04
  done: false
  episode_len_mean: 793.7529067213355
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 274.79104117762773
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 181
  episodes_total: 34145
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2621774483536191e-30
        cur_lr: 5.0e-05
        entropy: 0.10426718307038148
        entropy_coeff: 0.0005000000000000001
        kl: 0.003633318624148766
        model: {}
        policy_loss: -0.011164189025294036
        total_loss: 0.9336773902177811
        vf_explained_var: 0.9978461861610413
        vf_loss: 0.9448936929305395
    num_steps_sampled: 27181056
    num_steps_trained: 27181056
  iterations_since_restore: 168
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.422580645161293
    gpu_util_percent0: 0.3461290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14676148453668078
    mean_env_wait_ms: 1.2186424888186858
    mean_inference_ms: 4.3096294686244265
    mean_raw_obs_processing_ms: 0.3795077499846614
  time_since_restore: 4436.369859933853
  time_this_iter_s: 26.760172843933105
  time_total_s: 4436.369859933853
  timers:
    learn_throughput: 8314.952
    learn_time_ms: 19457.96
    sample_throughput: 23652.556
    sample_time_ms: 6840.36
    update_time_ms: 29.134
  timestamp: 1602725044
  timesteps_since_restore: 0
  timesteps_total: 27181056
  training_iteration: 168
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    168 |          4436.37 | 27181056 |  274.791 |              309.323 |              74.7778 |            793.753 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3138.601490842384
    time_step_min: 2920
  date: 2020-10-15_01-24-31
  done: false
  episode_len_mean: 793.6856694104301
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 274.96802556262713
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 236
  episodes_total: 34381
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.310887241768096e-31
        cur_lr: 5.0e-05
        entropy: 0.1081255233536164
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010027524850253636
        total_loss: .inf
        vf_explained_var: 0.9978724122047424
        vf_loss: 1.1150205632050831
    num_steps_sampled: 27342848
    num_steps_trained: 27342848
  iterations_since_restore: 169
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9741935483871
    gpu_util_percent0: 0.3164516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675550448144598
    mean_env_wait_ms: 1.218589110907109
    mean_inference_ms: 4.309273257554661
    mean_raw_obs_processing_ms: 0.3794879671745992
  time_since_restore: 4462.824617862701
  time_this_iter_s: 26.454757928848267
  time_total_s: 4462.824617862701
  timers:
    learn_throughput: 8312.966
    learn_time_ms: 19462.608
    sample_throughput: 23704.542
    sample_time_ms: 6825.359
    update_time_ms: 27.982
  timestamp: 1602725071
  timesteps_since_restore: 0
  timesteps_total: 27342848
  training_iteration: 169
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    169 |          4462.82 | 27342848 |  274.968 |              309.323 |              74.7778 |            793.686 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3137.482186785518
    time_step_min: 2920
  date: 2020-10-15_01-24-58
  done: false
  episode_len_mean: 793.6254517070914
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 275.1326739103327
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 210
  episodes_total: 34591
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.466330862652141e-31
        cur_lr: 5.0e-05
        entropy: 0.09769078592459361
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034645257789331176
        model: {}
        policy_loss: -0.008024831775401253
        total_loss: 0.8350283404191335
        vf_explained_var: 0.9982044696807861
        vf_loss: 0.8431020130713781
    num_steps_sampled: 27504640
    num_steps_trained: 27504640
  iterations_since_restore: 170
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.567741935483877
    gpu_util_percent0: 0.37000000000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14675074272869887
    mean_env_wait_ms: 1.2185434646164042
    mean_inference_ms: 4.308991941896488
    mean_raw_obs_processing_ms: 0.3794727767509562
  time_since_restore: 4489.382410526276
  time_this_iter_s: 26.55779266357422
  time_total_s: 4489.382410526276
  timers:
    learn_throughput: 8302.505
    learn_time_ms: 19487.13
    sample_throughput: 23711.657
    sample_time_ms: 6823.31
    update_time_ms: 29.944
  timestamp: 1602725098
  timesteps_since_restore: 0
  timesteps_total: 27504640
  training_iteration: 170
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    170 |          4489.38 | 27504640 |  275.133 |              309.323 |              74.7778 |            793.625 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3136.542496832892
    time_step_min: 2920
  date: 2020-10-15_01-25-25
  done: false
  episode_len_mean: 793.5883807880357
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 275.27325018955725
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 34770
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.733165431326071e-31
        cur_lr: 5.0e-05
        entropy: 0.09657883209486802
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034435519288914898
        model: {}
        policy_loss: -0.006704525895960008
        total_loss: 0.5905111978451411
        vf_explained_var: 0.9985852837562561
        vf_loss: 0.5972640117009481
    num_steps_sampled: 27666432
    num_steps_trained: 27666432
  iterations_since_restore: 171
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.525806451612905
    gpu_util_percent0: 0.3312903225806451
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1467467476835038
    mean_env_wait_ms: 1.218507813911119
    mean_inference_ms: 4.308752909653304
    mean_raw_obs_processing_ms: 0.37946156278369025
  time_since_restore: 4515.956285238266
  time_this_iter_s: 26.573874711990356
  time_total_s: 4515.956285238266
  timers:
    learn_throughput: 8302.028
    learn_time_ms: 19488.249
    sample_throughput: 23724.836
    sample_time_ms: 6819.52
    update_time_ms: 32.016
  timestamp: 1602725125
  timesteps_since_restore: 0
  timesteps_total: 27666432
  training_iteration: 171
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    171 |          4515.96 | 27666432 |  275.273 |              309.323 |              74.7778 |            793.588 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3135.4041140960717
    time_step_min: 2920
  date: 2020-10-15_01-25-52
  done: false
  episode_len_mean: 793.5483981595268
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 275.44426517756796
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 221
  episodes_total: 34991
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3665827156630353e-31
        cur_lr: 5.0e-05
        entropy: 0.10819186021884282
        entropy_coeff: 0.0005000000000000001
        kl: 0.003542952317123612
        model: {}
        policy_loss: -0.008832593487265209
        total_loss: 0.8390327244997025
        vf_explained_var: 0.9982366561889648
        vf_loss: 0.8479194243748983
    num_steps_sampled: 27828224
    num_steps_trained: 27828224
  iterations_since_restore: 172
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.96451612903226
    gpu_util_percent0: 0.3083870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14674118267031572
    mean_env_wait_ms: 1.2184567774415216
    mean_inference_ms: 4.308425990071353
    mean_raw_obs_processing_ms: 0.379443939986644
  time_since_restore: 4542.396062374115
  time_this_iter_s: 26.439777135849
  time_total_s: 4542.396062374115
  timers:
    learn_throughput: 8295.236
    learn_time_ms: 19504.206
    sample_throughput: 23752.296
    sample_time_ms: 6811.636
    update_time_ms: 32.586
  timestamp: 1602725152
  timesteps_since_restore: 0
  timesteps_total: 27828224
  training_iteration: 172
  trial_id: 9f737_00000
  
2020-10-15 01:25:53,738	WARNING util.py:136 -- The `process_trial` operation took 0.5129764080047607 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    172 |           4542.4 | 27828224 |  275.444 |              309.323 |              74.7778 |            793.548 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3134.305302061123
    time_step_min: 2920
  date: 2020-10-15_01-26-20
  done: false
  episode_len_mean: 793.4957260102804
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 275.6134626014783
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 35213
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1832913578315177e-31
        cur_lr: 5.0e-05
        entropy: 0.09851784321169059
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035531720301757255
        model: {}
        policy_loss: -0.007761814903157453
        total_loss: 0.6587038338184357
        vf_explained_var: 0.9986014366149902
        vf_loss: 0.6665148884057999
    num_steps_sampled: 27990016
    num_steps_trained: 27990016
  iterations_since_restore: 173
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.525
    gpu_util_percent0: 0.36375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146736382533117
    mean_env_wait_ms: 1.2184084693305461
    mean_inference_ms: 4.308141700684022
    mean_raw_obs_processing_ms: 0.37942907299696504
  time_since_restore: 4568.918481826782
  time_this_iter_s: 26.522419452667236
  time_total_s: 4568.918481826782
  timers:
    learn_throughput: 8295.436
    learn_time_ms: 19503.737
    sample_throughput: 23770.995
    sample_time_ms: 6806.278
    update_time_ms: 32.809
  timestamp: 1602725180
  timesteps_since_restore: 0
  timesteps_total: 27990016
  training_iteration: 173
  trial_id: 9f737_00000
  
2020-10-15 01:26:21,123	WARNING util.py:136 -- The `process_trial` operation took 0.5254058837890625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    173 |          4568.92 | 27990016 |  275.613 |              309.323 |              74.7778 |            793.496 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3133.346361605249
    time_step_min: 2920
  date: 2020-10-15_01-26-47
  done: false
  episode_len_mean: 793.459502217702
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 275.7545554708026
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 35397
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.916456789157588e-32
        cur_lr: 5.0e-05
        entropy: 0.08810987944404285
        entropy_coeff: 0.0005000000000000001
        kl: 0.004200216324534267
        model: {}
        policy_loss: -0.008411155121090511
        total_loss: 0.5135133564472198
        vf_explained_var: 0.9987574219703674
        vf_loss: 0.5219685733318329
    num_steps_sampled: 28151808
    num_steps_trained: 28151808
  iterations_since_restore: 174
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.163333333333338
    gpu_util_percent0: 0.2903333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8733333333333344
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14673223001610924
    mean_env_wait_ms: 1.2183684086357498
    mean_inference_ms: 4.307896693147044
    mean_raw_obs_processing_ms: 0.3794167745600915
  time_since_restore: 4595.122229814529
  time_this_iter_s: 26.203747987747192
  time_total_s: 4595.122229814529
  timers:
    learn_throughput: 8309.809
    learn_time_ms: 19470.003
    sample_throughput: 23746.176
    sample_time_ms: 6813.392
    update_time_ms: 30.576
  timestamp: 1602725207
  timesteps_since_restore: 0
  timesteps_total: 28151808
  training_iteration: 174
  trial_id: 9f737_00000
  
2020-10-15 01:26:48,036	WARNING util.py:136 -- The `process_trial` operation took 0.5109333992004395 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    174 |          4595.12 | 28151808 |  275.755 |              309.323 |              74.7778 |             793.46 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3132.347658557165
    time_step_min: 2920
  date: 2020-10-15_01-27-14
  done: false
  episode_len_mean: 793.4295507543618
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 275.9035183685817
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 196
  episodes_total: 35593
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.958228394578794e-32
        cur_lr: 5.0e-05
        entropy: 0.0922918717066447
        entropy_coeff: 0.0005000000000000001
        kl: 0.003717923342871169
        model: {}
        policy_loss: -0.009112246644993624
        total_loss: 0.45639027406771976
        vf_explained_var: 0.9990200400352478
        vf_loss: 0.46554867178201675
    num_steps_sampled: 28313600
    num_steps_trained: 28313600
  iterations_since_restore: 175
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.029032258064515
    gpu_util_percent0: 0.29709677419354835
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672836166036707
    mean_env_wait_ms: 1.2183240499645023
    mean_inference_ms: 4.307642004364375
    mean_raw_obs_processing_ms: 0.37940325163090544
  time_since_restore: 4621.4979038238525
  time_this_iter_s: 26.37567400932312
  time_total_s: 4621.4979038238525
  timers:
    learn_throughput: 8315.4
    learn_time_ms: 19456.91
    sample_throughput: 23778.076
    sample_time_ms: 6804.251
    update_time_ms: 32.18
  timestamp: 1602725234
  timesteps_since_restore: 0
  timesteps_total: 28313600
  training_iteration: 175
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    175 |           4621.5 | 28313600 |  275.904 |              309.323 |              74.7778 |             793.43 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3131.1971720784663
    time_step_min: 2920
  date: 2020-10-15_01-27-41
  done: false
  episode_len_mean: 793.4001786511835
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 276.07894966863813
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 231
  episodes_total: 35824
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.479114197289397e-32
        cur_lr: 5.0e-05
        entropy: 0.09476385327676932
        entropy_coeff: 0.0005000000000000001
        kl: 0.008486109552904963
        model: {}
        policy_loss: -0.008750838145109205
        total_loss: 0.5136016309261322
        vf_explained_var: 0.9989460110664368
        vf_loss: 0.5223998501896858
    num_steps_sampled: 28475392
    num_steps_trained: 28475392
  iterations_since_restore: 176
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.206451612903226
    gpu_util_percent0: 0.29999999999999993
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14672295226993928
    mean_env_wait_ms: 1.2182704335625936
    mean_inference_ms: 4.307329809029079
    mean_raw_obs_processing_ms: 0.3793871786990025
  time_since_restore: 4648.0246658325195
  time_this_iter_s: 26.526762008666992
  time_total_s: 4648.0246658325195
  timers:
    learn_throughput: 8311.861
    learn_time_ms: 19465.196
    sample_throughput: 23790.416
    sample_time_ms: 6800.722
    update_time_ms: 29.748
  timestamp: 1602725261
  timesteps_since_restore: 0
  timesteps_total: 28475392
  training_iteration: 176
  trial_id: 9f737_00000
  
2020-10-15 01:27:42,357	WARNING util.py:136 -- The `process_trial` operation took 0.5089209079742432 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    176 |          4648.02 | 28475392 |  276.079 |              309.323 |              74.7778 |              793.4 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3130.2877549886052
    time_step_min: 2920
  date: 2020-10-15_01-28-08
  done: false
  episode_len_mean: 793.3675180455302
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 276.21330181324623
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 196
  episodes_total: 36020
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.479114197289397e-32
        cur_lr: 5.0e-05
        entropy: 0.10181967914104462
        entropy_coeff: 0.0005000000000000001
        kl: 0.004104337111736338
        model: {}
        policy_loss: -0.008113448925238723
        total_loss: 0.9092183113098145
        vf_explained_var: 0.9979715943336487
        vf_loss: 0.917382667462031
    num_steps_sampled: 28637184
    num_steps_trained: 28637184
  iterations_since_restore: 177
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.054838709677423
    gpu_util_percent0: 0.3629032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671851794241977
    mean_env_wait_ms: 1.2182244584271789
    mean_inference_ms: 4.307075485956113
    mean_raw_obs_processing_ms: 0.3793734433953733
  time_since_restore: 4674.181489706039
  time_this_iter_s: 26.156823873519897
  time_total_s: 4674.181489706039
  timers:
    learn_throughput: 8323.886
    learn_time_ms: 19437.076
    sample_throughput: 23831.923
    sample_time_ms: 6788.877
    update_time_ms: 27.849
  timestamp: 1602725288
  timesteps_since_restore: 0
  timesteps_total: 28637184
  training_iteration: 177
  trial_id: 9f737_00000
  
2020-10-15 01:28:09,213	WARNING util.py:136 -- The `process_trial` operation took 0.5259449481964111 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    177 |          4674.18 | 28637184 |  276.213 |              309.323 |              74.7778 |            793.368 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3129.494691439947
    time_step_min: 2920
  date: 2020-10-15_01-28-35
  done: false
  episode_len_mean: 793.3337015964205
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 276.3366940129907
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 36206
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.395570986446985e-33
        cur_lr: 5.0e-05
        entropy: 0.09765564588208993
        entropy_coeff: 0.0005000000000000001
        kl: 0.0043293673467511935
        model: {}
        policy_loss: -0.008388961966071898
        total_loss: 0.7777091811100642
        vf_explained_var: 0.9981903433799744
        vf_loss: 0.7861469437678655
    num_steps_sampled: 28798976
    num_steps_trained: 28798976
  iterations_since_restore: 178
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.348387096774193
    gpu_util_percent0: 0.30064516129032254
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14671472505268857
    mean_env_wait_ms: 1.2181835520248945
    mean_inference_ms: 4.306833700732026
    mean_raw_obs_processing_ms: 0.3793621383462722
  time_since_restore: 4700.649770021439
  time_this_iter_s: 26.46828031539917
  time_total_s: 4700.649770021439
  timers:
    learn_throughput: 8333.893
    learn_time_ms: 19413.735
    sample_throughput: 23855.3
    sample_time_ms: 6782.225
    update_time_ms: 27.756
  timestamp: 1602725315
  timesteps_since_restore: 0
  timesteps_total: 28798976
  training_iteration: 178
  trial_id: 9f737_00000
  
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    178 |          4700.65 | 28798976 |  276.337 |              309.323 |              74.7778 |            793.334 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3128.444969503819
    time_step_min: 2920
  date: 2020-10-15_01-29-02
  done: false
  episode_len_mean: 793.3059885827204
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 276.49473187246264
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 230
  episodes_total: 36436
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.697785493223493e-33
        cur_lr: 5.0e-05
        entropy: 0.09435639530420303
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032874048532297215
        model: {}
        policy_loss: -0.009000834338318478
        total_loss: 0.783946638305982
        vf_explained_var: 0.998416006565094
        vf_loss: 0.7929946333169937
    num_steps_sampled: 28960768
    num_steps_trained: 28960768
  iterations_since_restore: 179
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.670967741935485
    gpu_util_percent0: 0.28548387096774197
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670939856645188
    mean_env_wait_ms: 1.2181282013859136
    mean_inference_ms: 4.306529752213918
    mean_raw_obs_processing_ms: 0.3793453483315165
  time_since_restore: 4727.2004137039185
  time_this_iter_s: 26.55064368247986
  time_total_s: 4727.2004137039185
  timers:
    learn_throughput: 8331.33
    learn_time_ms: 19419.708
    sample_throughput: 23843.606
    sample_time_ms: 6785.551
    update_time_ms: 27.897
  timestamp: 1602725342
  timesteps_since_restore: 0
  timesteps_total: 28960768
  training_iteration: 179
  trial_id: 9f737_00000
  
2020-10-15 01:29:03,790	WARNING util.py:136 -- The `process_trial` operation took 0.55059814453125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    179 |           4727.2 | 28960768 |  276.495 |              309.323 |              74.7778 |            793.306 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3127.4600885100804
    time_step_min: 2920
  date: 2020-10-15_01-29-29
  done: false
  episode_len_mean: 793.274697085471
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 276.64055741345334
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 36644
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8488927466117464e-33
        cur_lr: 5.0e-05
        entropy: 0.08865440885225932
        entropy_coeff: 0.0005000000000000001
        kl: 0.003634034190326929
        model: {}
        policy_loss: -0.009128385310759768
        total_loss: 0.5338593646883965
        vf_explained_var: 0.9988309741020203
        vf_loss: 0.5430320749680201
    num_steps_sampled: 29122560
    num_steps_trained: 29122560
  iterations_since_restore: 180
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.961290322580645
    gpu_util_percent0: 0.3129032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670519509150617
    mean_env_wait_ms: 1.2180783341212909
    mean_inference_ms: 4.306278113694325
    mean_raw_obs_processing_ms: 0.37933176275928876
  time_since_restore: 4753.353962182999
  time_this_iter_s: 26.1535484790802
  time_total_s: 4753.353962182999
  timers:
    learn_throughput: 8344.425
    learn_time_ms: 19389.233
    sample_throughput: 23872.239
    sample_time_ms: 6777.412
    update_time_ms: 25.901
  timestamp: 1602725369
  timesteps_since_restore: 0
  timesteps_total: 29122560
  training_iteration: 180
  trial_id: 9f737_00000
  
2020-10-15 01:29:30,638	WARNING util.py:136 -- The `process_trial` operation took 0.5231916904449463 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    180 |          4753.35 | 29122560 |  276.641 |              309.323 |              74.7778 |            793.275 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3126.660667681601
    time_step_min: 2920
  date: 2020-10-15_01-29-57
  done: false
  episode_len_mean: 793.2481668567704
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 276.7624279292847
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 178
  episodes_total: 36822
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.244463733058732e-34
        cur_lr: 5.0e-05
        entropy: 0.08436796069145203
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008803687417336429
        total_loss: .inf
        vf_explained_var: 0.9988777041435242
        vf_loss: 0.488923154771328
    num_steps_sampled: 29284352
    num_steps_trained: 29284352
  iterations_since_restore: 181
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.464516129032262
    gpu_util_percent0: 0.3441935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14670137302159786
    mean_env_wait_ms: 1.2180380891117868
    mean_inference_ms: 4.306061235632348
    mean_raw_obs_processing_ms: 0.37932174654157735
  time_since_restore: 4779.873112201691
  time_this_iter_s: 26.519150018692017
  time_total_s: 4779.873112201691
  timers:
    learn_throughput: 8343.511
    learn_time_ms: 19391.358
    sample_throughput: 23900.841
    sample_time_ms: 6769.302
    update_time_ms: 25.764
  timestamp: 1602725397
  timesteps_since_restore: 0
  timesteps_total: 29284352
  training_iteration: 181
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:29:57,859	WARNING util.py:136 -- The `process_trial` operation took 0.5218920707702637 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    181 |          4779.87 | 29284352 |  276.762 |              309.323 |              74.7778 |            793.248 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3125.638879130975
    time_step_min: 2920
  date: 2020-10-15_01-30-24
  done: false
  episode_len_mean: 793.2170333378324
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 276.9191292054026
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 37045
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3866695599588102e-33
        cur_lr: 5.0e-05
        entropy: 0.0883502767731746
        entropy_coeff: 0.0005000000000000001
        kl: 0.003522960818372667
        model: {}
        policy_loss: -0.006211847270606086
        total_loss: 0.531601756811142
        vf_explained_var: 0.9990924000740051
        vf_loss: 0.5378577609856924
    num_steps_sampled: 29446144
    num_steps_trained: 29446144
  iterations_since_restore: 182
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.478125
    gpu_util_percent0: 0.3525
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14669697581213725
    mean_env_wait_ms: 1.2179841430284744
    mean_inference_ms: 4.305769510634813
    mean_raw_obs_processing_ms: 0.3793060859387335
  time_since_restore: 4806.641075134277
  time_this_iter_s: 26.76796293258667
  time_total_s: 4806.641075134277
  timers:
    learn_throughput: 8338.373
    learn_time_ms: 19403.305
    sample_throughput: 23860.974
    sample_time_ms: 6780.612
    update_time_ms: 25.338
  timestamp: 1602725424
  timesteps_since_restore: 0
  timesteps_total: 29446144
  training_iteration: 182
  trial_id: 9f737_00000
  
2020-10-15 01:30:25,345	WARNING util.py:136 -- The `process_trial` operation took 0.5298721790313721 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    182 |          4806.64 | 29446144 |  276.919 |              309.323 |              74.7778 |            793.217 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3124.6059368703827
    time_step_min: 2920
  date: 2020-10-15_01-30-51
  done: false
  episode_len_mean: 793.1819499235166
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 277.0710266663088
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 37263
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.933347799794051e-34
        cur_lr: 5.0e-05
        entropy: 0.08796812159319718
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010682908119633794
        total_loss: .inf
        vf_explained_var: 0.9990804195404053
        vf_loss: 0.4426562860608101
    num_steps_sampled: 29607936
    num_steps_trained: 29607936
  iterations_since_restore: 183
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.358064516129037
    gpu_util_percent0: 0.34548387096774197
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466926365259552
    mean_env_wait_ms: 1.2179303884909807
    mean_inference_ms: 4.305522763494186
    mean_raw_obs_processing_ms: 0.37929295703608396
  time_since_restore: 4832.960638999939
  time_this_iter_s: 26.31956386566162
  time_total_s: 4832.960638999939
  timers:
    learn_throughput: 8349.073
    learn_time_ms: 19378.439
    sample_throughput: 23854.802
    sample_time_ms: 6782.366
    update_time_ms: 27.535
  timestamp: 1602725451
  timesteps_since_restore: 0
  timesteps_total: 29607936
  training_iteration: 183
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:30:52,517	WARNING util.py:136 -- The `process_trial` operation took 0.5065262317657471 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    183 |          4832.96 | 29607936 |  277.071 |              309.323 |              74.7778 |            793.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3123.717363412809
    time_step_min: 2920
  date: 2020-10-15_01-31-18
  done: false
  episode_len_mean: 793.1577837116155
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 277.2008752410621
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 187
  episodes_total: 37450
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0400021699691073e-33
        cur_lr: 5.0e-05
        entropy: 0.08323327576120694
        entropy_coeff: 0.0005000000000000001
        kl: 0.003009172137050579
        model: {}
        policy_loss: -0.009604743230738677
        total_loss: 0.31412489960591
        vf_explained_var: 0.9992542862892151
        vf_loss: 0.32377125322818756
    num_steps_sampled: 29769728
    num_steps_trained: 29769728
  iterations_since_restore: 184
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61612903225807
    gpu_util_percent0: 0.28645161290322585
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668851652438772
    mean_env_wait_ms: 1.2178840878027453
    mean_inference_ms: 4.305291846201783
    mean_raw_obs_processing_ms: 0.37928126872080203
  time_since_restore: 4859.2334089279175
  time_this_iter_s: 26.272769927978516
  time_total_s: 4859.2334089279175
  timers:
    learn_throughput: 8344.319
    learn_time_ms: 19389.48
    sample_throughput: 23878.952
    sample_time_ms: 6775.507
    update_time_ms: 29.755
  timestamp: 1602725478
  timesteps_since_restore: 0
  timesteps_total: 29769728
  training_iteration: 184
  trial_id: 9f737_00000
  
2020-10-15 01:31:19,680	WARNING util.py:136 -- The `process_trial` operation took 0.5179159641265869 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    184 |          4859.23 | 29769728 |  277.201 |              309.323 |              74.7778 |            793.158 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3122.815075777719
    time_step_min: 2920
  date: 2020-10-15_01-31-46
  done: false
  episode_len_mean: 793.1303654908627
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 277.3353324468655
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 198
  episodes_total: 37648
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.200010849845536e-34
        cur_lr: 5.0e-05
        entropy: 0.08848976778487365
        entropy_coeff: 0.0005000000000000001
        kl: 0.004977944850300749
        model: {}
        policy_loss: -0.0069766443921253085
        total_loss: 0.509889043867588
        vf_explained_var: 0.9989362359046936
        vf_loss: 0.5169099519650141
    num_steps_sampled: 29931520
    num_steps_trained: 29931520
  iterations_since_restore: 185
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.67419354838709
    gpu_util_percent0: 0.3403225806451612
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146685132290037
    mean_env_wait_ms: 1.217837772296297
    mean_inference_ms: 4.3050650325595825
    mean_raw_obs_processing_ms: 0.3792696718263544
  time_since_restore: 4885.7994956970215
  time_this_iter_s: 26.566086769104004
  time_total_s: 4885.7994956970215
  timers:
    learn_throughput: 8337.39
    learn_time_ms: 19405.594
    sample_throughput: 23895.524
    sample_time_ms: 6770.808
    update_time_ms: 36.458
  timestamp: 1602725506
  timesteps_since_restore: 0
  timesteps_total: 29931520
  training_iteration: 185
  trial_id: 9f737_00000
  
2020-10-15 01:31:47,007	WARNING util.py:136 -- The `process_trial` operation took 0.5362260341644287 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    185 |           4885.8 | 29931520 |  277.335 |              309.323 |              74.7778 |             793.13 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3121.818662297508
    time_step_min: 2920
  date: 2020-10-15_01-32-13
  done: false
  episode_len_mean: 793.0879115076956
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 277.4853626153027
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 231
  episodes_total: 37879
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.600005424922768e-34
        cur_lr: 5.0e-05
        entropy: 0.0913059829423825
        entropy_coeff: 0.0005000000000000001
        kl: 0.006766024433697264
        model: {}
        policy_loss: -0.01106312950529779
        total_loss: 0.5295437152187029
        vf_explained_var: 0.9989171028137207
        vf_loss: 0.5406525234381357
    num_steps_sampled: 30093312
    num_steps_trained: 30093312
  iterations_since_restore: 186
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.335483870967742
    gpu_util_percent0: 0.2661290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14668030175751562
    mean_env_wait_ms: 1.217777581026639
    mean_inference_ms: 4.3047882241170745
    mean_raw_obs_processing_ms: 0.37925517877903514
  time_since_restore: 4912.203943729401
  time_this_iter_s: 26.40444803237915
  time_total_s: 4912.203943729401
  timers:
    learn_throughput: 8341.441
    learn_time_ms: 19396.169
    sample_throughput: 23908.716
    sample_time_ms: 6767.072
    update_time_ms: 36.492
  timestamp: 1602725533
  timesteps_since_restore: 0
  timesteps_total: 30093312
  training_iteration: 186
  trial_id: 9f737_00000
  
2020-10-15 01:32:14,310	WARNING util.py:136 -- The `process_trial` operation took 0.5505239963531494 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    186 |           4912.2 | 30093312 |  277.485 |              309.323 |              74.7778 |            793.088 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3120.98517077275
    time_step_min: 2920
  date: 2020-10-15_01-32-40
  done: false
  episode_len_mean: 793.0629350424207
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 277.6125171761745
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 38071
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.600005424922768e-34
        cur_lr: 5.0e-05
        entropy: 0.08279486186802387
        entropy_coeff: 0.0005000000000000001
        kl: 0.004141854549137254
        model: {}
        policy_loss: -0.009294402101659216
        total_loss: 0.5253696590662003
        vf_explained_var: 0.99880051612854
        vf_loss: 0.5347054799397787
    num_steps_sampled: 30255104
    num_steps_trained: 30255104
  iterations_since_restore: 187
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.806666666666665
    gpu_util_percent0: 0.3033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.876666666666668
    vram_util_percent0: 0.10437848474909811
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667635499890047
    mean_env_wait_ms: 1.2177280998381987
    mean_inference_ms: 4.304562106426944
    mean_raw_obs_processing_ms: 0.3792426267104582
  time_since_restore: 4938.3734521865845
  time_this_iter_s: 26.169508457183838
  time_total_s: 4938.3734521865845
  timers:
    learn_throughput: 8340.526
    learn_time_ms: 19398.296
    sample_throughput: 23915.409
    sample_time_ms: 6765.178
    update_time_ms: 36.875
  timestamp: 1602725560
  timesteps_since_restore: 0
  timesteps_total: 30255104
  training_iteration: 187
  trial_id: 9f737_00000
  
2020-10-15 01:32:41,182	WARNING util.py:136 -- The `process_trial` operation took 0.5214993953704834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    187 |          4938.37 | 30255104 |  277.613 |              309.323 |              74.7778 |            793.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3120.2058923572044
    time_step_min: 2920
  date: 2020-10-15_01-33-07
  done: false
  episode_len_mean: 793.0398619860417
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 277.7329747272763
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 38257
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.300002712461384e-34
        cur_lr: 5.0e-05
        entropy: 0.0836948590974013
        entropy_coeff: 0.0005000000000000001
        kl: 0.004098805657122284
        model: {}
        policy_loss: -0.007735700850995879
        total_loss: 0.4161953777074814
        vf_explained_var: 0.9990634918212891
        vf_loss: 0.4239729270339012
    num_steps_sampled: 30416896
    num_steps_trained: 30416896
  iterations_since_restore: 188
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.406451612903222
    gpu_util_percent0: 0.3216129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14667278544852022
    mean_env_wait_ms: 1.2176826544348143
    mean_inference_ms: 4.30434263660056
    mean_raw_obs_processing_ms: 0.3792324984044435
  time_since_restore: 4964.841150045395
  time_this_iter_s: 26.467697858810425
  time_total_s: 4964.841150045395
  timers:
    learn_throughput: 8345.156
    learn_time_ms: 19387.533
    sample_throughput: 23885.219
    sample_time_ms: 6773.729
    update_time_ms: 38.09
  timestamp: 1602725587
  timesteps_since_restore: 0
  timesteps_total: 30416896
  training_iteration: 188
  trial_id: 9f737_00000
  
2020-10-15 01:33:08,409	WARNING util.py:136 -- The `process_trial` operation took 0.581228494644165 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    188 |          4964.84 | 30416896 |  277.733 |              309.323 |              74.7778 |             793.04 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3119.2149637219463
    time_step_min: 2920
  date: 2020-10-15_01-33-34
  done: false
  episode_len_mean: 793.0123925073394
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 277.8818367352829
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 234
  episodes_total: 38491
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.50001356230692e-35
        cur_lr: 5.0e-05
        entropy: 0.08863753080368042
        entropy_coeff: 0.0005000000000000001
        kl: 0.00427564939794441
        model: {}
        policy_loss: -0.008368352204949284
        total_loss: 0.5878398269414902
        vf_explained_var: 0.998814046382904
        vf_loss: 0.5962525059779485
    num_steps_sampled: 30578688
    num_steps_trained: 30578688
  iterations_since_restore: 189
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.332258064516136
    gpu_util_percent0: 0.3835483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666826351403697
    mean_env_wait_ms: 1.2176216524620804
    mean_inference_ms: 4.304068499462399
    mean_raw_obs_processing_ms: 0.3792174166143371
  time_since_restore: 4991.106750965118
  time_this_iter_s: 26.26560091972351
  time_total_s: 4991.106750965118
  timers:
    learn_throughput: 8361.121
    learn_time_ms: 19350.515
    sample_throughput: 23864.767
    sample_time_ms: 6779.534
    update_time_ms: 39.429
  timestamp: 1602725614
  timesteps_since_restore: 0
  timesteps_total: 30578688
  training_iteration: 189
  trial_id: 9f737_00000
  
2020-10-15 01:33:35,417	WARNING util.py:136 -- The `process_trial` operation took 0.5561082363128662 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    189 |          4991.11 | 30578688 |  277.882 |              309.323 |              74.7778 |            793.012 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3118.398277244626
    time_step_min: 2920
  date: 2020-10-15_01-34-01
  done: false
  episode_len_mean: 792.9899216993565
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.0075481538385
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 206
  episodes_total: 38697
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.25000678115346e-35
        cur_lr: 5.0e-05
        entropy: 0.0850138080616792
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035447473055683076
        model: {}
        policy_loss: -0.006158011519194891
        total_loss: 0.7149313390254974
        vf_explained_var: 0.9984879493713379
        vf_loss: 0.7211318711439768
    num_steps_sampled: 30740480
    num_steps_trained: 30740480
  iterations_since_restore: 190
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.55806451612903
    gpu_util_percent0: 0.31225806451612903
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666447218580797
    mean_env_wait_ms: 1.2175674796572842
    mean_inference_ms: 4.303847554296983
    mean_raw_obs_processing_ms: 0.379204810783381
  time_since_restore: 5017.5192902088165
  time_this_iter_s: 26.41253924369812
  time_total_s: 5017.5192902088165
  timers:
    learn_throughput: 8357.835
    learn_time_ms: 19358.124
    sample_throughput: 23810.389
    sample_time_ms: 6795.017
    update_time_ms: 41.114
  timestamp: 1602725641
  timesteps_since_restore: 0
  timesteps_total: 30740480
  training_iteration: 190
  trial_id: 9f737_00000
  
2020-10-15 01:34:02,544	WARNING util.py:136 -- The `process_trial` operation took 0.5229084491729736 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    190 |          5017.52 | 30740480 |  278.008 |              309.323 |              74.7778 |             792.99 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3117.6590458536084
    time_step_min: 2920
  date: 2020-10-15_01-34-28
  done: false
  episode_len_mean: 792.9756680984593
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.118309045339
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 38879
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.62500339057673e-35
        cur_lr: 5.0e-05
        entropy: 0.08337158337235451
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037976958168049655
        model: {}
        policy_loss: -0.009465747680224013
        total_loss: 0.7788745711247126
        vf_explained_var: 0.9982061386108398
        vf_loss: 0.7883819987376531
    num_steps_sampled: 30902272
    num_steps_trained: 30902272
  iterations_since_restore: 191
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.587096774193554
    gpu_util_percent0: 0.3361290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14666082990491625
    mean_env_wait_ms: 1.2175214546475768
    mean_inference_ms: 4.303642650011021
    mean_raw_obs_processing_ms: 0.3791955109158276
  time_since_restore: 5043.947866201401
  time_this_iter_s: 26.42857599258423
  time_total_s: 5043.947866201401
  timers:
    learn_throughput: 8364.943
    learn_time_ms: 19341.675
    sample_throughput: 23782.438
    sample_time_ms: 6803.003
    update_time_ms: 39.935
  timestamp: 1602725668
  timesteps_since_restore: 0
  timesteps_total: 30902272
  training_iteration: 191
  trial_id: 9f737_00000
  
2020-10-15 01:34:29,687	WARNING util.py:136 -- The `process_trial` operation took 0.5333566665649414 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    191 |          5043.95 | 30902272 |  278.118 |              309.323 |              74.7778 |            792.976 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3116.751388853332
    time_step_min: 2920
  date: 2020-10-15_01-34-56
  done: false
  episode_len_mean: 792.9614056625489
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.2546356684313
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 220
  episodes_total: 39099
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.12501695288365e-36
        cur_lr: 5.0e-05
        entropy: 0.08762575810154279
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007857469997058312
        total_loss: .inf
        vf_explained_var: 0.9989132881164551
        vf_loss: 0.5447061558564504
    num_steps_sampled: 31064064
    num_steps_trained: 31064064
  iterations_since_restore: 192
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.625806451612902
    gpu_util_percent0: 0.2938709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14665700164352932
    mean_env_wait_ms: 1.2174640375973753
    mean_inference_ms: 4.303384241127715
    mean_raw_obs_processing_ms: 0.3791812310740873
  time_since_restore: 5070.6394720077515
  time_this_iter_s: 26.691605806350708
  time_total_s: 5070.6394720077515
  timers:
    learn_throughput: 8368.65
    learn_time_ms: 19333.106
    sample_throughput: 23759.885
    sample_time_ms: 6809.461
    update_time_ms: 41.843
  timestamp: 1602725696
  timesteps_since_restore: 0
  timesteps_total: 31064064
  training_iteration: 192
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:34:57,127	WARNING util.py:136 -- The `process_trial` operation took 0.5614879131317139 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    192 |          5070.64 | 31064064 |  278.255 |              309.323 |              74.7778 |            792.961 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3115.8618345680898
    time_step_min: 2920
  date: 2020-10-15_01-35-23
  done: false
  episode_len_mean: 792.9477070987105
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.3892081020804
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 39317
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2187525429325476e-35
        cur_lr: 5.0e-05
        entropy: 0.0812802364428838
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033581307895171144
        model: {}
        policy_loss: -0.007272048465286692
        total_loss: 0.5096313382188479
        vf_explained_var: 0.9989507794380188
        vf_loss: 0.5169440334041914
    num_steps_sampled: 31225856
    num_steps_trained: 31225856
  iterations_since_restore: 193
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.49032258064516
    gpu_util_percent0: 0.33
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466530578797241
    mean_env_wait_ms: 1.2174052365437704
    mean_inference_ms: 4.3031619899334475
    mean_raw_obs_processing_ms: 0.3791691098103516
  time_since_restore: 5097.087938785553
  time_this_iter_s: 26.448466777801514
  time_total_s: 5097.087938785553
  timers:
    learn_throughput: 8365.657
    learn_time_ms: 19340.022
    sample_throughput: 23740.872
    sample_time_ms: 6814.914
    update_time_ms: 41.249
  timestamp: 1602725723
  timesteps_since_restore: 0
  timesteps_total: 31225856
  training_iteration: 193
  trial_id: 9f737_00000
  
2020-10-15 01:35:24,330	WARNING util.py:136 -- The `process_trial` operation took 0.5540521144866943 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    193 |          5097.09 | 31225856 |  278.389 |              309.323 |              74.7778 |            792.948 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3115.135957020856
    time_step_min: 2920
  date: 2020-10-15_01-35-50
  done: false
  episode_len_mean: 792.9416187751589
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.4995751075145
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 39499
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.093762714662738e-36
        cur_lr: 5.0e-05
        entropy: 0.07697107767065366
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006405234838894103
        total_loss: .inf
        vf_explained_var: 0.9988000392913818
        vf_loss: 0.5306244368354479
    num_steps_sampled: 31387648
    num_steps_trained: 31387648
  iterations_since_restore: 194
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.68064516129032
    gpu_util_percent0: 0.32193548387096776
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664927111523313
    mean_env_wait_ms: 1.2173573487135503
    mean_inference_ms: 4.302962587349614
    mean_raw_obs_processing_ms: 0.3791585017885843
  time_since_restore: 5123.653546571732
  time_this_iter_s: 26.56560778617859
  time_total_s: 5123.653546571732
  timers:
    learn_throughput: 8353.446
    learn_time_ms: 19368.295
    sample_throughput: 23743.895
    sample_time_ms: 6814.046
    update_time_ms: 40.882
  timestamp: 1602725750
  timesteps_since_restore: 0
  timesteps_total: 31387648
  training_iteration: 194
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:35:51,719	WARNING util.py:136 -- The `process_trial` operation took 0.5764310359954834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    194 |          5123.65 | 31387648 |    278.5 |              309.323 |              74.7778 |            792.942 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3114.3211619931412
    time_step_min: 2920
  date: 2020-10-15_01-36-18
  done: false
  episode_len_mean: 792.9416284577014
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.62226767091477
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 39694
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.14064407199411e-36
        cur_lr: 5.0e-05
        entropy: 0.07760118755201499
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007467014448290381
        total_loss: .inf
        vf_explained_var: 0.9994246959686279
        vf_loss: 0.2734355032444
    num_steps_sampled: 31549440
    num_steps_trained: 31549440
  iterations_since_restore: 195
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.412903225806453
    gpu_util_percent0: 0.30677419354838714
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14664618741424623
    mean_env_wait_ms: 1.2173070285414618
    mean_inference_ms: 4.302759484173049
    mean_raw_obs_processing_ms: 0.37914866484809623
  time_since_restore: 5150.154786348343
  time_this_iter_s: 26.501239776611328
  time_total_s: 5150.154786348343
  timers:
    learn_throughput: 8361.965
    learn_time_ms: 19348.563
    sample_throughput: 23671.781
    sample_time_ms: 6834.805
    update_time_ms: 33.7
  timestamp: 1602725778
  timesteps_since_restore: 0
  timesteps_total: 31549440
  training_iteration: 195
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:36:19,029	WARNING util.py:136 -- The `process_trial` operation took 0.5748987197875977 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    195 |          5150.15 | 31549440 |  278.622 |              309.323 |              74.7778 |            792.942 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3113.382299109941
    time_step_min: 2920
  date: 2020-10-15_01-36-45
  done: false
  episode_len_mean: 792.9430654008967
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.76472917436763
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 229
  episodes_total: 39923
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.371096610799116e-35
        cur_lr: 5.0e-05
        entropy: 0.07868409094711144
        entropy_coeff: 0.0005000000000000001
        kl: 0.004662000923417509
        model: {}
        policy_loss: -0.008344404176265622
        total_loss: 0.2915095128118992
        vf_explained_var: 0.9994255900382996
        vf_loss: 0.2998932624856631
    num_steps_sampled: 31711232
    num_steps_trained: 31711232
  iterations_since_restore: 196
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.429032258064513
    gpu_util_percent0: 0.2964516129032258
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466417219635496
    mean_env_wait_ms: 1.2172426957037557
    mean_inference_ms: 4.302505852873608
    mean_raw_obs_processing_ms: 0.3791352214755032
  time_since_restore: 5176.50620675087
  time_this_iter_s: 26.351420402526855
  time_total_s: 5176.50620675087
  timers:
    learn_throughput: 8363.414
    learn_time_ms: 19345.211
    sample_throughput: 23680.683
    sample_time_ms: 6832.235
    update_time_ms: 33.616
  timestamp: 1602725805
  timesteps_since_restore: 0
  timesteps_total: 31711232
  training_iteration: 196
  trial_id: 9f737_00000
  
2020-10-15 01:36:46,146	WARNING util.py:136 -- The `process_trial` operation took 0.5748677253723145 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    196 |          5176.51 | 31711232 |  278.765 |              309.323 |              74.7778 |            792.943 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3112.5626902160357
    time_step_min: 2920
  date: 2020-10-15_01-37-12
  done: false
  episode_len_mean: 792.9468397966305
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.88727319048314
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 40124
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.85548305399558e-36
        cur_lr: 5.0e-05
        entropy: 0.07638297602534294
        entropy_coeff: 0.0005000000000000001
        kl: 0.009828847677757343
        model: {}
        policy_loss: -0.00773686938919127
        total_loss: 0.3562139918406804
        vf_explained_var: 0.9992380738258362
        vf_loss: 0.3639890526731809
    num_steps_sampled: 31873024
    num_steps_trained: 31873024
  iterations_since_restore: 197
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.29032258064516
    gpu_util_percent0: 0.35064516129032264
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146638211640219
    mean_env_wait_ms: 1.217187101185628
    mean_inference_ms: 4.302301118807502
    mean_raw_obs_processing_ms: 0.3791237693601277
  time_since_restore: 5203.164620876312
  time_this_iter_s: 26.658414125442505
  time_total_s: 5203.164620876312
  timers:
    learn_throughput: 8349.07
    learn_time_ms: 19378.445
    sample_throughput: 23632.422
    sample_time_ms: 6846.188
    update_time_ms: 33.432
  timestamp: 1602725832
  timesteps_since_restore: 0
  timesteps_total: 31873024
  training_iteration: 197
  trial_id: 9f737_00000
  
2020-10-15 01:37:13,580	WARNING util.py:136 -- The `process_trial` operation took 0.576556921005249 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    197 |          5203.16 | 31873024 |  278.887 |              309.323 |              74.7778 |            792.947 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3111.868198187011
    time_step_min: 2920
  date: 2020-10-15_01-37-39
  done: false
  episode_len_mean: 792.9399796541201
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 278.99030726088256
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 40303
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.85548305399558e-36
        cur_lr: 5.0e-05
        entropy: 0.07605149348576863
        entropy_coeff: 0.0005000000000000001
        kl: 0.0050777323776856065
        model: {}
        policy_loss: -0.009927341248840094
        total_loss: 0.5209870611627897
        vf_explained_var: 0.9988350868225098
        vf_loss: 0.5309524511297544
    num_steps_sampled: 32034816
    num_steps_trained: 32034816
  iterations_since_restore: 198
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.464516129032262
    gpu_util_percent0: 0.3616129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663507420950142
    mean_env_wait_ms: 1.2171394886701272
    mean_inference_ms: 4.302118427039978
    mean_raw_obs_processing_ms: 0.3791155622384012
  time_since_restore: 5229.342242717743
  time_this_iter_s: 26.177621841430664
  time_total_s: 5229.342242717743
  timers:
    learn_throughput: 8357.117
    learn_time_ms: 19359.786
    sample_throughput: 23664.248
    sample_time_ms: 6836.98
    update_time_ms: 31.472
  timestamp: 1602725859
  timesteps_since_restore: 0
  timesteps_total: 32034816
  training_iteration: 198
  trial_id: 9f737_00000
  
2020-10-15 01:37:40,496	WARNING util.py:136 -- The `process_trial` operation took 0.549544095993042 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    198 |          5229.34 | 32034816 |   278.99 |              309.323 |              74.7778 |             792.94 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3111.0219994568033
    time_step_min: 2920
  date: 2020-10-15_01-38-06
  done: false
  episode_len_mean: 792.9230617430129
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.11468641868987
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 236
  episodes_total: 40539
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.85548305399558e-36
        cur_lr: 5.0e-05
        entropy: 0.09725277498364449
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009696828133504217
        total_loss: .inf
        vf_explained_var: 0.9974213242530823
        vf_loss: 1.3000514805316925
    num_steps_sampled: 32196608
    num_steps_trained: 32196608
  iterations_since_restore: 199
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.216129032258067
    gpu_util_percent0: 0.3022580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14663122468491863
    mean_env_wait_ms: 1.217074903445569
    mean_inference_ms: 4.301865076260689
    mean_raw_obs_processing_ms: 0.37910166830047176
  time_since_restore: 5255.533286571503
  time_this_iter_s: 26.191043853759766
  time_total_s: 5255.533286571503
  timers:
    learn_throughput: 8360.812
    learn_time_ms: 19351.231
    sample_throughput: 23655.518
    sample_time_ms: 6839.504
    update_time_ms: 29.484
  timestamp: 1602725886
  timesteps_since_restore: 0
  timesteps_total: 32196608
  training_iteration: 199
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:38:07,632	WARNING util.py:136 -- The `process_trial` operation took 0.5920548439025879 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    199 |          5255.53 | 32196608 |  279.115 |              309.323 |              74.7778 |            792.923 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3110.4361979166665
    time_step_min: 2920
  date: 2020-10-15_01-38-33
  done: false
  episode_len_mean: 792.8908988267635
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.2063028795638
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 203
  episodes_total: 40742
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0283224580993369e-35
        cur_lr: 5.0e-05
        entropy: 0.08863722843428452
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009318512746176566
        total_loss: .inf
        vf_explained_var: 0.9973430037498474
        vf_loss: 1.3116401731967926
    num_steps_sampled: 32358400
    num_steps_trained: 32358400
  iterations_since_restore: 200
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.01612903225807
    gpu_util_percent0: 0.2906451612903226
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662761457547308
    mean_env_wait_ms: 1.217016855010862
    mean_inference_ms: 4.3016660586819535
    mean_raw_obs_processing_ms: 0.37909054176981716
  time_since_restore: 5281.699223041534
  time_this_iter_s: 26.16593647003174
  time_total_s: 5281.699223041534
  timers:
    learn_throughput: 8369.881
    learn_time_ms: 19330.264
    sample_throughput: 23663.163
    sample_time_ms: 6837.294
    update_time_ms: 27.645
  timestamp: 1602725913
  timesteps_since_restore: 0
  timesteps_total: 32358400
  training_iteration: 200
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:38:34,738	WARNING util.py:136 -- The `process_trial` operation took 0.5953974723815918 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    200 |           5281.7 | 32358400 |  279.206 |              309.323 |              74.7778 |            792.891 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3109.780400117394
    time_step_min: 2920
  date: 2020-10-15_01-39-01
  done: false
  episode_len_mean: 792.897131407907
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.3077905576805
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 40926
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.542483687149006e-35
        cur_lr: 5.0e-05
        entropy: 0.07205589798589547
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008779713961606225
        total_loss: .inf
        vf_explained_var: 0.9989725947380066
        vf_loss: 0.46443136036396027
    num_steps_sampled: 32520192
    num_steps_trained: 32520192
  iterations_since_restore: 201
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.34516129032258
    gpu_util_percent0: 0.3512903225806452
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662403567409096
    mean_env_wait_ms: 1.2169667943807623
    mean_inference_ms: 4.3014827874028105
    mean_raw_obs_processing_ms: 0.3790811305161928
  time_since_restore: 5307.986885309219
  time_this_iter_s: 26.287662267684937
  time_total_s: 5307.986885309219
  timers:
    learn_throughput: 8376.514
    learn_time_ms: 19314.955
    sample_throughput: 23663.325
    sample_time_ms: 6837.247
    update_time_ms: 27.586
  timestamp: 1602725941
  timesteps_since_restore: 0
  timesteps_total: 32520192
  training_iteration: 201
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:39:01,824	WARNING util.py:136 -- The `process_trial` operation took 0.6100695133209229 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    201 |          5307.99 | 32520192 |  279.308 |              309.323 |              74.7778 |            792.897 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3109.032975590762
    time_step_min: 2920
  date: 2020-10-15_01-39-28
  done: false
  episode_len_mean: 792.9123246371173
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.42416751826164
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 203
  episodes_total: 41129
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.3137255307235084e-35
        cur_lr: 5.0e-05
        entropy: 0.07090826394657294
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008548042465311786
        total_loss: .inf
        vf_explained_var: 0.999167263507843
        vf_loss: 0.4097138022383054
    num_steps_sampled: 32681984
    num_steps_trained: 32681984
  iterations_since_restore: 202
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.238709677419358
    gpu_util_percent0: 0.34967741935483865
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14662101192042923
    mean_env_wait_ms: 1.216912249851262
    mean_inference_ms: 4.301282523704597
    mean_raw_obs_processing_ms: 0.3790711017996272
  time_since_restore: 5334.302370786667
  time_this_iter_s: 26.31548547744751
  time_total_s: 5334.302370786667
  timers:
    learn_throughput: 8386.199
    learn_time_ms: 19292.649
    sample_throughput: 23714.775
    sample_time_ms: 6822.413
    update_time_ms: 25.593
  timestamp: 1602725968
  timesteps_since_restore: 0
  timesteps_total: 32681984
  training_iteration: 202
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:39:28,970	WARNING util.py:136 -- The `process_trial` operation took 0.6389560699462891 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    202 |           5334.3 | 32681984 |  279.424 |              309.323 |              74.7778 |            792.912 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3108.157437436468
    time_step_min: 2920
  date: 2020-10-15_01-39-55
  done: false
  episode_len_mean: 792.9361398587871
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.5548868118266
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 41356
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.470588296085263e-35
        cur_lr: 5.0e-05
        entropy: 0.07126171266039212
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008386884869347947
        total_loss: .inf
        vf_explained_var: 0.9993535876274109
        vf_loss: 0.3390263418356578
    num_steps_sampled: 32843776
    num_steps_trained: 32843776
  iterations_since_restore: 203
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0
    gpu_util_percent0: 0.325483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466170192469784
    mean_env_wait_ms: 1.2168449654584916
    mean_inference_ms: 4.301053526869804
    mean_raw_obs_processing_ms: 0.37905864575077797
  time_since_restore: 5360.677631378174
  time_this_iter_s: 26.375260591506958
  time_total_s: 5360.677631378174
  timers:
    learn_throughput: 8392.036
    learn_time_ms: 19279.232
    sample_throughput: 23691.937
    sample_time_ms: 6828.99
    update_time_ms: 23.557
  timestamp: 1602725995
  timesteps_since_restore: 0
  timesteps_total: 32843776
  training_iteration: 203
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:39:56,108	WARNING util.py:136 -- The `process_trial` operation took 0.5678844451904297 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    203 |          5360.68 | 32843776 |  279.555 |              309.323 |              74.7778 |            792.936 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3107.4242833052276
    time_step_min: 2920
  date: 2020-10-15_01-40-22
  done: false
  episode_len_mean: 792.9577837681718
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.6638897884204
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 41548
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.205882444127895e-35
        cur_lr: 5.0e-05
        entropy: 0.06891318348546822
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00872472683840897
        total_loss: .inf
        vf_explained_var: 0.9994162917137146
        vf_loss: 0.2664998198548953
    num_steps_sampled: 33005568
    num_steps_trained: 33005568
  iterations_since_restore: 204
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54193548387097
    gpu_util_percent0: 0.2990322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661364078906594
    mean_env_wait_ms: 1.2167894750420556
    mean_inference_ms: 4.300868326944998
    mean_raw_obs_processing_ms: 0.3790482612167583
  time_since_restore: 5387.087016582489
  time_this_iter_s: 26.409385204315186
  time_total_s: 5387.087016582489
  timers:
    learn_throughput: 8407.141
    learn_time_ms: 19244.591
    sample_throughput: 23619.122
    sample_time_ms: 6850.043
    update_time_ms: 21.888
  timestamp: 1602726022
  timesteps_since_restore: 0
  timesteps_total: 33005568
  training_iteration: 204
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:40:23,333	WARNING util.py:136 -- The `process_trial` operation took 0.6221768856048584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    204 |          5387.09 | 33005568 |  279.664 |              309.323 |              74.7778 |            792.958 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3106.767898688029
    time_step_min: 2920
  date: 2020-10-15_01-40-49
  done: false
  episode_len_mean: 792.9699264335866
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.7639954213724
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 183
  episodes_total: 41731
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.808823666191841e-35
        cur_lr: 5.0e-05
        entropy: 0.06811186236639817
        entropy_coeff: 0.0005000000000000001
        kl: 0.004968660573164622
        model: {}
        policy_loss: -0.006599132790142903
        total_loss: 0.3629436021049817
        vf_explained_var: 0.9992592930793762
        vf_loss: 0.36957678695519763
    num_steps_sampled: 33167360
    num_steps_trained: 33167360
  iterations_since_restore: 205
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.580645161290324
    gpu_util_percent0: 0.35032258064516125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14661039919022822
    mean_env_wait_ms: 1.216738967986733
    mean_inference_ms: 4.300689280463208
    mean_raw_obs_processing_ms: 0.3790397989289366
  time_since_restore: 5413.733073949814
  time_this_iter_s: 26.64605736732483
  time_total_s: 5413.733073949814
  timers:
    learn_throughput: 8400.163
    learn_time_ms: 19260.579
    sample_throughput: 23642.124
    sample_time_ms: 6843.378
    update_time_ms: 23.134
  timestamp: 1602726049
  timesteps_since_restore: 0
  timesteps_total: 33167360
  training_iteration: 205
  trial_id: 9f737_00000
  
2020-10-15 01:40:50,771	WARNING util.py:136 -- The `process_trial` operation took 0.5927286148071289 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    205 |          5413.73 | 33167360 |  279.764 |              309.323 |              74.7778 |             792.97 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3105.925480654549
    time_step_min: 2920
  date: 2020-10-15_01-41-17
  done: false
  episode_len_mean: 792.9965443279314
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.891966615632
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 229
  episodes_total: 41960
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9044118330959205e-35
        cur_lr: 5.0e-05
        entropy: 0.07467252699037392
        entropy_coeff: 0.0005000000000000001
        kl: 0.005511083756573498
        model: {}
        policy_loss: -0.008574800739249136
        total_loss: 0.41664835065603256
        vf_explained_var: 0.9992724061012268
        vf_loss: 0.4252604891856511
    num_steps_sampled: 33329152
    num_steps_trained: 33329152
  iterations_since_restore: 206
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.87741935483871
    gpu_util_percent0: 0.2738709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1466072755109988
    mean_env_wait_ms: 1.2166734893433198
    mean_inference_ms: 4.300468214003537
    mean_raw_obs_processing_ms: 0.3790277605463024
  time_since_restore: 5440.1529042720795
  time_this_iter_s: 26.419830322265625
  time_total_s: 5440.1529042720795
  timers:
    learn_throughput: 8404.545
    learn_time_ms: 19250.536
    sample_throughput: 23595.201
    sample_time_ms: 6856.988
    update_time_ms: 25.38
  timestamp: 1602726077
  timesteps_since_restore: 0
  timesteps_total: 33329152
  training_iteration: 206
  trial_id: 9f737_00000
  
2020-10-15 01:41:18,021	WARNING util.py:136 -- The `process_trial` operation took 0.5712113380432129 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    206 |          5440.15 | 33329152 |  279.892 |              309.323 |              74.7778 |            792.997 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3105.2013008593267
    time_step_min: 2920
  date: 2020-10-15_01-41-44
  done: false
  episode_len_mean: 793.0195190209657
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 279.99977528822035
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 204
  episodes_total: 42164
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9044118330959205e-35
        cur_lr: 5.0e-05
        entropy: 0.07359488246341546
        entropy_coeff: 0.0005000000000000001
        kl: 0.0047821371505657835
        model: {}
        policy_loss: -0.006845095592628543
        total_loss: 0.4903220236301422
        vf_explained_var: 0.9990317225456238
        vf_loss: 0.49720392127831775
    num_steps_sampled: 33490944
    num_steps_trained: 33490944
  iterations_since_restore: 207
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.258064516129036
    gpu_util_percent0: 0.3487096774193548
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660400021756903
    mean_env_wait_ms: 1.2166127105729407
    mean_inference_ms: 4.30028897622655
    mean_raw_obs_processing_ms: 0.37901757926994917
  time_since_restore: 5466.474271535873
  time_this_iter_s: 26.321367263793945
  time_total_s: 5466.474271535873
  timers:
    learn_throughput: 8416.389
    learn_time_ms: 19223.446
    sample_throughput: 23618.306
    sample_time_ms: 6850.279
    update_time_ms: 25.348
  timestamp: 1602726104
  timesteps_since_restore: 0
  timesteps_total: 33490944
  training_iteration: 207
  trial_id: 9f737_00000
  
2020-10-15 01:41:45,136	WARNING util.py:136 -- The `process_trial` operation took 0.596651554107666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    207 |          5466.47 | 33490944 |      280 |              309.323 |              74.7778 |             793.02 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3104.5528847290175
    time_step_min: 2920
  date: 2020-10-15_01-42-11
  done: false
  episode_len_mean: 793.0356341653481
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.0970233184085
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 183
  episodes_total: 42347
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9522059165479602e-35
        cur_lr: 5.0e-05
        entropy: 0.07317305045823257
        entropy_coeff: 0.0005000000000000001
        kl: 0.007220014464110136
        model: {}
        policy_loss: -0.007455831340242487
        total_loss: 0.35194673885901767
        vf_explained_var: 0.9991986751556396
        vf_loss: 0.359439159433047
    num_steps_sampled: 33652736
    num_steps_trained: 33652736
  iterations_since_restore: 208
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.72258064516129
    gpu_util_percent0: 0.3667741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14660050502664856
    mean_env_wait_ms: 1.2165588892176251
    mean_inference_ms: 4.30011506015558
    mean_raw_obs_processing_ms: 0.3790086210807266
  time_since_restore: 5492.834908008575
  time_this_iter_s: 26.360636472702026
  time_total_s: 5492.834908008575
  timers:
    learn_throughput: 8410.289
    learn_time_ms: 19237.39
    sample_throughput: 23603.747
    sample_time_ms: 6854.505
    update_time_ms: 26.856
  timestamp: 1602726131
  timesteps_since_restore: 0
  timesteps_total: 33652736
  training_iteration: 208
  trial_id: 9f737_00000
  
2020-10-15 01:42:12,281	WARNING util.py:136 -- The `process_trial` operation took 0.5874176025390625 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    208 |          5492.83 | 33652736 |  280.097 |              309.323 |              74.7778 |            793.036 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3103.820604492532
    time_step_min: 2920
  date: 2020-10-15_01-42-38
  done: false
  episode_len_mean: 793.0507132282095
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.2026561291242
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 206
  episodes_total: 42553
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9522059165479602e-35
        cur_lr: 5.0e-05
        entropy: 0.08366782094041507
        entropy_coeff: 0.0005000000000000001
        kl: 0.005133055422144632
        model: {}
        policy_loss: -0.009684379600609342
        total_loss: 0.8509265134731928
        vf_explained_var: 0.9982805848121643
        vf_loss: 0.8606527199347814
    num_steps_sampled: 33814528
    num_steps_trained: 33814528
  iterations_since_restore: 209
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.296774193548384
    gpu_util_percent0: 0.3038709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659770862603685
    mean_env_wait_ms: 1.216499631012841
    mean_inference_ms: 4.299918584195624
    mean_raw_obs_processing_ms: 0.3789992665227699
  time_since_restore: 5519.440225601196
  time_this_iter_s: 26.60531759262085
  time_total_s: 5519.440225601196
  timers:
    learn_throughput: 8392.105
    learn_time_ms: 19279.072
    sample_throughput: 23610.024
    sample_time_ms: 6852.682
    update_time_ms: 26.834
  timestamp: 1602726158
  timesteps_since_restore: 0
  timesteps_total: 33814528
  training_iteration: 209
  trial_id: 9f737_00000
  
2020-10-15 01:42:39,674	WARNING util.py:136 -- The `process_trial` operation took 0.5821359157562256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    209 |          5519.44 | 33814528 |  280.203 |              309.323 |              74.7778 |            793.051 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3103.116394440024
    time_step_min: 2920
  date: 2020-10-15_01-43-06
  done: false
  episode_len_mean: 793.0632890676143
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.30635566362207
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 42772
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9522059165479602e-35
        cur_lr: 5.0e-05
        entropy: 0.08090397467215855
        entropy_coeff: 0.0005000000000000001
        kl: 0.0041309578421836095
        model: {}
        policy_loss: -0.011191504089462493
        total_loss: 0.8753270854552587
        vf_explained_var: 0.9983667731285095
        vf_loss: 0.8865590294202169
    num_steps_sampled: 33976320
    num_steps_trained: 33976320
  iterations_since_restore: 210
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.86774193548387
    gpu_util_percent0: 0.2916129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659397637790977
    mean_env_wait_ms: 1.2164316799919639
    mean_inference_ms: 4.299718667274938
    mean_raw_obs_processing_ms: 0.378987918840451
  time_since_restore: 5545.853789806366
  time_this_iter_s: 26.413564205169678
  time_total_s: 5545.853789806366
  timers:
    learn_throughput: 8377.541
    learn_time_ms: 19312.588
    sample_throughput: 23643.589
    sample_time_ms: 6842.954
    update_time_ms: 26.764
  timestamp: 1602726186
  timesteps_since_restore: 0
  timesteps_total: 33976320
  training_iteration: 210
  trial_id: 9f737_00000
  
2020-10-15 01:43:06,971	WARNING util.py:136 -- The `process_trial` operation took 0.6237859725952148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    210 |          5545.85 | 33976320 |  280.306 |              309.323 |              74.7778 |            793.063 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3102.4335592667644
    time_step_min: 2920
  date: 2020-10-15_01-43-33
  done: false
  episode_len_mean: 793.0856158804776
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.40637648740784
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 42971
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.761029582739801e-36
        cur_lr: 5.0e-05
        entropy: 0.0719805999348561
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035605813221385083
        model: {}
        policy_loss: -0.010058596313077336
        total_loss: 0.5554217944542567
        vf_explained_var: 0.9987854957580566
        vf_loss: 0.565516377488772
    num_steps_sampled: 34138112
    num_steps_trained: 34138112
  iterations_since_restore: 211
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.0
    gpu_util_percent0: 0.286875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14659081553913747
    mean_env_wait_ms: 1.216372508272575
    mean_inference_ms: 4.299547182665826
    mean_raw_obs_processing_ms: 0.3789784577360731
  time_since_restore: 5572.360810995102
  time_this_iter_s: 26.507021188735962
  time_total_s: 5572.360810995102
  timers:
    learn_throughput: 8372.437
    learn_time_ms: 19324.361
    sample_throughput: 23609.653
    sample_time_ms: 6852.79
    update_time_ms: 26.199
  timestamp: 1602726213
  timesteps_since_restore: 0
  timesteps_total: 34138112
  training_iteration: 211
  trial_id: 9f737_00000
  
2020-10-15 01:43:34,482	WARNING util.py:136 -- The `process_trial` operation took 0.6442666053771973 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    211 |          5572.36 | 34138112 |  280.406 |              309.323 |              74.7778 |            793.086 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3101.7937608720863
    time_step_min: 2920
  date: 2020-10-15_01-44-00
  done: false
  episode_len_mean: 793.1058327346882
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.50343749875645
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 43153
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.8805147913699006e-36
        cur_lr: 5.0e-05
        entropy: 0.07184026017785072
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005256985527618478
        total_loss: .inf
        vf_explained_var: 0.9992861151695251
        vf_loss: 0.3287944719195366
    num_steps_sampled: 34299904
    num_steps_trained: 34299904
  iterations_since_restore: 212
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.767741935483876
    gpu_util_percent0: 0.3116129032258064
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658781641045246
    mean_env_wait_ms: 1.216319458529392
    mean_inference_ms: 4.299380080134708
    mean_raw_obs_processing_ms: 0.3789705982524439
  time_since_restore: 5598.532405376434
  time_this_iter_s: 26.171594381332397
  time_total_s: 5598.532405376434
  timers:
    learn_throughput: 8380.489
    learn_time_ms: 19305.795
    sample_throughput: 23596.028
    sample_time_ms: 6856.747
    update_time_ms: 27.525
  timestamp: 1602726240
  timesteps_since_restore: 0
  timesteps_total: 34299904
  training_iteration: 212
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:44:01,627	WARNING util.py:136 -- The `process_trial` operation took 0.6019325256347656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    212 |          5598.53 | 34299904 |  280.503 |              309.323 |              74.7778 |            793.106 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3101.0061378005444
    time_step_min: 2920
  date: 2020-10-15_01-44-28
  done: false
  episode_len_mean: 793.1398238657322
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.6222085759848
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 43376
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.320772187054852e-36
        cur_lr: 5.0e-05
        entropy: 0.07405764237046242
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006808921287301928
        total_loss: .inf
        vf_explained_var: 0.9995395541191101
        vf_loss: 0.24525333940982819
    num_steps_sampled: 34461696
    num_steps_trained: 34461696
  iterations_since_restore: 213
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.599999999999998
    gpu_util_percent0: 0.28096774193548385
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14658506599969431
    mean_env_wait_ms: 1.2162535264380483
    mean_inference_ms: 4.2991773711417824
    mean_raw_obs_processing_ms: 0.37895942043481534
  time_since_restore: 5625.160454273224
  time_this_iter_s: 26.62804889678955
  time_total_s: 5625.160454273224
  timers:
    learn_throughput: 8370.264
    learn_time_ms: 19329.378
    sample_throughput: 23599.15
    sample_time_ms: 6855.84
    update_time_ms: 28.032
  timestamp: 1602726268
  timesteps_since_restore: 0
  timesteps_total: 34461696
  training_iteration: 213
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:44:29,067	WARNING util.py:136 -- The `process_trial` operation took 0.6118218898773193 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    213 |          5625.16 | 34461696 |  280.622 |              309.323 |              74.7778 |             793.14 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3100.315918798512
    time_step_min: 2920
  date: 2020-10-15_01-44-55
  done: false
  episode_len_mean: 793.1675385462555
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.7278310361322
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 43584
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.098115828058228e-35
        cur_lr: 5.0e-05
        entropy: 0.07228384353220463
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010511397160977745
        total_loss: .inf
        vf_explained_var: 0.9992570281028748
        vf_loss: 0.37579649935166043
    num_steps_sampled: 34623488
    num_steps_trained: 34623488
  iterations_since_restore: 214
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.88709677419355
    gpu_util_percent0: 0.3235483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465816974221239
    mean_env_wait_ms: 1.216187636104305
    mean_inference_ms: 4.299003446010226
    mean_raw_obs_processing_ms: 0.37895019324819146
  time_since_restore: 5651.719923257828
  time_this_iter_s: 26.559468984603882
  time_total_s: 5651.719923257828
  timers:
    learn_throughput: 8358.889
    learn_time_ms: 19355.682
    sample_throughput: 23648.575
    sample_time_ms: 6841.511
    update_time_ms: 29.613
  timestamp: 1602726295
  timesteps_since_restore: 0
  timesteps_total: 34623488
  training_iteration: 214
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:44:56,430	WARNING util.py:136 -- The `process_trial` operation took 0.5952386856079102 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    214 |          5651.72 | 34623488 |  280.728 |              309.323 |              74.7778 |            793.168 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3099.6945575120053
    time_step_min: 2920
  date: 2020-10-15_01-45-22
  done: false
  episode_len_mean: 793.1936346188996
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.81917788744687
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 43768
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6471737420873414e-35
        cur_lr: 5.0e-05
        entropy: 0.0715488667289416
        entropy_coeff: 0.0005000000000000001
        kl: 0.004264116655879964
        model: {}
        policy_loss: -0.008125849339800576
        total_loss: 0.370184063911438
        vf_explained_var: 0.9991686344146729
        vf_loss: 0.37834570556879044
    num_steps_sampled: 34785280
    num_steps_trained: 34785280
  iterations_since_restore: 215
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.716129032258067
    gpu_util_percent0: 0.2996774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465786415276948
    mean_env_wait_ms: 1.2161315263881378
    mean_inference_ms: 4.29884400263272
    mean_raw_obs_processing_ms: 0.37894216419793497
  time_since_restore: 5678.042319536209
  time_this_iter_s: 26.322396278381348
  time_total_s: 5678.042319536209
  timers:
    learn_throughput: 8363.227
    learn_time_ms: 19345.641
    sample_throughput: 23712.585
    sample_time_ms: 6823.044
    update_time_ms: 27.616
  timestamp: 1602726322
  timesteps_since_restore: 0
  timesteps_total: 34785280
  training_iteration: 215
  trial_id: 9f737_00000
  
2020-10-15 01:45:23,556	WARNING util.py:136 -- The `process_trial` operation took 0.5928795337677002 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    215 |          5678.04 | 34785280 |  280.819 |              309.323 |              74.7778 |            793.194 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3099.017118921872
    time_step_min: 2920
  date: 2020-10-15_01-45-49
  done: false
  episode_len_mean: 793.2217622708456
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 280.9182217020774
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 198
  episodes_total: 43966
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.235868710436707e-36
        cur_lr: 5.0e-05
        entropy: 0.07193105171124141
        entropy_coeff: 0.0005000000000000001
        kl: 0.0037130432319827378
        model: {}
        policy_loss: -0.006636794343648944
        total_loss: 0.38527532170216244
        vf_explained_var: 0.9992148280143738
        vf_loss: 0.39194808155298233
    num_steps_sampled: 34947072
    num_steps_trained: 34947072
  iterations_since_restore: 216
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32903225806452
    gpu_util_percent0: 0.3383870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465762786610097
    mean_env_wait_ms: 1.2160730172844614
    mean_inference_ms: 4.298667427504088
    mean_raw_obs_processing_ms: 0.3789341355681167
  time_since_restore: 5704.322285890579
  time_this_iter_s: 26.279966354370117
  time_total_s: 5704.322285890579
  timers:
    learn_throughput: 8366.364
    learn_time_ms: 19338.389
    sample_throughput: 23740.861
    sample_time_ms: 6814.917
    update_time_ms: 27.606
  timestamp: 1602726349
  timesteps_since_restore: 0
  timesteps_total: 34947072
  training_iteration: 216
  trial_id: 9f737_00000
  
2020-10-15 01:45:50,671	WARNING util.py:136 -- The `process_trial` operation took 0.6272830963134766 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    216 |          5704.32 | 34947072 |  280.918 |              309.323 |              74.7778 |            793.222 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3098.2562174405434
    time_step_min: 2920
  date: 2020-10-15_01-46-17
  done: false
  episode_len_mean: 793.2511088983434
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.0295299331689
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 44188
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1179343552183536e-36
        cur_lr: 5.0e-05
        entropy: 0.07209203578531742
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.005537383762809138
        total_loss: .inf
        vf_explained_var: 0.9992154240608215
        vf_loss: 0.4682893132170041
    num_steps_sampled: 35108864
    num_steps_trained: 35108864
  iterations_since_restore: 217
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.13225806451613
    gpu_util_percent0: 0.2803225806451612
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14657295590877809
    mean_env_wait_ms: 1.2160023222577734
    mean_inference_ms: 4.2984859685511365
    mean_raw_obs_processing_ms: 0.3789234357097229
  time_since_restore: 5730.907037496567
  time_this_iter_s: 26.58475160598755
  time_total_s: 5730.907037496567
  timers:
    learn_throughput: 8353.614
    learn_time_ms: 19367.905
    sample_throughput: 23760.342
    sample_time_ms: 6809.33
    update_time_ms: 29.061
  timestamp: 1602726377
  timesteps_since_restore: 0
  timesteps_total: 35108864
  training_iteration: 217
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:46:18,242	WARNING util.py:136 -- The `process_trial` operation took 0.6641802787780762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    217 |          5730.91 | 35108864 |   281.03 |              309.323 |              74.7778 |            793.251 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3097.591553741742
    time_step_min: 2920
  date: 2020-10-15_01-46-44
  done: false
  episode_len_mean: 793.2823221969406
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.13185335069124
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 44389
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.176901532827531e-36
        cur_lr: 5.0e-05
        entropy: 0.06902512659629186
        entropy_coeff: 0.0005000000000000001
        kl: 0.005151541205123067
        model: {}
        policy_loss: -0.007593152375193313
        total_loss: 0.2382563886543115
        vf_explained_var: 0.9995071887969971
        vf_loss: 0.24588405465086302
    num_steps_sampled: 35270656
    num_steps_trained: 35270656
  iterations_since_restore: 218
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.28125
    gpu_util_percent0: 0.3778125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465697700854361
    mean_env_wait_ms: 1.2159394373758126
    mean_inference_ms: 4.298320867957453
    mean_raw_obs_processing_ms: 0.37891407962816104
  time_since_restore: 5757.471812725067
  time_this_iter_s: 26.564775228500366
  time_total_s: 5757.471812725067
  timers:
    learn_throughput: 8349.501
    learn_time_ms: 19377.446
    sample_throughput: 23771.301
    sample_time_ms: 6806.19
    update_time_ms: 36.338
  timestamp: 1602726404
  timesteps_since_restore: 0
  timesteps_total: 35270656
  training_iteration: 218
  trial_id: 9f737_00000
  
2020-10-15 01:46:45,644	WARNING util.py:136 -- The `process_trial` operation took 0.629004716873169 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    218 |          5757.47 | 35270656 |  281.132 |              309.323 |              74.7778 |            793.282 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3097.019895358498
    time_step_min: 2920
  date: 2020-10-15_01-47-11
  done: false
  episode_len_mean: 793.30441318346
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.2173331891982
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 44571
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.176901532827531e-36
        cur_lr: 5.0e-05
        entropy: 0.07159287420411904
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007556386078552653
        total_loss: .inf
        vf_explained_var: 0.9991174340248108
        vf_loss: 0.4381818398833275
    num_steps_sampled: 35432448
    num_steps_trained: 35432448
  iterations_since_restore: 219
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.14838709677419
    gpu_util_percent0: 0.3632258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656687528963613
    mean_env_wait_ms: 1.2158832157848247
    mean_inference_ms: 4.2981643622922245
    mean_raw_obs_processing_ms: 0.37890692921482716
  time_since_restore: 5783.8194036483765
  time_this_iter_s: 26.347590923309326
  time_total_s: 5783.8194036483765
  timers:
    learn_throughput: 8360.604
    learn_time_ms: 19351.712
    sample_throughput: 23775.043
    sample_time_ms: 6805.119
    update_time_ms: 36.718
  timestamp: 1602726431
  timesteps_since_restore: 0
  timesteps_total: 35432448
  training_iteration: 219
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:47:12,830	WARNING util.py:136 -- The `process_trial` operation took 0.6324887275695801 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    219 |          5783.82 | 35432448 |  281.217 |              309.323 |              74.7778 |            793.304 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3096.347144006436
    time_step_min: 2920
  date: 2020-10-15_01-47-39
  done: false
  episode_len_mean: 793.3307953378288
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.31646884600923
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 215
  episodes_total: 44786
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.265352299241298e-36
        cur_lr: 5.0e-05
        entropy: 0.07543866770962875
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008498754623966912
        total_loss: .inf
        vf_explained_var: 0.9988247752189636
        vf_loss: 0.615699052810669
    num_steps_sampled: 35594240
    num_steps_trained: 35594240
  iterations_since_restore: 220
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.096774193548388
    gpu_util_percent0: 0.3016129032258065
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14656448572488429
    mean_env_wait_ms: 1.215817699603517
    mean_inference_ms: 4.297987934552738
    mean_raw_obs_processing_ms: 0.37889756276141096
  time_since_restore: 5810.181825876236
  time_this_iter_s: 26.362422227859497
  time_total_s: 5810.181825876236
  timers:
    learn_throughput: 8363.956
    learn_time_ms: 19343.956
    sample_throughput: 23769.974
    sample_time_ms: 6806.57
    update_time_ms: 36.715
  timestamp: 1602726459
  timesteps_since_restore: 0
  timesteps_total: 35594240
  training_iteration: 220
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:47:40,031	WARNING util.py:136 -- The `process_trial` operation took 0.6305751800537109 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    220 |          5810.18 | 35594240 |  281.316 |              309.323 |              74.7778 |            793.331 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3095.643783218805
    time_step_min: 2920
  date: 2020-10-15_01-48-06
  done: false
  episode_len_mean: 793.3614487279191
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.42344677751856
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 45005
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3898028448861943e-35
        cur_lr: 5.0e-05
        entropy: 0.06690955286224683
        entropy_coeff: 0.0005000000000000001
        kl: 0.004262192446428041
        model: {}
        policy_loss: -0.007250669412314892
        total_loss: 0.2639402089019616
        vf_explained_var: 0.9994639754295349
        vf_loss: 0.27122432986895245
    num_steps_sampled: 35756032
    num_steps_trained: 35756032
  iterations_since_restore: 221
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70322580645162
    gpu_util_percent0: 0.29096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465614343024671
    mean_env_wait_ms: 1.2157461445061708
    mean_inference_ms: 4.2978123882929244
    mean_raw_obs_processing_ms: 0.3788881231509357
  time_since_restore: 5836.717636108398
  time_this_iter_s: 26.535810232162476
  time_total_s: 5836.717636108398
  timers:
    learn_throughput: 8355.057
    learn_time_ms: 19364.559
    sample_throughput: 23837.67
    sample_time_ms: 6787.24
    update_time_ms: 36.868
  timestamp: 1602726486
  timesteps_since_restore: 0
  timesteps_total: 35756032
  training_iteration: 221
  trial_id: 9f737_00000
  
2020-10-15 01:48:07,508	WARNING util.py:136 -- The `process_trial` operation took 0.6168067455291748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    221 |          5836.72 | 35756032 |  281.423 |              309.323 |              74.7778 |            793.361 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3095.044496124031
    time_step_min: 2920
  date: 2020-10-15_01-48-33
  done: false
  episode_len_mean: 793.3828007435602
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.51152782136666
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 183
  episodes_total: 45188
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.949014224430972e-36
        cur_lr: 5.0e-05
        entropy: 0.06801940500736237
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008779244977631606
        total_loss: .inf
        vf_explained_var: 0.9993922114372253
        vf_loss: 0.2832076537112395
    num_steps_sampled: 35917824
    num_steps_trained: 35917824
  iterations_since_restore: 222
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.190625
    gpu_util_percent0: 0.314375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655845203971327
    mean_env_wait_ms: 1.2156886917030554
    mean_inference_ms: 4.297665403269403
    mean_raw_obs_processing_ms: 0.37888089484774096
  time_since_restore: 5863.147214889526
  time_this_iter_s: 26.42957878112793
  time_total_s: 5863.147214889526
  timers:
    learn_throughput: 8343.907
    learn_time_ms: 19390.437
    sample_throughput: 23847.664
    sample_time_ms: 6784.396
    update_time_ms: 36.628
  timestamp: 1602726513
  timesteps_since_restore: 0
  timesteps_total: 35917824
  training_iteration: 222
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:48:34,956	WARNING util.py:136 -- The `process_trial` operation took 0.6411993503570557 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    222 |          5863.15 | 35917824 |  281.512 |              309.323 |              74.7778 |            793.383 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3094.4390120189655
    time_step_min: 2920
  date: 2020-10-15_01-49-01
  done: false
  episode_len_mean: 793.4050856047419
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.6052008082944
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 45383
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0423521336646461e-35
        cur_lr: 5.0e-05
        entropy: 0.07032590359449387
        entropy_coeff: 0.0005000000000000001
        kl: 0.004573109365689258
        model: {}
        policy_loss: -0.007126828534334588
        total_loss: 0.29513639211654663
        vf_explained_var: 0.9993686676025391
        vf_loss: 0.3022983868916829
    num_steps_sampled: 36079616
    num_steps_trained: 36079616
  iterations_since_restore: 223
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.161290322580644
    gpu_util_percent0: 0.29838709677419356
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655625690958773
    mean_env_wait_ms: 1.2156291027301607
    mean_inference_ms: 4.297509527705982
    mean_raw_obs_processing_ms: 0.378873812160095
  time_since_restore: 5889.619291782379
  time_this_iter_s: 26.472076892852783
  time_total_s: 5889.619291782379
  timers:
    learn_throughput: 8347.861
    learn_time_ms: 19381.253
    sample_throughput: 23871.55
    sample_time_ms: 6777.608
    update_time_ms: 36.784
  timestamp: 1602726541
  timesteps_since_restore: 0
  timesteps_total: 36079616
  training_iteration: 223
  trial_id: 9f737_00000
  
2020-10-15 01:49:02,280	WARNING util.py:136 -- The `process_trial` operation took 0.6372690200805664 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    223 |          5889.62 | 36079616 |  281.605 |              309.323 |              74.7778 |            793.405 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3093.7425838160434
    time_step_min: 2920
  date: 2020-10-15_01-49-28
  done: false
  episode_len_mean: 793.4122857017583
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.69964254284855
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 231
  episodes_total: 45614
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.2117606683232304e-36
        cur_lr: 5.0e-05
        entropy: 0.09037090341250102
        entropy_coeff: 0.0005000000000000001
        kl: 0.005786324812409778
        model: {}
        policy_loss: -0.00992067830520682
        total_loss: 1.2283870577812195
        vf_explained_var: 0.9976277351379395
        vf_loss: 1.2383529047171276
    num_steps_sampled: 36241408
    num_steps_trained: 36241408
  iterations_since_restore: 224
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.33870967741936
    gpu_util_percent0: 0.3490322580645161
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655291615533442
    mean_env_wait_ms: 1.2155534072443124
    mean_inference_ms: 4.297324482046946
    mean_raw_obs_processing_ms: 0.3788633193096442
  time_since_restore: 5916.102433919907
  time_this_iter_s: 26.483142137527466
  time_total_s: 5916.102433919907
  timers:
    learn_throughput: 8347.288
    learn_time_ms: 19382.583
    sample_throughput: 23903.366
    sample_time_ms: 6768.587
    update_time_ms: 35.329
  timestamp: 1602726568
  timesteps_since_restore: 0
  timesteps_total: 36241408
  training_iteration: 224
  trial_id: 9f737_00000
  
2020-10-15 01:49:29,730	WARNING util.py:136 -- The `process_trial` operation took 0.6329762935638428 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    224 |           5916.1 | 36241408 |    281.7 |              309.323 |              74.7778 |            793.412 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3093.3397125005463
    time_step_min: 2920
  date: 2020-10-15_01-49-56
  done: false
  episode_len_mean: 793.4081463372042
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.76694827432624
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 198
  episodes_total: 45812
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.2117606683232304e-36
        cur_lr: 5.0e-05
        entropy: 0.08296942462523778
        entropy_coeff: 0.0005000000000000001
        kl: 0.004188739811070263
        model: {}
        policy_loss: -0.01040313889582952
        total_loss: 1.0601768841346104
        vf_explained_var: 0.9977259635925293
        vf_loss: 1.0706215351819992
    num_steps_sampled: 36403200
    num_steps_trained: 36403200
  iterations_since_restore: 225
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.578125
    gpu_util_percent0: 0.310625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14655006878914426
    mean_env_wait_ms: 1.2154909312033357
    mean_inference_ms: 4.297176419138426
    mean_raw_obs_processing_ms: 0.37885532803264427
  time_since_restore: 5942.7576315402985
  time_this_iter_s: 26.655197620391846
  time_total_s: 5942.7576315402985
  timers:
    learn_throughput: 8336.865
    learn_time_ms: 19406.815
    sample_throughput: 23873.923
    sample_time_ms: 6776.934
    update_time_ms: 35.501
  timestamp: 1602726596
  timesteps_since_restore: 0
  timesteps_total: 36403200
  training_iteration: 225
  trial_id: 9f737_00000
  
2020-10-15 01:49:57,245	WARNING util.py:136 -- The `process_trial` operation took 0.6545023918151855 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    225 |          5942.76 | 36403200 |  281.767 |              309.323 |              74.7778 |            793.408 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3092.9169187248394
    time_step_min: 2920
  date: 2020-10-15_01-50-23
  done: false
  episode_len_mean: 793.403822320788
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.83421390211555
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 181
  episodes_total: 45993
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6058803341616152e-36
        cur_lr: 5.0e-05
        entropy: 0.0793766484906276
        entropy_coeff: 0.0005000000000000001
        kl: 0.004161469036868463
        model: {}
        policy_loss: -0.010065817749515796
        total_loss: 0.9892927358547846
        vf_explained_var: 0.9978601336479187
        vf_loss: 0.9993982464075089
    num_steps_sampled: 36564992
    num_steps_trained: 36564992
  iterations_since_restore: 226
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.529032258064515
    gpu_util_percent0: 0.28935483870967743
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654738748924723
    mean_env_wait_ms: 1.2154336959032963
    mean_inference_ms: 4.2970296322638
    mean_raw_obs_processing_ms: 0.3788488357630674
  time_since_restore: 5969.386821269989
  time_this_iter_s: 26.62918972969055
  time_total_s: 5969.386821269989
  timers:
    learn_throughput: 8323.578
    learn_time_ms: 19437.795
    sample_throughput: 23859.928
    sample_time_ms: 6780.909
    update_time_ms: 35.045
  timestamp: 1602726623
  timesteps_since_restore: 0
  timesteps_total: 36564992
  training_iteration: 226
  trial_id: 9f737_00000
  
2020-10-15 01:50:24,731	WARNING util.py:136 -- The `process_trial` operation took 0.638394832611084 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    226 |          5969.39 | 36564992 |  281.834 |              309.323 |              74.7778 |            793.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3092.2932830809887
    time_step_min: 2920
  date: 2020-10-15_01-50-51
  done: false
  episode_len_mean: 793.4275294881506
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 281.9311030442942
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 212
  episodes_total: 46205
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3029401670808076e-36
        cur_lr: 5.0e-05
        entropy: 0.07105916241804759
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007929737582647553
        total_loss: .inf
        vf_explained_var: 0.9992243647575378
        vf_loss: 0.3994426230589549
    num_steps_sampled: 36726784
    num_steps_trained: 36726784
  iterations_since_restore: 227
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.677419354838708
    gpu_util_percent0: 0.31935483870967746
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654537541289514
    mean_env_wait_ms: 1.2153685643917318
    mean_inference_ms: 4.296865241311037
    mean_raw_obs_processing_ms: 0.3788404115320637
  time_since_restore: 5995.780412435532
  time_this_iter_s: 26.393591165542603
  time_total_s: 5995.780412435532
  timers:
    learn_throughput: 8333.766
    learn_time_ms: 19414.033
    sample_throughput: 23852.01
    sample_time_ms: 6783.16
    update_time_ms: 36.026
  timestamp: 1602726651
  timesteps_since_restore: 0
  timesteps_total: 36726784
  training_iteration: 227
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:50:52,063	WARNING util.py:136 -- The `process_trial` operation took 0.6357457637786865 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    227 |          5995.78 | 36726784 |  281.931 |              309.323 |              74.7778 |            793.428 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3091.6690165136033
    time_step_min: 2920
  date: 2020-10-15_01-51-18
  done: false
  episode_len_mean: 793.4506289850077
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.02863483186155
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 46424
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9544102506212107e-36
        cur_lr: 5.0e-05
        entropy: 0.066321878383557
        entropy_coeff: 0.0005000000000000001
        kl: 0.0038484074951459966
        model: {}
        policy_loss: -0.010361351228008667
        total_loss: 0.37821753571430844
        vf_explained_var: 0.999229907989502
        vf_loss: 0.3886120468378067
    num_steps_sampled: 36888576
    num_steps_trained: 36888576
  iterations_since_restore: 228
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.51612903225807
    gpu_util_percent0: 0.29967741935483866
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14654235729260617
    mean_env_wait_ms: 1.2152947312188607
    mean_inference_ms: 4.296698975766757
    mean_raw_obs_processing_ms: 0.37883128624112433
  time_since_restore: 6022.137595415115
  time_this_iter_s: 26.35718297958374
  time_total_s: 6022.137595415115
  timers:
    learn_throughput: 8336.076
    learn_time_ms: 19408.652
    sample_throughput: 23869.227
    sample_time_ms: 6778.267
    update_time_ms: 28.212
  timestamp: 1602726678
  timesteps_since_restore: 0
  timesteps_total: 36888576
  training_iteration: 228
  trial_id: 9f737_00000
  
2020-10-15 01:51:19,327	WARNING util.py:136 -- The `process_trial` operation took 0.6287846565246582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    228 |          6022.14 | 36888576 |  282.029 |              309.323 |              74.7778 |            793.451 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3091.116164218844
    time_step_min: 2920
  date: 2020-10-15_01-51-45
  done: false
  episode_len_mean: 793.465136236859
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.1113321587468
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 46610
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.772051253106053e-37
        cur_lr: 5.0e-05
        entropy: 0.06451980241884787
        entropy_coeff: 0.0005000000000000001
        kl: 0.003352529757345716
        model: {}
        policy_loss: -0.007302681991859572
        total_loss: 0.3221401944756508
        vf_explained_var: 0.9992740750312805
        vf_loss: 0.3294751321276029
    num_steps_sampled: 37050368
    num_steps_trained: 37050368
  iterations_since_restore: 229
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.77741935483871
    gpu_util_percent0: 0.33129032258064517
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653950917074104
    mean_env_wait_ms: 1.2152367840799811
    mean_inference_ms: 4.296563860041685
    mean_raw_obs_processing_ms: 0.37882492956537417
  time_since_restore: 6048.515106916428
  time_this_iter_s: 26.377511501312256
  time_total_s: 6048.515106916428
  timers:
    learn_throughput: 8332.064
    learn_time_ms: 19417.997
    sample_throughput: 23896.279
    sample_time_ms: 6770.594
    update_time_ms: 28.886
  timestamp: 1602726705
  timesteps_since_restore: 0
  timesteps_total: 37050368
  training_iteration: 229
  trial_id: 9f737_00000
  
2020-10-15 01:51:46,683	WARNING util.py:136 -- The `process_trial` operation took 0.6984837055206299 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    229 |          6048.52 | 37050368 |  282.111 |              309.323 |              74.7778 |            793.465 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3090.526985416756
    time_step_min: 2920
  date: 2020-10-15_01-52-13
  done: false
  episode_len_mean: 793.4875437996752
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.1984074571887
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 194
  episodes_total: 46804
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.886025626553027e-37
        cur_lr: 5.0e-05
        entropy: 0.0636204748104016
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006288318738370435
        total_loss: .inf
        vf_explained_var: 0.9989907741546631
        vf_loss: 0.48827440788348514
    num_steps_sampled: 37212160
    num_steps_trained: 37212160
  iterations_since_restore: 230
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.15625
    gpu_util_percent0: 0.313125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653747827672906
    mean_env_wait_ms: 1.2151769627496976
    mean_inference_ms: 4.296420994061722
    mean_raw_obs_processing_ms: 0.37881892784560134
  time_since_restore: 6074.965621709824
  time_this_iter_s: 26.450514793395996
  time_total_s: 6074.965621709824
  timers:
    learn_throughput: 8328.76
    learn_time_ms: 19425.701
    sample_throughput: 23902.128
    sample_time_ms: 6768.937
    update_time_ms: 30.262
  timestamp: 1602726733
  timesteps_since_restore: 0
  timesteps_total: 37212160
  training_iteration: 230
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:52:14,182	WARNING util.py:136 -- The `process_trial` operation took 0.6505396366119385 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    230 |          6074.97 | 37212160 |  282.198 |              309.323 |              74.7778 |            793.488 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3089.8380782160943
    time_step_min: 2920
  date: 2020-10-15_01-52-40
  done: false
  episode_len_mean: 793.5048686112765
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.29948584406867
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 232
  episodes_total: 47036
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.3290384398295396e-37
        cur_lr: 5.0e-05
        entropy: 0.06874275331695874
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009619857492604448
        total_loss: .inf
        vf_explained_var: 0.9989643692970276
        vf_loss: 0.5469795962174734
    num_steps_sampled: 37373952
    num_steps_trained: 37373952
  iterations_since_restore: 231
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.70645161290323
    gpu_util_percent0: 0.3203225806451612
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653456084016037
    mean_env_wait_ms: 1.2150996559870713
    mean_inference_ms: 4.296247814065649
    mean_raw_obs_processing_ms: 0.37880912564146674
  time_since_restore: 6101.463082313538
  time_this_iter_s: 26.49746060371399
  time_total_s: 6101.463082313538
  timers:
    learn_throughput: 8336.451
    learn_time_ms: 19407.779
    sample_throughput: 23855.622
    sample_time_ms: 6782.133
    update_time_ms: 30.469
  timestamp: 1602726760
  timesteps_since_restore: 0
  timesteps_total: 37373952
  training_iteration: 231
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:52:41,554	WARNING util.py:136 -- The `process_trial` operation took 0.6500561237335205 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    231 |          6101.46 | 37373952 |  282.299 |              309.323 |              74.7778 |            793.505 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3089.2724980399635
    time_step_min: 2920
  date: 2020-10-15_01-53-07
  done: false
  episode_len_mean: 793.5254176282526
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.3859229161467
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 47231
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0993557659744311e-36
        cur_lr: 5.0e-05
        entropy: 0.06473085656762123
        entropy_coeff: 0.0005000000000000001
        kl: 0.004416666323474298
        model: {}
        policy_loss: -0.008487024208686004
        total_loss: 0.3654348651568095
        vf_explained_var: 0.9991984367370605
        vf_loss: 0.37395424644152325
    num_steps_sampled: 37535744
    num_steps_trained: 37535744
  iterations_since_restore: 232
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.0258064516129
    gpu_util_percent0: 0.34838709677419355
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14653192669026022
    mean_env_wait_ms: 1.2150358622504172
    mean_inference_ms: 4.29610393448412
    mean_raw_obs_processing_ms: 0.37880129854027206
  time_since_restore: 6127.775009393692
  time_this_iter_s: 26.31192708015442
  time_total_s: 6127.775009393692
  timers:
    learn_throughput: 8339.752
    learn_time_ms: 19400.096
    sample_throughput: 23865.186
    sample_time_ms: 6779.415
    update_time_ms: 29.256
  timestamp: 1602726787
  timesteps_since_restore: 0
  timesteps_total: 37535744
  training_iteration: 232
  trial_id: 9f737_00000
  
2020-10-15 01:53:08,736	WARNING util.py:136 -- The `process_trial` operation took 0.6534309387207031 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    232 |          6127.78 | 37535744 |  282.386 |              309.323 |              74.7778 |            793.525 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3088.7449817421957
    time_step_min: 2920
  date: 2020-10-15_01-53-35
  done: false
  episode_len_mean: 793.5435621638721
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.46688971333066
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 47415
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.496778829872156e-37
        cur_lr: 5.0e-05
        entropy: 0.06477922201156616
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00853608724961911
        total_loss: .inf
        vf_explained_var: 0.9991405010223389
        vf_loss: 0.4002392267187436
    num_steps_sampled: 37697536
    num_steps_trained: 37697536
  iterations_since_restore: 233
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.625806451612906
    gpu_util_percent0: 0.363225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465294537133941
    mean_env_wait_ms: 1.2149768818270965
    mean_inference_ms: 4.295969647083042
    mean_raw_obs_processing_ms: 0.37879546801628483
  time_since_restore: 6154.365377187729
  time_this_iter_s: 26.590367794036865
  time_total_s: 6154.365377187729
  timers:
    learn_throughput: 8332.929
    learn_time_ms: 19415.982
    sample_throughput: 23875.888
    sample_time_ms: 6776.376
    update_time_ms: 28.602
  timestamp: 1602726815
  timesteps_since_restore: 0
  timesteps_total: 37697536
  training_iteration: 233
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:53:36,300	WARNING util.py:136 -- The `process_trial` operation took 0.6864347457885742 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    233 |          6154.37 | 37697536 |  282.467 |              309.323 |              74.7778 |            793.544 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3088.161111344626
    time_step_min: 2920
  date: 2020-10-15_01-54-02
  done: false
  episode_len_mean: 793.5662116757665
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.5543740640338
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 205
  episodes_total: 47620
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.245168244808232e-37
        cur_lr: 5.0e-05
        entropy: 0.06973858674367268
        entropy_coeff: 0.0005000000000000001
        kl: 0.004493356837580602
        model: {}
        policy_loss: -0.009044606810978925
        total_loss: 0.650698666771253
        vf_explained_var: 0.9987182021141052
        vf_loss: 0.6597781231005987
    num_steps_sampled: 37859328
    num_steps_trained: 37859328
  iterations_since_restore: 234
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.27741935483871
    gpu_util_percent0: 0.3203225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652774687171194
    mean_env_wait_ms: 1.2149136355960055
    mean_inference_ms: 4.29582040618835
    mean_raw_obs_processing_ms: 0.378787857956151
  time_since_restore: 6180.732321023941
  time_this_iter_s: 26.366943836212158
  time_total_s: 6180.732321023941
  timers:
    learn_throughput: 8339.628
    learn_time_ms: 19400.386
    sample_throughput: 23861.359
    sample_time_ms: 6780.502
    update_time_ms: 28.851
  timestamp: 1602726842
  timesteps_since_restore: 0
  timesteps_total: 37859328
  training_iteration: 234
  trial_id: 9f737_00000
  
2020-10-15 01:54:03,627	WARNING util.py:136 -- The `process_trial` operation took 0.6485574245452881 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    234 |          6180.73 | 37859328 |  282.554 |              309.323 |              74.7778 |            793.566 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3087.5371307840346
    time_step_min: 2920
  date: 2020-10-15_01-54-30
  done: false
  episode_len_mean: 793.5885623510723
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.6464882933258
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 47842
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.122584122404116e-37
        cur_lr: 5.0e-05
        entropy: 0.06917020678520203
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00876406611253818
        total_loss: .inf
        vf_explained_var: 0.9990059733390808
        vf_loss: 0.497096744676431
    num_steps_sampled: 38021120
    num_steps_trained: 38021120
  iterations_since_restore: 235
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.15483870967742
    gpu_util_percent0: 0.29225806451612907
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652437011182717
    mean_env_wait_ms: 1.2148366971205644
    mean_inference_ms: 4.295662411000573
    mean_raw_obs_processing_ms: 0.37877930078229605
  time_since_restore: 6207.147543430328
  time_this_iter_s: 26.41522240638733
  time_total_s: 6207.147543430328
  timers:
    learn_throughput: 8346.359
    learn_time_ms: 19384.741
    sample_throughput: 23899.691
    sample_time_ms: 6769.627
    update_time_ms: 29.957
  timestamp: 1602726870
  timesteps_since_restore: 0
  timesteps_total: 38021120
  training_iteration: 235
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:54:31,048	WARNING util.py:136 -- The `process_trial` operation took 0.6956503391265869 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    235 |          6207.15 | 38021120 |  282.646 |              309.323 |              74.7778 |            793.589 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3087.0229820394215
    time_step_min: 2920
  date: 2020-10-15_01-54-57
  done: false
  episode_len_mean: 793.6109468687541
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.72560107234904
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 190
  episodes_total: 48032
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.183876183606174e-37
        cur_lr: 5.0e-05
        entropy: 0.06346211613466342
        entropy_coeff: 0.0005000000000000001
        kl: 0.004353561904281378
        model: {}
        policy_loss: -0.00929041668617477
        total_loss: 0.44895487527052563
        vf_explained_var: 0.9989987015724182
        vf_loss: 0.45827702432870865
    num_steps_sampled: 38182912
    num_steps_trained: 38182912
  iterations_since_restore: 236
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.684375000000003
    gpu_util_percent0: 0.325625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652208696771743
    mean_env_wait_ms: 1.2147758154891268
    mean_inference_ms: 4.295532970297101
    mean_raw_obs_processing_ms: 0.37877322962040433
  time_since_restore: 6233.623663425446
  time_this_iter_s: 26.476119995117188
  time_total_s: 6233.623663425446
  timers:
    learn_throughput: 8354.931
    learn_time_ms: 19364.852
    sample_throughput: 23883.601
    sample_time_ms: 6774.188
    update_time_ms: 28.269
  timestamp: 1602726897
  timesteps_since_restore: 0
  timesteps_total: 38182912
  training_iteration: 236
  trial_id: 9f737_00000
  
2020-10-15 01:54:58,439	WARNING util.py:136 -- The `process_trial` operation took 0.6885495185852051 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    236 |          6233.62 | 38182912 |  282.726 |              309.323 |              74.7778 |            793.611 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3086.4934000249054
    time_step_min: 2920
  date: 2020-10-15_01-55-24
  done: false
  episode_len_mean: 793.63627125674
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.8034932485367
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 48220
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.091938091803087e-37
        cur_lr: 5.0e-05
        entropy: 0.06434088448683421
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010568188959344601
        total_loss: .inf
        vf_explained_var: 0.9993691444396973
        vf_loss: 0.30094047387441
    num_steps_sampled: 38344704
    num_steps_trained: 38344704
  iterations_since_restore: 237
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.890322580645158
    gpu_util_percent0: 0.3167741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14652015036864438
    mean_env_wait_ms: 1.2147156895482698
    mean_inference_ms: 4.295401042380801
    mean_raw_obs_processing_ms: 0.3787675669411451
  time_since_restore: 6260.138409852982
  time_this_iter_s: 26.51474642753601
  time_total_s: 6260.138409852982
  timers:
    learn_throughput: 8353.135
    learn_time_ms: 19369.016
    sample_throughput: 23847.585
    sample_time_ms: 6784.419
    update_time_ms: 25.704
  timestamp: 1602726924
  timesteps_since_restore: 0
  timesteps_total: 38344704
  training_iteration: 237
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:55:25,862	WARNING util.py:136 -- The `process_trial` operation took 0.6837880611419678 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    237 |          6260.14 | 38344704 |  282.803 |              309.323 |              74.7778 |            793.636 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3085.845399153155
    time_step_min: 2920
  date: 2020-10-15_01-55-52
  done: false
  episode_len_mean: 793.6672239077044
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.9001756778984
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 233
  episodes_total: 48453
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.637907137704632e-37
        cur_lr: 5.0e-05
        entropy: 0.06420455438395341
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039532328179727
        model: {}
        policy_loss: -0.008461341882745424
        total_loss: 0.32920654366413754
        vf_explained_var: 0.9993541836738586
        vf_loss: 0.3376999845107396
    num_steps_sampled: 38506496
    num_steps_trained: 38506496
  iterations_since_restore: 238
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.309677419354838
    gpu_util_percent0: 0.3729032258064515
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465175626453221
    mean_env_wait_ms: 1.2146376381771506
    mean_inference_ms: 4.2952338746993375
    mean_raw_obs_processing_ms: 0.3787584530708492
  time_since_restore: 6286.450631380081
  time_this_iter_s: 26.31222152709961
  time_total_s: 6286.450631380081
  timers:
    learn_throughput: 8356.628
    learn_time_ms: 19360.92
    sample_throughput: 23835.9
    sample_time_ms: 6787.744
    update_time_ms: 25.034
  timestamp: 1602726952
  timesteps_since_restore: 0
  timesteps_total: 38506496
  training_iteration: 238
  trial_id: 9f737_00000
  
2020-10-15 01:55:53,088	WARNING util.py:136 -- The `process_trial` operation took 0.6920232772827148 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    238 |          6286.45 | 38506496 |    282.9 |              309.323 |              74.7778 |            793.667 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3085.2909775364105
    time_step_min: 2920
  date: 2020-10-15_01-56-19
  done: false
  episode_len_mean: 793.6987461459404
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 282.9846636976133
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 197
  episodes_total: 48650
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.318953568852316e-37
        cur_lr: 5.0e-05
        entropy: 0.05820772765825192
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008255073422333226
        total_loss: .inf
        vf_explained_var: 0.9995768666267395
        vf_loss: 0.22452709823846817
    num_steps_sampled: 38668288
    num_steps_trained: 38668288
  iterations_since_restore: 239
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.674193548387095
    gpu_util_percent0: 0.2761290322580645
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1465151163337737
    mean_env_wait_ms: 1.214571156079036
    mean_inference_ms: 4.2951022318717085
    mean_raw_obs_processing_ms: 0.37875127868146186
  time_since_restore: 6312.885750293732
  time_this_iter_s: 26.435118913650513
  time_total_s: 6312.885750293732
  timers:
    learn_throughput: 8355.872
    learn_time_ms: 19362.672
    sample_throughput: 23821.827
    sample_time_ms: 6791.754
    update_time_ms: 23.993
  timestamp: 1602726979
  timesteps_since_restore: 0
  timesteps_total: 38668288
  training_iteration: 239
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:56:20,537	WARNING util.py:136 -- The `process_trial` operation took 0.7003319263458252 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    239 |          6312.89 | 38668288 |  282.985 |              309.323 |              74.7778 |            793.699 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3084.7937535863593
    time_step_min: 2920
  date: 2020-10-15_01-56-46
  done: false
  episode_len_mean: 793.719191546873
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.0597435219624
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 48834
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.4784303532784734e-37
        cur_lr: 5.0e-05
        entropy: 0.06263398968925078
        entropy_coeff: 0.0005000000000000001
        kl: 0.003521710071557512
        model: {}
        policy_loss: -0.008495070496185994
        total_loss: 0.34008913735548657
        vf_explained_var: 0.9992637038230896
        vf_loss: 0.3486155221859614
    num_steps_sampled: 38830080
    num_steps_trained: 38830080
  iterations_since_restore: 240
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.243750000000002
    gpu_util_percent0: 0.30093749999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651257088780933
    mean_env_wait_ms: 1.2145099858915862
    mean_inference_ms: 4.294975205664997
    mean_raw_obs_processing_ms: 0.37874542295529956
  time_since_restore: 6339.188338279724
  time_this_iter_s: 26.30258798599243
  time_total_s: 6339.188338279724
  timers:
    learn_throughput: 8368.507
    learn_time_ms: 19333.436
    sample_throughput: 23771.768
    sample_time_ms: 6806.057
    update_time_ms: 24.124
  timestamp: 1602727006
  timesteps_since_restore: 0
  timesteps_total: 38830080
  training_iteration: 240
  trial_id: 9f737_00000
  
2020-10-15 01:56:47,933	WARNING util.py:136 -- The `process_trial` operation took 0.6748411655426025 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    240 |          6339.19 | 38830080 |   283.06 |              309.323 |              74.7778 |            793.719 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3084.272393314149
    time_step_min: 2920
  date: 2020-10-15_01-57-14
  done: false
  episode_len_mean: 793.741929563391
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.1403654589412
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 203
  episodes_total: 49037
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7392151766392367e-37
        cur_lr: 5.0e-05
        entropy: 0.06700224926074345
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009604668492102064
        total_loss: .inf
        vf_explained_var: 0.9987528920173645
        vf_loss: 0.6274377753337225
    num_steps_sampled: 38991872
    num_steps_trained: 38991872
  iterations_since_restore: 241
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.88064516129032
    gpu_util_percent0: 0.3783870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14651122996053761
    mean_env_wait_ms: 1.2144459493305246
    mean_inference_ms: 4.29483952495726
    mean_raw_obs_processing_ms: 0.3787387222542332
  time_since_restore: 6365.585467815399
  time_this_iter_s: 26.39712953567505
  time_total_s: 6365.585467815399
  timers:
    learn_throughput: 8373.972
    learn_time_ms: 19320.82
    sample_throughput: 23768.883
    sample_time_ms: 6806.883
    update_time_ms: 25.346
  timestamp: 1602727034
  timesteps_since_restore: 0
  timesteps_total: 38991872
  training_iteration: 241
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:57:15,226	WARNING util.py:136 -- The `process_trial` operation took 0.6627161502838135 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    241 |          6365.59 | 38991872 |   283.14 |              309.323 |              74.7778 |            793.742 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3083.6771231206826
    time_step_min: 2920
  date: 2020-10-15_01-57-41
  done: false
  episode_len_mean: 793.7677331600959
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.22937831766853
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 221
  episodes_total: 49258
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6088227649588553e-37
        cur_lr: 5.0e-05
        entropy: 0.06186483769367138
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008788373631735643
        total_loss: .inf
        vf_explained_var: 0.9994125962257385
        vf_loss: 0.3123856907089551
    num_steps_sampled: 39153664
    num_steps_trained: 39153664
  iterations_since_restore: 242
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.90645161290323
    gpu_util_percent0: 0.362258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650812664152185
    mean_env_wait_ms: 1.214368625266729
    mean_inference_ms: 4.294694063996623
    mean_raw_obs_processing_ms: 0.3787310124637599
  time_since_restore: 6392.037896633148
  time_this_iter_s: 26.452428817749023
  time_total_s: 6392.037896633148
  timers:
    learn_throughput: 8368.359
    learn_time_ms: 19333.779
    sample_throughput: 23769.889
    sample_time_ms: 6806.595
    update_time_ms: 25.385
  timestamp: 1602727061
  timesteps_since_restore: 0
  timesteps_total: 39153664
  training_iteration: 242
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:57:42,645	WARNING util.py:136 -- The `process_trial` operation took 0.6612505912780762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    242 |          6392.04 | 39153664 |  283.229 |              309.323 |              74.7778 |            793.768 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3083.170663590553
    time_step_min: 2920
  date: 2020-10-15_01-58-09
  done: false
  episode_len_mean: 793.7928656650017
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.30569001168175
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 193
  episodes_total: 49451
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.913234147438284e-37
        cur_lr: 5.0e-05
        entropy: 0.06041446110854546
        entropy_coeff: 0.0005000000000000001
        kl: 0.004398979712277651
        model: {}
        policy_loss: -0.009446103145213177
        total_loss: 0.3611543079217275
        vf_explained_var: 0.9992952942848206
        vf_loss: 0.3706306243936221
    num_steps_sampled: 39315456
    num_steps_trained: 39315456
  iterations_since_restore: 243
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.634375
    gpu_util_percent0: 0.3440625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650590735978705
    mean_env_wait_ms: 1.2143044823095266
    mean_inference_ms: 4.294569137924266
    mean_raw_obs_processing_ms: 0.3787248509179989
  time_since_restore: 6418.522286891937
  time_this_iter_s: 26.484390258789062
  time_total_s: 6418.522286891937
  timers:
    learn_throughput: 8372.964
    learn_time_ms: 19323.145
    sample_throughput: 23777.899
    sample_time_ms: 6804.302
    update_time_ms: 26.85
  timestamp: 1602727089
  timesteps_since_restore: 0
  timesteps_total: 39315456
  training_iteration: 243
  trial_id: 9f737_00000
  
2020-10-15 01:58:10,211	WARNING util.py:136 -- The `process_trial` operation took 0.6864862442016602 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    243 |          6418.52 | 39315456 |  283.306 |              309.323 |              74.7778 |            793.793 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3082.6974273156175
    time_step_min: 2920
  date: 2020-10-15_01-58-36
  done: false
  episode_len_mean: 793.8111854299299
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.3778987798852
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 49636
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.956617073719142e-37
        cur_lr: 5.0e-05
        entropy: 0.06191537125657002
        entropy_coeff: 0.0005000000000000001
        kl: 0.00444030063226819
        model: {}
        policy_loss: -0.008250071747170296
        total_loss: 0.37029725064833957
        vf_explained_var: 0.9992126822471619
        vf_loss: 0.3785782704750697
    num_steps_sampled: 39477248
    num_steps_trained: 39477248
  iterations_since_restore: 244
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.03870967741936
    gpu_util_percent0: 0.33096774193548384
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650408817553556
    mean_env_wait_ms: 1.2142432958359513
    mean_inference_ms: 4.294448710358016
    mean_raw_obs_processing_ms: 0.37871945456615735
  time_since_restore: 6444.945636034012
  time_this_iter_s: 26.423349142074585
  time_total_s: 6444.945636034012
  timers:
    learn_throughput: 8374.54
    learn_time_ms: 19319.509
    sample_throughput: 23759.143
    sample_time_ms: 6809.673
    update_time_ms: 28.298
  timestamp: 1602727116
  timesteps_since_restore: 0
  timesteps_total: 39477248
  training_iteration: 244
  trial_id: 9f737_00000
  
2020-10-15 01:58:37,571	WARNING util.py:136 -- The `process_trial` operation took 0.6982786655426025 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    244 |          6444.95 | 39477248 |  283.378 |              309.323 |              74.7778 |            793.811 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3082.0853822531762
    time_step_min: 2920
  date: 2020-10-15_01-59-03
  done: false
  episode_len_mean: 793.8449690138585
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.4686687577323
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 225
  episodes_total: 49861
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.78308536859571e-38
        cur_lr: 5.0e-05
        entropy: 0.05888119246810675
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00821173458401366
        total_loss: .inf
        vf_explained_var: 0.999478280544281
        vf_loss: 0.2742651129762332
    num_steps_sampled: 39639040
    num_steps_trained: 39639040
  iterations_since_restore: 245
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.351612903225806
    gpu_util_percent0: 0.3348387096774193
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14650186681523808
    mean_env_wait_ms: 1.2141686855840768
    mean_inference_ms: 4.294301445224535
    mean_raw_obs_processing_ms: 0.3787117081420748
  time_since_restore: 6471.316770076752
  time_this_iter_s: 26.371134042739868
  time_total_s: 6471.316770076752
  timers:
    learn_throughput: 8381.367
    learn_time_ms: 19303.773
    sample_throughput: 23725.942
    sample_time_ms: 6819.202
    update_time_ms: 28.241
  timestamp: 1602727143
  timesteps_since_restore: 0
  timesteps_total: 39639040
  training_iteration: 245
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:59:04,859	WARNING util.py:136 -- The `process_trial` operation took 0.6844263076782227 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    245 |          6471.32 | 39639040 |  283.469 |              309.323 |              74.7778 |            793.845 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3081.5520949944025
    time_step_min: 2920
  date: 2020-10-15_01-59-31
  done: false
  episode_len_mean: 793.8655467220647
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.54535446753084
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 50062
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.467462805289356e-37
        cur_lr: 5.0e-05
        entropy: 0.05877321089307467
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008187075358970711
        total_loss: .inf
        vf_explained_var: 0.9989163875579834
        vf_loss: 0.5360946829120318
    num_steps_sampled: 39800832
    num_steps_trained: 39800832
  iterations_since_restore: 246
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.61935483870968
    gpu_util_percent0: 0.3409677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146499458758695
    mean_env_wait_ms: 1.214098764697024
    mean_inference_ms: 4.2941719806933465
    mean_raw_obs_processing_ms: 0.378704668533741
  time_since_restore: 6497.6369597911835
  time_this_iter_s: 26.320189714431763
  time_total_s: 6497.6369597911835
  timers:
    learn_throughput: 8390.1
    learn_time_ms: 19283.68
    sample_throughput: 23738.475
    sample_time_ms: 6815.602
    update_time_ms: 27.972
  timestamp: 1602727171
  timesteps_since_restore: 0
  timesteps_total: 39800832
  training_iteration: 246
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:59:32,095	WARNING util.py:136 -- The `process_trial` operation took 0.6784241199493408 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    246 |          6497.64 | 39800832 |  283.545 |              309.323 |              74.7778 |            793.866 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3081.073643333665
    time_step_min: 2920
  date: 2020-10-15_01-59-58
  done: false
  episode_len_mean: 793.8849222931964
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.61693346816617
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 50253
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2011942079340342e-37
        cur_lr: 5.0e-05
        entropy: 0.05591506293664376
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008465389129317677
        total_loss: .inf
        vf_explained_var: 0.9990746974945068
        vf_loss: 0.43237626552581787
    num_steps_sampled: 39962624
    num_steps_trained: 39962624
  iterations_since_restore: 247
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.09354838709678
    gpu_util_percent0: 0.29193548387096774
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146496968884323
    mean_env_wait_ms: 1.2140358106577758
    mean_inference_ms: 4.294047970443247
    mean_raw_obs_processing_ms: 0.37869889061542933
  time_since_restore: 6524.230689764023
  time_this_iter_s: 26.593729972839355
  time_total_s: 6524.230689764023
  timers:
    learn_throughput: 8385.504
    learn_time_ms: 19294.248
    sample_throughput: 23755.701
    sample_time_ms: 6810.66
    update_time_ms: 28.014
  timestamp: 1602727198
  timesteps_since_restore: 0
  timesteps_total: 39962624
  training_iteration: 247
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 01:59:59,606	WARNING util.py:136 -- The `process_trial` operation took 0.6771945953369141 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    247 |          6524.23 | 39962624 |  283.617 |              309.323 |              74.7778 |            793.885 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3080.5843697312307
    time_step_min: 2920
  date: 2020-10-15_02-00-25
  done: false
  episode_len_mean: 793.9048223098725
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.69302383035955
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 200
  episodes_total: 50453
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3017913119010515e-37
        cur_lr: 5.0e-05
        entropy: 0.05648048284153143
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008503304076536248
        total_loss: .inf
        vf_explained_var: 0.9990152716636658
        vf_loss: 0.5027126769224802
    num_steps_sampled: 40124416
    num_steps_trained: 40124416
  iterations_since_restore: 248
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.874193548387098
    gpu_util_percent0: 0.31290322580645163
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146495590551841
    mean_env_wait_ms: 1.2139697828735267
    mean_inference_ms: 4.293916129278455
    mean_raw_obs_processing_ms: 0.3786926218053351
  time_since_restore: 6550.421587467194
  time_this_iter_s: 26.190897703170776
  time_total_s: 6550.421587467194
  timers:
    learn_throughput: 8392.323
    learn_time_ms: 19278.572
    sample_throughput: 23763.915
    sample_time_ms: 6808.306
    update_time_ms: 29.962
  timestamp: 1602727225
  timesteps_since_restore: 0
  timesteps_total: 40124416
  training_iteration: 248
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:00:26,758	WARNING util.py:136 -- The `process_trial` operation took 0.7139756679534912 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    248 |          6550.42 | 40124416 |  283.693 |              309.323 |              74.7778 |            793.905 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3080.032543443918
    time_step_min: 2920
  date: 2020-10-15_02-00-53
  done: false
  episode_len_mean: 793.9295157662102
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.7761656981831
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 225
  episodes_total: 50678
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.952686967851576e-37
        cur_lr: 5.0e-05
        entropy: 0.059069667321940265
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00782113968549917
        total_loss: .inf
        vf_explained_var: 0.9993965029716492
        vf_loss: 0.304643082122008
    num_steps_sampled: 40286208
    num_steps_trained: 40286208
  iterations_since_restore: 249
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.464516129032262
    gpu_util_percent0: 0.32419354838709685
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14649272138649005
    mean_env_wait_ms: 1.2138910528629159
    mean_inference_ms: 4.293784540400996
    mean_raw_obs_processing_ms: 0.3786850482978363
  time_since_restore: 6576.887253046036
  time_this_iter_s: 26.465665578842163
  time_total_s: 6576.887253046036
  timers:
    learn_throughput: 8393.99
    learn_time_ms: 19274.743
    sample_throughput: 23749.364
    sample_time_ms: 6812.477
    update_time_ms: 31.759
  timestamp: 1602727253
  timesteps_since_restore: 0
  timesteps_total: 40286208
  training_iteration: 249
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:00:54,341	WARNING util.py:136 -- The `process_trial` operation took 0.7948040962219238 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    249 |          6576.89 | 40286208 |  283.776 |              309.323 |              74.7778 |             793.93 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3079.5748293297133
    time_step_min: 2920
  date: 2020-10-15_02-01-20
  done: false
  episode_len_mean: 793.9535062024495
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.8436632032873
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 50867
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.429030451777364e-37
        cur_lr: 5.0e-05
        entropy: 0.05610303860157728
        entropy_coeff: 0.0005000000000000001
        kl: 0.004503374240205933
        model: {}
        policy_loss: -0.009128836701468876
        total_loss: 0.5733734791477522
        vf_explained_var: 0.9989013075828552
        vf_loss: 0.5825303768118223
    num_steps_sampled: 40448000
    num_steps_trained: 40448000
  iterations_since_restore: 250
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.000000000000004
    gpu_util_percent0: 0.3241935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146490812553122
    mean_env_wait_ms: 1.2138279783283628
    mean_inference_ms: 4.293667909569016
    mean_raw_obs_processing_ms: 0.37867879898389945
  time_since_restore: 6602.762730836868
  time_this_iter_s: 25.87547779083252
  time_total_s: 6602.762730836868
  timers:
    learn_throughput: 8406.612
    learn_time_ms: 19245.802
    sample_throughput: 23804.084
    sample_time_ms: 6796.817
    update_time_ms: 32.654
  timestamp: 1602727280
  timesteps_since_restore: 0
  timesteps_total: 40448000
  training_iteration: 250
  trial_id: 9f737_00000
  
2020-10-15 02:01:21,179	WARNING util.py:136 -- The `process_trial` operation took 0.7214949131011963 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    250 |          6602.76 | 40448000 |  283.844 |              309.323 |              74.7778 |            793.954 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3079.131941585808
    time_step_min: 2920
  date: 2020-10-15_02-01-47
  done: false
  episode_len_mean: 793.9852897968777
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.91280620040925
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 51053
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.714515225888682e-37
        cur_lr: 5.0e-05
        entropy: 0.05284845735877752
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007001691837407027
        total_loss: .inf
        vf_explained_var: 0.9993686676025391
        vf_loss: 0.3207749178012212
    num_steps_sampled: 40609792
    num_steps_trained: 40609792
  iterations_since_restore: 251
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.512903225806458
    gpu_util_percent0: 0.3448387096774194
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648901579877266
    mean_env_wait_ms: 1.2137652846262625
    mean_inference_ms: 4.293556170317618
    mean_raw_obs_processing_ms: 0.3786739547377375
  time_since_restore: 6629.1040070056915
  time_this_iter_s: 26.341276168823242
  time_total_s: 6629.1040070056915
  timers:
    learn_throughput: 8404.584
    learn_time_ms: 19250.447
    sample_throughput: 23842.893
    sample_time_ms: 6785.754
    update_time_ms: 31.785
  timestamp: 1602727307
  timesteps_since_restore: 0
  timesteps_total: 40609792
  training_iteration: 251
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:01:48,539	WARNING util.py:136 -- The `process_trial` operation took 0.7758839130401611 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    251 |           6629.1 | 40609792 |  283.913 |              309.323 |              74.7778 |            793.985 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3078.5912710805746
    time_step_min: 2917
  date: 2020-10-15_02-02-14
  done: false
  episode_len_mean: 794.0203822898382
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 283.9913677441471
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 217
  episodes_total: 51270
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.571772838833023e-37
        cur_lr: 5.0e-05
        entropy: 0.05404324519137541
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008040605306935808
        total_loss: .inf
        vf_explained_var: 0.9987453818321228
        vf_loss: 0.7097285886605581
    num_steps_sampled: 40771584
    num_steps_trained: 40771584
  iterations_since_restore: 252
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.858064516129033
    gpu_util_percent0: 0.29774193548387096
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648744746612632
    mean_env_wait_ms: 1.2136936487273406
    mean_inference_ms: 4.293425806646824
    mean_raw_obs_processing_ms: 0.3786674633155866
  time_since_restore: 6655.40168762207
  time_this_iter_s: 26.297680616378784
  time_total_s: 6655.40168762207
  timers:
    learn_throughput: 8410.109
    learn_time_ms: 19237.801
    sample_throughput: 23856.575
    sample_time_ms: 6781.862
    update_time_ms: 31.828
  timestamp: 1602727334
  timesteps_since_restore: 0
  timesteps_total: 40771584
  training_iteration: 252
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:02:15,836	WARNING util.py:136 -- The `process_trial` operation took 0.7454197406768799 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    252 |           6655.4 | 40771584 |  283.991 |              309.323 |              74.7778 |             794.02 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3078.090962110461
    time_step_min: 2917
  date: 2020-10-15_02-02-42
  done: false
  episode_len_mean: 794.0569380500029
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.0697004428573
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 51477
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.357659258249534e-37
        cur_lr: 5.0e-05
        entropy: 0.04791497873763243
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009774020669283345
        total_loss: .inf
        vf_explained_var: 0.9995271563529968
        vf_loss: 0.23515931765238443
    num_steps_sampled: 40933376
    num_steps_trained: 40933376
  iterations_since_restore: 253
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.325
    gpu_util_percent0: 0.3209375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648443514879214
    mean_env_wait_ms: 1.213620327655735
    mean_inference_ms: 4.293298338049551
    mean_raw_obs_processing_ms: 0.3786599641069383
  time_since_restore: 6682.07683634758
  time_this_iter_s: 26.675148725509644
  time_total_s: 6682.07683634758
  timers:
    learn_throughput: 8405.551
    learn_time_ms: 19248.233
    sample_throughput: 23856.161
    sample_time_ms: 6781.98
    update_time_ms: 30.603
  timestamp: 1602727362
  timesteps_since_restore: 0
  timesteps_total: 40933376
  training_iteration: 253
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:02:43,474	WARNING util.py:136 -- The `process_trial` operation took 0.6982517242431641 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    253 |          6682.08 | 40933376 |   284.07 |              309.323 |              74.7778 |            794.057 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3077.6365485183032
    time_step_min: 2917
  date: 2020-10-15_02-03-10
  done: false
  episode_len_mean: 794.0897073623906
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.1388470522363
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 51668
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2536488887374301e-36
        cur_lr: 5.0e-05
        entropy: 0.048961978405714035
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009910274801465372
        total_loss: .inf
        vf_explained_var: 0.999321460723877
        vf_loss: 0.31906373302141827
    num_steps_sampled: 41095168
    num_steps_trained: 41095168
  iterations_since_restore: 254
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.719354838709677
    gpu_util_percent0: 0.3209677419354838
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464826463447331
    mean_env_wait_ms: 1.2135561228104341
    mean_inference_ms: 4.293187623664749
    mean_raw_obs_processing_ms: 0.3786547078573356
  time_since_restore: 6708.677795171738
  time_this_iter_s: 26.600958824157715
  time_total_s: 6708.677795171738
  timers:
    learn_throughput: 8398.179
    learn_time_ms: 19265.128
    sample_throughput: 23855.694
    sample_time_ms: 6782.112
    update_time_ms: 30.645
  timestamp: 1602727390
  timesteps_since_restore: 0
  timesteps_total: 41095168
  training_iteration: 254
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:03:11,061	WARNING util.py:136 -- The `process_trial` operation took 0.7408027648925781 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    254 |          6708.68 | 41095168 |  284.139 |              309.323 |              74.7778 |             794.09 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3077.1540346544207
    time_step_min: 2917
  date: 2020-10-15_02-03-37
  done: false
  episode_len_mean: 794.1246722196514
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.2126108376686
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 196
  episodes_total: 51864
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.880473333106145e-36
        cur_lr: 5.0e-05
        entropy: 0.05513257874796788
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0067018988096
        total_loss: .inf
        vf_explained_var: 0.9994567036628723
        vf_loss: 0.2837526897589366
    num_steps_sampled: 41256960
    num_steps_trained: 41256960
  iterations_since_restore: 255
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.815624999999997
    gpu_util_percent0: 0.3303125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14648100035955755
    mean_env_wait_ms: 1.2134907662950014
    mean_inference_ms: 4.293064901508916
    mean_raw_obs_processing_ms: 0.37864934408942597
  time_since_restore: 6735.4483387470245
  time_this_iter_s: 26.770543575286865
  time_total_s: 6735.4483387470245
  timers:
    learn_throughput: 8386.48
    learn_time_ms: 19292.004
    sample_throughput: 23811.85
    sample_time_ms: 6794.6
    update_time_ms: 31.029
  timestamp: 1602727417
  timesteps_since_restore: 0
  timesteps_total: 41256960
  training_iteration: 255
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:03:38,985	WARNING util.py:136 -- The `process_trial` operation took 0.732999324798584 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    255 |          6735.45 | 41256960 |  284.213 |              309.323 |              74.7778 |            794.125 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3076.626914135301
    time_step_min: 2917
  date: 2020-10-15_02-04-05
  done: false
  episode_len_mean: 794.1538446769704
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.2863665938448
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 221
  episodes_total: 52085
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.820709999659217e-36
        cur_lr: 5.0e-05
        entropy: 0.060299210560818516
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011148526653414592
        total_loss: .inf
        vf_explained_var: 0.9989049434661865
        vf_loss: 0.5759093364079794
    num_steps_sampled: 41418752
    num_steps_trained: 41418752
  iterations_since_restore: 256
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.745161290322585
    gpu_util_percent0: 0.33258064516129027
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647880348290077
    mean_env_wait_ms: 1.2134147850038033
    mean_inference_ms: 4.292943356529175
    mean_raw_obs_processing_ms: 0.3786425007097369
  time_since_restore: 6761.980712413788
  time_this_iter_s: 26.532373666763306
  time_total_s: 6761.980712413788
  timers:
    learn_throughput: 8375.305
    learn_time_ms: 19317.744
    sample_throughput: 23804.927
    sample_time_ms: 6796.576
    update_time_ms: 31.313
  timestamp: 1602727445
  timesteps_since_restore: 0
  timesteps_total: 41418752
  training_iteration: 256
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:04:06,500	WARNING util.py:136 -- The `process_trial` operation took 0.7401432991027832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    256 |          6761.98 | 41418752 |  284.286 |              309.323 |              74.7778 |            794.154 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3076.145082877158
    time_step_min: 2917
  date: 2020-10-15_02-04-33
  done: false
  episode_len_mean: 794.1819677147885
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.35669757014716
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 52284
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2310649994888276e-36
        cur_lr: 5.0e-05
        entropy: 0.05405699058125416
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006129725122567227
        total_loss: .inf
        vf_explained_var: 0.9993528723716736
        vf_loss: 0.31068049371242523
    num_steps_sampled: 41580544
    num_steps_trained: 41580544
  iterations_since_restore: 257
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.819354838709682
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647670472158997
    mean_env_wait_ms: 1.2133443645075874
    mean_inference_ms: 4.292823638871511
    mean_raw_obs_processing_ms: 0.3786358247562509
  time_since_restore: 6788.508185148239
  time_this_iter_s: 26.527472734451294
  time_total_s: 6788.508185148239
  timers:
    learn_throughput: 8376.666
    learn_time_ms: 19314.604
    sample_throughput: 23816.865
    sample_time_ms: 6793.169
    update_time_ms: 31.388
  timestamp: 1602727473
  timesteps_since_restore: 0
  timesteps_total: 41580544
  training_iteration: 257
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:04:34,049	WARNING util.py:136 -- The `process_trial` operation took 0.7043201923370361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    257 |          6788.51 | 41580544 |  284.357 |              309.323 |              74.7778 |            794.182 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3075.6928358892196
    time_step_min: 2917
  date: 2020-10-15_02-05-00
  done: false
  episode_len_mean: 794.2103457477224
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.4235945780374
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 52466
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.34659749923324e-36
        cur_lr: 5.0e-05
        entropy: 0.054820731903115906
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042578383775738375
        model: {}
        policy_loss: -0.00588527406337865
        total_loss: 0.22152798622846603
        vf_explained_var: 0.9995211958885193
        vf_loss: 0.22744067634145418
    num_steps_sampled: 41742336
    num_steps_trained: 41742336
  iterations_since_restore: 258
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.39375
    gpu_util_percent0: 0.29125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647466959664285
    mean_env_wait_ms: 1.2132829194543806
    mean_inference_ms: 4.292720506701989
    mean_raw_obs_processing_ms: 0.3786312440519551
  time_since_restore: 6815.290510892868
  time_this_iter_s: 26.782325744628906
  time_total_s: 6815.290510892868
  timers:
    learn_throughput: 8354.744
    learn_time_ms: 19365.286
    sample_throughput: 23795.384
    sample_time_ms: 6799.302
    update_time_ms: 32.894
  timestamp: 1602727500
  timesteps_since_restore: 0
  timesteps_total: 41742336
  training_iteration: 258
  trial_id: 9f737_00000
  
2020-10-15 02:05:01,814	WARNING util.py:136 -- The `process_trial` operation took 0.7165248394012451 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    258 |          6815.29 | 41742336 |  284.424 |              309.323 |              74.7778 |             794.21 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3075.200771320554
    time_step_min: 2917
  date: 2020-10-15_02-05-28
  done: false
  episode_len_mean: 794.2398291409587
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.49658233977163
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 209
  episodes_total: 52675
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.17329874961662e-36
        cur_lr: 5.0e-05
        entropy: 0.058113422244787216
        entropy_coeff: 0.0005000000000000001
        kl: 0.0033706101627709963
        model: {}
        policy_loss: -0.0059754800361891585
        total_loss: 0.3531038984656334
        vf_explained_var: 0.9992890357971191
        vf_loss: 0.35910844306151074
    num_steps_sampled: 41904128
    num_steps_trained: 41904128
  iterations_since_restore: 259
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.428125
    gpu_util_percent0: 0.3575
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647348806164345
    mean_env_wait_ms: 1.2132147171483276
    mean_inference_ms: 4.292607691235448
    mean_raw_obs_processing_ms: 0.37862557616830567
  time_since_restore: 6842.094863653183
  time_this_iter_s: 26.80435276031494
  time_total_s: 6842.094863653183
  timers:
    learn_throughput: 8341.451
    learn_time_ms: 19396.146
    sample_throughput: 23824.643
    sample_time_ms: 6790.952
    update_time_ms: 33.16
  timestamp: 1602727528
  timesteps_since_restore: 0
  timesteps_total: 41904128
  training_iteration: 259
  trial_id: 9f737_00000
  
2020-10-15 02:05:29,636	WARNING util.py:136 -- The `process_trial` operation took 0.7107744216918945 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    259 |          6842.09 | 41904128 |  284.497 |              309.323 |              74.7778 |             794.24 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3074.707910021378
    time_step_min: 2917
  date: 2020-10-15_02-05-56
  done: false
  episode_len_mean: 794.2691558748464
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.5716518622275
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 220
  episodes_total: 52895
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.58664937480831e-36
        cur_lr: 5.0e-05
        entropy: 0.0549432709813118
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0066217404188743485
        total_loss: .inf
        vf_explained_var: 0.9992043972015381
        vf_loss: 0.4036439408858617
    num_steps_sampled: 42065920
    num_steps_trained: 42065920
  iterations_since_restore: 260
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.93870967741936
    gpu_util_percent0: 0.30129032258064514
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14647054865818057
    mean_env_wait_ms: 1.21313527699931
    mean_inference_ms: 4.292479251415952
    mean_raw_obs_processing_ms: 0.3786182935221053
  time_since_restore: 6868.706673383713
  time_this_iter_s: 26.611809730529785
  time_total_s: 6868.706673383713
  timers:
    learn_throughput: 8311.816
    learn_time_ms: 19465.301
    sample_throughput: 23807.475
    sample_time_ms: 6795.849
    update_time_ms: 31.3
  timestamp: 1602727556
  timesteps_since_restore: 0
  timesteps_total: 42065920
  training_iteration: 260
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:05:57,244	WARNING util.py:136 -- The `process_trial` operation took 0.7462418079376221 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    260 |          6868.71 | 42065920 |  284.572 |              309.323 |              74.7778 |            794.269 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3074.2953097311765
    time_step_min: 2917
  date: 2020-10-15_02-06-23
  done: false
  episode_len_mean: 794.296190942657
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.63519643728364
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 53084
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.379974062212465e-36
        cur_lr: 5.0e-05
        entropy: 0.051399804962178074
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00984744699235307
        total_loss: .inf
        vf_explained_var: 0.9990639090538025
        vf_loss: 0.47412554919719696
    num_steps_sampled: 42227712
    num_steps_trained: 42227712
  iterations_since_restore: 261
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.234375
    gpu_util_percent0: 0.23375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646900034384694
    mean_env_wait_ms: 1.2130710389213424
    mean_inference_ms: 4.292375939520488
    mean_raw_obs_processing_ms: 0.37861310600224873
  time_since_restore: 6895.297696113586
  time_this_iter_s: 26.591022729873657
  time_total_s: 6895.297696113586
  timers:
    learn_throughput: 8302.035
    learn_time_ms: 19488.235
    sample_throughput: 23805.652
    sample_time_ms: 6796.369
    update_time_ms: 32.613
  timestamp: 1602727583
  timesteps_since_restore: 0
  timesteps_total: 42227712
  training_iteration: 261
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:06:24,982	WARNING util.py:136 -- The `process_trial` operation took 0.7257165908813477 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    261 |           6895.3 | 42227712 |  284.635 |              309.323 |              74.7778 |            794.296 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3073.842118115561
    time_step_min: 2917
  date: 2020-10-15_02-06-51
  done: false
  episode_len_mean: 794.3280211735556
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.70256910813276
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 190
  episodes_total: 53274
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.569961093318698e-36
        cur_lr: 5.0e-05
        entropy: 0.052110519570608936
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007875840564035267
        total_loss: .inf
        vf_explained_var: 0.9995552897453308
        vf_loss: 0.22512581323583922
    num_steps_sampled: 42389504
    num_steps_trained: 42389504
  iterations_since_restore: 262
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.15806451612903
    gpu_util_percent0: 0.3767741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646724117938187
    mean_env_wait_ms: 1.2130063949041834
    mean_inference_ms: 4.2922719220831205
    mean_raw_obs_processing_ms: 0.3786084615737066
  time_since_restore: 6921.618757247925
  time_this_iter_s: 26.32106113433838
  time_total_s: 6921.618757247925
  timers:
    learn_throughput: 8305.319
    learn_time_ms: 19480.529
    sample_throughput: 23780.018
    sample_time_ms: 6803.695
    update_time_ms: 34.966
  timestamp: 1602727611
  timesteps_since_restore: 0
  timesteps_total: 42389504
  training_iteration: 262
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:06:52,342	WARNING util.py:136 -- The `process_trial` operation took 0.789405345916748 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    262 |          6921.62 | 42389504 |  284.703 |              309.323 |              74.7778 |            794.328 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3073.325943802148
    time_step_min: 2917
  date: 2020-10-15_02-07-18
  done: false
  episode_len_mean: 794.3674568159724
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.7797038658476
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 53492
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.354941639978046e-36
        cur_lr: 5.0e-05
        entropy: 0.051682861832280956
        entropy_coeff: 0.0005000000000000001
        kl: 0.003312780308381965
        model: {}
        policy_loss: -0.006597839868239437
        total_loss: 0.1882843735317389
        vf_explained_var: 0.9996251463890076
        vf_loss: 0.19490804771582285
    num_steps_sampled: 42551296
    num_steps_trained: 42551296
  iterations_since_restore: 263
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.629032258064516
    gpu_util_percent0: 0.36096774193548387
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646575186053373
    mean_env_wait_ms: 1.2129334582435851
    mean_inference_ms: 4.292158770558294
    mean_raw_obs_processing_ms: 0.37860253529989896
  time_since_restore: 6947.970552206039
  time_this_iter_s: 26.351794958114624
  time_total_s: 6947.970552206039
  timers:
    learn_throughput: 8317.052
    learn_time_ms: 19453.046
    sample_throughput: 23770.552
    sample_time_ms: 6806.405
    update_time_ms: 34.801
  timestamp: 1602727638
  timesteps_since_restore: 0
  timesteps_total: 42551296
  training_iteration: 263
  trial_id: 9f737_00000
  
2020-10-15 02:07:19,673	WARNING util.py:136 -- The `process_trial` operation took 0.731041669845581 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    263 |          6947.97 | 42551296 |   284.78 |              309.323 |              74.7778 |            794.367 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3072.8566130565237
    time_step_min: 2917
  date: 2020-10-15_02-07-45
  done: false
  episode_len_mean: 794.4035234743095
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.85104523078707
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 205
  episodes_total: 53697
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.677470819989023e-36
        cur_lr: 5.0e-05
        entropy: 0.05026842436442772
        entropy_coeff: 0.0005000000000000001
        kl: 0.0034272088669240475
        model: {}
        policy_loss: -0.00775867686024867
        total_loss: 0.15540529104570547
        vf_explained_var: 0.9996752738952637
        vf_loss: 0.16318910072247186
    num_steps_sampled: 42713088
    num_steps_trained: 42713088
  iterations_since_restore: 264
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.612903225806456
    gpu_util_percent0: 0.34451612903225803
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14646354591945693
    mean_env_wait_ms: 1.2128595427382236
    mean_inference_ms: 4.292041585253077
    mean_raw_obs_processing_ms: 0.37859615048588546
  time_since_restore: 6974.195565462112
  time_this_iter_s: 26.225013256072998
  time_total_s: 6974.195565462112
  timers:
    learn_throughput: 8333.199
    learn_time_ms: 19415.353
    sample_throughput: 23764.009
    sample_time_ms: 6808.279
    update_time_ms: 32.424
  timestamp: 1602727665
  timesteps_since_restore: 0
  timesteps_total: 42713088
  training_iteration: 264
  trial_id: 9f737_00000
  
2020-10-15 02:07:46,926	WARNING util.py:136 -- The `process_trial` operation took 0.779503345489502 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    264 |           6974.2 | 42713088 |  284.851 |              309.323 |              74.7778 |            794.404 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3072.4425459205468
    time_step_min: 2917
  date: 2020-10-15_02-08-13
  done: false
  episode_len_mean: 794.4336036821885
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.9125135282222
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 53881
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3387354099945115e-36
        cur_lr: 5.0e-05
        entropy: 0.051068851413826145
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007911400529944027
        total_loss: .inf
        vf_explained_var: 0.9994378685951233
        vf_loss: 0.2717503719031811
    num_steps_sampled: 42874880
    num_steps_trained: 42874880
  iterations_since_restore: 265
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.864516129032264
    gpu_util_percent0: 0.2903225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464614744457996
    mean_env_wait_ms: 1.2127966751910293
    mean_inference_ms: 4.29194382593385
    mean_raw_obs_processing_ms: 0.37859137395556564
  time_since_restore: 7000.397998571396
  time_this_iter_s: 26.202433109283447
  time_total_s: 7000.397998571396
  timers:
    learn_throughput: 8349.242
    learn_time_ms: 19378.046
    sample_throughput: 23825.862
    sample_time_ms: 6790.604
    update_time_ms: 30.138
  timestamp: 1602727693
  timesteps_since_restore: 0
  timesteps_total: 42874880
  training_iteration: 265
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:08:14,133	WARNING util.py:136 -- The `process_trial` operation took 0.7452683448791504 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    265 |           7000.4 | 42874880 |  284.913 |              309.323 |              74.7778 |            794.434 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3071.994504477833
    time_step_min: 2917
  date: 2020-10-15_02-08-40
  done: false
  episode_len_mean: 794.4660145704671
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 284.98056467937386
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 54082
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0081031149917673e-36
        cur_lr: 5.0e-05
        entropy: 0.05336501418302456
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009572030442844456
        total_loss: .inf
        vf_explained_var: 0.9992785453796387
        vf_loss: 0.3527329663435618
    num_steps_sampled: 43036672
    num_steps_trained: 43036672
  iterations_since_restore: 266
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.98125
    gpu_util_percent0: 0.29781250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645993715410865
    mean_env_wait_ms: 1.2127284895505688
    mean_inference_ms: 4.291832434746281
    mean_raw_obs_processing_ms: 0.3785861532827588
  time_since_restore: 7026.971448898315
  time_this_iter_s: 26.573450326919556
  time_total_s: 7026.971448898315
  timers:
    learn_throughput: 8347.169
    learn_time_ms: 19382.859
    sample_throughput: 23855.078
    sample_time_ms: 6782.288
    update_time_ms: 31.841
  timestamp: 1602727720
  timesteps_since_restore: 0
  timesteps_total: 43036672
  training_iteration: 266
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:08:41,791	WARNING util.py:136 -- The `process_trial` operation took 0.7220458984375 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    266 |          7026.97 | 43036672 |  284.981 |              309.323 |              74.7778 |            794.466 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3071.4757614283344
    time_step_min: 2917
  date: 2020-10-15_02-09-08
  done: false
  episode_len_mean: 794.5021634659645
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.0565677767902
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 229
  episodes_total: 54311
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.012154672487651e-36
        cur_lr: 5.0e-05
        entropy: 0.054307615074018635
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00608379888581112
        total_loss: .inf
        vf_explained_var: 0.9994283318519592
        vf_loss: 0.304606889684995
    num_steps_sampled: 43198464
    num_steps_trained: 43198464
  iterations_since_restore: 267
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.045161290322575
    gpu_util_percent0: 0.29096774193548386
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464577378660903
    mean_env_wait_ms: 1.2126477902844317
    mean_inference_ms: 4.291715515358335
    mean_raw_obs_processing_ms: 0.3785794956494125
  time_since_restore: 7053.720048427582
  time_this_iter_s: 26.748599529266357
  time_total_s: 7053.720048427582
  timers:
    learn_throughput: 8341.818
    learn_time_ms: 19395.293
    sample_throughput: 23836.295
    sample_time_ms: 6787.632
    update_time_ms: 34.124
  timestamp: 1602727748
  timesteps_since_restore: 0
  timesteps_total: 43198464
  training_iteration: 267
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:09:09,674	WARNING util.py:136 -- The `process_trial` operation took 0.7791144847869873 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    267 |          7053.72 | 43198464 |  285.057 |              309.323 |              74.7778 |            794.502 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3071.0521454932705
    time_step_min: 2917
  date: 2020-10-15_02-09-36
  done: false
  episode_len_mean: 794.5370543659749
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.1202596783045
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 190
  episodes_total: 54501
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.518232008731477e-36
        cur_lr: 5.0e-05
        entropy: 0.05432743734369675
        entropy_coeff: 0.0005000000000000001
        kl: 0.006382687909839054
        model: {}
        policy_loss: -0.008830250648315996
        total_loss: 0.2332798739274343
        vf_explained_var: 0.999553382396698
        vf_loss: 0.2421372818450133
    num_steps_sampled: 43360256
    num_steps_trained: 43360256
  iterations_since_restore: 268
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.471875
    gpu_util_percent0: 0.3496875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464559958195749
    mean_env_wait_ms: 1.21258257700109
    mean_inference_ms: 4.291615902549477
    mean_raw_obs_processing_ms: 0.37857445209186463
  time_since_restore: 7080.09681224823
  time_this_iter_s: 26.376763820648193
  time_total_s: 7080.09681224823
  timers:
    learn_throughput: 8362.972
    learn_time_ms: 19346.233
    sample_throughput: 23814.86
    sample_time_ms: 6793.741
    update_time_ms: 30.891
  timestamp: 1602727776
  timesteps_since_restore: 0
  timesteps_total: 43360256
  training_iteration: 268
  trial_id: 9f737_00000
  
2020-10-15 02:09:37,221	WARNING util.py:136 -- The `process_trial` operation took 0.8432362079620361 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    268 |           7080.1 | 43360256 |   285.12 |              309.323 |              74.7778 |            794.537 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3070.6596819354722
    time_step_min: 2917
  date: 2020-10-15_02-10-03
  done: false
  episode_len_mean: 794.5705089519212
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.17781424271794
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 180
  episodes_total: 54681
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.518232008731477e-36
        cur_lr: 5.0e-05
        entropy: 0.056491303257644176
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008856914675561711
        total_loss: .inf
        vf_explained_var: 0.9994174838066101
        vf_loss: 0.28255702555179596
    num_steps_sampled: 43522048
    num_steps_trained: 43522048
  iterations_since_restore: 269
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.164516129032258
    gpu_util_percent0: 0.3632258064516129
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645419334711576
    mean_env_wait_ms: 1.2125198672455
    mean_inference_ms: 4.291520898456844
    mean_raw_obs_processing_ms: 0.37856998639329414
  time_since_restore: 7106.74195766449
  time_this_iter_s: 26.645145416259766
  time_total_s: 7106.74195766449
  timers:
    learn_throughput: 8369.349
    learn_time_ms: 19331.492
    sample_throughput: 23783.843
    sample_time_ms: 6802.601
    update_time_ms: 29.252
  timestamp: 1602727803
  timesteps_since_restore: 0
  timesteps_total: 43522048
  training_iteration: 269
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:10:04,966	WARNING util.py:136 -- The `process_trial` operation took 0.8169207572937012 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    269 |          7106.74 | 43522048 |  285.178 |              309.323 |              74.7778 |            794.571 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3070.194913863823
    time_step_min: 2917
  date: 2020-10-15_02-10-31
  done: false
  episode_len_mean: 794.6059060353779
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.24598065621507
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 212
  episodes_total: 54893
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.777348013097215e-36
        cur_lr: 5.0e-05
        entropy: 0.058352661008636154
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007419189768067251
        total_loss: .inf
        vf_explained_var: 0.999271810054779
        vf_loss: 0.37180932114521664
    num_steps_sampled: 43683840
    num_steps_trained: 43683840
  iterations_since_restore: 270
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.4
    gpu_util_percent0: 0.3275
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14645322039158834
    mean_env_wait_ms: 1.2124498307636038
    mean_inference_ms: 4.291416154720282
    mean_raw_obs_processing_ms: 0.37856497500417874
  time_since_restore: 7133.5731019973755
  time_this_iter_s: 26.831144332885742
  time_total_s: 7133.5731019973755
  timers:
    learn_throughput: 8369.069
    learn_time_ms: 19332.138
    sample_throughput: 23714.608
    sample_time_ms: 6822.461
    update_time_ms: 28.802
  timestamp: 1602727831
  timesteps_since_restore: 0
  timesteps_total: 43683840
  training_iteration: 270
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:10:32,809	WARNING util.py:136 -- The `process_trial` operation took 0.7547626495361328 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    270 |          7133.57 | 43683840 |  285.246 |              309.323 |              74.7778 |            794.606 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3069.7338296713274
    time_step_min: 2917
  date: 2020-10-15_02-10-59
  done: false
  episode_len_mean: 794.6456049938303
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.3165160349961
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 215
  episodes_total: 55108
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0166022019645822e-35
        cur_lr: 5.0e-05
        entropy: 0.0548128942027688
        entropy_coeff: 0.0005000000000000001
        kl: 0.0057091137471919256
        model: {}
        policy_loss: -0.006514176183069746
        total_loss: 0.16652228807409605
        vf_explained_var: 0.9996638298034668
        vf_loss: 0.17306387424468994
    num_steps_sampled: 43845632
    num_steps_trained: 43845632
  iterations_since_restore: 271
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.10625
    gpu_util_percent0: 0.2884375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464504987387405
    mean_env_wait_ms: 1.2123714339574099
    mean_inference_ms: 4.29130442455399
    mean_raw_obs_processing_ms: 0.3785584069688383
  time_since_restore: 7160.1585195064545
  time_this_iter_s: 26.58541750907898
  time_total_s: 7160.1585195064545
  timers:
    learn_throughput: 8368.883
    learn_time_ms: 19332.569
    sample_throughput: 23724.728
    sample_time_ms: 6819.551
    update_time_ms: 29.473
  timestamp: 1602727859
  timesteps_since_restore: 0
  timesteps_total: 43845632
  training_iteration: 271
  trial_id: 9f737_00000
  
2020-10-15 02:11:00,624	WARNING util.py:136 -- The `process_trial` operation took 0.78806471824646 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    271 |          7160.16 | 43845632 |  285.317 |              309.323 |              74.7778 |            794.646 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3069.3554300812566
    time_step_min: 2917
  date: 2020-10-15_02-11-27
  done: false
  episode_len_mean: 794.6754317750249
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.37423972978723
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 187
  episodes_total: 55295
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0166022019645822e-35
        cur_lr: 5.0e-05
        entropy: 0.05760692246258259
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007712428555047761
        total_loss: .inf
        vf_explained_var: 0.999176561832428
        vf_loss: 0.38873159637053806
    num_steps_sampled: 44007424
    num_steps_trained: 44007424
  iterations_since_restore: 272
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.932258064516137
    gpu_util_percent0: 0.3729032258064516
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644899608115042
    mean_env_wait_ms: 1.2123069839815075
    mean_inference_ms: 4.2912111441062235
    mean_raw_obs_processing_ms: 0.3785538632376088
  time_since_restore: 7186.543378353119
  time_this_iter_s: 26.38485884666443
  time_total_s: 7186.543378353119
  timers:
    learn_throughput: 8365.778
    learn_time_ms: 19339.742
    sample_throughput: 23729.794
    sample_time_ms: 6818.095
    update_time_ms: 28.957
  timestamp: 1602727887
  timesteps_since_restore: 0
  timesteps_total: 44007424
  training_iteration: 272
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:11:28,077	WARNING util.py:136 -- The `process_trial` operation took 0.8156158924102783 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    272 |          7186.54 | 44007424 |  285.374 |              309.323 |              74.7778 |            794.675 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3068.9760861332034
    time_step_min: 2917
  date: 2020-10-15_02-11-54
  done: false
  episode_len_mean: 794.7097518337629
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.4353219145152
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 55487
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5249033029468732e-35
        cur_lr: 5.0e-05
        entropy: 0.05428362482537826
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006135753957399477
        total_loss: .inf
        vf_explained_var: 0.9995468258857727
        vf_loss: 0.22612930089235306
    num_steps_sampled: 44169216
    num_steps_trained: 44169216
  iterations_since_restore: 273
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.267741935483873
    gpu_util_percent0: 0.405483870967742
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464473107279127
    mean_env_wait_ms: 1.2122415887303026
    mean_inference_ms: 4.291113637850723
    mean_raw_obs_processing_ms: 0.3785495527672211
  time_since_restore: 7212.8412845134735
  time_this_iter_s: 26.297906160354614
  time_total_s: 7212.8412845134735
  timers:
    learn_throughput: 8368.262
    learn_time_ms: 19334.003
    sample_throughput: 23731.217
    sample_time_ms: 6817.687
    update_time_ms: 29.11
  timestamp: 1602727914
  timesteps_since_restore: 0
  timesteps_total: 44169216
  training_iteration: 273
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:11:55,394	WARNING util.py:136 -- The `process_trial` operation took 0.7654330730438232 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    273 |          7212.84 | 44169216 |  285.435 |              309.323 |              74.7778 |             794.71 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3068.5535755986275
    time_step_min: 2917
  date: 2020-10-15_02-12-21
  done: false
  episode_len_mean: 794.7412533433859
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.49605158157044
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 220
  episodes_total: 55707
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2873549544203103e-35
        cur_lr: 5.0e-05
        entropy: 0.06333496297399203
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010084113474780073
        total_loss: .inf
        vf_explained_var: 0.998329222202301
        vf_loss: 0.894017348686854
    num_steps_sampled: 44331008
    num_steps_trained: 44331008
  iterations_since_restore: 274
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.961290322580645
    gpu_util_percent0: 0.370967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644603773991616
    mean_env_wait_ms: 1.2121664438206408
    mean_inference_ms: 4.291011286199785
    mean_raw_obs_processing_ms: 0.378544262204007
  time_since_restore: 7239.007628917694
  time_this_iter_s: 26.16634440422058
  time_total_s: 7239.007628917694
  timers:
    learn_throughput: 8365.588
    learn_time_ms: 19340.183
    sample_throughput: 23776.615
    sample_time_ms: 6804.669
    update_time_ms: 29.453
  timestamp: 1602727941
  timesteps_since_restore: 0
  timesteps_total: 44331008
  training_iteration: 274
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:12:22,629	WARNING util.py:136 -- The `process_trial` operation took 0.805924654006958 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    274 |          7239.01 | 44331008 |  285.496 |              309.323 |              74.7778 |            794.741 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3068.1644205816556
    time_step_min: 2917
  date: 2020-10-15_02-12-49
  done: false
  episode_len_mean: 794.7722533221255
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.5533685359307
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 206
  episodes_total: 55913
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.431032431630465e-35
        cur_lr: 5.0e-05
        entropy: 0.059990561567246914
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007244446237261097
        total_loss: .inf
        vf_explained_var: 0.9986424446105957
        vf_loss: 0.6869809329509735
    num_steps_sampled: 44492800
    num_steps_trained: 44492800
  iterations_since_restore: 275
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.1875
    gpu_util_percent0: 0.31781250000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464440583128968
    mean_env_wait_ms: 1.2120918996486951
    mean_inference_ms: 4.290906491832313
    mean_raw_obs_processing_ms: 0.37853812960325856
  time_since_restore: 7265.839691400528
  time_this_iter_s: 26.832062482833862
  time_total_s: 7265.839691400528
  timers:
    learn_throughput: 8343.458
    learn_time_ms: 19391.481
    sample_throughput: 23747.395
    sample_time_ms: 6813.042
    update_time_ms: 31.752
  timestamp: 1602727969
  timesteps_since_restore: 0
  timesteps_total: 44492800
  training_iteration: 275
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:12:50,505	WARNING util.py:136 -- The `process_trial` operation took 0.7811274528503418 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    275 |          7265.84 | 44492800 |  285.553 |              309.323 |              74.7778 |            794.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3067.8391637083682
    time_step_min: 2917
  date: 2020-10-15_02-13-17
  done: false
  episode_len_mean: 794.7953115250914
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.6046677668926
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 56095
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.146548647445697e-35
        cur_lr: 5.0e-05
        entropy: 0.059398689617713295
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009020907627321625
        total_loss: .inf
        vf_explained_var: 0.9986433386802673
        vf_loss: 0.6594372888406118
    num_steps_sampled: 44654592
    num_steps_trained: 44654592
  iterations_since_restore: 276
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.60322580645162
    gpu_util_percent0: 0.2967741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14644218933414313
    mean_env_wait_ms: 1.212029039126064
    mean_inference_ms: 4.290819166007676
    mean_raw_obs_processing_ms: 0.37853386328586336
  time_since_restore: 7292.334473371506
  time_this_iter_s: 26.494781970977783
  time_total_s: 7292.334473371506
  timers:
    learn_throughput: 8345.889
    learn_time_ms: 19385.833
    sample_throughput: 23741.052
    sample_time_ms: 6814.862
    update_time_ms: 31.938
  timestamp: 1602727997
  timesteps_since_restore: 0
  timesteps_total: 44654592
  training_iteration: 276
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:13:18,137	WARNING util.py:136 -- The `process_trial` operation took 0.7751777172088623 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    276 |          7292.33 | 44654592 |  285.605 |              309.323 |              74.7778 |            794.795 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3067.4832912652423
    time_step_min: 2917
  date: 2020-10-15_02-13-44
  done: false
  episode_len_mean: 794.8232556487139
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.6623876967773
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 56296
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.719822971168548e-35
        cur_lr: 5.0e-05
        entropy: 0.06120381069680055
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007615926180733368
        total_loss: .inf
        vf_explained_var: 0.9990870952606201
        vf_loss: 0.46862587581078213
    num_steps_sampled: 44816384
    num_steps_trained: 44816384
  iterations_since_restore: 277
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.684375000000003
    gpu_util_percent0: 0.2915625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464408489397647
    mean_env_wait_ms: 1.2119603735246003
    mean_inference_ms: 4.290717355670483
    mean_raw_obs_processing_ms: 0.3785289539791339
  time_since_restore: 7318.661640405655
  time_this_iter_s: 26.32716703414917
  time_total_s: 7318.661640405655
  timers:
    learn_throughput: 8362.426
    learn_time_ms: 19347.497
    sample_throughput: 23745.134
    sample_time_ms: 6813.691
    update_time_ms: 30.152
  timestamp: 1602728024
  timesteps_since_restore: 0
  timesteps_total: 44816384
  training_iteration: 277
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:13:45,705	WARNING util.py:136 -- The `process_trial` operation took 0.8134984970092773 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    277 |          7318.66 | 44816384 |  285.662 |              309.323 |              74.7778 |            794.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3067.1234775527546
    time_step_min: 2917
  date: 2020-10-15_02-14-12
  done: false
  episode_len_mean: 794.8377030039273
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.7098347877458
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 230
  episodes_total: 56526
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1579734456752821e-34
        cur_lr: 5.0e-05
        entropy: 0.085781825085481
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01107660685859931
        total_loss: .inf
        vf_explained_var: 0.9971111416816711
        vf_loss: 1.5574995080629985
    num_steps_sampled: 44978176
    num_steps_trained: 44978176
  iterations_since_restore: 278
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.590322580645157
    gpu_util_percent0: 0.3267741935483871
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464385449776603
    mean_env_wait_ms: 1.2118790779040625
    mean_inference_ms: 4.290607650853711
    mean_raw_obs_processing_ms: 0.37852302956384865
  time_since_restore: 7345.300649404526
  time_this_iter_s: 26.63900899887085
  time_total_s: 7345.300649404526
  timers:
    learn_throughput: 8348.811
    learn_time_ms: 19379.046
    sample_throughput: 23745.965
    sample_time_ms: 6813.452
    update_time_ms: 32.063
  timestamp: 1602728052
  timesteps_since_restore: 0
  timesteps_total: 44978176
  training_iteration: 278
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:14:13,408	WARNING util.py:136 -- The `process_trial` operation took 0.801053524017334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    278 |           7345.3 | 44978176 |   285.71 |              309.323 |              74.7778 |            794.838 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3066.9814210349878
    time_step_min: 2917
  date: 2020-10-15_02-14-39
  done: false
  episode_len_mean: 794.8379617385172
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.7233375810472
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 56715
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7369601685129233e-34
        cur_lr: 5.0e-05
        entropy: 0.08233312082787354
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009723865368869156
        total_loss: .inf
        vf_explained_var: 0.9952919483184814
        vf_loss: 2.518989900747935
    num_steps_sampled: 45139968
    num_steps_trained: 45139968
  iterations_since_restore: 279
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.667741935483875
    gpu_util_percent0: 0.31258064516129036
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643714851960418
    mean_env_wait_ms: 1.2118153733136126
    mean_inference_ms: 4.2905218767674205
    mean_raw_obs_processing_ms: 0.3785187700342368
  time_since_restore: 7371.708243608475
  time_this_iter_s: 26.407594203948975
  time_total_s: 7371.708243608475
  timers:
    learn_throughput: 8357.122
    learn_time_ms: 19359.775
    sample_throughput: 23761.298
    sample_time_ms: 6809.056
    update_time_ms: 31.947
  timestamp: 1602728079
  timesteps_since_restore: 0
  timesteps_total: 45139968
  training_iteration: 279
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:14:40,934	WARNING util.py:136 -- The `process_trial` operation took 0.781104326248169 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    279 |          7371.71 | 45139968 |  285.723 |              309.323 |              74.7778 |            794.838 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3066.7568717795402
    time_step_min: 2917
  date: 2020-10-15_02-15-07
  done: false
  episode_len_mean: 794.8551343561625
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.7622068739272
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 56901
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.6054402527693844e-34
        cur_lr: 5.0e-05
        entropy: 0.06369507902612288
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008125227022295197
        total_loss: .inf
        vf_explained_var: 0.9979148507118225
        vf_loss: 1.0362310359875362
    num_steps_sampled: 45301760
    num_steps_trained: 45301760
  iterations_since_restore: 280
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.75
    gpu_util_percent0: 0.29500000000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464353907151417
    mean_env_wait_ms: 1.2117520362611511
    mean_inference_ms: 4.290428958991743
    mean_raw_obs_processing_ms: 0.3785144840403098
  time_since_restore: 7398.168782949448
  time_this_iter_s: 26.4605393409729
  time_total_s: 7398.168782949448
  timers:
    learn_throughput: 8372.846
    learn_time_ms: 19323.417
    sample_throughput: 23797.287
    sample_time_ms: 6798.758
    update_time_ms: 33.726
  timestamp: 1602728107
  timesteps_since_restore: 0
  timesteps_total: 45301760
  training_iteration: 280
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:15:08,460	WARNING util.py:136 -- The `process_trial` operation took 0.7919821739196777 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    280 |          7398.17 | 45301760 |  285.762 |              309.323 |              74.7778 |            794.855 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3066.3524773985564
    time_step_min: 2917
  date: 2020-10-15_02-15-34
  done: false
  episode_len_mean: 794.8899219105648
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.8271198167196
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 213
  episodes_total: 57114
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9081603791540755e-34
        cur_lr: 5.0e-05
        entropy: 0.052666439985235534
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006376946568101023
        total_loss: .inf
        vf_explained_var: 0.9994368553161621
        vf_loss: 0.2966914276281993
    num_steps_sampled: 45463552
    num_steps_trained: 45463552
  iterations_since_restore: 281
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.551612903225806
    gpu_util_percent0: 0.3754838709677419
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643454031426556
    mean_env_wait_ms: 1.2116813095572059
    mean_inference_ms: 4.290333587566754
    mean_raw_obs_processing_ms: 0.3785103589903331
  time_since_restore: 7424.682363033295
  time_this_iter_s: 26.513580083847046
  time_total_s: 7424.682363033295
  timers:
    learn_throughput: 8378.335
    learn_time_ms: 19310.757
    sample_throughput: 23774.812
    sample_time_ms: 6805.185
    update_time_ms: 31.414
  timestamp: 1602728134
  timesteps_since_restore: 0
  timesteps_total: 45463552
  training_iteration: 281
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:15:36,066	WARNING util.py:136 -- The `process_trial` operation took 0.8018097877502441 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    281 |          7424.68 | 45463552 |  285.827 |              309.323 |              74.7778 |             794.89 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3065.9449778305343
    time_step_min: 2917
  date: 2020-10-15_02-16-02
  done: false
  episode_len_mean: 794.9248656758077
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.88835233219777
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 210
  episodes_total: 57324
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.862240568731114e-34
        cur_lr: 5.0e-05
        entropy: 0.05643254332244396
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006323957776961227
        total_loss: .inf
        vf_explained_var: 0.9989128112792969
        vf_loss: 0.548063687980175
    num_steps_sampled: 45625344
    num_steps_trained: 45625344
  iterations_since_restore: 282
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.674193548387098
    gpu_util_percent0: 0.34096774193548396
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643221455101782
    mean_env_wait_ms: 1.2116055969448938
    mean_inference_ms: 4.290235807451959
    mean_raw_obs_processing_ms: 0.37850407007206516
  time_since_restore: 7450.962708950043
  time_this_iter_s: 26.280345916748047
  time_total_s: 7450.962708950043
  timers:
    learn_throughput: 8383.073
    learn_time_ms: 19299.843
    sample_throughput: 23769.708
    sample_time_ms: 6806.647
    update_time_ms: 31.58
  timestamp: 1602728162
  timesteps_since_restore: 0
  timesteps_total: 45625344
  training_iteration: 282
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:16:03,406	WARNING util.py:136 -- The `process_trial` operation took 0.7973387241363525 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    282 |          7450.96 | 45625344 |  285.888 |              309.323 |              74.7778 |            794.925 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3065.5767825451508
    time_step_min: 2917
  date: 2020-10-15_02-16-30
  done: false
  episode_len_mean: 794.9579739880372
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 285.9439191258811
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 57512
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.79336085309667e-34
        cur_lr: 5.0e-05
        entropy: 0.05143563232074181
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00748752864698569
        total_loss: .inf
        vf_explained_var: 0.9994874596595764
        vf_loss: 0.23973059033354124
    num_steps_sampled: 45787136
    num_steps_trained: 45787136
  iterations_since_restore: 283
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.040625
    gpu_util_percent0: 0.2834375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14643079973061826
    mean_env_wait_ms: 1.2115415545213757
    mean_inference_ms: 4.290153476930678
    mean_raw_obs_processing_ms: 0.37850008685523906
  time_since_restore: 7477.564019918442
  time_this_iter_s: 26.601310968399048
  time_total_s: 7477.564019918442
  timers:
    learn_throughput: 8371.565
    learn_time_ms: 19326.374
    sample_throughput: 23768.404
    sample_time_ms: 6807.02
    update_time_ms: 33.797
  timestamp: 1602728190
  timesteps_since_restore: 0
  timesteps_total: 45787136
  training_iteration: 283
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:16:31,237	WARNING util.py:136 -- The `process_trial` operation took 0.7983107566833496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    283 |          7477.56 | 45787136 |  285.944 |              309.323 |              74.7778 |            794.958 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3065.190678480398
    time_step_min: 2917
  date: 2020-10-15_02-16-57
  done: false
  episode_len_mean: 794.9875933530869
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.00165033397866
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 57711
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3190041279645005e-33
        cur_lr: 5.0e-05
        entropy: 0.05597696484376987
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007602937473469258
        total_loss: .inf
        vf_explained_var: 0.999238908290863
        vf_loss: 0.37296853959560394
    num_steps_sampled: 45948928
    num_steps_trained: 45948928
  iterations_since_restore: 284
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.95161290322581
    gpu_util_percent0: 0.2809677419354839
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642942740900633
    mean_env_wait_ms: 1.2114740022898314
    mean_inference_ms: 4.290057860316252
    mean_raw_obs_processing_ms: 0.3784957670900466
  time_since_restore: 7504.016424655914
  time_this_iter_s: 26.452404737472534
  time_total_s: 7504.016424655914
  timers:
    learn_throughput: 8361.627
    learn_time_ms: 19349.344
    sample_throughput: 23755.322
    sample_time_ms: 6810.768
    update_time_ms: 34.037
  timestamp: 1602728217
  timesteps_since_restore: 0
  timesteps_total: 45948928
  training_iteration: 284
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:16:58,778	WARNING util.py:136 -- The `process_trial` operation took 0.8167507648468018 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    284 |          7504.02 | 45948928 |  286.002 |              309.323 |              74.7778 |            794.988 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3064.7706260364844
    time_step_min: 2917
  date: 2020-10-15_02-17-25
  done: false
  episode_len_mean: 795.0243414010979
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.0655650172965
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 215
  episodes_total: 57926
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9785061919467505e-33
        cur_lr: 5.0e-05
        entropy: 0.04880044267823299
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006387982642157415
        total_loss: .inf
        vf_explained_var: 0.9995811581611633
        vf_loss: 0.23517673338452974
    num_steps_sampled: 46110720
    num_steps_trained: 46110720
  iterations_since_restore: 285
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.36875
    gpu_util_percent0: 0.283125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464280830765808
    mean_env_wait_ms: 1.2114008657378679
    mean_inference_ms: 4.289968296175735
    mean_raw_obs_processing_ms: 0.3784911041540713
  time_since_restore: 7530.9571397304535
  time_this_iter_s: 26.940715074539185
  time_total_s: 7530.9571397304535
  timers:
    learn_throughput: 8368.595
    learn_time_ms: 19333.233
    sample_throughput: 23665.315
    sample_time_ms: 6836.672
    update_time_ms: 34.311
  timestamp: 1602728245
  timesteps_since_restore: 0
  timesteps_total: 46110720
  training_iteration: 285
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:17:26,787	WARNING util.py:136 -- The `process_trial` operation took 0.7984857559204102 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    285 |          7530.96 | 46110720 |  286.066 |              309.323 |              74.7778 |            795.024 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3064.386107763815
    time_step_min: 2917
  date: 2020-10-15_02-17-53
  done: false
  episode_len_mean: 795.0601259289843
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.1259519221947
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 58128
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9677592879201255e-33
        cur_lr: 5.0e-05
        entropy: 0.05085622643431028
        entropy_coeff: 0.0005000000000000001
        kl: 0.006161314435303211
        model: {}
        policy_loss: -0.008829273991674805
        total_loss: 0.2642391696572304
        vf_explained_var: 0.9994640350341797
        vf_loss: 0.273093876739343
    num_steps_sampled: 46272512
    num_steps_trained: 46272512
  iterations_since_restore: 286
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.822580645161292
    gpu_util_percent0: 0.3119354838709677
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642614170359333
    mean_env_wait_ms: 1.2113300522940287
    mean_inference_ms: 4.28987332675994
    mean_raw_obs_processing_ms: 0.3784859718829365
  time_since_restore: 7557.358961820602
  time_this_iter_s: 26.401822090148926
  time_total_s: 7557.358961820602
  timers:
    learn_throughput: 8369.735
    learn_time_ms: 19330.6
    sample_throughput: 23684.6
    sample_time_ms: 6831.105
    update_time_ms: 32.692
  timestamp: 1602728273
  timesteps_since_restore: 0
  timesteps_total: 46272512
  training_iteration: 286
  trial_id: 9f737_00000
  
2020-10-15 02:17:54,338	WARNING util.py:136 -- The `process_trial` operation took 0.7921926975250244 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    286 |          7557.36 | 46272512 |  286.126 |              309.323 |              74.7778 |             795.06 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3064.043534963535
    time_step_min: 2917
  date: 2020-10-15_02-18-21
  done: false
  episode_len_mean: 795.0890538987876
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.17904284211966
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 58313
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9677592879201255e-33
        cur_lr: 5.0e-05
        entropy: 0.055525547514359154
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007168952327144022
        total_loss: .inf
        vf_explained_var: 0.9993581175804138
        vf_loss: 0.297407902777195
    num_steps_sampled: 46434304
    num_steps_trained: 46434304
  iterations_since_restore: 287
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6875
    gpu_util_percent0: 0.265
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464246713059101
    mean_env_wait_ms: 1.2112663473064575
    mean_inference_ms: 4.289794445545506
    mean_raw_obs_processing_ms: 0.37848206421381203
  time_since_restore: 7584.107618570328
  time_this_iter_s: 26.748656749725342
  time_total_s: 7584.107618570328
  timers:
    learn_throughput: 8354.719
    learn_time_ms: 19365.342
    sample_throughput: 23664.266
    sample_time_ms: 6836.975
    update_time_ms: 32.28
  timestamp: 1602728301
  timesteps_since_restore: 0
  timesteps_total: 46434304
  training_iteration: 287
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:18:22,215	WARNING util.py:136 -- The `process_trial` operation took 0.8256709575653076 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    287 |          7584.11 | 46434304 |  286.179 |              309.323 |              74.7778 |            795.089 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3063.6450068399454
    time_step_min: 2917
  date: 2020-10-15_02-18-48
  done: false
  episode_len_mean: 795.1227998222769
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.23883042461944
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 205
  episodes_total: 58518
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.451638931880189e-33
        cur_lr: 5.0e-05
        entropy: 0.05919317062944174
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010667012807971332
        total_loss: .inf
        vf_explained_var: 0.9995005130767822
        vf_loss: 0.2647930371264617
    num_steps_sampled: 46596096
    num_steps_trained: 46596096
  iterations_since_restore: 288
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.577419354838717
    gpu_util_percent0: 0.2503225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642364801079094
    mean_env_wait_ms: 1.211198713959363
    mean_inference_ms: 4.289701273866926
    mean_raw_obs_processing_ms: 0.37847806277337537
  time_since_restore: 7610.473515748978
  time_this_iter_s: 26.365897178649902
  time_total_s: 7610.473515748978
  timers:
    learn_throughput: 8365.956
    learn_time_ms: 19339.332
    sample_throughput: 23671.699
    sample_time_ms: 6834.828
    update_time_ms: 32.164
  timestamp: 1602728328
  timesteps_since_restore: 0
  timesteps_total: 46596096
  training_iteration: 288
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:18:49,780	WARNING util.py:136 -- The `process_trial` operation took 0.8098080158233643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    288 |          7610.47 | 46596096 |  286.239 |              309.323 |              74.7778 |            795.123 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3063.266767747321
    time_step_min: 2917
  date: 2020-10-15_02-19-16
  done: false
  episode_len_mean: 795.1562047772273
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.2973919180569
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 58737
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.677458397820282e-33
        cur_lr: 5.0e-05
        entropy: 0.06188429519534111
        entropy_coeff: 0.0005000000000000001
        kl: 0.005383455504973729
        model: {}
        policy_loss: -0.00939125105999968
        total_loss: 0.5557838305830956
        vf_explained_var: 0.9990043640136719
        vf_loss: 0.5652060409386953
    num_steps_sampled: 46757888
    num_steps_trained: 46757888
  iterations_since_restore: 289
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.678125
    gpu_util_percent0: 0.31375000000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14642134553436792
    mean_env_wait_ms: 1.2111208878707165
    mean_inference_ms: 4.289604719934704
    mean_raw_obs_processing_ms: 0.37847220599519155
  time_since_restore: 7636.961549520493
  time_this_iter_s: 26.488033771514893
  time_total_s: 7636.961549520493
  timers:
    learn_throughput: 8367.731
    learn_time_ms: 19335.229
    sample_throughput: 23674.339
    sample_time_ms: 6834.066
    update_time_ms: 32.91
  timestamp: 1602728356
  timesteps_since_restore: 0
  timesteps_total: 46757888
  training_iteration: 289
  trial_id: 9f737_00000
  
2020-10-15 02:19:17,413	WARNING util.py:136 -- The `process_trial` operation took 0.8390543460845947 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    289 |          7636.96 | 46757888 |  286.297 |              309.323 |              74.7778 |            795.156 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3062.9017150619798
    time_step_min: 2917
  date: 2020-10-15_02-19-43
  done: false
  episode_len_mean: 795.1845981536791
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.35155245092795
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 58928
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.677458397820282e-33
        cur_lr: 5.0e-05
        entropy: 0.06091508797059456
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010418010778569927
        total_loss: .inf
        vf_explained_var: 0.9993415474891663
        vf_loss: 0.31744860112667084
    num_steps_sampled: 46919680
    num_steps_trained: 46919680
  iterations_since_restore: 290
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.958064516129028
    gpu_util_percent0: 0.37516129032258067
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464198336490376
    mean_env_wait_ms: 1.2110551536829643
    mean_inference_ms: 4.289523020501196
    mean_raw_obs_processing_ms: 0.3784680473326269
  time_since_restore: 7663.436020851135
  time_this_iter_s: 26.4744713306427
  time_total_s: 7663.436020851135
  timers:
    learn_throughput: 8355.996
    learn_time_ms: 19362.384
    sample_throughput: 23736.122
    sample_time_ms: 6816.278
    update_time_ms: 31.229
  timestamp: 1602728383
  timesteps_since_restore: 0
  timesteps_total: 46919680
  training_iteration: 290
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:19:45,045	WARNING util.py:136 -- The `process_trial` operation took 0.7878386974334717 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    290 |          7663.44 | 46919680 |  286.352 |              309.323 |              74.7778 |            795.185 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3062.556411341515
    time_step_min: 2917
  date: 2020-10-15_02-20-11
  done: false
  episode_len_mean: 795.2121529951111
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.4039852109989
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 59113
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0016187596730427e-32
        cur_lr: 5.0e-05
        entropy: 0.060146536057194076
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00718425668310374
        total_loss: .inf
        vf_explained_var: 0.9995034337043762
        vf_loss: 0.2600071442623933
    num_steps_sampled: 47081472
    num_steps_trained: 47081472
  iterations_since_restore: 291
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.96875
    gpu_util_percent0: 0.284375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641829898721934
    mean_env_wait_ms: 1.2109923585780757
    mean_inference_ms: 4.289438634466435
    mean_raw_obs_processing_ms: 0.37846432090365417
  time_since_restore: 7689.988011837006
  time_this_iter_s: 26.55199098587036
  time_total_s: 7689.988011837006
  timers:
    learn_throughput: 8356.967
    learn_time_ms: 19360.133
    sample_throughput: 23748.115
    sample_time_ms: 6812.835
    update_time_ms: 31.987
  timestamp: 1602728411
  timesteps_since_restore: 0
  timesteps_total: 47081472
  training_iteration: 291
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:20:12,697	WARNING util.py:136 -- The `process_trial` operation took 0.8089425563812256 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    291 |          7689.99 | 47081472 |  286.404 |              309.323 |              74.7778 |            795.212 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3062.171776968225
    time_step_min: 2917
  date: 2020-10-15_02-20-39
  done: false
  episode_len_mean: 795.2352941176471
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.45790281033834
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 217
  episodes_total: 59330
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5024281395095634e-32
        cur_lr: 5.0e-05
        entropy: 0.06867837843795617
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010024837761496505
        total_loss: .inf
        vf_explained_var: 0.9986957907676697
        vf_loss: 0.7228539486726125
    num_steps_sampled: 47243264
    num_steps_trained: 47243264
  iterations_since_restore: 292
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7483870967742
    gpu_util_percent0: 0.3683870967741936
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641760035270476
    mean_env_wait_ms: 1.2109204144874965
    mean_inference_ms: 4.289357294240091
    mean_raw_obs_processing_ms: 0.3784605630893871
  time_since_restore: 7716.451936006546
  time_this_iter_s: 26.463924169540405
  time_total_s: 7716.451936006546
  timers:
    learn_throughput: 8350.757
    learn_time_ms: 19374.53
    sample_throughput: 23733.315
    sample_time_ms: 6817.084
    update_time_ms: 29.735
  timestamp: 1602728439
  timesteps_since_restore: 0
  timesteps_total: 47243264
  training_iteration: 292
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:20:40,279	WARNING util.py:136 -- The `process_trial` operation took 0.8386406898498535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    292 |          7716.45 | 47243264 |  286.458 |              309.323 |              74.7778 |            795.235 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3061.8484924876475
    time_step_min: 2917
  date: 2020-10-15_02-21-06
  done: false
  episode_len_mean: 795.2534094726235
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.5076716102916
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 210
  episodes_total: 59540
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2536422092643458e-32
        cur_lr: 5.0e-05
        entropy: 0.06324220386644204
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010067535273265094
        total_loss: .inf
        vf_explained_var: 0.999196469783783
        vf_loss: 0.4034599761168162
    num_steps_sampled: 47405056
    num_steps_trained: 47405056
  iterations_since_restore: 293
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.945161290322588
    gpu_util_percent0: 0.29161290322580646
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641561981413787
    mean_env_wait_ms: 1.2108453782278144
    mean_inference_ms: 4.2892611934627
    mean_raw_obs_processing_ms: 0.3784545649069082
  time_since_restore: 7742.914528369904
  time_this_iter_s: 26.462592363357544
  time_total_s: 7742.914528369904
  timers:
    learn_throughput: 8354.313
    learn_time_ms: 19366.285
    sample_throughput: 23749.909
    sample_time_ms: 6812.321
    update_time_ms: 27.455
  timestamp: 1602728466
  timesteps_since_restore: 0
  timesteps_total: 47405056
  training_iteration: 293
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:21:08,003	WARNING util.py:136 -- The `process_trial` operation took 0.8836627006530762 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    293 |          7742.91 | 47405056 |  286.508 |              309.323 |              74.7778 |            795.253 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3061.52015412967
    time_step_min: 2917
  date: 2020-10-15_02-21-34
  done: false
  episode_len_mean: 795.2785795606751
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.56041969385797
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 59728
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.3804633138965186e-32
        cur_lr: 5.0e-05
        entropy: 0.06116164103150368
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00750797596507861
        total_loss: .inf
        vf_explained_var: 0.9992713928222656
        vf_loss: 0.3486984396974246
    num_steps_sampled: 47566848
    num_steps_trained: 47566848
  iterations_since_restore: 294
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.734375
    gpu_util_percent0: 0.31
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1464139565489776
    mean_env_wait_ms: 1.2107818401442196
    mean_inference_ms: 4.289185530693497
    mean_raw_obs_processing_ms: 0.3784512140490277
  time_since_restore: 7769.592625379562
  time_this_iter_s: 26.678097009658813
  time_total_s: 7769.592625379562
  timers:
    learn_throughput: 8349.224
    learn_time_ms: 19378.088
    sample_throughput: 23713.883
    sample_time_ms: 6822.67
    update_time_ms: 27.487
  timestamp: 1602728494
  timesteps_since_restore: 0
  timesteps_total: 47566848
  training_iteration: 294
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:21:35,824	WARNING util.py:136 -- The `process_trial` operation took 0.8428158760070801 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    294 |          7769.59 | 47566848 |   286.56 |              309.323 |              74.7778 |            795.279 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3061.189690583933
    time_step_min: 2917
  date: 2020-10-15_02-22-02
  done: false
  episode_len_mean: 795.2993742177722
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.6118329730345
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 197
  episodes_total: 59925
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.070694970844778e-32
        cur_lr: 5.0e-05
        entropy: 0.06468563651045163
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008141912568438178
        total_loss: .inf
        vf_explained_var: 0.999136745929718
        vf_loss: 0.4306246464451154
    num_steps_sampled: 47728640
    num_steps_trained: 47728640
  iterations_since_restore: 295
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.175
    gpu_util_percent0: 0.29312499999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641284766093637
    mean_env_wait_ms: 1.21071560994226
    mean_inference_ms: 4.289098539676768
    mean_raw_obs_processing_ms: 0.3784473637931813
  time_since_restore: 7795.93236374855
  time_this_iter_s: 26.339738368988037
  time_total_s: 7795.93236374855
  timers:
    learn_throughput: 8361.002
    learn_time_ms: 19350.791
    sample_throughput: 23833.956
    sample_time_ms: 6788.298
    update_time_ms: 27.258
  timestamp: 1602728522
  timesteps_since_restore: 0
  timesteps_total: 47728640
  training_iteration: 295
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:22:03,515	WARNING util.py:136 -- The `process_trial` operation took 0.8472518920898438 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    295 |          7795.93 | 47728640 |  286.612 |              309.323 |              74.7778 |            795.299 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3060.7874064215603
    time_step_min: 2917
  date: 2020-10-15_02-22-29
  done: false
  episode_len_mean: 795.3270100418966
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.67123099721016
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 60148
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.606042456267165e-32
        cur_lr: 5.0e-05
        entropy: 0.06454741396009922
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008739075166279994
        total_loss: .inf
        vf_explained_var: 0.9988684058189392
        vf_loss: 0.6038772960503896
    num_steps_sampled: 47890432
    num_steps_trained: 47890432
  iterations_since_restore: 296
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.638709677419353
    gpu_util_percent0: 0.33999999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14641118785686547
    mean_env_wait_ms: 1.210639237007587
    mean_inference_ms: 4.289013254667197
    mean_raw_obs_processing_ms: 0.37844243079887363
  time_since_restore: 7822.347942352295
  time_this_iter_s: 26.415578603744507
  time_total_s: 7822.347942352295
  timers:
    learn_throughput: 8365.922
    learn_time_ms: 19339.41
    sample_throughput: 23798.741
    sample_time_ms: 6798.343
    update_time_ms: 28.952
  timestamp: 1602728549
  timesteps_since_restore: 0
  timesteps_total: 47890432
  training_iteration: 296
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:22:31,049	WARNING util.py:136 -- The `process_trial` operation took 0.8332695960998535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    296 |          7822.35 | 47890432 |  286.671 |              309.323 |              74.7778 |            795.327 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3060.4651012387444
    time_step_min: 2917
  date: 2020-10-15_02-22-57
  done: false
  episode_len_mean: 795.3474254652724
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.72037372113607
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 193
  episodes_total: 60341
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1409063684400748e-31
        cur_lr: 5.0e-05
        entropy: 0.06125227486093839
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006847026287384021
        total_loss: .inf
        vf_explained_var: 0.9993037581443787
        vf_loss: 0.33546893298625946
    num_steps_sampled: 48052224
    num_steps_trained: 48052224
  iterations_since_restore: 297
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3
    gpu_util_percent0: 0.3341935483870968
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640987354033794
    mean_env_wait_ms: 1.210572634592698
    mean_inference_ms: 4.288931590339968
    mean_raw_obs_processing_ms: 0.378438209842113
  time_since_restore: 7848.6630601882935
  time_this_iter_s: 26.315117835998535
  time_total_s: 7848.6630601882935
  timers:
    learn_throughput: 8383.025
    learn_time_ms: 19299.954
    sample_throughput: 23830.858
    sample_time_ms: 6789.181
    update_time_ms: 30.803
  timestamp: 1602728577
  timesteps_since_restore: 0
  timesteps_total: 48052224
  training_iteration: 297
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:22:58,536	WARNING util.py:136 -- The `process_trial` operation took 0.8492894172668457 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    297 |          7848.66 | 48052224 |   286.72 |              309.323 |              74.7778 |            795.347 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3060.136464325861
    time_step_min: 2917
  date: 2020-10-15_02-23-24
  done: false
  episode_len_mean: 795.3690731868495
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.7687439403117
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 60530
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.7113595526601124e-31
        cur_lr: 5.0e-05
        entropy: 0.06398554642995198
        entropy_coeff: 0.0005000000000000001
        kl: 0.004049557241766403
        model: {}
        policy_loss: -0.010001787794559883
        total_loss: 0.5647018601497015
        vf_explained_var: 0.9988035559654236
        vf_loss: 0.574735646446546
    num_steps_sampled: 48214016
    num_steps_trained: 48214016
  iterations_since_restore: 298
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.835483870967746
    gpu_util_percent0: 0.30580645161290326
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.870967741935485
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640825944163904
    mean_env_wait_ms: 1.2105090892667536
    mean_inference_ms: 4.288856640888194
    mean_raw_obs_processing_ms: 0.3784348951378183
  time_since_restore: 7874.966199874878
  time_this_iter_s: 26.303139686584473
  time_total_s: 7874.966199874878
  timers:
    learn_throughput: 8383.278
    learn_time_ms: 19299.371
    sample_throughput: 23854.412
    sample_time_ms: 6782.477
    update_time_ms: 29.194
  timestamp: 1602728604
  timesteps_since_restore: 0
  timesteps_total: 48214016
  training_iteration: 298
  trial_id: 9f737_00000
  
2020-10-15 02:23:26,089	WARNING util.py:136 -- The `process_trial` operation took 0.8256278038024902 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    298 |          7874.97 | 48214016 |  286.769 |              309.323 |              74.7778 |            795.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3059.7966886326194
    time_step_min: 2917
  date: 2020-10-15_02-23-52
  done: false
  episode_len_mean: 795.3912213112055
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.81895413684407
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 60738
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.556797763300562e-32
        cur_lr: 5.0e-05
        entropy: 0.06704578238228957
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009398080641403794
        total_loss: .inf
        vf_explained_var: 0.9987568259239197
        vf_loss: 0.6451302170753479
    num_steps_sampled: 48375808
    num_steps_trained: 48375808
  iterations_since_restore: 299
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.359375
    gpu_util_percent0: 0.265
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640745087339443
    mean_env_wait_ms: 1.2104405424774165
    mean_inference_ms: 4.28877420552433
    mean_raw_obs_processing_ms: 0.37843056419715193
  time_since_restore: 7901.786769866943
  time_this_iter_s: 26.82056999206543
  time_total_s: 7901.786769866943
  timers:
    learn_throughput: 8370.47
    learn_time_ms: 19328.902
    sample_throughput: 23818.183
    sample_time_ms: 6792.794
    update_time_ms: 30.738
  timestamp: 1602728632
  timesteps_since_restore: 0
  timesteps_total: 48375808
  training_iteration: 299
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:23:54,030	WARNING util.py:136 -- The `process_trial` operation took 0.8116493225097656 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    299 |          7901.79 | 48375808 |  286.819 |              309.323 |              74.7778 |            795.391 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3059.40309924817
    time_step_min: 2917
  date: 2020-10-15_02-24-20
  done: false
  episode_len_mean: 795.4189743421484
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.87807317217056
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 60956
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2835196644950844e-31
        cur_lr: 5.0e-05
        entropy: 0.0596668083841602
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008217796916142106
        total_loss: .inf
        vf_explained_var: 0.999561607837677
        vf_loss: 0.22063873956600824
    num_steps_sampled: 48537600
    num_steps_trained: 48537600
  iterations_since_restore: 300
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.662499999999998
    gpu_util_percent0: 0.28125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640555447907172
    mean_env_wait_ms: 1.210364599004133
    mean_inference_ms: 4.288684673799288
    mean_raw_obs_processing_ms: 0.3784257910253157
  time_since_restore: 7928.340667486191
  time_this_iter_s: 26.553897619247437
  time_total_s: 7928.340667486191
  timers:
    learn_throughput: 8373.045
    learn_time_ms: 19322.957
    sample_throughput: 23802.012
    sample_time_ms: 6797.409
    update_time_ms: 32.74
  timestamp: 1602728660
  timesteps_since_restore: 0
  timesteps_total: 48537600
  training_iteration: 300
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:24:21,844	WARNING util.py:136 -- The `process_trial` operation took 0.8591048717498779 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    300 |          7928.34 | 48537600 |  286.878 |              309.323 |              74.7778 |            795.419 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3059.0870006382665
    time_step_min: 2917
  date: 2020-10-15_02-24-48
  done: false
  episode_len_mean: 795.4442191001128
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.9252406963272
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 61141
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9252794967426263e-31
        cur_lr: 5.0e-05
        entropy: 0.05584517773240805
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011211806112745156
        total_loss: .inf
        vf_explained_var: 0.999380886554718
        vf_loss: 0.3164503872394562
    num_steps_sampled: 48699392
    num_steps_trained: 48699392
  iterations_since_restore: 301
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.7
    gpu_util_percent0: 0.30387096774193556
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640404595969295
    mean_env_wait_ms: 1.2103016944993334
    mean_inference_ms: 4.288614582425894
    mean_raw_obs_processing_ms: 0.37842234596305135
  time_since_restore: 7955.112305164337
  time_this_iter_s: 26.771637678146362
  time_total_s: 7955.112305164337
  timers:
    learn_throughput: 8366.84
    learn_time_ms: 19337.288
    sample_throughput: 23743.865
    sample_time_ms: 6814.055
    update_time_ms: 31.564
  timestamp: 1602728688
  timesteps_since_restore: 0
  timesteps_total: 48699392
  training_iteration: 301
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:24:49,843	WARNING util.py:136 -- The `process_trial` operation took 0.8538961410522461 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    301 |          7955.11 | 48699392 |  286.925 |              309.323 |              74.7778 |            795.444 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3058.7443795477534
    time_step_min: 2917
  date: 2020-10-15_02-25-16
  done: false
  episode_len_mean: 795.47414074219
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 286.97535025464975
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 61332
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.88791924511394e-31
        cur_lr: 5.0e-05
        entropy: 0.05599693084756533
        entropy_coeff: 0.0005000000000000001
        kl: 0.004099860709781448
        model: {}
        policy_loss: -0.007922993749768162
        total_loss: 0.4274475947022438
        vf_explained_var: 0.9991376996040344
        vf_loss: 0.43539859106143314
    num_steps_sampled: 48861184
    num_steps_trained: 48861184
  iterations_since_restore: 302
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.546875
    gpu_util_percent0: 0.355625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640267287252853
    mean_env_wait_ms: 1.210237733141558
    mean_inference_ms: 4.2885371596209065
    mean_raw_obs_processing_ms: 0.3784191682808653
  time_since_restore: 7981.645245313644
  time_this_iter_s: 26.53294014930725
  time_total_s: 7981.645245313644
  timers:
    learn_throughput: 8368.781
    learn_time_ms: 19332.804
    sample_throughput: 23741.5
    sample_time_ms: 6814.734
    update_time_ms: 31.718
  timestamp: 1602728716
  timesteps_since_restore: 0
  timesteps_total: 48861184
  training_iteration: 302
  trial_id: 9f737_00000
  
2020-10-15 02:25:17,519	WARNING util.py:136 -- The `process_trial` operation took 0.8501689434051514 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    302 |          7981.65 | 48861184 |  286.975 |              309.323 |              74.7778 |            795.474 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3058.3637353883173
    time_step_min: 2917
  date: 2020-10-15_02-25-44
  done: false
  episode_len_mean: 795.5075958210798
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.03067541550325
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 215
  episodes_total: 61547
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.44395962255697e-31
        cur_lr: 5.0e-05
        entropy: 0.05109684603909651
        entropy_coeff: 0.0005000000000000001
        kl: 0.0032273814819442728
        model: {}
        policy_loss: -0.007164327013015281
        total_loss: 0.2330336074034373
        vf_explained_var: 0.9995736479759216
        vf_loss: 0.2402234822511673
    num_steps_sampled: 49022976
    num_steps_trained: 49022976
  iterations_since_restore: 303
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.015625
    gpu_util_percent0: 0.29437500000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146402076635858
    mean_env_wait_ms: 1.2101673418775067
    mean_inference_ms: 4.288466048561453
    mean_raw_obs_processing_ms: 0.3784153875446716
  time_since_restore: 8008.2367033958435
  time_this_iter_s: 26.591458082199097
  time_total_s: 8008.2367033958435
  timers:
    learn_throughput: 8371.12
    learn_time_ms: 19327.403
    sample_throughput: 23681.509
    sample_time_ms: 6831.997
    update_time_ms: 31.797
  timestamp: 1602728744
  timesteps_since_restore: 0
  timesteps_total: 49022976
  training_iteration: 303
  trial_id: 9f737_00000
  
2020-10-15 02:25:45,438	WARNING util.py:136 -- The `process_trial` operation took 0.8447864055633545 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    303 |          8008.24 | 49022976 |  287.031 |              309.323 |              74.7778 |            795.508 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3057.9965000972197
    time_step_min: 2917
  date: 2020-10-15_02-26-12
  done: false
  episode_len_mean: 795.5412280985847
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.08683198209377
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 61754
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.21979811278485e-32
        cur_lr: 5.0e-05
        entropy: 0.055821553183098636
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00850887813915809
        total_loss: .inf
        vf_explained_var: 0.9997549653053284
        vf_loss: 0.15066883775095144
    num_steps_sampled: 49184768
    num_steps_trained: 49184768
  iterations_since_restore: 304
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.709677419354847
    gpu_util_percent0: 0.3383870967741935
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14640010675537485
    mean_env_wait_ms: 1.2100935388585445
    mean_inference_ms: 4.2883736508412325
    mean_raw_obs_processing_ms: 0.378409804670152
  time_since_restore: 8034.799136161804
  time_this_iter_s: 26.562432765960693
  time_total_s: 8034.799136161804
  timers:
    learn_throughput: 8376.571
    learn_time_ms: 19314.824
    sample_throughput: 23678.779
    sample_time_ms: 6832.785
    update_time_ms: 31.611
  timestamp: 1602728772
  timesteps_since_restore: 0
  timesteps_total: 49184768
  training_iteration: 304
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:26:13,170	WARNING util.py:136 -- The `process_trial` operation took 0.8753936290740967 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    304 |           8034.8 | 49184768 |  287.087 |              309.323 |              74.7778 |            795.541 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3057.783187413175
    time_step_min: 2917
  date: 2020-10-15_02-26-39
  done: false
  episode_len_mean: 795.5450729691335
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.1096371502706
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 190
  episodes_total: 61944
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0829697169177273e-31
        cur_lr: 5.0e-05
        entropy: 0.11468125445147355
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.014098927572680017
        total_loss: .inf
        vf_explained_var: 0.9952261447906494
        vf_loss: 2.163328925768534
    num_steps_sampled: 49346560
    num_steps_trained: 49346560
  iterations_since_restore: 305
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.93125
    gpu_util_percent0: 0.275
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639866224819717
    mean_env_wait_ms: 1.2100310990145549
    mean_inference_ms: 4.288308018024677
    mean_raw_obs_processing_ms: 0.37840706817263536
  time_since_restore: 8061.4732167720795
  time_this_iter_s: 26.67408061027527
  time_total_s: 8061.4732167720795
  timers:
    learn_throughput: 8372.148
    learn_time_ms: 19325.028
    sample_throughput: 23623.448
    sample_time_ms: 6848.789
    update_time_ms: 29.211
  timestamp: 1602728799
  timesteps_since_restore: 0
  timesteps_total: 49346560
  training_iteration: 305
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:26:41,025	WARNING util.py:136 -- The `process_trial` operation took 0.8484063148498535 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    305 |          8061.47 | 49346560 |   287.11 |              309.323 |              74.7778 |            795.545 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3058.0990534144694
    time_step_min: 2917
  date: 2020-10-15_02-27-07
  done: false
  episode_len_mean: 795.4781517472167
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.05734268484446
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 212
  episodes_total: 62156
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6244545753765913e-31
        cur_lr: 5.0e-05
        entropy: 0.15031808987259865
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.012805982783902436
        total_loss: .inf
        vf_explained_var: 0.9901579022407532
        vf_loss: 5.407870848973592
    num_steps_sampled: 49508352
    num_steps_trained: 49508352
  iterations_since_restore: 306
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3125
    gpu_util_percent0: 0.3609375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639766184246786
    mean_env_wait_ms: 1.2099630326914235
    mean_inference_ms: 4.288226182835631
    mean_raw_obs_processing_ms: 0.3784031790954996
  time_since_restore: 8088.315300941467
  time_this_iter_s: 26.842084169387817
  time_total_s: 8088.315300941467
  timers:
    learn_throughput: 8361.112
    learn_time_ms: 19350.535
    sample_throughput: 23568.507
    sample_time_ms: 6864.754
    update_time_ms: 29.42
  timestamp: 1602728827
  timesteps_since_restore: 0
  timesteps_total: 49508352
  training_iteration: 306
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:27:09,231	WARNING util.py:136 -- The `process_trial` operation took 0.886284351348877 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    306 |          8088.32 | 49508352 |  287.057 |              309.323 |              74.7778 |            795.478 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3058.3454708779136
    time_step_min: 2917
  date: 2020-10-15_02-27-35
  done: false
  episode_len_mean: 795.4305134740858
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.02624960711813
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 62379
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.436681863064887e-31
        cur_lr: 5.0e-05
        entropy: 0.11941007835169633
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011814214860351058
        total_loss: .inf
        vf_explained_var: 0.9911243915557861
        vf_loss: 4.998524030049642
    num_steps_sampled: 49670144
    num_steps_trained: 49670144
  iterations_since_restore: 307
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.864516129032257
    gpu_util_percent0: 0.29935483870967744
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463958981796063
    mean_env_wait_ms: 1.209887133486677
    mean_inference_ms: 4.288139355404075
    mean_raw_obs_processing_ms: 0.37839853089550446
  time_since_restore: 8114.86704659462
  time_this_iter_s: 26.551745653152466
  time_total_s: 8114.86704659462
  timers:
    learn_throughput: 8349.696
    learn_time_ms: 19376.994
    sample_throughput: 23570.734
    sample_time_ms: 6864.105
    update_time_ms: 28.987
  timestamp: 1602728855
  timesteps_since_restore: 0
  timesteps_total: 49670144
  training_iteration: 307
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:27:37,086	WARNING util.py:136 -- The `process_trial` operation took 0.8970179557800293 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    307 |          8114.87 | 49670144 |  287.026 |              309.323 |              74.7778 |            795.431 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3058.2846359289288
    time_step_min: 2917
  date: 2020-10-15_02-28-03
  done: false
  episode_len_mean: 795.4171528121853
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.03656363206926
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 62567
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.6550227945973295e-31
        cur_lr: 5.0e-05
        entropy: 0.08462541364133358
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039686608749131365
        model: {}
        policy_loss: -0.010461072883723924
        total_loss: 2.3735650181770325
        vf_explained_var: 0.9950666427612305
        vf_loss: 2.3840683698654175
    num_steps_sampled: 49831936
    num_steps_trained: 49831936
  iterations_since_restore: 308
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.868750000000002
    gpu_util_percent0: 0.27937500000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639448551245757
    mean_env_wait_ms: 1.2098257228460036
    mean_inference_ms: 4.28807231630965
    mean_raw_obs_processing_ms: 0.3783952509551219
  time_since_restore: 8141.351105928421
  time_this_iter_s: 26.48405933380127
  time_total_s: 8141.351105928421
  timers:
    learn_throughput: 8345.728
    learn_time_ms: 19386.206
    sample_throughput: 23546.678
    sample_time_ms: 6871.118
    update_time_ms: 30.868
  timestamp: 1602728883
  timesteps_since_restore: 0
  timesteps_total: 49831936
  training_iteration: 308
  trial_id: 9f737_00000
  
2020-10-15 02:28:04,697	WARNING util.py:136 -- The `process_trial` operation took 0.8339054584503174 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    308 |          8141.35 | 49831936 |  287.037 |              309.323 |              74.7778 |            795.417 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3058.0731026785716
    time_step_min: 2917
  date: 2020-10-15_02-28-30
  done: false
  episode_len_mean: 795.430016890277
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.07368693145804
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 62758
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8275113972986648e-31
        cur_lr: 5.0e-05
        entropy: 0.06753294418255489
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011061659538730359
        total_loss: .inf
        vf_explained_var: 0.9980425834655762
        vf_loss: 0.943828339378039
    num_steps_sampled: 49993728
    num_steps_trained: 49993728
  iterations_since_restore: 309
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.731250000000003
    gpu_util_percent0: 0.2990625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639332452997572
    mean_env_wait_ms: 1.2097647369490703
    mean_inference_ms: 4.28800369025093
    mean_raw_obs_processing_ms: 0.3783924201917021
  time_since_restore: 8167.655078172684
  time_this_iter_s: 26.303972244262695
  time_total_s: 8167.655078172684
  timers:
    learn_throughput: 8364.579
    learn_time_ms: 19342.515
    sample_throughput: 23564.124
    sample_time_ms: 6866.031
    update_time_ms: 29.373
  timestamp: 1602728910
  timesteps_since_restore: 0
  timesteps_total: 49993728
  training_iteration: 309
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:28:32,352	WARNING util.py:136 -- The `process_trial` operation took 0.8821346759796143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    309 |          8167.66 | 49993728 |  287.074 |              309.323 |              74.7778 |             795.43 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3057.7436887342515
    time_step_min: 2917
  date: 2020-10-15_02-28-59
  done: false
  episode_len_mean: 795.4481192740668
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.1263744926119
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 62981
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.741267095947997e-31
        cur_lr: 5.0e-05
        entropy: 0.06200203858315945
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008282620498600105
        total_loss: .inf
        vf_explained_var: 0.998878002166748
        vf_loss: 0.620267296830813
    num_steps_sampled: 50155520
    num_steps_trained: 50155520
  iterations_since_restore: 310
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.528125
    gpu_util_percent0: 0.3371875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463923612503905
    mean_env_wait_ms: 1.2096922105417098
    mean_inference_ms: 4.287924230104618
    mean_raw_obs_processing_ms: 0.3783883646427142
  time_since_restore: 8194.489364862442
  time_this_iter_s: 26.8342866897583
  time_total_s: 8194.489364862442
  timers:
    learn_throughput: 8354.521
    learn_time_ms: 19365.802
    sample_throughput: 23557.286
    sample_time_ms: 6868.024
    update_time_ms: 28.627
  timestamp: 1602728939
  timesteps_since_restore: 0
  timesteps_total: 50155520
  training_iteration: 310
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:29:00,397	WARNING util.py:136 -- The `process_trial` operation took 0.8923859596252441 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    310 |          8194.49 | 50155520 |  287.126 |              309.323 |              74.7778 |            795.448 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3057.4043233826906
    time_step_min: 2917
  date: 2020-10-15_02-29-26
  done: false
  episode_len_mean: 795.4699365335613
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.17591757276483
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 63183
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.111900643921996e-31
        cur_lr: 5.0e-05
        entropy: 0.058210671258469425
        entropy_coeff: 0.0005000000000000001
        kl: 0.004774598482375343
        model: {}
        policy_loss: -0.008642333130410407
        total_loss: 0.3626844709118207
        vf_explained_var: 0.9992673397064209
        vf_loss: 0.37135590612888336
    num_steps_sampled: 50317312
    num_steps_trained: 50317312
  iterations_since_restore: 311
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.196774193548393
    gpu_util_percent0: 0.27064516129032257
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14639063897282642
    mean_env_wait_ms: 1.2096247642433353
    mean_inference_ms: 4.287842182450172
    mean_raw_obs_processing_ms: 0.3783835353567027
  time_since_restore: 8221.03705072403
  time_this_iter_s: 26.547685861587524
  time_total_s: 8221.03705072403
  timers:
    learn_throughput: 8353.633
    learn_time_ms: 19367.861
    sample_throughput: 23647.235
    sample_time_ms: 6841.899
    update_time_ms: 29.132
  timestamp: 1602728966
  timesteps_since_restore: 0
  timesteps_total: 50317312
  training_iteration: 311
  trial_id: 9f737_00000
  
2020-10-15 02:29:28,204	WARNING util.py:136 -- The `process_trial` operation took 0.8574869632720947 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    311 |          8221.04 | 50317312 |  287.176 |              309.323 |              74.7778 |             795.47 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3057.141000742109
    time_step_min: 2917
  date: 2020-10-15_02-29-54
  done: false
  episode_len_mean: 795.4735289012324
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.21169148364555
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 188
  episodes_total: 63371
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.055950321960998e-31
        cur_lr: 5.0e-05
        entropy: 0.077826543400685
        entropy_coeff: 0.0005000000000000001
        kl: 0.005009303412710627
        model: {}
        policy_loss: -0.013171553708768139
        total_loss: 1.1222182313601177
        vf_explained_var: 0.997560441493988
        vf_loss: 1.1354287266731262
    num_steps_sampled: 50479104
    num_steps_trained: 50479104
  iterations_since_restore: 312
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.890625
    gpu_util_percent0: 0.3221875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463892274133897
    mean_env_wait_ms: 1.2095644435324917
    mean_inference_ms: 4.287777079378833
    mean_raw_obs_processing_ms: 0.3783809208523491
  time_since_restore: 8247.580377101898
  time_this_iter_s: 26.543326377868652
  time_total_s: 8247.580377101898
  timers:
    learn_throughput: 8347.712
    learn_time_ms: 19381.597
    sample_throughput: 23663.333
    sample_time_ms: 6837.245
    update_time_ms: 29.631
  timestamp: 1602728994
  timesteps_since_restore: 0
  timesteps_total: 50479104
  training_iteration: 312
  trial_id: 9f737_00000
  
2020-10-15 02:29:55,954	WARNING util.py:136 -- The `process_trial` operation took 0.9020993709564209 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    312 |          8247.58 | 50479104 |  287.212 |              309.323 |              74.7778 |            795.474 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3057.002187495082
    time_step_min: 2917
  date: 2020-10-15_02-30-22
  done: false
  episode_len_mean: 795.4556392633019
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.2288435383228
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 210
  episodes_total: 63581
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.055950321960998e-31
        cur_lr: 5.0e-05
        entropy: 0.08562086150050163
        entropy_coeff: 0.0005000000000000001
        kl: 0.005501979379914701
        model: {}
        policy_loss: -0.01052004446197922
        total_loss: 2.260508288939794
        vf_explained_var: 0.9955795407295227
        vf_loss: 2.271071215470632
    num_steps_sampled: 50640896
    num_steps_trained: 50640896
  iterations_since_restore: 313
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.890625
    gpu_util_percent0: 0.3490625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638827682718947
    mean_env_wait_ms: 1.2094992173102435
    mean_inference_ms: 4.287706628184959
    mean_raw_obs_processing_ms: 0.3783775224844422
  time_since_restore: 8274.314441680908
  time_this_iter_s: 26.73406457901001
  time_total_s: 8274.314441680908
  timers:
    learn_throughput: 8343.848
    learn_time_ms: 19390.573
    sample_throughput: 23645.148
    sample_time_ms: 6842.503
    update_time_ms: 29.524
  timestamp: 1602729022
  timesteps_since_restore: 0
  timesteps_total: 50640896
  training_iteration: 313
  trial_id: 9f737_00000
  
2020-10-15 02:30:23,957	WARNING util.py:136 -- The `process_trial` operation took 0.8704190254211426 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    313 |          8274.31 | 50640896 |  287.229 |              309.323 |              74.7778 |            795.456 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3056.8403701380175
    time_step_min: 2917
  date: 2020-10-15_02-30-50
  done: false
  episode_len_mean: 795.4610175867582
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.2647180922362
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 217
  episodes_total: 63798
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.055950321960998e-31
        cur_lr: 5.0e-05
        entropy: 0.06573932617902756
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011574610660318285
        total_loss: .inf
        vf_explained_var: 0.9981677532196045
        vf_loss: 0.9454284409681956
    num_steps_sampled: 50802688
    num_steps_trained: 50802688
  iterations_since_restore: 314
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9375
    gpu_util_percent0: 0.27375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463866238484251
    mean_env_wait_ms: 1.209427307757609
    mean_inference_ms: 4.287624437177437
    mean_raw_obs_processing_ms: 0.3783732055030762
  time_since_restore: 8300.967567443848
  time_this_iter_s: 26.653125762939453
  time_total_s: 8300.967567443848
  timers:
    learn_throughput: 8336.836
    learn_time_ms: 19406.883
    sample_throughput: 23681.494
    sample_time_ms: 6832.001
    update_time_ms: 31.504
  timestamp: 1602729050
  timesteps_since_restore: 0
  timesteps_total: 50802688
  training_iteration: 314
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:30:51,901	WARNING util.py:136 -- The `process_trial` operation took 0.8797299861907959 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    314 |          8300.97 | 50802688 |  287.265 |              309.323 |              74.7778 |            795.461 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3056.5283585357083
    time_step_min: 2917
  date: 2020-10-15_02-31-18
  done: false
  episode_len_mean: 795.4883804522793
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.31262505499456
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 63987
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0839254829414973e-31
        cur_lr: 5.0e-05
        entropy: 0.05319378493974606
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007188191293001485
        total_loss: .inf
        vf_explained_var: 0.9996693730354309
        vf_loss: 0.15451321254173914
    num_steps_sampled: 50964480
    num_steps_trained: 50964480
  iterations_since_restore: 315
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.096874999999997
    gpu_util_percent0: 0.36
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638525860523333
    mean_env_wait_ms: 1.2093664424537163
    mean_inference_ms: 4.287558437666486
    mean_raw_obs_processing_ms: 0.3783700147786129
  time_since_restore: 8327.288178682327
  time_this_iter_s: 26.320611238479614
  time_total_s: 8327.288178682327
  timers:
    learn_throughput: 8342.048
    learn_time_ms: 19394.757
    sample_throughput: 23741.833
    sample_time_ms: 6814.638
    update_time_ms: 34.152
  timestamp: 1602729078
  timesteps_since_restore: 0
  timesteps_total: 50964480
  training_iteration: 315
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:31:19,421	WARNING util.py:136 -- The `process_trial` operation took 0.8971407413482666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    315 |          8327.29 | 50964480 |  287.313 |              309.323 |              74.7778 |            795.488 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3056.2481991954846
    time_step_min: 2917
  date: 2020-10-15_02-31-46
  done: false
  episode_len_mean: 795.4993767140364
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.3544348685055
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 64176
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.625888224412247e-31
        cur_lr: 5.0e-05
        entropy: 0.06445507643123467
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008799253848944014
        total_loss: .inf
        vf_explained_var: 0.9985455870628357
        vf_loss: 0.7033633937438329
    num_steps_sampled: 51126272
    num_steps_trained: 51126272
  iterations_since_restore: 316
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9125
    gpu_util_percent0: 0.2459375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638420917372397
    mean_env_wait_ms: 1.209306399226874
    mean_inference_ms: 4.287493426692419
    mean_raw_obs_processing_ms: 0.3783675806613244
  time_since_restore: 8354.210401058197
  time_this_iter_s: 26.92222237586975
  time_total_s: 8354.210401058197
  timers:
    learn_throughput: 8335.57
    learn_time_ms: 19409.831
    sample_throughput: 23763.638
    sample_time_ms: 6808.385
    update_time_ms: 32.289
  timestamp: 1602729106
  timesteps_since_restore: 0
  timesteps_total: 51126272
  training_iteration: 316
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:31:47,633	WARNING util.py:136 -- The `process_trial` operation took 0.8734207153320312 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    316 |          8354.21 | 51126272 |  287.354 |              309.323 |              74.7778 |            795.499 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3055.930061997918
    time_step_min: 2917
  date: 2020-10-15_02-32-14
  done: false
  episode_len_mean: 795.5175712400031
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.40524979588577
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 64395
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.938832336618371e-31
        cur_lr: 5.0e-05
        entropy: 0.0640398534014821
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008332235806544
        total_loss: .inf
        vf_explained_var: 0.9992530345916748
        vf_loss: 0.38459690908590954
    num_steps_sampled: 51288064
    num_steps_trained: 51288064
  iterations_since_restore: 317
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.209375
    gpu_util_percent0: 0.34125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638298776905712
    mean_env_wait_ms: 1.2092371082711049
    mean_inference_ms: 4.287417160165475
    mean_raw_obs_processing_ms: 0.37836347524499536
  time_since_restore: 8380.733309745789
  time_this_iter_s: 26.522908687591553
  time_total_s: 8380.733309745789
  timers:
    learn_throughput: 8333.01
    learn_time_ms: 19415.793
    sample_throughput: 23801.063
    sample_time_ms: 6797.68
    update_time_ms: 32.518
  timestamp: 1602729134
  timesteps_since_restore: 0
  timesteps_total: 51288064
  training_iteration: 317
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:32:15,491	WARNING util.py:136 -- The `process_trial` operation took 0.9162461757659912 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    317 |          8380.73 | 51288064 |  287.405 |              309.323 |              74.7778 |            795.518 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3055.662887763478
    time_step_min: 2917
  date: 2020-10-15_02-32-41
  done: false
  episode_len_mean: 795.52166173944
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.4378422890349
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 212
  episodes_total: 64607
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0408248504927552e-30
        cur_lr: 5.0e-05
        entropy: 0.0837915154794852
        entropy_coeff: 0.0005000000000000001
        kl: 0.005638086353428662
        model: {}
        policy_loss: -0.011860732202573368
        total_loss: 1.4726516902446747
        vf_explained_var: 0.9970652461051941
        vf_loss: 1.4845542808373768
    num_steps_sampled: 51449856
    num_steps_trained: 51449856
  iterations_since_restore: 318
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.015625
    gpu_util_percent0: 0.3815625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638152727371215
    mean_env_wait_ms: 1.209167995812516
    mean_inference_ms: 4.287341806680688
    mean_raw_obs_processing_ms: 0.37835884425471544
  time_since_restore: 8407.168589115143
  time_this_iter_s: 26.435279369354248
  time_total_s: 8407.168589115143
  timers:
    learn_throughput: 8336.542
    learn_time_ms: 19407.568
    sample_throughput: 23791.373
    sample_time_ms: 6800.448
    update_time_ms: 32.022
  timestamp: 1602729161
  timesteps_since_restore: 0
  timesteps_total: 51449856
  training_iteration: 318
  trial_id: 9f737_00000
  
2020-10-15 02:32:43,245	WARNING util.py:136 -- The `process_trial` operation took 0.9982526302337646 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    318 |          8407.17 | 51449856 |  287.438 |              309.323 |              74.7778 |            795.522 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3055.5803243243245
    time_step_min: 2917
  date: 2020-10-15_02-33-09
  done: false
  episode_len_mean: 795.5106655553498
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.450462206806
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 181
  episodes_total: 64788
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0408248504927552e-30
        cur_lr: 5.0e-05
        entropy: 0.07792356548209985
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009772158965157965
        total_loss: .inf
        vf_explained_var: 0.9969246983528137
        vf_loss: 1.4962186018625896
    num_steps_sampled: 51611648
    num_steps_trained: 51611648
  iterations_since_restore: 319
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.090625
    gpu_util_percent0: 0.364375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14638030499796179
    mean_env_wait_ms: 1.2091112045974284
    mean_inference_ms: 4.287281793605757
    mean_raw_obs_processing_ms: 0.37835647616230955
  time_since_restore: 8433.603957414627
  time_this_iter_s: 26.435368299484253
  time_total_s: 8433.603957414627
  timers:
    learn_throughput: 8331.749
    learn_time_ms: 19418.733
    sample_throughput: 23789.625
    sample_time_ms: 6800.948
    update_time_ms: 32.018
  timestamp: 1602729189
  timesteps_since_restore: 0
  timesteps_total: 51611648
  training_iteration: 319
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:33:10,908	WARNING util.py:136 -- The `process_trial` operation took 0.9160690307617188 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    319 |           8433.6 | 51611648 |   287.45 |              309.323 |              74.7778 |            795.511 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3055.303483735895
    time_step_min: 2917
  date: 2020-10-15_02-33-37
  done: false
  episode_len_mean: 795.5253473237226
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.497658555492
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 209
  episodes_total: 64997
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.561237275739133e-30
        cur_lr: 5.0e-05
        entropy: 0.06500373718639214
        entropy_coeff: 0.0005000000000000001
        kl: 0.004126939398702234
        model: {}
        policy_loss: -0.007925171754322946
        total_loss: 0.5390878766775131
        vf_explained_var: 0.9989137053489685
        vf_loss: 0.5470455338557562
    num_steps_sampled: 51773440
    num_steps_trained: 51773440
  iterations_since_restore: 320
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.38125
    gpu_util_percent0: 0.27749999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637927079437865
    mean_env_wait_ms: 1.209047107886238
    mean_inference_ms: 4.287213671646279
    mean_raw_obs_processing_ms: 0.37835374667859745
  time_since_restore: 8460.231759786606
  time_this_iter_s: 26.62780237197876
  time_total_s: 8460.231759786606
  timers:
    learn_throughput: 8343.992
    learn_time_ms: 19390.238
    sample_throughput: 23766.494
    sample_time_ms: 6807.567
    update_time_ms: 32.244
  timestamp: 1602729217
  timesteps_since_restore: 0
  timesteps_total: 51773440
  training_iteration: 320
  trial_id: 9f737_00000
  
2020-10-15 02:33:38,805	WARNING util.py:136 -- The `process_trial` operation took 0.9491763114929199 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    320 |          8460.23 | 51773440 |  287.498 |              309.323 |              74.7778 |            795.525 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3054.973688651601
    time_step_min: 2917
  date: 2020-10-15_02-34-05
  done: false
  episode_len_mean: 795.5432925987826
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.5483173165903
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 65219
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.806186378695665e-31
        cur_lr: 5.0e-05
        entropy: 0.0591657996798555
        entropy_coeff: 0.0005000000000000001
        kl: 0.004317259687619905
        model: {}
        policy_loss: -0.007985874491472108
        total_loss: 0.5036735807855924
        vf_explained_var: 0.9989911913871765
        vf_loss: 0.5116890370845795
    num_steps_sampled: 51935232
    num_steps_trained: 51935232
  iterations_since_restore: 321
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.06875
    gpu_util_percent0: 0.30125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637790066051015
    mean_env_wait_ms: 1.2089751574452723
    mean_inference_ms: 4.287134035191086
    mean_raw_obs_processing_ms: 0.37834934416607985
  time_since_restore: 8486.968470811844
  time_this_iter_s: 26.736711025238037
  time_total_s: 8486.968470811844
  timers:
    learn_throughput: 8342.742
    learn_time_ms: 19393.145
    sample_throughput: 23752.733
    sample_time_ms: 6811.511
    update_time_ms: 33.492
  timestamp: 1602729245
  timesteps_since_restore: 0
  timesteps_total: 51935232
  training_iteration: 321
  trial_id: 9f737_00000
  
2020-10-15 02:34:06,829	WARNING util.py:136 -- The `process_trial` operation took 0.9523451328277588 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    321 |          8486.97 | 51935232 |  287.548 |              309.323 |              74.7778 |            795.543 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3054.6893587076247
    time_step_min: 2917
  date: 2020-10-15_02-34-33
  done: false
  episode_len_mean: 795.5613705164664
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.59254595306334
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 187
  episodes_total: 65406
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9030931893478324e-31
        cur_lr: 5.0e-05
        entropy: 0.05820522333184878
        entropy_coeff: 0.0005000000000000001
        kl: 0.005763437986994783
        model: {}
        policy_loss: -0.005795113975182176
        total_loss: 0.3417143647869428
        vf_explained_var: 0.9993370175361633
        vf_loss: 0.34753858546415967
    num_steps_sampled: 52097024
    num_steps_trained: 52097024
  iterations_since_restore: 322
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.929032258064517
    gpu_util_percent0: 0.3938709677419354
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637649809836417
    mean_env_wait_ms: 1.2089156815852113
    mean_inference_ms: 4.28707105735959
    mean_raw_obs_processing_ms: 0.3783461539035461
  time_since_restore: 8513.150214195251
  time_this_iter_s: 26.181743383407593
  time_total_s: 8513.150214195251
  timers:
    learn_throughput: 8353.935
    learn_time_ms: 19367.16
    sample_throughput: 23791.36
    sample_time_ms: 6800.452
    update_time_ms: 32.861
  timestamp: 1602729273
  timesteps_since_restore: 0
  timesteps_total: 52097024
  training_iteration: 322
  trial_id: 9f737_00000
  
2020-10-15 02:34:34,262	WARNING util.py:136 -- The `process_trial` operation took 0.9408278465270996 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    322 |          8513.15 | 52097024 |  287.593 |              309.323 |              74.7778 |            795.561 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3054.434309074546
    time_step_min: 2917
  date: 2020-10-15_02-35-00
  done: false
  episode_len_mean: 795.5799832304291
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.6329173278636
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 65595
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.9030931893478324e-31
        cur_lr: 5.0e-05
        entropy: 0.06341700690488021
        entropy_coeff: 0.0005000000000000001
        kl: 0.004736181697808206
        model: {}
        policy_loss: -0.008902221360282661
        total_loss: 0.6349354386329651
        vf_explained_var: 0.9986860156059265
        vf_loss: 0.6438693553209305
    num_steps_sampled: 52258816
    num_steps_trained: 52258816
  iterations_since_restore: 323
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.725
    gpu_util_percent0: 0.3365625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637542649435467
    mean_env_wait_ms: 1.2088567290510315
    mean_inference_ms: 4.287010652283475
    mean_raw_obs_processing_ms: 0.37834408731968067
  time_since_restore: 8539.62974858284
  time_this_iter_s: 26.4795343875885
  time_total_s: 8539.62974858284
  timers:
    learn_throughput: 8362.798
    learn_time_ms: 19346.636
    sample_throughput: 23843.639
    sample_time_ms: 6785.541
    update_time_ms: 40.529
  timestamp: 1602729300
  timesteps_since_restore: 0
  timesteps_total: 52258816
  training_iteration: 323
  trial_id: 9f737_00000
  
2020-10-15 02:35:01,949	WARNING util.py:136 -- The `process_trial` operation took 0.8881485462188721 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    323 |          8539.63 | 52258816 |  287.633 |              309.323 |              74.7778 |             795.58 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3054.1127175978713
    time_step_min: 2917
  date: 2020-10-15_02-35-28
  done: false
  episode_len_mean: 795.6020239162475
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.6811002769248
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 65813
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9515465946739162e-31
        cur_lr: 5.0e-05
        entropy: 0.06251690195252498
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009952658860129304
        total_loss: .inf
        vf_explained_var: 0.999320924282074
        vf_loss: 0.3615172232190768
    num_steps_sampled: 52420608
    num_steps_trained: 52420608
  iterations_since_restore: 324
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79375
    gpu_util_percent0: 0.33718750000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637442920072447
    mean_env_wait_ms: 1.208789030667513
    mean_inference_ms: 4.286936617685145
    mean_raw_obs_processing_ms: 0.37834043655851496
  time_since_restore: 8566.327955961227
  time_this_iter_s: 26.69820737838745
  time_total_s: 8566.327955961227
  timers:
    learn_throughput: 8363.135
    learn_time_ms: 19345.856
    sample_throughput: 23856.491
    sample_time_ms: 6781.886
    update_time_ms: 40.435
  timestamp: 1602729328
  timesteps_since_restore: 0
  timesteps_total: 52420608
  training_iteration: 324
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:35:29,882	WARNING util.py:136 -- The `process_trial` operation took 0.9213337898254395 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    324 |          8566.33 | 52420608 |  287.681 |              309.323 |              74.7778 |            795.602 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3053.786475304245
    time_step_min: 2917
  date: 2020-10-15_02-35-56
  done: false
  episode_len_mean: 795.6261189621484
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.72999959149826
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 66021
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.9273198920108743e-31
        cur_lr: 5.0e-05
        entropy: 0.05815344428022703
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007566721178591251
        total_loss: .inf
        vf_explained_var: 0.9994474053382874
        vf_loss: 0.2815694808959961
    num_steps_sampled: 52582400
    num_steps_trained: 52582400
  iterations_since_restore: 325
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.381249999999998
    gpu_util_percent0: 0.2284375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637295058593103
    mean_env_wait_ms: 1.208722141363856
    mean_inference_ms: 4.286865951833113
    mean_raw_obs_processing_ms: 0.3783362947363003
  time_since_restore: 8592.990823030472
  time_this_iter_s: 26.662867069244385
  time_total_s: 8592.990823030472
  timers:
    learn_throughput: 8353.924
    learn_time_ms: 19367.185
    sample_throughput: 23844.216
    sample_time_ms: 6785.377
    update_time_ms: 39.691
  timestamp: 1602729356
  timesteps_since_restore: 0
  timesteps_total: 52582400
  training_iteration: 325
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:35:57,829	WARNING util.py:136 -- The `process_trial` operation took 0.905780553817749 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    325 |          8592.99 | 52582400 |   287.73 |              309.323 |              74.7778 |            795.626 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3053.5033778167362
    time_step_min: 2917
  date: 2020-10-15_02-36-24
  done: false
  episode_len_mean: 795.6474586511592
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.77160548922495
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 66205
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.3909798380163115e-31
        cur_lr: 5.0e-05
        entropy: 0.05884972121566534
        entropy_coeff: 0.0005000000000000001
        kl: 0.004236923899346341
        model: {}
        policy_loss: -0.008118980636936612
        total_loss: 0.30420657247304916
        vf_explained_var: 0.9993286728858948
        vf_loss: 0.31235498934984207
    num_steps_sampled: 52744192
    num_steps_trained: 52744192
  iterations_since_restore: 326
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.521875
    gpu_util_percent0: 0.3259375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637163179063087
    mean_env_wait_ms: 1.208664060003058
    mean_inference_ms: 4.28680681060428
    mean_raw_obs_processing_ms: 0.37833359353896445
  time_since_restore: 8619.59279179573
  time_this_iter_s: 26.60196876525879
  time_total_s: 8619.59279179573
  timers:
    learn_throughput: 8357.877
    learn_time_ms: 19358.026
    sample_throughput: 23928.642
    sample_time_ms: 6761.437
    update_time_ms: 39.182
  timestamp: 1602729384
  timesteps_since_restore: 0
  timesteps_total: 52744192
  training_iteration: 326
  trial_id: 9f737_00000
  
2020-10-15 02:36:25,802	WARNING util.py:136 -- The `process_trial` operation took 0.8792598247528076 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    326 |          8619.59 | 52744192 |  287.772 |              309.323 |              74.7778 |            795.647 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3053.213260001507
    time_step_min: 2917
  date: 2020-10-15_02-36-52
  done: false
  episode_len_mean: 795.6691414544524
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.81615592699427
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 198
  episodes_total: 66403
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1954899190081557e-31
        cur_lr: 5.0e-05
        entropy: 0.06252290153255065
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00823608716018498
        total_loss: .inf
        vf_explained_var: 0.9992201924324036
        vf_loss: 0.42587263137102127
    num_steps_sampled: 52905984
    num_steps_trained: 52905984
  iterations_since_restore: 327
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.099999999999998
    gpu_util_percent0: 0.3203225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14637063607828157
    mean_env_wait_ms: 1.2086031753288171
    mean_inference_ms: 4.286747867511409
    mean_raw_obs_processing_ms: 0.37833120382011526
  time_since_restore: 8646.038341522217
  time_this_iter_s: 26.445549726486206
  time_total_s: 8646.038341522217
  timers:
    learn_throughput: 8361.856
    learn_time_ms: 19348.815
    sample_throughput: 23916.237
    sample_time_ms: 6764.944
    update_time_ms: 37.356
  timestamp: 1602729412
  timesteps_since_restore: 0
  timesteps_total: 52905984
  training_iteration: 327
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:36:53,583	WARNING util.py:136 -- The `process_trial` operation took 0.9237728118896484 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    327 |          8646.04 | 52905984 |  287.816 |              309.323 |              74.7778 |            795.669 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3052.902070912613
    time_step_min: 2917
  date: 2020-10-15_02-37-20
  done: false
  episode_len_mean: 795.694733366353
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.86669522911586
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 224
  episodes_total: 66627
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.293234878512234e-31
        cur_lr: 5.0e-05
        entropy: 0.06312044089039166
        entropy_coeff: 0.0005000000000000001
        kl: 0.005499538073005776
        model: {}
        policy_loss: -0.00747885797075772
        total_loss: 0.31022847692171734
        vf_explained_var: 0.9994023442268372
        vf_loss: 0.3177388980984688
    num_steps_sampled: 53067776
    num_steps_trained: 53067776
  iterations_since_restore: 328
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.821875
    gpu_util_percent0: 0.271875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636954494187265
    mean_env_wait_ms: 1.2085327872403242
    mean_inference_ms: 4.286671042010592
    mean_raw_obs_processing_ms: 0.3783271711433583
  time_since_restore: 8672.65213060379
  time_this_iter_s: 26.613789081573486
  time_total_s: 8672.65213060379
  timers:
    learn_throughput: 8349.812
    learn_time_ms: 19376.723
    sample_throughput: 23951.585
    sample_time_ms: 6754.96
    update_time_ms: 37.393
  timestamp: 1602729440
  timesteps_since_restore: 0
  timesteps_total: 53067776
  training_iteration: 328
  trial_id: 9f737_00000
  
2020-10-15 02:37:21,498	WARNING util.py:136 -- The `process_trial` operation took 0.8839325904846191 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    328 |          8672.65 | 53067776 |  287.867 |              309.323 |              74.7778 |            795.695 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3052.644821752085
    time_step_min: 2917
  date: 2020-10-15_02-37-48
  done: false
  episode_len_mean: 795.7104763044877
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.906333147568
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 200
  episodes_total: 66827
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.293234878512234e-31
        cur_lr: 5.0e-05
        entropy: 0.06817417715986569
        entropy_coeff: 0.0005000000000000001
        kl: 0.004811525034407775
        model: {}
        policy_loss: -0.007971316098216144
        total_loss: 0.6904131124416987
        vf_explained_var: 0.9985252022743225
        vf_loss: 0.6984185129404068
    num_steps_sampled: 53229568
    num_steps_trained: 53229568
  iterations_since_restore: 329
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.915625
    gpu_util_percent0: 0.3325
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636803288337258
    mean_env_wait_ms: 1.2084687232854292
    mean_inference_ms: 4.28660789838142
    mean_raw_obs_processing_ms: 0.378323745361531
  time_since_restore: 8699.290113210678
  time_this_iter_s: 26.637982606887817
  time_total_s: 8699.290113210678
  timers:
    learn_throughput: 8335.077
    learn_time_ms: 19410.979
    sample_throughput: 24005.903
    sample_time_ms: 6739.676
    update_time_ms: 38.242
  timestamp: 1602729468
  timesteps_since_restore: 0
  timesteps_total: 53229568
  training_iteration: 329
  trial_id: 9f737_00000
  
2020-10-15 02:37:49,377	WARNING util.py:136 -- The `process_trial` operation took 0.9161496162414551 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    329 |          8699.29 | 53229568 |  287.906 |              309.323 |              74.7778 |             795.71 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3052.4527386214313
    time_step_min: 2917
  date: 2020-10-15_02-38-15
  done: false
  episode_len_mean: 795.7228457153091
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.9391183723333
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 67006
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.646617439256117e-31
        cur_lr: 5.0e-05
        entropy: 0.06769804532329242
        entropy_coeff: 0.0005000000000000001
        kl: 0.005271129387741287
        model: {}
        policy_loss: -0.01088847778737545
        total_loss: 0.5557021846373876
        vf_explained_var: 0.9987909197807312
        vf_loss: 0.5666245073080063
    num_steps_sampled: 53391360
    num_steps_trained: 53391360
  iterations_since_restore: 330
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.165625
    gpu_util_percent0: 0.2859375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636702493097056
    mean_env_wait_ms: 1.2084134938186828
    mean_inference_ms: 4.286551801463157
    mean_raw_obs_processing_ms: 0.3783215446072582
  time_since_restore: 8725.815495967865
  time_this_iter_s: 26.52538275718689
  time_total_s: 8725.815495967865
  timers:
    learn_throughput: 8334.675
    learn_time_ms: 19411.914
    sample_throughput: 24047.305
    sample_time_ms: 6728.072
    update_time_ms: 38.608
  timestamp: 1602729495
  timesteps_since_restore: 0
  timesteps_total: 53391360
  training_iteration: 330
  trial_id: 9f737_00000
  
2020-10-15 02:38:17,209	WARNING util.py:136 -- The `process_trial` operation took 0.9258975982666016 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    330 |          8725.82 | 53391360 |  287.939 |              309.323 |              74.7778 |            795.723 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3052.1712022866673
    time_step_min: 2917
  date: 2020-10-15_02-38-43
  done: false
  episode_len_mean: 795.7473590239548
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 287.9839768011915
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 204
  episodes_total: 67210
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.646617439256117e-31
        cur_lr: 5.0e-05
        entropy: 0.06138577312231064
        entropy_coeff: 0.0005000000000000001
        kl: 0.004243339644744992
        model: {}
        policy_loss: -0.010395692018695021
        total_loss: 0.2797408526142438
        vf_explained_var: 0.9994789958000183
        vf_loss: 0.2901672460138798
    num_steps_sampled: 53553152
    num_steps_trained: 53553152
  iterations_since_restore: 331
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.209375
    gpu_util_percent0: 0.36375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636611011011177
    mean_env_wait_ms: 1.208350652147018
    mean_inference_ms: 4.286489095658494
    mean_raw_obs_processing_ms: 0.37831844111366564
  time_since_restore: 8752.380944490433
  time_this_iter_s: 26.56544852256775
  time_total_s: 8752.380944490433
  timers:
    learn_throughput: 8337.813
    learn_time_ms: 19404.61
    sample_throughput: 24049.872
    sample_time_ms: 6727.354
    update_time_ms: 39.161
  timestamp: 1602729523
  timesteps_since_restore: 0
  timesteps_total: 53553152
  training_iteration: 331
  trial_id: 9f737_00000
  
2020-10-15 02:38:45,169	WARNING util.py:136 -- The `process_trial` operation took 0.9021852016448975 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    331 |          8752.38 | 53553152 |  287.984 |              309.323 |              74.7778 |            795.747 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3051.849389456817
    time_step_min: 2917
  date: 2020-10-15_02-39-11
  done: false
  episode_len_mean: 795.7705710514999
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.0335639863199
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 67437
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.233087196280585e-32
        cur_lr: 5.0e-05
        entropy: 0.058483776015539966
        entropy_coeff: 0.0005000000000000001
        kl: 0.0035691854815619686
        model: {}
        policy_loss: -0.009702968217122057
        total_loss: 0.341906301677227
        vf_explained_var: 0.999342143535614
        vf_loss: 0.3516385133067767
    num_steps_sampled: 53714944
    num_steps_trained: 53714944
  iterations_since_restore: 332
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.543750000000003
    gpu_util_percent0: 0.26
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463646914823656
    mean_env_wait_ms: 1.2082784285733972
    mean_inference_ms: 4.286413742269851
    mean_raw_obs_processing_ms: 0.37831444288345284
  time_since_restore: 8779.007676362991
  time_this_iter_s: 26.626731872558594
  time_total_s: 8779.007676362991
  timers:
    learn_throughput: 8327.06
    learn_time_ms: 19429.667
    sample_throughput: 23992.578
    sample_time_ms: 6743.419
    update_time_ms: 40.835
  timestamp: 1602729551
  timesteps_since_restore: 0
  timesteps_total: 53714944
  training_iteration: 332
  trial_id: 9f737_00000
  
2020-10-15 02:39:13,191	WARNING util.py:136 -- The `process_trial` operation took 0.9017281532287598 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    332 |          8779.01 | 53714944 |  288.034 |              309.323 |              74.7778 |            795.771 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3051.5732485018866
    time_step_min: 2917
  date: 2020-10-15_02-39-39
  done: false
  episode_len_mean: 795.789332032001
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.07405629875797
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 67623
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.1165435981402925e-32
        cur_lr: 5.0e-05
        entropy: 0.06550642289221287
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010602766669762786
        total_loss: .inf
        vf_explained_var: 0.9992754459381104
        vf_loss: 0.3500601276755333
    num_steps_sampled: 53876736
    num_steps_trained: 53876736
  iterations_since_restore: 333
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.609677419354842
    gpu_util_percent0: 0.3122580645161291
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463634948540655
    mean_env_wait_ms: 1.2082207482214455
    mean_inference_ms: 4.286359226659906
    mean_raw_obs_processing_ms: 0.37831207156159047
  time_since_restore: 8805.559997320175
  time_this_iter_s: 26.552320957183838
  time_total_s: 8805.559997320175
  timers:
    learn_throughput: 8316.671
    learn_time_ms: 19453.937
    sample_throughput: 24029.214
    sample_time_ms: 6733.137
    update_time_ms: 33.567
  timestamp: 1602729579
  timesteps_since_restore: 0
  timesteps_total: 53876736
  training_iteration: 333
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:39:41,092	WARNING util.py:136 -- The `process_trial` operation took 0.9122903347015381 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    333 |          8805.56 | 53876736 |  288.074 |              309.323 |              74.7778 |            795.789 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3051.3644304114964
    time_step_min: 2917
  date: 2020-10-15_02-40-07
  done: false
  episode_len_mean: 795.7851802698518
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.1002459900934
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 67815
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.174815397210439e-32
        cur_lr: 5.0e-05
        entropy: 0.10698160715401173
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.0108191454783082
        total_loss: .inf
        vf_explained_var: 0.996036946773529
        vf_loss: 1.856870283683141
    num_steps_sampled: 54038528
    num_steps_trained: 54038528
  iterations_since_restore: 334
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.634375
    gpu_util_percent0: 0.29093749999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14636260228371775
    mean_env_wait_ms: 1.2081621814467656
    mean_inference_ms: 4.286303016281684
    mean_raw_obs_processing_ms: 0.3783100905864678
  time_since_restore: 8831.93615603447
  time_this_iter_s: 26.376158714294434
  time_total_s: 8831.93615603447
  timers:
    learn_throughput: 8331.088
    learn_time_ms: 19420.274
    sample_throughput: 24024.581
    sample_time_ms: 6734.436
    update_time_ms: 31.683
  timestamp: 1602729607
  timesteps_since_restore: 0
  timesteps_total: 54038528
  training_iteration: 334
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:40:08,739	WARNING util.py:136 -- The `process_trial` operation took 0.9375674724578857 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    334 |          8831.94 | 54038528 |    288.1 |              309.323 |              74.7778 |            795.785 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3051.412462689859
    time_step_min: 2917
  date: 2020-10-15_02-40-35
  done: false
  episode_len_mean: 795.7370571206043
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.0913993793243
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 234
  episodes_total: 68049
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.262223095815658e-32
        cur_lr: 5.0e-05
        entropy: 0.12795317297180495
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.011067871004343033
        total_loss: .inf
        vf_explained_var: 0.9931125640869141
        vf_loss: 3.7521493434906006
    num_steps_sampled: 54200320
    num_steps_trained: 54200320
  iterations_since_restore: 335
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.370967741935484
    gpu_util_percent0: 0.3570967741935484
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.877419354838711
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463613272804826
    mean_env_wait_ms: 1.2080900885400196
    mean_inference_ms: 4.286229401649136
    mean_raw_obs_processing_ms: 0.3783059088834394
  time_since_restore: 8858.37077498436
  time_this_iter_s: 26.434618949890137
  time_total_s: 8858.37077498436
  timers:
    learn_throughput: 8333.072
    learn_time_ms: 19415.649
    sample_throughput: 24055.886
    sample_time_ms: 6725.672
    update_time_ms: 30.022
  timestamp: 1602729635
  timesteps_since_restore: 0
  timesteps_total: 54200320
  training_iteration: 335
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:40:36,553	WARNING util.py:136 -- The `process_trial` operation took 0.9612488746643066 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    335 |          8858.37 | 54200320 |  288.091 |              309.323 |              74.7778 |            795.737 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3051.3494795484535
    time_step_min: 2917
  date: 2020-10-15_02-41-03
  done: false
  episode_len_mean: 795.7149367014418
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.10558536365886
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 199
  episodes_total: 68248
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.3893334643723486e-31
        cur_lr: 5.0e-05
        entropy: 0.09474877205987771
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01063384604640305
        total_loss: .inf
        vf_explained_var: 0.9962942004203796
        vf_loss: 1.7539482712745667
    num_steps_sampled: 54362112
    num_steps_trained: 54362112
  iterations_since_restore: 336
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.653125000000003
    gpu_util_percent0: 0.3078125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635998876477854
    mean_env_wait_ms: 1.208028176185137
    mean_inference_ms: 4.286166173444411
    mean_raw_obs_processing_ms: 0.3783024449663778
  time_since_restore: 8885.070262908936
  time_this_iter_s: 26.699487924575806
  time_total_s: 8885.070262908936
  timers:
    learn_throughput: 8334.464
    learn_time_ms: 19412.407
    sample_throughput: 24015.637
    sample_time_ms: 6736.944
    update_time_ms: 31.875
  timestamp: 1602729663
  timesteps_since_restore: 0
  timesteps_total: 54362112
  training_iteration: 336
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:41:04,617	WARNING util.py:136 -- The `process_trial` operation took 0.9450864791870117 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    336 |          8885.07 | 54362112 |  288.106 |              309.323 |              74.7778 |            795.715 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3051.2282677038706
    time_step_min: 2917
  date: 2020-10-15_02-41-31
  done: false
  episode_len_mean: 795.7122042468616
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.1267637132427
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 179
  episodes_total: 68427
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0840001965585234e-31
        cur_lr: 5.0e-05
        entropy: 0.08063710729281108
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008443649741820991
        total_loss: .inf
        vf_explained_var: 0.9973017573356628
        vf_loss: 1.248758316040039
    num_steps_sampled: 54523904
    num_steps_trained: 54523904
  iterations_since_restore: 337
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.403125
    gpu_util_percent0: 0.349375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635897472397572
    mean_env_wait_ms: 1.2079743437959858
    mean_inference_ms: 4.286112463701701
    mean_raw_obs_processing_ms: 0.3783002961417205
  time_since_restore: 8911.54205918312
  time_this_iter_s: 26.47179627418518
  time_total_s: 8911.54205918312
  timers:
    learn_throughput: 8334.475
    learn_time_ms: 19412.38
    sample_throughput: 24008.287
    sample_time_ms: 6739.006
    update_time_ms: 31.928
  timestamp: 1602729691
  timesteps_since_restore: 0
  timesteps_total: 54523904
  training_iteration: 337
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:41:32,352	WARNING util.py:136 -- The `process_trial` operation took 0.9357235431671143 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    337 |          8911.54 | 54523904 |  288.127 |              309.323 |              74.7778 |            795.712 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3050.988264792921
    time_step_min: 2917
  date: 2020-10-15_02-41-58
  done: false
  episode_len_mean: 795.7215123479274
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.16661552510607
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 68635
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.1260002948377844e-31
        cur_lr: 5.0e-05
        entropy: 0.06576636371513207
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008199600681109587
        total_loss: .inf
        vf_explained_var: 0.9989103674888611
        vf_loss: 0.5766256203254064
    num_steps_sampled: 54685696
    num_steps_trained: 54685696
  iterations_since_restore: 338
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.790625
    gpu_util_percent0: 0.3059375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463580713050735
    mean_env_wait_ms: 1.2079119301613284
    mean_inference_ms: 4.2860510022243865
    mean_raw_obs_processing_ms: 0.3782973545895578
  time_since_restore: 8938.178427934647
  time_this_iter_s: 26.63636875152588
  time_total_s: 8938.178427934647
  timers:
    learn_throughput: 8336.535
    learn_time_ms: 19407.584
    sample_throughput: 23991.054
    sample_time_ms: 6743.847
    update_time_ms: 33.102
  timestamp: 1602729718
  timesteps_since_restore: 0
  timesteps_total: 54685696
  training_iteration: 338
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:42:00,283	WARNING util.py:136 -- The `process_trial` operation took 0.9656870365142822 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    338 |          8938.18 | 54685696 |  288.167 |              309.323 |              74.7778 |            795.722 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3050.659576322901
    time_step_min: 2917
  date: 2020-10-15_02-42-26
  done: false
  episode_len_mean: 795.7435525092936
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.2161226578049
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 229
  episodes_total: 68864
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.689000442256678e-31
        cur_lr: 5.0e-05
        entropy: 0.05807966087013483
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007272186242820074
        total_loss: .inf
        vf_explained_var: 0.999523937702179
        vf_loss: 0.265092412630717
    num_steps_sampled: 54847488
    num_steps_trained: 54847488
  iterations_since_restore: 339
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.88125
    gpu_util_percent0: 0.3640625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635661197153788
    mean_env_wait_ms: 1.2078409871490794
    mean_inference_ms: 4.285978327130766
    mean_raw_obs_processing_ms: 0.3782935134996366
  time_since_restore: 8964.856568336487
  time_this_iter_s: 26.67814040184021
  time_total_s: 8964.856568336487
  timers:
    learn_throughput: 8340.434
    learn_time_ms: 19398.511
    sample_throughput: 23945.085
    sample_time_ms: 6756.794
    update_time_ms: 31.457
  timestamp: 1602729746
  timesteps_since_restore: 0
  timesteps_total: 54847488
  training_iteration: 339
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:42:28,247	WARNING util.py:136 -- The `process_trial` operation took 0.9517247676849365 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    339 |          8964.86 | 54847488 |  288.216 |              309.323 |              74.7778 |            795.744 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3050.3900127521447
    time_step_min: 2917
  date: 2020-10-15_02-42-54
  done: false
  episode_len_mean: 795.7608116328245
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.25586748345484
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 69046
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.033500663385015e-31
        cur_lr: 5.0e-05
        entropy: 0.056568604273100696
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009865931264357641
        total_loss: .inf
        vf_explained_var: 0.9994645714759827
        vf_loss: 0.2495333900054296
    num_steps_sampled: 55009280
    num_steps_trained: 55009280
  iterations_since_restore: 340
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.54375
    gpu_util_percent0: 0.2625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635545163254188
    mean_env_wait_ms: 1.207786284544229
    mean_inference_ms: 4.285928057208157
    mean_raw_obs_processing_ms: 0.3782910830758882
  time_since_restore: 8991.271156787872
  time_this_iter_s: 26.414588451385498
  time_total_s: 8991.271156787872
  timers:
    learn_throughput: 8347.903
    learn_time_ms: 19381.155
    sample_throughput: 23916.424
    sample_time_ms: 6764.891
    update_time_ms: 29.377
  timestamp: 1602729774
  timesteps_since_restore: 0
  timesteps_total: 55009280
  training_iteration: 340
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:42:55,959	WARNING util.py:136 -- The `process_trial` operation took 0.9551923274993896 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    340 |          8991.27 | 55009280 |  288.256 |              309.323 |              74.7778 |            795.761 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3050.110072254335
    time_step_min: 2917
  date: 2020-10-15_02-43-22
  done: false
  episode_len_mean: 795.7721771281666
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.2976897721546
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 69238
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0550250995077523e-30
        cur_lr: 5.0e-05
        entropy: 0.05819374291847149
        entropy_coeff: 0.0005000000000000001
        kl: 0.0042475618538446724
        model: {}
        policy_loss: -0.006586617622815538
        total_loss: 0.3776979446411133
        vf_explained_var: 0.9991950392723083
        vf_loss: 0.38431366533041
    num_steps_sampled: 55171072
    num_steps_trained: 55171072
  iterations_since_restore: 341
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6875
    gpu_util_percent0: 0.293125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635459250653893
    mean_env_wait_ms: 1.2077291842531752
    mean_inference_ms: 4.285874804327554
    mean_raw_obs_processing_ms: 0.3782892398055663
  time_since_restore: 9017.755443811417
  time_this_iter_s: 26.48428702354431
  time_total_s: 9017.755443811417
  timers:
    learn_throughput: 8354.089
    learn_time_ms: 19366.804
    sample_throughput: 23897.23
    sample_time_ms: 6770.324
    update_time_ms: 29.008
  timestamp: 1602729802
  timesteps_since_restore: 0
  timesteps_total: 55171072
  training_iteration: 341
  trial_id: 9f737_00000
  
2020-10-15 02:43:23,918	WARNING util.py:136 -- The `process_trial` operation took 0.9586620330810547 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    341 |          9017.76 | 55171072 |  288.298 |              309.323 |              74.7778 |            795.772 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3049.792780594607
    time_step_min: 2917
  date: 2020-10-15_02-43-50
  done: false
  episode_len_mean: 795.7861708560076
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.34468988058
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 224
  episodes_total: 69462
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.275125497538762e-31
        cur_lr: 5.0e-05
        entropy: 0.05732334032654762
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009331359178759158
        total_loss: .inf
        vf_explained_var: 0.9993963241577148
        vf_loss: 0.3523792376120885
    num_steps_sampled: 55332864
    num_steps_trained: 55332864
  iterations_since_restore: 342
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.784374999999997
    gpu_util_percent0: 0.30125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635359831006048
    mean_env_wait_ms: 1.2076615637939527
    mean_inference_ms: 4.28580752913852
    mean_raw_obs_processing_ms: 0.37828575205657583
  time_since_restore: 9044.517497777939
  time_this_iter_s: 26.762053966522217
  time_total_s: 9044.517497777939
  timers:
    learn_throughput: 8344.537
    learn_time_ms: 19388.972
    sample_throughput: 23963.895
    sample_time_ms: 6751.49
    update_time_ms: 29.67
  timestamp: 1602729830
  timesteps_since_restore: 0
  timesteps_total: 55332864
  training_iteration: 342
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:43:52,060	WARNING util.py:136 -- The `process_trial` operation took 1.0075569152832031 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    342 |          9044.52 | 55332864 |  288.345 |              309.323 |              74.7778 |            795.786 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3049.491648354805
    time_step_min: 2917
  date: 2020-10-15_02-44-18
  done: false
  episode_len_mean: 795.8047226010192
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.3881360073135
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 203
  episodes_total: 69665
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.912688246308141e-31
        cur_lr: 5.0e-05
        entropy: 0.05640241131186485
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006569571162496383
        total_loss: .inf
        vf_explained_var: 0.998936653137207
        vf_loss: 0.5759745066364607
    num_steps_sampled: 55494656
    num_steps_trained: 55494656
  iterations_since_restore: 343
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5875
    gpu_util_percent0: 0.3665625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635228814926415
    mean_env_wait_ms: 1.207598689344337
    mean_inference_ms: 4.285747355451643
    mean_raw_obs_processing_ms: 0.378282076491685
  time_since_restore: 9071.29949593544
  time_this_iter_s: 26.78199815750122
  time_total_s: 9071.29949593544
  timers:
    learn_throughput: 8336.563
    learn_time_ms: 19407.518
    sample_throughput: 23979.961
    sample_time_ms: 6746.967
    update_time_ms: 29.504
  timestamp: 1602729858
  timesteps_since_restore: 0
  timesteps_total: 55494656
  training_iteration: 343
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:44:20,159	WARNING util.py:136 -- The `process_trial` operation took 0.9402213096618652 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    343 |           9071.3 | 55494656 |  288.388 |              309.323 |              74.7778 |            795.805 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3049.240387370351
    time_step_min: 2917
  date: 2020-10-15_02-44-46
  done: false
  episode_len_mean: 795.8194209787807
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.4244005011022
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 177
  episodes_total: 69842
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1869032369462214e-30
        cur_lr: 5.0e-05
        entropy: 0.06038896397997936
        entropy_coeff: 0.0005000000000000001
        kl: 0.004986391363975902
        model: {}
        policy_loss: -0.005233056261204183
        total_loss: 0.2529036303361257
        vf_explained_var: 0.9994555115699768
        vf_loss: 0.2581668831408024
    num_steps_sampled: 55656448
    num_steps_trained: 55656448
  iterations_since_restore: 344
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.793548387096774
    gpu_util_percent0: 0.32096774193548383
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8806451612903237
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14635120727923934
    mean_env_wait_ms: 1.2075460900067967
    mean_inference_ms: 4.285698186020047
    mean_raw_obs_processing_ms: 0.3782799635131988
  time_since_restore: 9097.57805299759
  time_this_iter_s: 26.278557062149048
  time_total_s: 9097.57805299759
  timers:
    learn_throughput: 8331.614
    learn_time_ms: 19419.046
    sample_throughput: 24026.263
    sample_time_ms: 6733.964
    update_time_ms: 29.961
  timestamp: 1602729886
  timesteps_since_restore: 0
  timesteps_total: 55656448
  training_iteration: 344
  trial_id: 9f737_00000
  
2020-10-15 02:44:47,704	WARNING util.py:136 -- The `process_trial` operation took 0.9314963817596436 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    344 |          9097.58 | 55656448 |  288.424 |              309.323 |              74.7778 |            795.819 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3048.958992158375
    time_step_min: 2917
  date: 2020-10-15_02-45-14
  done: false
  episode_len_mean: 795.833916258619
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.46536097170656
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 70049
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.934516184731107e-31
        cur_lr: 5.0e-05
        entropy: 0.07135867017010848
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01060857872168223
        total_loss: .inf
        vf_explained_var: 0.9992900490760803
        vf_loss: 0.3517821008960406
    num_steps_sampled: 55818240
    num_steps_trained: 55818240
  iterations_since_restore: 345
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.79375
    gpu_util_percent0: 0.2740625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463503611611504
    mean_env_wait_ms: 1.2074844081289902
    mean_inference_ms: 4.285642284631008
    mean_raw_obs_processing_ms: 0.37827759058679783
  time_since_restore: 9124.386204719543
  time_this_iter_s: 26.808151721954346
  time_total_s: 9124.386204719543
  timers:
    learn_throughput: 8320.327
    learn_time_ms: 19445.39
    sample_throughput: 24002.686
    sample_time_ms: 6740.579
    update_time_ms: 31.551
  timestamp: 1602729914
  timesteps_since_restore: 0
  timesteps_total: 55818240
  training_iteration: 345
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:45:15,923	WARNING util.py:136 -- The `process_trial` operation took 0.9827783107757568 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    345 |          9124.39 | 55818240 |  288.465 |              309.323 |              74.7778 |            795.834 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3048.675626806384
    time_step_min: 2917
  date: 2020-10-15_02-45-42
  done: false
  episode_len_mean: 795.8491355389541
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.5095178034346
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 226
  episodes_total: 70275
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.901774277096662e-31
        cur_lr: 5.0e-05
        entropy: 0.06879583125313123
        entropy_coeff: 0.0005000000000000001
        kl: 0.005636563912654917
        model: {}
        policy_loss: -0.011375370255943077
        total_loss: 0.4655763531724612
        vf_explained_var: 0.9990909099578857
        vf_loss: 0.4769861126939456
    num_steps_sampled: 55980032
    num_steps_trained: 55980032
  iterations_since_restore: 346
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.115625
    gpu_util_percent0: 0.286875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634913788582715
    mean_env_wait_ms: 1.2074156166316172
    mean_inference_ms: 4.285571927138911
    mean_raw_obs_processing_ms: 0.3782733857748704
  time_since_restore: 9151.18171453476
  time_this_iter_s: 26.795509815216064
  time_total_s: 9151.18171453476
  timers:
    learn_throughput: 8312.284
    learn_time_ms: 19464.204
    sample_throughput: 24041.549
    sample_time_ms: 6729.683
    update_time_ms: 32.089
  timestamp: 1602729942
  timesteps_since_restore: 0
  timesteps_total: 55980032
  training_iteration: 346
  trial_id: 9f737_00000
  
2020-10-15 02:45:44,138	WARNING util.py:136 -- The `process_trial` operation took 0.9888274669647217 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    346 |          9151.18 | 55980032 |   288.51 |              309.323 |              74.7778 |            795.849 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3048.4507440222637
    time_step_min: 2917
  date: 2020-10-15_02-46-10
  done: false
  episode_len_mean: 795.8648142366532
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.54682091829085
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 70466
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.901774277096662e-31
        cur_lr: 5.0e-05
        entropy: 0.06547182984650135
        entropy_coeff: 0.0005000000000000001
        kl: 0.005120703368447721
        model: {}
        policy_loss: -0.01063225963540996
        total_loss: 0.29070183386405307
        vf_explained_var: 0.9993565678596497
        vf_loss: 0.30136683334906894
    num_steps_sampled: 56141824
    num_steps_trained: 56141824
  iterations_since_restore: 347
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.903125000000003
    gpu_util_percent0: 0.280625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634774945952259
    mean_env_wait_ms: 1.2073584536910664
    mean_inference_ms: 4.2855202944068305
    mean_raw_obs_processing_ms: 0.3782710392224728
  time_since_restore: 9177.813356399536
  time_this_iter_s: 26.63164186477661
  time_total_s: 9177.813356399536
  timers:
    learn_throughput: 8305.838
    learn_time_ms: 19479.311
    sample_throughput: 24052.719
    sample_time_ms: 6726.558
    update_time_ms: 34.268
  timestamp: 1602729970
  timesteps_since_restore: 0
  timesteps_total: 56141824
  training_iteration: 347
  trial_id: 9f737_00000
  
2020-10-15 02:46:12,269	WARNING util.py:136 -- The `process_trial` operation took 1.0283246040344238 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    347 |          9177.81 | 56141824 |  288.547 |              309.323 |              74.7778 |            795.865 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3048.224502223355
    time_step_min: 2917
  date: 2020-10-15_02-46-39
  done: false
  episode_len_mean: 795.873634150484
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.5807454606073
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 70652
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.901774277096662e-31
        cur_lr: 5.0e-05
        entropy: 0.07655092515051365
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009878397133434191
        total_loss: .inf
        vf_explained_var: 0.9983232021331787
        vf_loss: 0.8428117781877518
    num_steps_sampled: 56303616
    num_steps_trained: 56303616
  iterations_since_restore: 348
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.012500000000003
    gpu_util_percent0: 0.264375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634722165042363
    mean_env_wait_ms: 1.2073038320323748
    mean_inference_ms: 4.285471609146366
    mean_raw_obs_processing_ms: 0.37826967878685125
  time_since_restore: 9204.646014213562
  time_this_iter_s: 26.83265781402588
  time_total_s: 9204.646014213562
  timers:
    learn_throughput: 8301.649
    learn_time_ms: 19489.14
    sample_throughput: 24014.154
    sample_time_ms: 6737.36
    update_time_ms: 32.061
  timestamp: 1602729999
  timesteps_since_restore: 0
  timesteps_total: 56303616
  training_iteration: 348
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:46:40,489	WARNING util.py:136 -- The `process_trial` operation took 0.9627082347869873 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    348 |          9204.65 | 56303616 |  288.581 |              309.323 |              74.7778 |            795.874 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3047.99233462266
    time_step_min: 2917
  date: 2020-10-15_02-47-07
  done: false
  episode_len_mean: 795.8728624640216
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.6150963042012
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 224
  episodes_total: 70876
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.335266141564499e-30
        cur_lr: 5.0e-05
        entropy: 0.08014378262062867
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01037901220358132
        total_loss: .inf
        vf_explained_var: 0.9979510307312012
        vf_loss: 1.0977667272090912
    num_steps_sampled: 56465408
    num_steps_trained: 56465408
  iterations_since_restore: 349
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.828125
    gpu_util_percent0: 0.33781249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463461960366126
    mean_env_wait_ms: 1.2072374657005598
    mean_inference_ms: 4.285408740135853
    mean_raw_obs_processing_ms: 0.3782661694159291
  time_since_restore: 9231.255586862564
  time_this_iter_s: 26.609572649002075
  time_total_s: 9231.255586862564
  timers:
    learn_throughput: 8303.13
    learn_time_ms: 19485.664
    sample_throughput: 24024.257
    sample_time_ms: 6734.527
    update_time_ms: 31.93
  timestamp: 1602730027
  timesteps_since_restore: 0
  timesteps_total: 56465408
  training_iteration: 349
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:47:08,514	WARNING util.py:136 -- The `process_trial` operation took 0.9843900203704834 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    349 |          9231.26 | 56465408 |  288.615 |              309.323 |              74.7778 |            795.873 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3047.7616795461904
    time_step_min: 2917
  date: 2020-10-15_02-47-35
  done: false
  episode_len_mean: 795.8815154542001
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.6516851808983
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 205
  episodes_total: 71081
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.0028992123467484e-30
        cur_lr: 5.0e-05
        entropy: 0.06857382878661156
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009547196289834877
        total_loss: .inf
        vf_explained_var: 0.998999297618866
        vf_loss: 0.4872501865029335
    num_steps_sampled: 56627200
    num_steps_trained: 56627200
  iterations_since_restore: 350
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.796875
    gpu_util_percent0: 0.27781249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634492174759325
    mean_env_wait_ms: 1.2071742598035022
    mean_inference_ms: 4.285346138446238
    mean_raw_obs_processing_ms: 0.37826268096384635
  time_since_restore: 9257.766681432724
  time_this_iter_s: 26.511094570159912
  time_total_s: 9257.766681432724
  timers:
    learn_throughput: 8293.345
    learn_time_ms: 19508.654
    sample_throughput: 24048.016
    sample_time_ms: 6727.873
    update_time_ms: 33.038
  timestamp: 1602730055
  timesteps_since_restore: 0
  timesteps_total: 56627200
  training_iteration: 350
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:47:36,498	WARNING util.py:136 -- The `process_trial` operation took 1.0389411449432373 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    350 |          9257.77 | 56627200 |  288.652 |              309.323 |              74.7778 |            795.882 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3047.521558441558
    time_step_min: 2917
  date: 2020-10-15_02-48-03
  done: false
  episode_len_mean: 795.9026984550187
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.69002940735817
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 71263
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0043488185201226e-30
        cur_lr: 5.0e-05
        entropy: 0.06333648009846608
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006850370661898826
        total_loss: .inf
        vf_explained_var: 0.999680757522583
        vf_loss: 0.14796175186832747
    num_steps_sampled: 56788992
    num_steps_trained: 56788992
  iterations_since_restore: 351
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.2875
    gpu_util_percent0: 0.28312499999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463437333198715
    mean_env_wait_ms: 1.2071211806769908
    mean_inference_ms: 4.285299902636008
    mean_raw_obs_processing_ms: 0.3782606778581302
  time_since_restore: 9284.389788627625
  time_this_iter_s: 26.623107194900513
  time_total_s: 9284.389788627625
  timers:
    learn_throughput: 8289.832
    learn_time_ms: 19516.92
    sample_throughput: 24041.5
    sample_time_ms: 6729.696
    update_time_ms: 33.338
  timestamp: 1602730083
  timesteps_since_restore: 0
  timesteps_total: 56788992
  training_iteration: 351
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:48:04,494	WARNING util.py:136 -- The `process_trial` operation took 1.0382390022277832 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    351 |          9284.39 | 56788992 |   288.69 |              309.323 |              74.7778 |            795.903 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3047.2450508218294
    time_step_min: 2917
  date: 2020-10-15_02-48-30
  done: false
  episode_len_mean: 795.9241296317027
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.731143575009
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 201
  episodes_total: 71464
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5065232277801844e-30
        cur_lr: 5.0e-05
        entropy: 0.06715309744079907
        entropy_coeff: 0.0005000000000000001
        kl: 0.0040237448799113435
        model: {}
        policy_loss: -0.008851053051936711
        total_loss: 0.2690552497903506
        vf_explained_var: 0.9994257092475891
        vf_loss: 0.2779398784041405
    num_steps_sampled: 56950784
    num_steps_trained: 56950784
  iterations_since_restore: 352
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.59375
    gpu_util_percent0: 0.30874999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634283671007026
    mean_env_wait_ms: 1.207062090549935
    mean_inference_ms: 4.285246281518758
    mean_raw_obs_processing_ms: 0.3782587477171457
  time_since_restore: 9310.814466953278
  time_this_iter_s: 26.424678325653076
  time_total_s: 9310.814466953278
  timers:
    learn_throughput: 8305.649
    learn_time_ms: 19479.753
    sample_throughput: 24019.374
    sample_time_ms: 6735.896
    update_time_ms: 31.789
  timestamp: 1602730110
  timesteps_since_restore: 0
  timesteps_total: 56950784
  training_iteration: 352
  trial_id: 9f737_00000
  
2020-10-15 02:48:32,221	WARNING util.py:136 -- The `process_trial` operation took 0.9599151611328125 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    352 |          9310.81 | 56950784 |  288.731 |              309.323 |              74.7778 |            795.924 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3046.9538590369852
    time_step_min: 2917
  date: 2020-10-15_02-48-58
  done: false
  episode_len_mean: 795.9451930588104
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.7743238376398
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 224
  episodes_total: 71688
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2532616138900922e-30
        cur_lr: 5.0e-05
        entropy: 0.06757176791628201
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008076323535836613
        total_loss: .inf
        vf_explained_var: 0.9992229342460632
        vf_loss: 0.40117280681927997
    num_steps_sampled: 57112576
    num_steps_trained: 57112576
  iterations_since_restore: 353
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.206451612903226
    gpu_util_percent0: 0.2787096774193549
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.874193548387098
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463417450862245
    mean_env_wait_ms: 1.2069943352289758
    mean_inference_ms: 4.285183406381704
    mean_raw_obs_processing_ms: 0.378254615570509
  time_since_restore: 9337.074436426163
  time_this_iter_s: 26.259969472885132
  time_total_s: 9337.074436426163
  timers:
    learn_throughput: 8325.533
    learn_time_ms: 19433.23
    sample_throughput: 24008.896
    sample_time_ms: 6738.836
    update_time_ms: 31.644
  timestamp: 1602730138
  timesteps_since_restore: 0
  timesteps_total: 57112576
  training_iteration: 353
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:48:59,882	WARNING util.py:136 -- The `process_trial` operation took 0.969928503036499 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    353 |          9337.07 | 57112576 |  288.774 |              309.323 |              74.7778 |            795.945 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3046.689915790939
    time_step_min: 2917
  date: 2020-10-15_02-49-26
  done: false
  episode_len_mean: 795.9653603772797
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.8136943071211
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 71883
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.379892420835138e-30
        cur_lr: 5.0e-05
        entropy: 0.06155322305858135
        entropy_coeff: 0.0005000000000000001
        kl: 0.003996451967395842
        model: {}
        policy_loss: -0.007795992501390477
        total_loss: 0.26820852359135944
        vf_explained_var: 0.9994456171989441
        vf_loss: 0.27603528027733165
    num_steps_sampled: 57274368
    num_steps_trained: 57274368
  iterations_since_restore: 354
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.796875
    gpu_util_percent0: 0.3234375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14634050391153286
    mean_env_wait_ms: 1.2069365731922197
    mean_inference_ms: 4.285127902410118
    mean_raw_obs_processing_ms: 0.3782520128113339
  time_since_restore: 9363.739926815033
  time_this_iter_s: 26.66549038887024
  time_total_s: 9363.739926815033
  timers:
    learn_throughput: 8313.989
    learn_time_ms: 19460.213
    sample_throughput: 23981.432
    sample_time_ms: 6746.553
    update_time_ms: 33.547
  timestamp: 1602730166
  timesteps_since_restore: 0
  timesteps_total: 57274368
  training_iteration: 354
  trial_id: 9f737_00000
  
2020-10-15 02:49:27,955	WARNING util.py:136 -- The `process_trial` operation took 0.9653124809265137 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    354 |          9363.74 | 57274368 |  288.814 |              309.323 |              74.7778 |            795.965 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3046.4510364744597
    time_step_min: 2917
  date: 2020-10-15_02-49-54
  done: false
  episode_len_mean: 795.9820013599589
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.8492336529139
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 178
  episodes_total: 72061
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.689946210417569e-30
        cur_lr: 5.0e-05
        entropy: 0.06452987529337406
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008561738210119074
        total_loss: .inf
        vf_explained_var: 0.9993934035301208
        vf_loss: 0.27869947751363117
    num_steps_sampled: 57436160
    num_steps_trained: 57436160
  iterations_since_restore: 355
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.8125
    gpu_util_percent0: 0.333125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463395029775719
    mean_env_wait_ms: 1.2068840727337524
    mean_inference_ms: 4.285079504758286
    mean_raw_obs_processing_ms: 0.37825031649048246
  time_since_restore: 9390.151951313019
  time_this_iter_s: 26.41202449798584
  time_total_s: 9390.151951313019
  timers:
    learn_throughput: 8331.94
    learn_time_ms: 19418.288
    sample_throughput: 23967.439
    sample_time_ms: 6750.492
    update_time_ms: 33.077
  timestamp: 1602730194
  timesteps_since_restore: 0
  timesteps_total: 57436160
  training_iteration: 355
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:49:55,682	WARNING util.py:136 -- The `process_trial` operation took 0.968332052230835 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    355 |          9390.15 | 57436160 |  288.849 |              309.323 |              74.7778 |            795.982 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3046.1722710597355
    time_step_min: 2917
  date: 2020-10-15_02-50-22
  done: false
  episode_len_mean: 796.0040955820292
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.8915861533437
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 212
  episodes_total: 72273
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.5349193156263536e-30
        cur_lr: 5.0e-05
        entropy: 0.06410428540160258
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008138772502510013
        total_loss: .inf
        vf_explained_var: 0.9996083378791809
        vf_loss: 0.21646800761421522
    num_steps_sampled: 57597952
    num_steps_trained: 57597952
  iterations_since_restore: 356
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.44375
    gpu_util_percent0: 0.35562499999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463388303404559
    mean_env_wait_ms: 1.2068220377722063
    mean_inference_ms: 4.285029122539992
    mean_raw_obs_processing_ms: 0.3782476335596588
  time_since_restore: 9416.890911579132
  time_this_iter_s: 26.73896026611328
  time_total_s: 9416.890911579132
  timers:
    learn_throughput: 8338.432
    learn_time_ms: 19403.169
    sample_throughput: 23937.111
    sample_time_ms: 6759.045
    update_time_ms: 32.841
  timestamp: 1602730222
  timesteps_since_restore: 0
  timesteps_total: 57597952
  training_iteration: 356
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:50:23,745	WARNING util.py:136 -- The `process_trial` operation took 0.9770481586456299 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    356 |          9416.89 | 57597952 |  288.892 |              309.323 |              74.7778 |            796.004 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3045.8986667402764
    time_step_min: 2917
  date: 2020-10-15_02-50-50
  done: false
  episode_len_mean: 796.024044032445
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.9343701875568
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 72492
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.8023789734395296e-30
        cur_lr: 5.0e-05
        entropy: 0.06522705064465602
        entropy_coeff: 0.0005000000000000001
        kl: 0.004351841441045205
        model: {}
        policy_loss: -0.008759802207350731
        total_loss: 0.2834951455394427
        vf_explained_var: 0.9994195103645325
        vf_loss: 0.29228756576776505
    num_steps_sampled: 57759744
    num_steps_trained: 57759744
  iterations_since_restore: 357
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.878125
    gpu_util_percent0: 0.3234375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633760634004694
    mean_env_wait_ms: 1.2067567631655596
    mean_inference_ms: 4.284966565475149
    mean_raw_obs_processing_ms: 0.378244279535115
  time_since_restore: 9443.327682971954
  time_this_iter_s: 26.436771392822266
  time_total_s: 9443.327682971954
  timers:
    learn_throughput: 8352.0
    learn_time_ms: 19371.647
    sample_throughput: 23917.753
    sample_time_ms: 6764.515
    update_time_ms: 30.247
  timestamp: 1602730250
  timesteps_since_restore: 0
  timesteps_total: 57759744
  training_iteration: 357
  trial_id: 9f737_00000
  
2020-10-15 02:50:51,531	WARNING util.py:136 -- The `process_trial` operation took 1.0040276050567627 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    357 |          9443.33 | 57759744 |  288.934 |              309.323 |              74.7778 |            796.024 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3045.6541717715404
    time_step_min: 2917
  date: 2020-10-15_02-51-17
  done: false
  episode_len_mean: 796.0414964020858
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 288.97175035949954
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 189
  episodes_total: 72681
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9011894867197648e-30
        cur_lr: 5.0e-05
        entropy: 0.06213834478209416
        entropy_coeff: 0.0005000000000000001
        kl: 0.005306640950342019
        model: {}
        policy_loss: -0.009041636173302928
        total_loss: 0.1951240636408329
        vf_explained_var: 0.9995556473731995
        vf_loss: 0.20419676477710405
    num_steps_sampled: 57921536
    num_steps_trained: 57921536
  iterations_since_restore: 358
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.651612903225804
    gpu_util_percent0: 0.3203225806451613
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8774193548387106
    vram_util_percent0: 0.10437848474909812
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633639835998818
    mean_env_wait_ms: 1.206701079450986
    mean_inference_ms: 4.284917492492332
    mean_raw_obs_processing_ms: 0.37824214480088447
  time_since_restore: 9469.50673031807
  time_this_iter_s: 26.179047346115112
  time_total_s: 9469.50673031807
  timers:
    learn_throughput: 8369.736
    learn_time_ms: 19330.598
    sample_throughput: 24005.588
    sample_time_ms: 6739.764
    update_time_ms: 30.393
  timestamp: 1602730277
  timesteps_since_restore: 0
  timesteps_total: 57921536
  training_iteration: 358
  trial_id: 9f737_00000
  
2020-10-15 02:51:19,038	WARNING util.py:136 -- The `process_trial` operation took 0.9902770519256592 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    358 |          9469.51 | 57921536 |  288.972 |              309.323 |              74.7778 |            796.041 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3045.3935720365744
    time_step_min: 2917
  date: 2020-10-15_02-51-45
  done: false
  episode_len_mean: 796.0645891651573
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.0109376325414
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 72876
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9011894867197648e-30
        cur_lr: 5.0e-05
        entropy: 0.0594357056543231
        entropy_coeff: 0.0005000000000000001
        kl: 0.005252052564173937
        model: {}
        policy_loss: -0.007364990373995776
        total_loss: 0.14939945377409458
        vf_explained_var: 0.9996814727783203
        vf_loss: 0.15679416432976723
    num_steps_sampled: 58083328
    num_steps_trained: 58083328
  iterations_since_restore: 359
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.778125000000003
    gpu_util_percent0: 0.3303125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633560896159317
    mean_env_wait_ms: 1.2066449447890357
    mean_inference_ms: 4.284868655709185
    mean_raw_obs_processing_ms: 0.3782407289601465
  time_since_restore: 9496.042026042938
  time_this_iter_s: 26.535295724868774
  time_total_s: 9496.042026042938
  timers:
    learn_throughput: 8377.448
    learn_time_ms: 19312.802
    sample_throughput: 24008.829
    sample_time_ms: 6738.854
    update_time_ms: 39.491
  timestamp: 1602730305
  timesteps_since_restore: 0
  timesteps_total: 58083328
  training_iteration: 359
  trial_id: 9f737_00000
  
2020-10-15 02:51:46,929	WARNING util.py:136 -- The `process_trial` operation took 1.0109593868255615 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    359 |          9496.04 | 58083328 |  289.011 |              309.323 |              74.7778 |            796.065 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3045.1014782370653
    time_step_min: 2917
  date: 2020-10-15_02-52-13
  done: false
  episode_len_mean: 796.0908369586035
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.05570230748765
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 73098
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9011894867197648e-30
        cur_lr: 5.0e-05
        entropy: 0.05946210895975431
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00713437106848384
        total_loss: .inf
        vf_explained_var: 0.9995601773262024
        vf_loss: 0.24874461566408476
    num_steps_sampled: 58245120
    num_steps_trained: 58245120
  iterations_since_restore: 360
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.896875
    gpu_util_percent0: 0.3084375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633473352360984
    mean_env_wait_ms: 1.206579049122803
    mean_inference_ms: 4.284811195956285
    mean_raw_obs_processing_ms: 0.37823711460962234
  time_since_restore: 9522.551652669907
  time_this_iter_s: 26.509626626968384
  time_total_s: 9522.551652669907
  timers:
    learn_throughput: 8379.861
    learn_time_ms: 19307.242
    sample_throughput: 24022.62
    sample_time_ms: 6734.986
    update_time_ms: 38.545
  timestamp: 1602730333
  timesteps_since_restore: 0
  timesteps_total: 58245120
  training_iteration: 360
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:52:14,788	WARNING util.py:136 -- The `process_trial` operation took 0.9819216728210449 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    360 |          9522.55 | 58245120 |  289.056 |              309.323 |              74.7778 |            796.091 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3044.846542357149
    time_step_min: 2917
  date: 2020-10-15_02-52-41
  done: false
  episode_len_mean: 796.1144537218947
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.0949859597944
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 198
  episodes_total: 73296
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.851784230079647e-30
        cur_lr: 5.0e-05
        entropy: 0.060905796786149345
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006839940655481769
        total_loss: .inf
        vf_explained_var: 0.9996911883354187
        vf_loss: 0.15906320015589395
    num_steps_sampled: 58406912
    num_steps_trained: 58406912
  iterations_since_restore: 361
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.140625
    gpu_util_percent0: 0.27749999999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633361626437869
    mean_env_wait_ms: 1.2065196132221234
    mean_inference_ms: 4.284755872224253
    mean_raw_obs_processing_ms: 0.3782343481986294
  time_since_restore: 9548.948823213577
  time_this_iter_s: 26.397170543670654
  time_total_s: 9548.948823213577
  timers:
    learn_throughput: 8383.864
    learn_time_ms: 19298.023
    sample_throughput: 24097.036
    sample_time_ms: 6714.187
    update_time_ms: 36.604
  timestamp: 1602730361
  timesteps_since_restore: 0
  timesteps_total: 58406912
  training_iteration: 361
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:52:42,609	WARNING util.py:136 -- The `process_trial` operation took 0.9848637580871582 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    361 |          9548.95 | 58406912 |  289.095 |              309.323 |              74.7778 |            796.114 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3044.6187940848063
    time_step_min: 2917
  date: 2020-10-15_02-53-09
  done: false
  episode_len_mean: 796.1345473351843
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.13076584891877
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 180
  episodes_total: 73476
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2776763451194715e-30
        cur_lr: 5.0e-05
        entropy: 0.058759771597882114
        entropy_coeff: 0.0005000000000000001
        kl: 0.004177543819726755
        model: {}
        policy_loss: -0.007696786100192791
        total_loss: 0.18921058004101118
        vf_explained_var: 0.9996006488800049
        vf_loss: 0.19693675016363463
    num_steps_sampled: 58568704
    num_steps_trained: 58568704
  iterations_since_restore: 362
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.11875
    gpu_util_percent0: 0.28156250000000005
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463325873080133
    mean_env_wait_ms: 1.2064673895043865
    mean_inference_ms: 4.2847135119091755
    mean_raw_obs_processing_ms: 0.3782326255385857
  time_since_restore: 9575.630795240402
  time_this_iter_s: 26.68197202682495
  time_total_s: 9575.630795240402
  timers:
    learn_throughput: 8377.889
    learn_time_ms: 19311.786
    sample_throughput: 24063.065
    sample_time_ms: 6723.665
    update_time_ms: 35.945
  timestamp: 1602730389
  timesteps_since_restore: 0
  timesteps_total: 58568704
  training_iteration: 362
  trial_id: 9f737_00000
  
2020-10-15 02:53:10,742	WARNING util.py:136 -- The `process_trial` operation took 1.0631699562072754 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    362 |          9575.63 | 58568704 |  289.131 |              309.323 |              74.7778 |            796.135 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3044.3465643671916
    time_step_min: 2917
  date: 2020-10-15_02-53-37
  done: false
  episode_len_mean: 796.1588805342165
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.1698046728585
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 73678
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1388381725597357e-30
        cur_lr: 5.0e-05
        entropy: 0.058558824472129345
        entropy_coeff: 0.0005000000000000001
        kl: 0.004680612939409912
        model: {}
        policy_loss: -0.0043689587619155645
        total_loss: 0.23670028150081635
        vf_explained_var: 0.999531090259552
        vf_loss: 0.24109851196408272
    num_steps_sampled: 58730496
    num_steps_trained: 58730496
  iterations_since_restore: 363
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.36875
    gpu_util_percent0: 0.2846875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.146331746593464
    mean_env_wait_ms: 1.2064087392234337
    mean_inference_ms: 4.284662855621142
    mean_raw_obs_processing_ms: 0.3782305909744239
  time_since_restore: 9602.22884440422
  time_this_iter_s: 26.59804916381836
  time_total_s: 9602.22884440422
  timers:
    learn_throughput: 8371.885
    learn_time_ms: 19325.636
    sample_throughput: 24034.655
    sample_time_ms: 6731.613
    update_time_ms: 38.155
  timestamp: 1602730417
  timesteps_since_restore: 0
  timesteps_total: 58730496
  training_iteration: 363
  trial_id: 9f737_00000
  
2020-10-15 02:53:38,776	WARNING util.py:136 -- The `process_trial` operation took 1.0000035762786865 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    363 |          9602.23 | 58730496 |   289.17 |              309.323 |              74.7778 |            796.159 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3044.053665470791
    time_step_min: 2917
  date: 2020-10-15_02-54-05
  done: false
  episode_len_mean: 796.1854863807964
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.2143426607386
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 225
  episodes_total: 73903
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0694190862798679e-30
        cur_lr: 5.0e-05
        entropy: 0.05928088538348675
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00754935546622922
        total_loss: .inf
        vf_explained_var: 0.9997360706329346
        vf_loss: 0.13816745082537332
    num_steps_sampled: 58892288
    num_steps_trained: 58892288
  iterations_since_restore: 364
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.375
    gpu_util_percent0: 0.27531249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14633066677680096
    mean_env_wait_ms: 1.2063420370421907
    mean_inference_ms: 4.284603590393994
    mean_raw_obs_processing_ms: 0.37822709573884483
  time_since_restore: 9628.94088602066
  time_this_iter_s: 26.71204161643982
  time_total_s: 9628.94088602066
  timers:
    learn_throughput: 8380.527
    learn_time_ms: 19305.707
    sample_throughput: 23970.657
    sample_time_ms: 6749.586
    update_time_ms: 35.906
  timestamp: 1602730445
  timesteps_since_restore: 0
  timesteps_total: 58892288
  training_iteration: 364
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:54:06,871	WARNING util.py:136 -- The `process_trial` operation took 1.0302119255065918 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    364 |          9628.94 | 58892288 |  289.214 |              309.323 |              74.7778 |            796.185 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3043.817019322954
    time_step_min: 2917
  date: 2020-10-15_02-54-33
  done: false
  episode_len_mean: 796.2060732842972
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.25051241206177
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 74095
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6041286294198015e-30
        cur_lr: 5.0e-05
        entropy: 0.05907756214340528
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007284804966426843
        total_loss: .inf
        vf_explained_var: 0.9993400573730469
        vf_loss: 0.3367946371436119
    num_steps_sampled: 59054080
    num_steps_trained: 59054080
  iterations_since_restore: 365
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.721874999999997
    gpu_util_percent0: 0.30656249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632959730519207
    mean_env_wait_ms: 1.2062851183283587
    mean_inference_ms: 4.284553609772681
    mean_raw_obs_processing_ms: 0.37822470525396895
  time_since_restore: 9655.757911920547
  time_this_iter_s: 26.817025899887085
  time_total_s: 9655.757911920547
  timers:
    learn_throughput: 8365.467
    learn_time_ms: 19340.463
    sample_throughput: 23981.469
    sample_time_ms: 6746.542
    update_time_ms: 44.108
  timestamp: 1602730473
  timesteps_since_restore: 0
  timesteps_total: 59054080
  training_iteration: 365
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:54:35,032	WARNING util.py:136 -- The `process_trial` operation took 0.9921746253967285 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    365 |          9655.76 | 59054080 |  289.251 |              309.323 |              74.7778 |            796.206 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3043.594900391967
    time_step_min: 2917
  date: 2020-10-15_02-55-01
  done: false
  episode_len_mean: 796.2251915076939
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.2846248127283
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 74279
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.4061929441297032e-30
        cur_lr: 5.0e-05
        entropy: 0.056626539677381516
        entropy_coeff: 0.0005000000000000001
        kl: 0.004395067847023408
        model: {}
        policy_loss: -0.008871398548459789
        total_loss: 0.20398551101485887
        vf_explained_var: 0.9995562434196472
        vf_loss: 0.21288521960377693
    num_steps_sampled: 59215872
    num_steps_trained: 59215872
  iterations_since_restore: 366
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.875
    gpu_util_percent0: 0.3359375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632885179210076
    mean_env_wait_ms: 1.206232243005525
    mean_inference_ms: 4.284511666173775
    mean_raw_obs_processing_ms: 0.37822342332275766
  time_since_restore: 9682.269548654556
  time_this_iter_s: 26.51163673400879
  time_total_s: 9682.269548654556
  timers:
    learn_throughput: 8384.869
    learn_time_ms: 19295.709
    sample_throughput: 23928.617
    sample_time_ms: 6761.444
    update_time_ms: 42.435
  timestamp: 1602730501
  timesteps_since_restore: 0
  timesteps_total: 59215872
  training_iteration: 366
  trial_id: 9f737_00000
  
2020-10-15 02:55:02,886	WARNING util.py:136 -- The `process_trial` operation took 0.9845316410064697 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    366 |          9682.27 | 59215872 |  289.285 |              309.323 |              74.7778 |            796.225 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3043.3271633130053
    time_step_min: 2917
  date: 2020-10-15_02-55-29
  done: false
  episode_len_mean: 796.2502685140432
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.3250875111678
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 205
  episodes_total: 74484
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2030964720648516e-30
        cur_lr: 5.0e-05
        entropy: 0.05656571500003338
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006064304785127206
        total_loss: .inf
        vf_explained_var: 0.999784529209137
        vf_loss: 0.11251368435720603
    num_steps_sampled: 59377664
    num_steps_trained: 59377664
  iterations_since_restore: 367
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.278125000000003
    gpu_util_percent0: 0.28125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.86875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632802954629634
    mean_env_wait_ms: 1.2061725945415578
    mean_inference_ms: 4.2844617744043605
    mean_raw_obs_processing_ms: 0.37822103962252523
  time_since_restore: 9708.634302854538
  time_this_iter_s: 26.36475419998169
  time_total_s: 9708.634302854538
  timers:
    learn_throughput: 8383.378
    learn_time_ms: 19299.142
    sample_throughput: 23946.107
    sample_time_ms: 6756.505
    update_time_ms: 44.232
  timestamp: 1602730529
  timesteps_since_restore: 0
  timesteps_total: 59377664
  training_iteration: 367
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:55:30,831	WARNING util.py:136 -- The `process_trial` operation took 1.0501575469970703 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    367 |          9708.63 | 59377664 |  289.325 |              309.323 |              74.7778 |             796.25 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3043.037366905511
    time_step_min: 2917
  date: 2020-10-15_02-55-57
  done: false
  episode_len_mean: 796.2743531049623
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.3670847667876
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 74703
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8046447080972773e-30
        cur_lr: 5.0e-05
        entropy: 0.05968419462442398
        entropy_coeff: 0.0005000000000000001
        kl: 0.005202712452349563
        model: {}
        policy_loss: -0.008070259384112433
        total_loss: 0.1626421424249808
        vf_explained_var: 0.9996685981750488
        vf_loss: 0.1707422435283661
    num_steps_sampled: 59539456
    num_steps_trained: 59539456
  iterations_since_restore: 368
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.603125
    gpu_util_percent0: 0.3125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632689303820082
    mean_env_wait_ms: 1.2061076424895143
    mean_inference_ms: 4.28440572791052
    mean_raw_obs_processing_ms: 0.37821780538747
  time_since_restore: 9735.379870176315
  time_this_iter_s: 26.745567321777344
  time_total_s: 9735.379870176315
  timers:
    learn_throughput: 8372.852
    learn_time_ms: 19323.404
    sample_throughput: 23866.621
    sample_time_ms: 6779.007
    update_time_ms: 51.984
  timestamp: 1602730557
  timesteps_since_restore: 0
  timesteps_total: 59539456
  training_iteration: 368
  trial_id: 9f737_00000
  
2020-10-15 02:55:58,945	WARNING util.py:136 -- The `process_trial` operation took 1.0096747875213623 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    368 |          9735.38 | 59539456 |  289.367 |              309.323 |              74.7778 |            796.274 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3042.7990301508207
    time_step_min: 2917
  date: 2020-10-15_02-56-25
  done: false
  episode_len_mean: 796.2965217971828
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.4030410251119
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 74895
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.8046447080972773e-30
        cur_lr: 5.0e-05
        entropy: 0.060277419785658516
        entropy_coeff: 0.0005000000000000001
        kl: 0.003964158027277638
        model: {}
        policy_loss: -0.008506860758643597
        total_loss: 0.20142260566353798
        vf_explained_var: 0.9995877146720886
        vf_loss: 0.20995960757136345
    num_steps_sampled: 59701248
    num_steps_trained: 59701248
  iterations_since_restore: 369
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.665625
    gpu_util_percent0: 0.30156249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632572373780944
    mean_env_wait_ms: 1.2060519463120578
    mean_inference_ms: 4.284360538736896
    mean_raw_obs_processing_ms: 0.37821584564034566
  time_since_restore: 9762.051327228546
  time_this_iter_s: 26.671457052230835
  time_total_s: 9762.051327228546
  timers:
    learn_throughput: 8362.337
    learn_time_ms: 19347.702
    sample_throughput: 23874.951
    sample_time_ms: 6776.642
    update_time_ms: 42.829
  timestamp: 1602730585
  timesteps_since_restore: 0
  timesteps_total: 59701248
  training_iteration: 369
  trial_id: 9f737_00000
  
2020-10-15 02:56:27,140	WARNING util.py:136 -- The `process_trial` operation took 1.0516605377197266 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    369 |          9762.05 | 59701248 |  289.403 |              309.323 |              74.7778 |            796.297 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3042.577055821329
    time_step_min: 2917
  date: 2020-10-15_02-56-53
  done: false
  episode_len_mean: 796.3167645609409
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.4381829240582
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 75081
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 9.023223540486386e-31
        cur_lr: 5.0e-05
        entropy: 0.05950537541260322
        entropy_coeff: 0.0005000000000000001
        kl: 0.0046953188721090555
        model: {}
        policy_loss: -0.007870071812552245
        total_loss: 0.20323686425884566
        vf_explained_var: 0.9996318221092224
        vf_loss: 0.21113669499754906
    num_steps_sampled: 59863040
    num_steps_trained: 59863040
  iterations_since_restore: 370
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.9
    gpu_util_percent0: 0.358125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632525852217215
    mean_env_wait_ms: 1.2059989126998427
    mean_inference_ms: 4.284321696964059
    mean_raw_obs_processing_ms: 0.37821476263177367
  time_since_restore: 9788.139304161072
  time_this_iter_s: 26.087976932525635
  time_total_s: 9788.139304161072
  timers:
    learn_throughput: 8385.254
    learn_time_ms: 19294.823
    sample_throughput: 23846.649
    sample_time_ms: 6784.685
    update_time_ms: 44.92
  timestamp: 1602730613
  timesteps_since_restore: 0
  timesteps_total: 59863040
  training_iteration: 370
  trial_id: 9f737_00000
  
2020-10-15 02:56:54,677	WARNING util.py:136 -- The `process_trial` operation took 1.0935451984405518 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    370 |          9788.14 | 59863040 |  289.438 |              309.323 |              74.7778 |            796.317 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3042.305872193437
    time_step_min: 2917
  date: 2020-10-15_02-57-21
  done: false
  episode_len_mean: 796.3395256812025
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.47938901953086
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 227
  episodes_total: 75308
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.511611770243193e-31
        cur_lr: 5.0e-05
        entropy: 0.06274569903810819
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007404180767480284
        total_loss: .inf
        vf_explained_var: 0.9995136260986328
        vf_loss: 0.27014240249991417
    num_steps_sampled: 60024832
    num_steps_trained: 60024832
  iterations_since_restore: 371
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.109375
    gpu_util_percent0: 0.2953125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632418134517206
    mean_env_wait_ms: 1.2059322032180215
    mean_inference_ms: 4.284264127939912
    mean_raw_obs_processing_ms: 0.3782112255681198
  time_since_restore: 9814.76307606697
  time_this_iter_s: 26.623771905899048
  time_total_s: 9814.76307606697
  timers:
    learn_throughput: 8384.903
    learn_time_ms: 19295.632
    sample_throughput: 23767.526
    sample_time_ms: 6807.271
    update_time_ms: 45.015
  timestamp: 1602730641
  timesteps_since_restore: 0
  timesteps_total: 60024832
  training_iteration: 371
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:57:22,767	WARNING util.py:136 -- The `process_trial` operation took 1.0711326599121094 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    371 |          9814.76 | 60024832 |  289.479 |              309.323 |              74.7778 |             796.34 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3042.0711010997748
    time_step_min: 2917
  date: 2020-10-15_02-57-49
  done: false
  episode_len_mean: 796.3579355829846
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.51414326022314
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 200
  episodes_total: 75508
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.76741765536479e-31
        cur_lr: 5.0e-05
        entropy: 0.058100646982590355
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008255147443075353
        total_loss: .inf
        vf_explained_var: 0.9994605183601379
        vf_loss: 0.26712941378355026
    num_steps_sampled: 60186624
    num_steps_trained: 60186624
  iterations_since_restore: 372
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.734375
    gpu_util_percent0: 0.3271875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632307286711138
    mean_env_wait_ms: 1.2058733300080713
    mean_inference_ms: 4.284212354900502
    mean_raw_obs_processing_ms: 0.37820886830457884
  time_since_restore: 9841.574385166168
  time_this_iter_s: 26.811309099197388
  time_total_s: 9841.574385166168
  timers:
    learn_throughput: 8378.197
    learn_time_ms: 19311.076
    sample_throughput: 23781.773
    sample_time_ms: 6803.193
    update_time_ms: 53.694
  timestamp: 1602730669
  timesteps_since_restore: 0
  timesteps_total: 60186624
  training_iteration: 372
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:57:50,988	WARNING util.py:136 -- The `process_trial` operation took 1.0469906330108643 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    372 |          9841.57 | 60186624 |  289.514 |              309.323 |              74.7778 |            796.358 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3041.8948592880465
    time_step_min: 2917
  date: 2020-10-15_02-58-17
  done: false
  episode_len_mean: 796.3691421474718
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.5427190025744
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 181
  episodes_total: 75689
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.0151126483047183e-30
        cur_lr: 5.0e-05
        entropy: 0.06236537297566732
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.010523996374104172
        total_loss: .inf
        vf_explained_var: 0.9988077282905579
        vf_loss: 0.5664885118603706
    num_steps_sampled: 60348416
    num_steps_trained: 60348416
  iterations_since_restore: 373
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.650000000000002
    gpu_util_percent0: 0.3521875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632210048326422
    mean_env_wait_ms: 1.205821154650409
    mean_inference_ms: 4.284172165225912
    mean_raw_obs_processing_ms: 0.37820714621466744
  time_since_restore: 9868.248860359192
  time_this_iter_s: 26.67447519302368
  time_total_s: 9868.248860359192
  timers:
    learn_throughput: 8375.447
    learn_time_ms: 19317.417
    sample_throughput: 23749.818
    sample_time_ms: 6812.347
    update_time_ms: 53.375
  timestamp: 1602730697
  timesteps_since_restore: 0
  timesteps_total: 60348416
  training_iteration: 373
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:58:19,174	WARNING util.py:136 -- The `process_trial` operation took 1.0358686447143555 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    373 |          9868.25 | 60348416 |  289.543 |              309.323 |              74.7778 |            796.369 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3041.6430558302022
    time_step_min: 2917
  date: 2020-10-15_02-58-45
  done: false
  episode_len_mean: 796.3894957374198
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.58010513738975
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 204
  episodes_total: 75893
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.5226689724570778e-30
        cur_lr: 5.0e-05
        entropy: 0.05652405911435684
        entropy_coeff: 0.0005000000000000001
        kl: 0.00476144696585834
        model: {}
        policy_loss: -0.006732412126439158
        total_loss: 0.2069220965107282
        vf_explained_var: 0.9996068477630615
        vf_loss: 0.2136827732125918
    num_steps_sampled: 60510208
    num_steps_trained: 60510208
  iterations_since_restore: 374
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6375
    gpu_util_percent0: 0.32875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14632119152459133
    mean_env_wait_ms: 1.2057630086501772
    mean_inference_ms: 4.284125697101541
    mean_raw_obs_processing_ms: 0.3782054937625733
  time_since_restore: 9894.684697628021
  time_this_iter_s: 26.435837268829346
  time_total_s: 9894.684697628021
  timers:
    learn_throughput: 8378.618
    learn_time_ms: 19310.106
    sample_throughput: 23793.408
    sample_time_ms: 6799.867
    update_time_ms: 53.881
  timestamp: 1602730725
  timesteps_since_restore: 0
  timesteps_total: 60510208
  training_iteration: 374
  trial_id: 9f737_00000
  
2020-10-15 02:58:47,000	WARNING util.py:136 -- The `process_trial` operation took 1.02469801902771 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    374 |          9894.68 | 60510208 |   289.58 |              309.323 |              74.7778 |            796.389 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3041.3835537211808
    time_step_min: 2917
  date: 2020-10-15_02-59-14
  done: false
  episode_len_mean: 796.4093094750118
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.61693276768955
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 223
  episodes_total: 76116
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 7.613344862285389e-31
        cur_lr: 5.0e-05
        entropy: 0.05745017838974794
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00825227128128366
        total_loss: .inf
        vf_explained_var: 0.9994432926177979
        vf_loss: 0.29713886727889377
    num_steps_sampled: 60672000
    num_steps_trained: 60672000
  iterations_since_restore: 375
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.1375
    gpu_util_percent0: 0.2953125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463201634108367
    mean_env_wait_ms: 1.205697570747412
    mean_inference_ms: 4.284074621062663
    mean_raw_obs_processing_ms: 0.37820219752488493
  time_since_restore: 9921.717650413513
  time_this_iter_s: 27.032952785491943
  time_total_s: 9921.717650413513
  timers:
    learn_throughput: 8368.604
    learn_time_ms: 19333.213
    sample_throughput: 23778.956
    sample_time_ms: 6803.999
    update_time_ms: 46.331
  timestamp: 1602730754
  timesteps_since_restore: 0
  timesteps_total: 60672000
  training_iteration: 375
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 02:59:15,567	WARNING util.py:136 -- The `process_trial` operation took 1.0709869861602783 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    375 |          9921.72 | 60672000 |  289.617 |              309.323 |              74.7778 |            796.409 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3041.176100381544
    time_step_min: 2917
  date: 2020-10-15_02-59-42
  done: false
  episode_len_mean: 796.4271691981077
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.64956959480395
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 191
  episodes_total: 76307
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1420017293428083e-30
        cur_lr: 5.0e-05
        entropy: 0.056759477593004704
        entropy_coeff: 0.0005000000000000001
        kl: 0.004658918711356819
        model: {}
        policy_loss: -0.010793720978350999
        total_loss: 0.21916984270016351
        vf_explained_var: 0.9995452761650085
        vf_loss: 0.22999194885293642
    num_steps_sampled: 60833792
    num_steps_trained: 60833792
  iterations_since_restore: 376
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.709375
    gpu_util_percent0: 0.31656249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631931063631645
    mean_env_wait_ms: 1.2056418887427143
    mean_inference_ms: 4.284026467792106
    mean_raw_obs_processing_ms: 0.3781999415333847
  time_since_restore: 9948.207046031952
  time_this_iter_s: 26.48939561843872
  time_total_s: 9948.207046031952
  timers:
    learn_throughput: 8357.92
    learn_time_ms: 19357.925
    sample_throughput: 23846.975
    sample_time_ms: 6784.592
    update_time_ms: 45.945
  timestamp: 1602730782
  timesteps_since_restore: 0
  timesteps_total: 60833792
  training_iteration: 376
  trial_id: 9f737_00000
  
2020-10-15 02:59:43,547	WARNING util.py:136 -- The `process_trial` operation took 1.029512882232666 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    376 |          9948.21 | 60833792 |   289.65 |              309.323 |              74.7778 |            796.427 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3040.952636948674
    time_step_min: 2917
  date: 2020-10-15_03-00-10
  done: false
  episode_len_mean: 796.448267747418
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.68367555803826
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 183
  episodes_total: 76490
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.710008646714042e-31
        cur_lr: 5.0e-05
        entropy: 0.05530309770256281
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052484923508018255
        model: {}
        policy_loss: -0.006903803994646296
        total_loss: 0.0928850428511699
        vf_explained_var: 0.9997946619987488
        vf_loss: 0.09981649865706761
    num_steps_sampled: 60995584
    num_steps_trained: 60995584
  iterations_since_restore: 377
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.578787878787878
    gpu_util_percent0: 0.23363636363636361
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8757575757575755
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631856776006213
    mean_env_wait_ms: 1.2055894663046072
    mean_inference_ms: 4.28398789329307
    mean_raw_obs_processing_ms: 0.3781988328141734
  time_since_restore: 9974.879332304
  time_this_iter_s: 26.67228627204895
  time_total_s: 9974.879332304
  timers:
    learn_throughput: 8349.789
    learn_time_ms: 19376.777
    sample_throughput: 23838.933
    sample_time_ms: 6786.881
    update_time_ms: 44.629
  timestamp: 1602730810
  timesteps_since_restore: 0
  timesteps_total: 60995584
  training_iteration: 377
  trial_id: 9f737_00000
  
2020-10-15 03:00:11,625	WARNING util.py:136 -- The `process_trial` operation took 1.0234880447387695 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    377 |          9974.88 | 60995584 |  289.684 |              309.323 |              74.7778 |            796.448 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3040.705293910503
    time_step_min: 2917
  date: 2020-10-15_03-00-38
  done: false
  episode_len_mean: 796.4611854883915
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.7220708604735
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 221
  episodes_total: 76711
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.710008646714042e-31
        cur_lr: 5.0e-05
        entropy: 0.060873979702591896
        entropy_coeff: 0.0005000000000000001
        kl: 0.004831873928196728
        model: {}
        policy_loss: -0.00770192750011726
        total_loss: 0.41984130442142487
        vf_explained_var: 0.9991667866706848
        vf_loss: 0.42757366597652435
    num_steps_sampled: 61157376
    num_steps_trained: 61157376
  iterations_since_restore: 378
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.571875
    gpu_util_percent0: 0.371875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631782502499233
    mean_env_wait_ms: 1.205526645401199
    mean_inference_ms: 4.283936903384617
    mean_raw_obs_processing_ms: 0.37819623641510025
  time_since_restore: 10001.60590171814
  time_this_iter_s: 26.726569414138794
  time_total_s: 10001.60590171814
  timers:
    learn_throughput: 8341.329
    learn_time_ms: 19396.429
    sample_throughput: 23920.26
    sample_time_ms: 6763.806
    update_time_ms: 37.406
  timestamp: 1602730838
  timesteps_since_restore: 0
  timesteps_total: 61157376
  training_iteration: 378
  trial_id: 9f737_00000
  
2020-10-15 03:00:39,758	WARNING util.py:136 -- The `process_trial` operation took 1.0300579071044922 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    378 |          10001.6 | 61157376 |  289.722 |              309.323 |              74.7778 |            796.461 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3040.459860565542
    time_step_min: 2917
  date: 2020-10-15_03-01-06
  done: false
  episode_len_mean: 796.4769630785231
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.75837548653465
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 209
  episodes_total: 76920
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.855004323357021e-31
        cur_lr: 5.0e-05
        entropy: 0.05769563093781471
        entropy_coeff: 0.0005000000000000001
        kl: 0.005303316089945535
        model: {}
        policy_loss: -0.00891517637016174
        total_loss: 0.3065975084900856
        vf_explained_var: 0.9993930459022522
        vf_loss: 0.315541535615921
    num_steps_sampled: 61319168
    num_steps_trained: 61319168
  iterations_since_restore: 379
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.89375
    gpu_util_percent0: 0.2278125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631679374666456
    mean_env_wait_ms: 1.205465617052069
    mean_inference_ms: 4.283889490374565
    mean_raw_obs_processing_ms: 0.37819363600320544
  time_since_restore: 10027.97689652443
  time_this_iter_s: 26.370994806289673
  time_total_s: 10027.97689652443
  timers:
    learn_throughput: 8352.153
    learn_time_ms: 19371.293
    sample_throughput: 23945.113
    sample_time_ms: 6756.786
    update_time_ms: 37.478
  timestamp: 1602730866
  timesteps_since_restore: 0
  timesteps_total: 61319168
  training_iteration: 379
  trial_id: 9f737_00000
  
2020-10-15 03:01:07,705	WARNING util.py:136 -- The `process_trial` operation took 1.0219266414642334 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    379 |            10028 | 61319168 |  289.758 |              309.323 |              74.7778 |            796.477 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3040.237034345846
    time_step_min: 2917
  date: 2020-10-15_03-01-34
  done: false
  episode_len_mean: 796.4947540430829
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.79224606289597
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 187
  episodes_total: 77107
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.855004323357021e-31
        cur_lr: 5.0e-05
        entropy: 0.060936685651540756
        entropy_coeff: 0.0005000000000000001
        kl: 0.007348845984476308
        model: {}
        policy_loss: -0.00795948512313771
        total_loss: 0.1964349610110124
        vf_explained_var: 0.9996386170387268
        vf_loss: 0.2044249065220356
    num_steps_sampled: 61480960
    num_steps_trained: 61480960
  iterations_since_restore: 380
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.328125
    gpu_util_percent0: 0.255625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463157266329533
    mean_env_wait_ms: 1.2054122609767224
    mean_inference_ms: 4.2838490551708635
    mean_raw_obs_processing_ms: 0.37819201748088815
  time_since_restore: 10054.766763687134
  time_this_iter_s: 26.789867162704468
  time_total_s: 10054.766763687134
  timers:
    learn_throughput: 8318.723
    learn_time_ms: 19449.138
    sample_throughput: 23974.517
    sample_time_ms: 6748.499
    update_time_ms: 36.985
  timestamp: 1602730894
  timesteps_since_restore: 0
  timesteps_total: 61480960
  training_iteration: 380
  trial_id: 9f737_00000
  
2020-10-15 03:01:36,101	WARNING util.py:136 -- The `process_trial` operation took 1.0240881443023682 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    380 |          10054.8 | 61480960 |  289.792 |              309.323 |              74.7778 |            796.495 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3040.0150265974657
    time_step_min: 2917
  date: 2020-10-15_03-02-02
  done: false
  episode_len_mean: 796.5135379878656
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.8248605510219
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 194
  episodes_total: 77301
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.855004323357021e-31
        cur_lr: 5.0e-05
        entropy: 0.06307689535121123
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008449694030180884
        total_loss: .inf
        vf_explained_var: 0.9995918869972229
        vf_loss: 0.21645139654477438
    num_steps_sampled: 61642752
    num_steps_trained: 61642752
  iterations_since_restore: 381
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6375
    gpu_util_percent0: 0.2828125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631509970826026
    mean_env_wait_ms: 1.2053577448666137
    mean_inference_ms: 4.283809474907404
    mean_raw_obs_processing_ms: 0.3781906668736646
  time_since_restore: 10081.386919498444
  time_this_iter_s: 26.620155811309814
  time_total_s: 10081.386919498444
  timers:
    learn_throughput: 8319.785
    learn_time_ms: 19446.656
    sample_throughput: 23969.311
    sample_time_ms: 6749.965
    update_time_ms: 36.586
  timestamp: 1602730922
  timesteps_since_restore: 0
  timesteps_total: 61642752
  training_iteration: 381
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:02:04,178	WARNING util.py:136 -- The `process_trial` operation took 1.0760188102722168 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    381 |          10081.4 | 61642752 |  289.825 |              309.323 |              74.7778 |            796.514 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3039.772385844454
    time_step_min: 2917
  date: 2020-10-15_03-02-30
  done: false
  episode_len_mean: 796.5324690402476
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.8609352034275
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 219
  episodes_total: 77520
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.2825064850355305e-31
        cur_lr: 5.0e-05
        entropy: 0.06420532831301291
        entropy_coeff: 0.0005000000000000001
        kl: 0.004187056639542182
        model: {}
        policy_loss: -0.008532451213492701
        total_loss: 0.3739231899380684
        vf_explained_var: 0.9992806315422058
        vf_loss: 0.3824877366423607
    num_steps_sampled: 61804544
    num_steps_trained: 61804544
  iterations_since_restore: 382
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.596875
    gpu_util_percent0: 0.335625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631427471815495
    mean_env_wait_ms: 1.2052948121003513
    mean_inference_ms: 4.283759588173783
    mean_raw_obs_processing_ms: 0.378187797790252
  time_since_restore: 10108.03109550476
  time_this_iter_s: 26.64417600631714
  time_total_s: 10108.03109550476
  timers:
    learn_throughput: 8320.897
    learn_time_ms: 19444.058
    sample_throughput: 23993.905
    sample_time_ms: 6743.046
    update_time_ms: 29.357
  timestamp: 1602730950
  timesteps_since_restore: 0
  timesteps_total: 61804544
  training_iteration: 382
  trial_id: 9f737_00000
  
2020-10-15 03:02:32,256	WARNING util.py:136 -- The `process_trial` operation took 1.0548474788665771 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    382 |            10108 | 61804544 |  289.861 |              309.323 |              74.7778 |            796.532 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3039.5692549302303
    time_step_min: 2917
  date: 2020-10-15_03-02-58
  done: false
  episode_len_mean: 796.55142688042
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.8932592958223
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 77722
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1412532425177653e-31
        cur_lr: 5.0e-05
        entropy: 0.0620024964834253
        entropy_coeff: 0.0005000000000000001
        kl: 0.006711660302244127
        model: {}
        policy_loss: -0.008130639888501415
        total_loss: 0.34145527333021164
        vf_explained_var: 0.9993179440498352
        vf_loss: 0.34961691747109097
    num_steps_sampled: 61966336
    num_steps_trained: 61966336
  iterations_since_restore: 383
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.890625
    gpu_util_percent0: 0.30062500000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631317179858863
    mean_env_wait_ms: 1.2052356480543356
    mean_inference_ms: 4.283709371105999
    mean_raw_obs_processing_ms: 0.37818528257427375
  time_since_restore: 10134.637343883514
  time_this_iter_s: 26.606248378753662
  time_total_s: 10134.637343883514
  timers:
    learn_throughput: 8318.645
    learn_time_ms: 19449.322
    sample_throughput: 24043.543
    sample_time_ms: 6729.125
    update_time_ms: 29.162
  timestamp: 1602730978
  timesteps_since_restore: 0
  timesteps_total: 61966336
  training_iteration: 383
  trial_id: 9f737_00000
  
2020-10-15 03:03:00,436	WARNING util.py:136 -- The `process_trial` operation took 1.114257574081421 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    383 |          10134.6 | 61966336 |  289.893 |              309.323 |              74.7778 |            796.551 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3039.4478675724613
    time_step_min: 2917
  date: 2020-10-15_03-03-26
  done: false
  episode_len_mean: 796.5580243110376
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.90499239380597
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 185
  episodes_total: 77907
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.1412532425177653e-31
        cur_lr: 5.0e-05
        entropy: 0.08623662901421388
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.01131964176602196
        total_loss: .inf
        vf_explained_var: 0.9970410466194153
        vf_loss: 1.4815564254919689
    num_steps_sampled: 62128128
    num_steps_trained: 62128128
  iterations_since_restore: 384
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.675000000000004
    gpu_util_percent0: 0.354375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631228397758494
    mean_env_wait_ms: 1.205183247405803
    mean_inference_ms: 4.283673012171427
    mean_raw_obs_processing_ms: 0.37818396496708107
  time_since_restore: 10161.031187057495
  time_this_iter_s: 26.393843173980713
  time_total_s: 10161.031187057495
  timers:
    learn_throughput: 8320.104
    learn_time_ms: 19445.91
    sample_throughput: 24049.024
    sample_time_ms: 6727.591
    update_time_ms: 28.428
  timestamp: 1602731006
  timesteps_since_restore: 0
  timesteps_total: 62128128
  training_iteration: 384
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:03:28,277	WARNING util.py:136 -- The `process_trial` operation took 1.0661118030548096 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    384 |            10161 | 62128128 |  289.905 |              309.323 |              74.7778 |            796.558 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3039.5183482125703
    time_step_min: 2917
  date: 2020-10-15_03-03-54
  done: false
  episode_len_mean: 796.5567589712076
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.89770617803805
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 204
  episodes_total: 78111
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2118798637766477e-31
        cur_lr: 5.0e-05
        entropy: 0.08522132659951846
        entropy_coeff: 0.0005000000000000001
        kl: 0.005046335902685921
        model: {}
        policy_loss: -0.011908572264170894
        total_loss: 2.03079417347908
        vf_explained_var: 0.9963490962982178
        vf_loss: 2.0427453418572745
    num_steps_sampled: 62289920
    num_steps_trained: 62289920
  iterations_since_restore: 385
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.571875
    gpu_util_percent0: 0.3209375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631158680970385
    mean_env_wait_ms: 1.2051264973616662
    mean_inference_ms: 4.283630843607633
    mean_raw_obs_processing_ms: 0.3781822070310183
  time_since_restore: 10187.550326824188
  time_this_iter_s: 26.519139766693115
  time_total_s: 10187.550326824188
  timers:
    learn_throughput: 8343.603
    learn_time_ms: 19391.142
    sample_throughput: 24064.957
    sample_time_ms: 6723.137
    update_time_ms: 26.296
  timestamp: 1602731034
  timesteps_since_restore: 0
  timesteps_total: 62289920
  training_iteration: 385
  trial_id: 9f737_00000
  
2020-10-15 03:03:56,245	WARNING util.py:136 -- The `process_trial` operation took 1.0783348083496094 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    385 |          10187.6 | 62289920 |  289.898 |              309.323 |              74.7778 |            796.557 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3039.332285998927
    time_step_min: 2917
  date: 2020-10-15_03-04-22
  done: false
  episode_len_mean: 796.5741459429097
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.9281323679526
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 221
  episodes_total: 78332
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2118798637766477e-31
        cur_lr: 5.0e-05
        entropy: 0.0652887299656868
        entropy_coeff: 0.0005000000000000001
        kl: 0.005769484870446225
        model: {}
        policy_loss: -0.00817600944234679
        total_loss: 0.4470681498448054
        vf_explained_var: 0.9991374611854553
        vf_loss: 0.45527681211630505
    num_steps_sampled: 62451712
    num_steps_trained: 62451712
  iterations_since_restore: 386
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.471875
    gpu_util_percent0: 0.333125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14631067336673348
    mean_env_wait_ms: 1.205063345497525
    mean_inference_ms: 4.283580934563497
    mean_raw_obs_processing_ms: 0.37817921914440034
  time_since_restore: 10214.086613178253
  time_this_iter_s: 26.53628635406494
  time_total_s: 10214.086613178253
  timers:
    learn_throughput: 8349.866
    learn_time_ms: 19376.599
    sample_throughput: 24036.397
    sample_time_ms: 6731.125
    update_time_ms: 35.221
  timestamp: 1602731062
  timesteps_since_restore: 0
  timesteps_total: 62451712
  training_iteration: 386
  trial_id: 9f737_00000
  
2020-10-15 03:04:24,277	WARNING util.py:136 -- The `process_trial` operation took 1.1229243278503418 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    386 |          10214.1 | 62451712 |  289.928 |              309.323 |              74.7778 |            796.574 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3039.136266340494
    time_step_min: 2917
  date: 2020-10-15_03-04-50
  done: false
  episode_len_mean: 796.5914242779278
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.9585814077817
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 78524
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2118798637766477e-31
        cur_lr: 5.0e-05
        entropy: 0.062085891452928386
        entropy_coeff: 0.0005000000000000001
        kl: 0.004817397372486691
        model: {}
        policy_loss: -0.00876125782087911
        total_loss: 0.27814941356579465
        vf_explained_var: 0.9994053244590759
        vf_loss: 0.28694172327717143
    num_steps_sampled: 62613504
    num_steps_trained: 62613504
  iterations_since_restore: 387
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.6875
    gpu_util_percent0: 0.3471875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630959399640958
    mean_env_wait_ms: 1.2050091439633976
    mean_inference_ms: 4.283539240077089
    mean_raw_obs_processing_ms: 0.3781774229519877
  time_since_restore: 10240.559261083603
  time_this_iter_s: 26.47264790534973
  time_total_s: 10240.559261083603
  timers:
    learn_throughput: 8358.405
    learn_time_ms: 19356.804
    sample_throughput: 24008.587
    sample_time_ms: 6738.922
    update_time_ms: 35.165
  timestamp: 1602731090
  timesteps_since_restore: 0
  timesteps_total: 62613504
  training_iteration: 387
  trial_id: 9f737_00000
  
2020-10-15 03:04:52,193	WARNING util.py:136 -- The `process_trial` operation took 1.0694093704223633 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    387 |          10240.6 | 62613504 |  289.959 |              309.323 |              74.7778 |            796.591 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3038.951024506813
    time_step_min: 2917
  date: 2020-10-15_03-05-18
  done: false
  episode_len_mean: 796.6067971032905
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 289.98735609172655
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 78710
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.6059399318883238e-31
        cur_lr: 5.0e-05
        entropy: 0.0616902249554793
        entropy_coeff: 0.0005000000000000001
        kl: 0.004536660426917176
        model: {}
        policy_loss: -0.00822500986396335
        total_loss: 0.30501696467399597
        vf_explained_var: 0.9993646144866943
        vf_loss: 0.3132728114724159
    num_steps_sampled: 62775296
    num_steps_trained: 62775296
  iterations_since_restore: 388
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.99375
    gpu_util_percent0: 0.27718750000000003
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463091212195949
    mean_env_wait_ms: 1.2049577801320455
    mean_inference_ms: 4.283504296616487
    mean_raw_obs_processing_ms: 0.3781765282387024
  time_since_restore: 10266.97629070282
  time_this_iter_s: 26.41702961921692
  time_total_s: 10266.97629070282
  timers:
    learn_throughput: 8374.423
    learn_time_ms: 19319.778
    sample_throughput: 23993.92
    sample_time_ms: 6743.042
    update_time_ms: 35.594
  timestamp: 1602731118
  timesteps_since_restore: 0
  timesteps_total: 62775296
  training_iteration: 388
  trial_id: 9f737_00000
  
2020-10-15 03:05:20,129	WARNING util.py:136 -- The `process_trial` operation took 1.1387848854064941 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    388 |            10267 | 62775296 |  289.987 |              309.323 |              74.7778 |            796.607 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3038.723127820311
    time_step_min: 2917
  date: 2020-10-15_03-05-46
  done: false
  episode_len_mean: 796.6249081464589
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.02049188195144
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 220
  episodes_total: 78930
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.029699659441619e-32
        cur_lr: 5.0e-05
        entropy: 0.06341554193447034
        entropy_coeff: 0.0005000000000000001
        kl: 0.0045873553802569704
        model: {}
        policy_loss: -0.009872557469255602
        total_loss: 0.5406313414374987
        vf_explained_var: 0.9989693760871887
        vf_loss: 0.5505356118083
    num_steps_sampled: 62937088
    num_steps_trained: 62937088
  iterations_since_restore: 389
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.015625
    gpu_util_percent0: 0.31375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630841326156963
    mean_env_wait_ms: 1.2048955477132894
    mean_inference_ms: 4.283456290053079
    mean_raw_obs_processing_ms: 0.3781738763914545
  time_since_restore: 10293.399019956589
  time_this_iter_s: 26.42272925376892
  time_total_s: 10293.399019956589
  timers:
    learn_throughput: 8381.469
    learn_time_ms: 19303.538
    sample_throughput: 23954.379
    sample_time_ms: 6754.172
    update_time_ms: 37.666
  timestamp: 1602731146
  timesteps_since_restore: 0
  timesteps_total: 62937088
  training_iteration: 389
  trial_id: 9f737_00000
  
2020-10-15 03:05:48,018	WARNING util.py:136 -- The `process_trial` operation took 1.0837411880493164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    389 |          10293.4 | 62937088 |   290.02 |              309.323 |              74.7778 |            796.625 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3038.516062149964
    time_step_min: 2917
  date: 2020-10-15_03-06-14
  done: false
  episode_len_mean: 796.6456398397715
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.05386784176727
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 207
  episodes_total: 79137
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.0148498297208096e-32
        cur_lr: 5.0e-05
        entropy: 0.059044926427304745
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009451691381400451
        total_loss: .inf
        vf_explained_var: 0.999644935131073
        vf_loss: 0.19055577740073204
    num_steps_sampled: 63098880
    num_steps_trained: 63098880
  iterations_since_restore: 390
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.971875
    gpu_util_percent0: 0.2834375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463072133176256
    mean_env_wait_ms: 1.2048364554562738
    mean_inference_ms: 4.283407200155177
    mean_raw_obs_processing_ms: 0.3781714351818928
  time_since_restore: 10319.885895729065
  time_this_iter_s: 26.486875772476196
  time_total_s: 10319.885895729065
  timers:
    learn_throughput: 8396.322
    learn_time_ms: 19269.39
    sample_throughput: 23939.231
    sample_time_ms: 6758.446
    update_time_ms: 36.693
  timestamp: 1602731174
  timesteps_since_restore: 0
  timesteps_total: 63098880
  training_iteration: 390
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:06:16,018	WARNING util.py:136 -- The `process_trial` operation took 1.1307182312011719 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    390 |          10319.9 | 63098880 |  290.054 |              309.323 |              74.7778 |            796.646 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3038.3191736245994
    time_step_min: 2917
  date: 2020-10-15_03-06-42
  done: false
  episode_len_mean: 796.6578589077707
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.0819388479113
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 187
  episodes_total: 79324
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.022274744581215e-32
        cur_lr: 5.0e-05
        entropy: 0.0669141144802173
        entropy_coeff: 0.0005000000000000001
        kl: 0.004681107743332784
        model: {}
        policy_loss: -0.010108219207419703
        total_loss: 0.5205400337775549
        vf_explained_var: 0.9988470077514648
        vf_loss: 0.5306817069649696
    num_steps_sampled: 63260672
    num_steps_trained: 63260672
  iterations_since_restore: 391
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.878124999999997
    gpu_util_percent0: 0.28218750000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463062845236856
    mean_env_wait_ms: 1.20478431771646
    mean_inference_ms: 4.283371721134317
    mean_raw_obs_processing_ms: 0.3781699868152301
  time_since_restore: 10346.488398551941
  time_this_iter_s: 26.602502822875977
  time_total_s: 10346.488398551941
  timers:
    learn_throughput: 8396.046
    learn_time_ms: 19270.023
    sample_throughput: 23965.491
    sample_time_ms: 6751.041
    update_time_ms: 39.117
  timestamp: 1602731202
  timesteps_since_restore: 0
  timesteps_total: 63260672
  training_iteration: 391
  trial_id: 9f737_00000
  
2020-10-15 03:06:44,132	WARNING util.py:136 -- The `process_trial` operation took 1.112471342086792 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    391 |          10346.5 | 63260672 |  290.082 |              309.323 |              74.7778 |            796.658 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3038.179833666755
    time_step_min: 2917
  date: 2020-10-15_03-07-10
  done: false
  episode_len_mean: 796.6679074914798
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.10516155429826
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 193
  episodes_total: 79517
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.0111373722906073e-32
        cur_lr: 5.0e-05
        entropy: 0.06746762928863366
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009055282071737262
        total_loss: .inf
        vf_explained_var: 0.9987338185310364
        vf_loss: 0.6472459634145101
    num_steps_sampled: 63422464
    num_steps_trained: 63422464
  iterations_since_restore: 392
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.984375
    gpu_util_percent0: 0.285625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630581500308282
    mean_env_wait_ms: 1.204731847052624
    mean_inference_ms: 4.283336208223543
    mean_raw_obs_processing_ms: 0.37816872933489315
  time_since_restore: 10373.17181277275
  time_this_iter_s: 26.683414220809937
  time_total_s: 10373.17181277275
  timers:
    learn_throughput: 8405.027
    learn_time_ms: 19249.433
    sample_throughput: 23915.927
    sample_time_ms: 6765.032
    update_time_ms: 46.472
  timestamp: 1602731230
  timesteps_since_restore: 0
  timesteps_total: 63422464
  training_iteration: 392
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:07:12,317	WARNING util.py:136 -- The `process_trial` operation took 1.0865237712860107 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    392 |          10373.2 | 63422464 |  290.105 |              309.323 |              74.7778 |            796.668 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3037.9572663513295
    time_step_min: 2917
  date: 2020-10-15_03-07-38
  done: false
  episode_len_mean: 796.685845424562
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.13877795524627
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 224
  episodes_total: 79741
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.5167060584359115e-32
        cur_lr: 5.0e-05
        entropy: 0.06125968291113774
        entropy_coeff: 0.0005000000000000001
        kl: 0.004230380795585613
        model: {}
        policy_loss: -0.00823301982988293
        total_loss: 0.5730938290556272
        vf_explained_var: 0.9989622235298157
        vf_loss: 0.5813574890295664
    num_steps_sampled: 63584256
    num_steps_trained: 63584256
  iterations_since_restore: 393
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.56875
    gpu_util_percent0: 0.2784375
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463048959519919
    mean_env_wait_ms: 1.2046689027246664
    mean_inference_ms: 4.28328866391407
    mean_raw_obs_processing_ms: 0.3781660966882615
  time_since_restore: 10399.753201007843
  time_this_iter_s: 26.581388235092163
  time_total_s: 10399.753201007843
  timers:
    learn_throughput: 8408.777
    learn_time_ms: 19240.847
    sample_throughput: 23919.363
    sample_time_ms: 6764.06
    update_time_ms: 44.601
  timestamp: 1602731258
  timesteps_since_restore: 0
  timesteps_total: 63584256
  training_iteration: 393
  trial_id: 9f737_00000
  
2020-10-15 03:07:40,408	WARNING util.py:136 -- The `process_trial` operation took 1.1345160007476807 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    393 |          10399.8 | 63584256 |  290.139 |              309.323 |              74.7778 |            796.686 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3037.7467429257977
    time_step_min: 2917
  date: 2020-10-15_03-08-06
  done: false
  episode_len_mean: 796.7028433469684
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.16891472612565
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 200
  episodes_total: 79941
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.2583530292179558e-32
        cur_lr: 5.0e-05
        entropy: 0.060712021154661976
        entropy_coeff: 0.0005000000000000001
        kl: 0.003922394263402869
        model: {}
        policy_loss: -0.010277828458735408
        total_loss: 0.47015032172203064
        vf_explained_var: 0.9990419745445251
        vf_loss: 0.48045851786931354
    num_steps_sampled: 63746048
    num_steps_trained: 63746048
  iterations_since_restore: 394
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.096875
    gpu_util_percent0: 0.31906249999999997
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463037614129628
    mean_env_wait_ms: 1.2046117308642403
    mean_inference_ms: 4.283243513138888
    mean_raw_obs_processing_ms: 0.3781638720040163
  time_since_restore: 10426.229320764542
  time_this_iter_s: 26.47611975669861
  time_total_s: 10426.229320764542
  timers:
    learn_throughput: 8408.981
    learn_time_ms: 19240.382
    sample_throughput: 23927.39
    sample_time_ms: 6761.79
    update_time_ms: 45.887
  timestamp: 1602731286
  timesteps_since_restore: 0
  timesteps_total: 63746048
  training_iteration: 394
  trial_id: 9f737_00000
  
2020-10-15 03:08:08,352	WARNING util.py:136 -- The `process_trial` operation took 1.0898916721343994 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    394 |          10426.2 | 63746048 |  290.169 |              309.323 |              74.7778 |            796.703 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3037.5458518555515
    time_step_min: 2917
  date: 2020-10-15_03-08-35
  done: false
  episode_len_mean: 796.7176430942812
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.1983069253732
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 181
  episodes_total: 80122
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1291765146089779e-32
        cur_lr: 5.0e-05
        entropy: 0.061628819753726326
        entropy_coeff: 0.0005000000000000001
        kl: 0.00523802040455242
        model: {}
        policy_loss: -0.00890371985345458
        total_loss: 0.204816784709692
        vf_explained_var: 0.9995421767234802
        vf_loss: 0.2137513185540835
    num_steps_sampled: 63907840
    num_steps_trained: 63907840
  iterations_since_restore: 395
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.978125
    gpu_util_percent0: 0.28625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1463030559225502
    mean_env_wait_ms: 1.2045616240197226
    mean_inference_ms: 4.283208790142938
    mean_raw_obs_processing_ms: 0.37816281757575276
  time_since_restore: 10452.948558330536
  time_this_iter_s: 26.719237565994263
  time_total_s: 10452.948558330536
  timers:
    learn_throughput: 8396.089
    learn_time_ms: 19269.924
    sample_throughput: 23931.797
    sample_time_ms: 6760.545
    update_time_ms: 46.05
  timestamp: 1602731315
  timesteps_since_restore: 0
  timesteps_total: 63907840
  training_iteration: 395
  trial_id: 9f737_00000
  
2020-10-15 03:08:36,688	WARNING util.py:136 -- The `process_trial` operation took 1.1150271892547607 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    395 |          10452.9 | 63907840 |  290.198 |              309.323 |              74.7778 |            796.718 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3037.3043402453454
    time_step_min: 2917
  date: 2020-10-15_03-09-03
  done: false
  episode_len_mean: 796.7360611454819
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.232949413722
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 211
  episodes_total: 80333
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1291765146089779e-32
        cur_lr: 5.0e-05
        entropy: 0.06465149267266194
        entropy_coeff: 0.0005000000000000001
        kl: 0.0052374874940142035
        model: {}
        policy_loss: -0.007659254634442429
        total_loss: 0.253723348180453
        vf_explained_var: 0.9994933009147644
        vf_loss: 0.26141492649912834
    num_steps_sampled: 64069632
    num_steps_trained: 64069632
  iterations_since_restore: 396
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.145454545454545
    gpu_util_percent0: 0.26090909090909087
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8757575757575755
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630235571560368
    mean_env_wait_ms: 1.2045033175570727
    mean_inference_ms: 4.283168769806441
    mean_raw_obs_processing_ms: 0.37816097964068374
  time_since_restore: 10479.501156568527
  time_this_iter_s: 26.552598237991333
  time_total_s: 10479.501156568527
  timers:
    learn_throughput: 8386.359
    learn_time_ms: 19292.281
    sample_throughput: 23973.185
    sample_time_ms: 6748.874
    update_time_ms: 37.387
  timestamp: 1602731343
  timesteps_since_restore: 0
  timesteps_total: 64069632
  training_iteration: 396
  trial_id: 9f737_00000
  
2020-10-15 03:09:04,853	WARNING util.py:136 -- The `process_trial` operation took 1.0604114532470703 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    396 |          10479.5 | 64069632 |  290.233 |              309.323 |              74.7778 |            796.736 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3037.0841851626446
    time_step_min: 2917
  date: 2020-10-15_03-09-31
  done: false
  episode_len_mean: 796.7477250437611
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.26413067372215
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 218
  episodes_total: 80551
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.1291765146089779e-32
        cur_lr: 5.0e-05
        entropy: 0.06635478821893533
        entropy_coeff: 0.0005000000000000001
        kl: 0.004575717922610541
        model: {}
        policy_loss: -0.007182757564199467
        total_loss: 0.43069089700778324
        vf_explained_var: 0.9991649985313416
        vf_loss: 0.43790681411822635
    num_steps_sampled: 64231424
    num_steps_trained: 64231424
  iterations_since_restore: 397
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.487499999999997
    gpu_util_percent0: 0.2953125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.88125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630145122749233
    mean_env_wait_ms: 1.2044428016327764
    mean_inference_ms: 4.283120384834724
    mean_raw_obs_processing_ms: 0.3781585838357492
  time_since_restore: 10506.420483589172
  time_this_iter_s: 26.91932702064514
  time_total_s: 10506.420483589172
  timers:
    learn_throughput: 8371.419
    learn_time_ms: 19326.711
    sample_throughput: 23939.468
    sample_time_ms: 6758.379
    update_time_ms: 37.356
  timestamp: 1602731371
  timesteps_since_restore: 0
  timesteps_total: 64231424
  training_iteration: 397
  trial_id: 9f737_00000
  
2020-10-15 03:09:33,356	WARNING util.py:136 -- The `process_trial` operation took 1.0899138450622559 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    397 |          10506.4 | 64231424 |  290.264 |              309.323 |              74.7778 |            796.748 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3036.914776036181
    time_step_min: 2917
  date: 2020-10-15_03-09-59
  done: false
  episode_len_mean: 796.7571306490966
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.29060767315485
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 192
  episodes_total: 80743
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6458825730448894e-33
        cur_lr: 5.0e-05
        entropy: 0.06709124023715655
        entropy_coeff: 0.0005000000000000001
        kl: 0.005112812737934291
        model: {}
        policy_loss: -0.00957642476229618
        total_loss: 0.5493471994996071
        vf_explained_var: 0.9988442063331604
        vf_loss: 0.5589571769038836
    num_steps_sampled: 64393216
    num_steps_trained: 64393216
  iterations_since_restore: 398
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.63125
    gpu_util_percent0: 0.2928125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630049194742226
    mean_env_wait_ms: 1.2043894344513038
    mean_inference_ms: 4.283082851429006
    mean_raw_obs_processing_ms: 0.378156854404539
  time_since_restore: 10533.035314321518
  time_this_iter_s: 26.61483073234558
  time_total_s: 10533.035314321518
  timers:
    learn_throughput: 8360.985
    learn_time_ms: 19350.83
    sample_throughput: 23926.403
    sample_time_ms: 6762.069
    update_time_ms: 35.421
  timestamp: 1602731399
  timesteps_since_restore: 0
  timesteps_total: 64393216
  training_iteration: 398
  trial_id: 9f737_00000
  
2020-10-15 03:10:01,567	WARNING util.py:136 -- The `process_trial` operation took 1.1133005619049072 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    398 |            10533 | 64393216 |  290.291 |              309.323 |              74.7778 |            796.757 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3036.7437168535434
    time_step_min: 2917
  date: 2020-10-15_03-10-28
  done: false
  episode_len_mean: 796.7704901827528
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.3188712240721
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 80929
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 5.6458825730448894e-33
        cur_lr: 5.0e-05
        entropy: 0.06354599601278703
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008814639198438575
        total_loss: .inf
        vf_explained_var: 0.9994454979896545
        vf_loss: 0.2733009507258733
    num_steps_sampled: 64555008
    num_steps_trained: 64555008
  iterations_since_restore: 399
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.05454545454546
    gpu_util_percent0: 0.3033333333333334
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8757575757575755
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14630011062415596
    mean_env_wait_ms: 1.2043391871025837
    mean_inference_ms: 4.283048254543969
    mean_raw_obs_processing_ms: 0.378155783582328
  time_since_restore: 10559.63945555687
  time_this_iter_s: 26.604141235351562
  time_total_s: 10559.63945555687
  timers:
    learn_throughput: 8346.792
    learn_time_ms: 19383.735
    sample_throughput: 23947.5
    sample_time_ms: 6756.112
    update_time_ms: 33.069
  timestamp: 1602731428
  timesteps_since_restore: 0
  timesteps_total: 64555008
  training_iteration: 399
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:10:29,846	WARNING util.py:136 -- The `process_trial` operation took 1.1213703155517578 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    399 |          10559.6 | 64555008 |  290.319 |              309.323 |              74.7778 |             796.77 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3036.5248434186515
    time_step_min: 2917
  date: 2020-10-15_03-10-56
  done: false
  episode_len_mean: 796.7888620511177
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.35248823233445
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 217
  episodes_total: 81146
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 8.468823859567333e-33
        cur_lr: 5.0e-05
        entropy: 0.06158976908773184
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.00904318958055228
        total_loss: .inf
        vf_explained_var: 0.9994585514068604
        vf_loss: 0.2836419753730297
    num_steps_sampled: 64716800
    num_steps_trained: 64716800
  iterations_since_restore: 400
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.621875
    gpu_util_percent0: 0.28125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629919745015085
    mean_env_wait_ms: 1.2042790267227579
    mean_inference_ms: 4.283001207411469
    mean_raw_obs_processing_ms: 0.3781534727547747
  time_since_restore: 10586.41865324974
  time_this_iter_s: 26.779197692871094
  time_total_s: 10586.41865324974
  timers:
    learn_throughput: 8336.107
    learn_time_ms: 19408.581
    sample_throughput: 23905.428
    sample_time_ms: 6768.003
    update_time_ms: 32.412
  timestamp: 1602731456
  timesteps_since_restore: 0
  timesteps_total: 64716800
  training_iteration: 400
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:10:58,228	WARNING util.py:136 -- The `process_trial` operation took 1.0965585708618164 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    400 |          10586.4 | 64716800 |  290.352 |              309.323 |              74.7778 |            796.789 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3036.3039083060726
    time_step_min: 2917
  date: 2020-10-15_03-11-24
  done: false
  episode_len_mean: 796.8088430524142
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.3849893864399
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 206
  episodes_total: 81352
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2703235789351003e-32
        cur_lr: 5.0e-05
        entropy: 0.05809082121898731
        entropy_coeff: 0.0005000000000000001
        kl: 0.0053687083612506585
        model: {}
        policy_loss: -0.0080076281252938
        total_loss: 0.1516143468519052
        vf_explained_var: 0.9996962547302246
        vf_loss: 0.15965102364619574
    num_steps_sampled: 64878592
    num_steps_trained: 64878592
  iterations_since_restore: 401
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.396969696969695
    gpu_util_percent0: 0.3245454545454546
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8727272727272726
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462981689331512
    mean_env_wait_ms: 1.2042210820033308
    mean_inference_ms: 4.282958511510892
    mean_raw_obs_processing_ms: 0.37815114597756566
  time_since_restore: 10613.088600635529
  time_this_iter_s: 26.669947385787964
  time_total_s: 10613.088600635529
  timers:
    learn_throughput: 8331.067
    learn_time_ms: 19420.323
    sample_throughput: 23914.048
    sample_time_ms: 6765.563
    update_time_ms: 29.917
  timestamp: 1602731484
  timesteps_since_restore: 0
  timesteps_total: 64878592
  training_iteration: 401
  trial_id: 9f737_00000
  
2020-10-15 03:11:26,459	WARNING util.py:136 -- The `process_trial` operation took 1.1021332740783691 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    401 |          10613.1 | 64878592 |  290.385 |              309.323 |              74.7778 |            796.809 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3036.110861616236
    time_step_min: 2917
  date: 2020-10-15_03-11-53
  done: false
  episode_len_mean: 796.8230352237049
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.4129889208014
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 184
  episodes_total: 81536
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.2703235789351003e-32
        cur_lr: 5.0e-05
        entropy: 0.05883107924213012
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007709367258939892
        total_loss: .inf
        vf_explained_var: 0.9994933605194092
        vf_loss: 0.242492056141297
    num_steps_sampled: 65040384
    num_steps_trained: 65040384
  iterations_since_restore: 402
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.925
    gpu_util_percent0: 0.306875
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629733687850607
    mean_env_wait_ms: 1.2041709420347784
    mean_inference_ms: 4.282927697654569
    mean_raw_obs_processing_ms: 0.37814990255906183
  time_since_restore: 10639.760772943497
  time_this_iter_s: 26.67217230796814
  time_total_s: 10639.760772943497
  timers:
    learn_throughput: 8327.694
    learn_time_ms: 19428.187
    sample_throughput: 23947.14
    sample_time_ms: 6756.214
    update_time_ms: 28.901
  timestamp: 1602731513
  timesteps_since_restore: 0
  timesteps_total: 65040384
  training_iteration: 402
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:11:54,677	WARNING util.py:136 -- The `process_trial` operation took 1.1358189582824707 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.8/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    402 |          10639.8 | 65040384 |  290.413 |              309.323 |              74.7778 |            796.823 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3035.8998776009794
    time_step_min: 2917
  date: 2020-10-15_03-12-21
  done: false
  episode_len_mean: 796.8402089603367
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.443806411765
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 202
  episodes_total: 81738
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 1.9054853684026505e-32
        cur_lr: 5.0e-05
        entropy: 0.06015664742638668
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009175410035823006
        total_loss: .inf
        vf_explained_var: 0.9995682835578918
        vf_loss: 0.2169381616016229
    num_steps_sampled: 65202176
    num_steps_trained: 65202176
  iterations_since_restore: 403
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.657575757575756
    gpu_util_percent0: 0.35878787878787877
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.8757575757575755
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629660355710547
    mean_env_wait_ms: 1.204116527884866
    mean_inference_ms: 4.282891456231739
    mean_raw_obs_processing_ms: 0.37814864787203445
  time_since_restore: 10666.367316246033
  time_this_iter_s: 26.60654330253601
  time_total_s: 10666.367316246033
  timers:
    learn_throughput: 8322.596
    learn_time_ms: 19440.087
    sample_throughput: 23952.55
    sample_time_ms: 6754.688
    update_time_ms: 29.139
  timestamp: 1602731541
  timesteps_since_restore: 0
  timesteps_total: 65202176
  training_iteration: 403
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:12:23,041	WARNING util.py:136 -- The `process_trial` operation took 1.1961266994476318 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    403 |          10666.4 | 65202176 |  290.444 |              309.323 |              74.7778 |             796.84 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3035.6861282683526
    time_step_min: 2917
  date: 2020-10-15_03-12-49
  done: false
  episode_len_mean: 796.8606027330405
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.47664048981767
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 222
  episodes_total: 81960
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 2.858228052603976e-32
        cur_lr: 5.0e-05
        entropy: 0.06449495845784743
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.006967006731429137
        total_loss: .inf
        vf_explained_var: 0.9992306232452393
        vf_loss: 0.4105289503931999
    num_steps_sampled: 65363968
    num_steps_trained: 65363968
  iterations_since_restore: 404
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.706249999999997
    gpu_util_percent0: 0.25
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.871875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462959869555323
    mean_env_wait_ms: 1.204055222601716
    mean_inference_ms: 4.2828470544933515
    mean_raw_obs_processing_ms: 0.378146092848785
  time_since_restore: 10692.881209611893
  time_this_iter_s: 26.513893365859985
  time_total_s: 10692.881209611893
  timers:
    learn_throughput: 8320.79
    learn_time_ms: 19444.308
    sample_throughput: 23952.785
    sample_time_ms: 6754.622
    update_time_ms: 27.768
  timestamp: 1602731569
  timesteps_since_restore: 0
  timesteps_total: 65363968
  training_iteration: 404
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:12:51,137	WARNING util.py:136 -- The `process_trial` operation took 1.1868534088134766 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    404 |          10692.9 | 65363968 |  290.477 |              309.323 |              74.7778 |            796.861 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3035.4966937418562
    time_step_min: 2917
  date: 2020-10-15_03-13-17
  done: false
  episode_len_mean: 796.8783032073519
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.5049693576259
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 195
  episodes_total: 82155
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.287342078905964e-32
        cur_lr: 5.0e-05
        entropy: 0.06533569780488808
        entropy_coeff: 0.0005000000000000001
        kl: 0.005928137999338408
        model: {}
        policy_loss: -0.007671467930776998
        total_loss: 0.28252197429537773
        vf_explained_var: 0.9994205832481384
        vf_loss: 0.2902261018753052
    num_steps_sampled: 65525760
    num_steps_trained: 65525760
  iterations_since_restore: 405
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.915625
    gpu_util_percent0: 0.27281249999999996
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629494355692932
    mean_env_wait_ms: 1.2040011078033699
    mean_inference_ms: 4.282806424747662
    mean_raw_obs_processing_ms: 0.37814420137976507
  time_since_restore: 10719.40005850792
  time_this_iter_s: 26.51884889602661
  time_total_s: 10719.40005850792
  timers:
    learn_throughput: 8331.395
    learn_time_ms: 19419.558
    sample_throughput: 23974.385
    sample_time_ms: 6748.536
    update_time_ms: 29.441
  timestamp: 1602731597
  timesteps_since_restore: 0
  timesteps_total: 65525760
  training_iteration: 405
  trial_id: 9f737_00000
  
2020-10-15 03:13:19,221	WARNING util.py:136 -- The `process_trial` operation took 1.166839838027954 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    405 |          10719.4 | 65525760 |  290.505 |              309.323 |              74.7778 |            796.878 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3035.332021045213
    time_step_min: 2917
  date: 2020-10-15_03-13-45
  done: false
  episode_len_mean: 796.8959762925538
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.5314026623523
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 182
  episodes_total: 82337
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.287342078905964e-32
        cur_lr: 5.0e-05
        entropy: 0.06395845053096612
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.009205860238580499
        total_loss: .inf
        vf_explained_var: 0.9994290471076965
        vf_loss: 0.2872546873986721
    num_steps_sampled: 65687552
    num_steps_trained: 65687552
  iterations_since_restore: 406
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.940625
    gpu_util_percent0: 0.29781250000000004
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1462943564904677
    mean_env_wait_ms: 1.2039518869486168
    mean_inference_ms: 4.282774709120137
    mean_raw_obs_processing_ms: 0.3781432690550357
  time_since_restore: 10745.918889760971
  time_this_iter_s: 26.518831253051758
  time_total_s: 10745.918889760971
  timers:
    learn_throughput: 8338.521
    learn_time_ms: 19402.962
    sample_throughput: 23963.194
    sample_time_ms: 6751.688
    update_time_ms: 28.992
  timestamp: 1602731625
  timesteps_since_restore: 0
  timesteps_total: 65687552
  training_iteration: 406
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:13:47,307	WARNING util.py:136 -- The `process_trial` operation took 1.1614699363708496 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    406 |          10745.9 | 65687552 |  290.531 |              309.323 |              74.7778 |            796.896 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3035.1240379604155
    time_step_min: 2917
  date: 2020-10-15_03-14-13
  done: false
  episode_len_mean: 796.9164698043492
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.56332542212976
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 208
  episodes_total: 82545
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 6.431013118358945e-32
        cur_lr: 5.0e-05
        entropy: 0.06387939397245646
        entropy_coeff: 0.0005000000000000001
        kl: 0.0039702630989874406
        model: {}
        policy_loss: -0.009056380348435292
        total_loss: 0.2292010709643364
        vf_explained_var: 0.9995669722557068
        vf_loss: 0.23828939100106558
    num_steps_sampled: 65849344
    num_steps_trained: 65849344
  iterations_since_restore: 407
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.703125
    gpu_util_percent0: 0.29125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629362778422136
    mean_env_wait_ms: 1.2038953950507025
    mean_inference_ms: 4.282735880434076
    mean_raw_obs_processing_ms: 0.37814134184209774
  time_since_restore: 10772.500865221024
  time_this_iter_s: 26.58197546005249
  time_total_s: 10772.500865221024
  timers:
    learn_throughput: 8347.424
    learn_time_ms: 19382.268
    sample_throughput: 24016.325
    sample_time_ms: 6736.751
    update_time_ms: 31.277
  timestamp: 1602731653
  timesteps_since_restore: 0
  timesteps_total: 65849344
  training_iteration: 407
  trial_id: 9f737_00000
  
2020-10-15 03:14:15,489	WARNING util.py:136 -- The `process_trial` operation took 1.1162848472595215 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    407 |          10772.5 | 65849344 |  290.563 |              309.323 |              74.7778 |            796.916 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3034.897143616957
    time_step_min: 2917
  date: 2020-10-15_03-14-42
  done: false
  episode_len_mean: 796.9375460641576
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.5974296215341
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 220
  episodes_total: 82765
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 3.2155065591794724e-32
        cur_lr: 5.0e-05
        entropy: 0.05691630393266678
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.008341272730225077
        total_loss: .inf
        vf_explained_var: 0.9995887279510498
        vf_loss: 0.2331764449675878
    num_steps_sampled: 66011136
    num_steps_trained: 66011136
  iterations_since_restore: 408
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.1125
    gpu_util_percent0: 0.31125
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.878125
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629277078605393
    mean_env_wait_ms: 1.203834615461073
    mean_inference_ms: 4.282692529548138
    mean_raw_obs_processing_ms: 0.37813926347835136
  time_since_restore: 10799.06071472168
  time_this_iter_s: 26.559849500656128
  time_total_s: 10799.06071472168
  timers:
    learn_throughput: 8349.864
    learn_time_ms: 19376.603
    sample_throughput: 24011.11
    sample_time_ms: 6738.214
    update_time_ms: 31.416
  timestamp: 1602731682
  timesteps_since_restore: 0
  timesteps_total: 66011136
  training_iteration: 408
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:14:43,598	WARNING util.py:136 -- The `process_trial` operation took 1.154482364654541 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 28.9/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | RUNNING  | 172.17.0.4:10000 |    408 |          10799.1 | 66011136 |  290.597 |              309.323 |              74.7778 |            796.938 |
+-------------------------+----------+------------------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_9f737_00000:
  custom_metrics:
    time_step_max: 4459
    time_step_mean: 3034.7133621989315
    time_step_min: 2917
  date: 2020-10-15_03-15-10
  done: true
  episode_len_mean: 796.9542139335271
  episode_reward_max: 309.32323232323205
  episode_reward_mean: 290.62612453816905
  episode_reward_min: 74.77777777777752
  episodes_this_iter: 186
  episodes_total: 82951
  experiment_id: d4b965977176421598950bd1542303ae
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 4.8232598387692094e-32
        cur_lr: 5.0e-05
        entropy: 0.0657297894358635
        entropy_coeff: 0.0005000000000000001
        kl: .inf
        model: {}
        policy_loss: -0.007584637001855299
        total_loss: .inf
        vf_explained_var: 0.9994227886199951
        vf_loss: 0.27785464624563855
    num_steps_sampled: 66172928
    num_steps_trained: 66172928
  iterations_since_restore: 409
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.721874999999997
    gpu_util_percent0: 0.375625
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.875
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 10000
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14629191381847287
    mean_env_wait_ms: 1.203784167188072
    mean_inference_ms: 4.282659039903279
    mean_raw_obs_processing_ms: 0.37813755161198853
  time_since_restore: 10825.52372932434
  time_this_iter_s: 26.463014602661133
  time_total_s: 10825.52372932434
  timers:
    learn_throughput: 8358.25
    learn_time_ms: 19357.163
    sample_throughput: 23993.059
    sample_time_ms: 6743.284
    update_time_ms: 31.65
  timestamp: 1602731710
  timesteps_since_restore: 0
  timesteps_total: 66172928
  training_iteration: 409
  trial_id: 9f737_00000
  
WARNING:root:NaN or Inf found in input tensor.
WARNING:root:NaN or Inf found in input tensor.
2020-10-15 03:15:11,889	WARNING util.py:136 -- The `process_trial` operation took 1.323237657546997 seconds to complete, which may be a performance bottleneck.
== Status ==
Memory usage on this node: 25.7/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | TERMINATED |       |    409 |          10825.5 | 66172928 |  290.626 |              309.323 |              74.7778 |            796.954 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 25.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/557.57 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |       ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_9f737_00000 | TERMINATED |       |    409 |          10825.5 | 66172928 |  290.626 |              309.323 |              74.7778 |            796.954 |
+-------------------------+------------+-------+--------+------------------+----------+----------+----------------------+----------------------+--------------------+


