2020-10-11 18:41:31,385	INFO services.py:1164 -- View the Ray dashboard at [1m[32mhttp://127.0.0.1:8265[39m[22m
== Status ==
Memory usage on this node: 11.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+-------+
| Trial name              | status   | loc   |
|-------------------------+----------+-------|
| PPO_jss_env_622ba_00000 | RUNNING  |       |
+-------------------------+----------+-------+


[2m[36m(pid=70886)[0m 2020-10-11 18:41:34,174	INFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
[2m[36m(pid=70863)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70863)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70892)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70892)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70912)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70912)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70896)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70896)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70877)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70877)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70895)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70895)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70919)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70919)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70853)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70853)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70882)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70882)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70881)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70881)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70906)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70906)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70901)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70901)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70864)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70864)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70868)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70868)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70858)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70858)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70866)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70866)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70796)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70796)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70905)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70905)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70884)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70884)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70792)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70792)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70820)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70820)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70798)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70798)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70807)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70807)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70799)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70799)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70887)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70887)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70890)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70890)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70801)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70801)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70803)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70803)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70862)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70862)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70848)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70848)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70782)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70782)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70786)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70786)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70784)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70784)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70788)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70788)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70889)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70889)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70802)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70802)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70850)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70850)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70872)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70872)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70785)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70785)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70916)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70916)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70795)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70795)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70861)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70861)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70899)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70899)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70852)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70852)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70811)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70811)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70844)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70844)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70797)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70797)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70789)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70789)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70817)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70817)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70793)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70793)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70783)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70783)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70791)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70791)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70894)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70894)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70854)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70854)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70842)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70842)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70812)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70812)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70822)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70822)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70860)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70860)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70891)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70891)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70871)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70871)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70805)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70805)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70846)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70846)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70865)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70865)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70787)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70787)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70845)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70845)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70856)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70856)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70859)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70859)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70875)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70875)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70888)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70888)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70815)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70815)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70810)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70810)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70879)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70879)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70794)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70794)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70867)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70867)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70855)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70855)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70883)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70883)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70821)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70821)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70874)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70874)[0m   tensor = torch.from_numpy(np.asarray(item))
[2m[36m(pid=70857)[0m /root/miniconda3/lib/python3.8/site-packages/ray/rllib/utils/torch_ops.py:65: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629401015/work/torch/csrc/utils/tensor_numpy.cpp:141.)
[2m[36m(pid=70857)[0m   tensor = torch.from_numpy(np.asarray(item))
Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4054
    time_step_mean: 3615.0923076923077
    time_step_min: 3379
  date: 2020-10-11_18-42-10
  done: false
  episode_len_mean: 891.1139240506329
  episode_reward_max: 258.59595959595964
  episode_reward_mean: 216.07678046285614
  episode_reward_min: 145.7171717171716
  episodes_this_iter: 158
  episodes_total: 158
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.20000000000000004
        cur_lr: 5.0e-05
        entropy: 1.1845979293187459
        entropy_coeff: 0.0001
        kl: 0.004941714345477521
        model: {}
        policy_loss: -0.010662895229567463
        total_loss: 502.23693593343097
        vf_explained_var: 0.5664147734642029
        vf_loss: 502.24672444661456
    num_steps_sampled: 161792
    num_steps_trained: 161792
  iterations_since_restore: 1
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 25.20810810810811
    gpu_util_percent0: 0.34891891891891885
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.581081081081081
    vram_util_percent0: 0.08933146002676999
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1664800252322615
    mean_env_wait_ms: 1.1663706356377173
    mean_inference_ms: 5.595255395666101
    mean_raw_obs_processing_ms: 0.44349043852902365
  time_since_restore: 31.27917718887329
  time_this_iter_s: 31.27917718887329
  time_total_s: 31.27917718887329
  timers:
    learn_throughput: 7253.058
    learn_time_ms: 22306.728
    sample_throughput: 18320.966
    sample_time_ms: 8830.975
    update_time_ms: 41.172
  timestamp: 1602441730
  timesteps_since_restore: 0
  timesteps_total: 161792
  training_iteration: 1
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 27.6/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      1 |          31.2792 | 161792 |  216.077 |              258.596 |              145.717 |            891.114 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4055
    time_step_mean: 3620.9201388888887
    time_step_min: 3341
  date: 2020-10-11_18-42-41
  done: false
  episode_len_mean: 890.6012658227849
  episode_reward_max: 262.6868686868683
  episode_reward_mean: 215.81786216596322
  episode_reward_min: 116.4747474747471
  episodes_this_iter: 158
  episodes_total: 316
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1522138218084972
        entropy_coeff: 0.0001
        kl: 0.007931098264331618
        model: {}
        policy_loss: -0.011280511661122242
        total_loss: 126.10309092203777
        vf_explained_var: 0.8166090846061707
        vf_loss: 126.1136926015218
    num_steps_sampled: 323584
    num_steps_trained: 323584
  iterations_since_restore: 2
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.47714285714286
    gpu_util_percent0: 0.3782857142857143
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7628571428571433
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16288274247959839
    mean_env_wait_ms: 1.1639329801148912
    mean_inference_ms: 5.408732157946938
    mean_raw_obs_processing_ms: 0.43382407019505514
  time_since_restore: 61.52647805213928
  time_this_iter_s: 30.24730086326599
  time_total_s: 61.52647805213928
  timers:
    learn_throughput: 7218.964
    learn_time_ms: 22412.08
    sample_throughput: 19651.705
    sample_time_ms: 8232.975
    update_time_ms: 42.627
  timestamp: 1602441761
  timesteps_since_restore: 0
  timesteps_total: 323584
  training_iteration: 2
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      2 |          61.5265 | 323584 |  215.818 |              262.687 |              116.475 |            890.601 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4055
    time_step_mean: 3616.4058295964123
    time_step_min: 3341
  date: 2020-10-11_18-43-10
  done: false
  episode_len_mean: 884.4451476793249
  episode_reward_max: 262.6868686868683
  episode_reward_mean: 218.03752717043835
  episode_reward_min: 116.4747474747471
  episodes_this_iter: 158
  episodes_total: 474
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1405681769053142
        entropy_coeff: 0.0001
        kl: 0.009609605185687542
        model: {}
        policy_loss: -0.0148115831737717
        total_loss: 52.60168711344401
        vf_explained_var: 0.9039597511291504
        vf_loss: 52.61565113067627
    num_steps_sampled: 485376
    num_steps_trained: 485376
  iterations_since_restore: 3
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.373529411764704
    gpu_util_percent0: 0.3391176470588236
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7735294117647062
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.16031659588832678
    mean_env_wait_ms: 1.1642242883286065
    mean_inference_ms: 5.246955067526088
    mean_raw_obs_processing_ms: 0.4259677486543139
  time_since_restore: 90.91913604736328
  time_this_iter_s: 29.392657995224
  time_total_s: 90.91913604736328
  timers:
    learn_throughput: 7241.098
    learn_time_ms: 22343.574
    sample_throughput: 20603.946
    sample_time_ms: 7852.477
    update_time_ms: 43.784
  timestamp: 1602441790
  timesteps_since_restore: 0
  timesteps_total: 485376
  training_iteration: 3
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.1/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      3 |          90.9191 | 485376 |  218.038 |              262.687 |              116.475 |            884.445 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4327
    time_step_mean: 3615.4139072847684
    time_step_min: 3304
  date: 2020-10-11_18-43-40
  done: false
  episode_len_mean: 879.493670886076
  episode_reward_max: 265.41414141414066
  episode_reward_mean: 217.98894003324364
  episode_reward_min: 110.4141414141415
  episodes_this_iter: 158
  episodes_total: 632
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.1268143057823181
        entropy_coeff: 0.0001
        kl: 0.008136198157444596
        model: {}
        policy_loss: -0.01660729798216683
        total_loss: 42.11880366007487
        vf_explained_var: 0.9290847182273865
        vf_loss: 42.13470904032389
    num_steps_sampled: 647168
    num_steps_trained: 647168
  iterations_since_restore: 4
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.10294117647059
    gpu_util_percent0: 0.36147058823529415
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7764705882352945
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15846372141219398
    mean_env_wait_ms: 1.1653484678602273
    mean_inference_ms: 5.1257152726721955
    mean_raw_obs_processing_ms: 0.419894565832805
  time_since_restore: 120.28875994682312
  time_this_iter_s: 29.36962389945984
  time_total_s: 120.28875994682312
  timers:
    learn_throughput: 7244.936
    learn_time_ms: 22331.735
    sample_throughput: 21188.169
    sample_time_ms: 7635.959
    update_time_ms: 43.039
  timestamp: 1602441820
  timesteps_since_restore: 0
  timesteps_total: 647168
  training_iteration: 4
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      4 |          120.289 | 647168 |  217.989 |              265.414 |              110.414 |            879.494 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4327
    time_step_mean: 3611.709973753281
    time_step_min: 3304
  date: 2020-10-11_18-44-09
  done: false
  episode_len_mean: 874.4481012658227
  episode_reward_max: 274.0505050505043
  episode_reward_mean: 218.87674210459
  episode_reward_min: 110.4141414141415
  episodes_this_iter: 158
  episodes_total: 790
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0861701965332031
        entropy_coeff: 0.0001
        kl: 0.007670978588672976
        model: {}
        policy_loss: -0.013802881830542901
        total_loss: 30.548245588938396
        vf_explained_var: 0.9537122845649719
        vf_loss: 30.561390558878582
    num_steps_sampled: 808960
    num_steps_trained: 808960
  iterations_since_restore: 5
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.070588235294117
    gpu_util_percent0: 0.41
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7735294117647062
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1570728592199228
    mean_env_wait_ms: 1.166939153381436
    mean_inference_ms: 5.03346713101367
    mean_raw_obs_processing_ms: 0.41508064448091997
  time_since_restore: 149.78548908233643
  time_this_iter_s: 29.496729135513306
  time_total_s: 149.78548908233643
  timers:
    learn_throughput: 7243.359
    learn_time_ms: 22336.597
    sample_throughput: 21526.756
    sample_time_ms: 7515.856
    update_time_ms: 41.81
  timestamp: 1602441849
  timesteps_since_restore: 0
  timesteps_total: 808960
  training_iteration: 5
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      5 |          149.785 | 808960 |  218.877 |              274.051 |              110.414 |            874.448 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3603.586592178771
    time_step_min: 3234
  date: 2020-10-11_18-44-39
  done: false
  episode_len_mean: 865.5980036297641
  episode_reward_max: 276.0202020202017
  episode_reward_mean: 220.3214724376247
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 312
  episodes_total: 1102
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0862921973069508
        entropy_coeff: 0.0001
        kl: 0.007875574054196477
        model: {}
        policy_loss: -0.013222926047092187
        total_loss: 36.798745473225914
        vf_explained_var: 0.9591273665428162
        vf_loss: 36.811290423075356
    num_steps_sampled: 970752
    num_steps_trained: 970752
  iterations_since_restore: 6
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.885714285714286
    gpu_util_percent0: 0.376
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.765714285714286
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1552253727421544
    mean_env_wait_ms: 1.1708524925769397
    mean_inference_ms: 4.909764741179798
    mean_raw_obs_processing_ms: 0.4090652128889821
  time_since_restore: 179.2623269557953
  time_this_iter_s: 29.476837873458862
  time_total_s: 179.2623269557953
  timers:
    learn_throughput: 7239.352
    learn_time_ms: 22348.961
    sample_throughput: 21785.609
    sample_time_ms: 7426.554
    update_time_ms: 41.277
  timestamp: 1602441879
  timesteps_since_restore: 0
  timesteps_total: 970752
  training_iteration: 6
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |     ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      6 |          179.262 | 970752 |  220.321 |               276.02 |              109.051 |            865.598 |
+-------------------------+----------+------------------+--------+------------------+--------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3596.4053398058254
    time_step_min: 3234
  date: 2020-10-11_18-45-08
  done: false
  episode_len_mean: 861.0213607594936
  episode_reward_max: 276.0202020202017
  episode_reward_mean: 221.51969856795785
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 162
  episodes_total: 1264
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0740437110265095
        entropy_coeff: 0.0001
        kl: 0.008695898888011774
        model: {}
        policy_loss: -0.015221895941067487
        total_loss: 17.440695921579998
        vf_explained_var: 0.9710730910301208
        vf_loss: 17.45515553156535
    num_steps_sampled: 1132544
    num_steps_trained: 1132544
  iterations_since_restore: 7
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.32058823529412
    gpu_util_percent0: 0.3317647058823529
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7852941176470587
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.154530025534869
    mean_env_wait_ms: 1.1725547609258773
    mean_inference_ms: 4.863332721949732
    mean_raw_obs_processing_ms: 0.40682603817267987
  time_since_restore: 208.61806631088257
  time_this_iter_s: 29.35573935508728
  time_total_s: 208.61806631088257
  timers:
    learn_throughput: 7246.158
    learn_time_ms: 22327.969
    sample_throughput: 21939.752
    sample_time_ms: 7374.377
    update_time_ms: 41.206
  timestamp: 1602441908
  timesteps_since_restore: 0
  timesteps_total: 1132544
  training_iteration: 7
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      7 |          208.618 | 1132544 |   221.52 |               276.02 |              109.051 |            861.021 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3589.5918220946915
    time_step_min: 3234
  date: 2020-10-11_18-45-38
  done: false
  episode_len_mean: 856.4817158931083
  episode_reward_max: 276.0202020202017
  episode_reward_mean: 222.6242168520648
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 158
  episodes_total: 1422
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0498243769009907
        entropy_coeff: 0.0001
        kl: 0.007762666131990652
        model: {}
        policy_loss: -0.01506129972403869
        total_loss: 18.5674090385437
        vf_explained_var: 0.9686254858970642
        vf_loss: 18.581799030303955
    num_steps_sampled: 1294336
    num_steps_trained: 1294336
  iterations_since_restore: 8
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.511764705882356
    gpu_util_percent0: 0.3070588235294118
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7794117647058822
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15395558370788992
    mean_env_wait_ms: 1.1740950383699602
    mean_inference_ms: 4.824259445476165
    mean_raw_obs_processing_ms: 0.4048965586263207
  time_since_restore: 237.8823263645172
  time_this_iter_s: 29.264260053634644
  time_total_s: 237.8823263645172
  timers:
    learn_throughput: 7250.709
    learn_time_ms: 22313.955
    sample_throughput: 22092.896
    sample_time_ms: 7323.259
    update_time_ms: 40.269
  timestamp: 1602441938
  timesteps_since_restore: 0
  timesteps_total: 1294336
  training_iteration: 8
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      8 |          237.882 | 1294336 |  222.624 |               276.02 |              109.051 |            856.482 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3583.8647778493237
    time_step_min: 3234
  date: 2020-10-11_18-46-07
  done: false
  episode_len_mean: 852.1638203668564
  episode_reward_max: 276.0202020202017
  episode_reward_mean: 223.5045329959939
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 159
  episodes_total: 1581
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 1.0068772335847218
        entropy_coeff: 0.0001
        kl: 0.009047625819221139
        model: {}
        policy_loss: -0.015237496777748069
        total_loss: 19.367111682891846
        vf_explained_var: 0.9707355499267578
        vf_loss: 19.381545066833496
    num_steps_sampled: 1456128
    num_steps_trained: 1456128
  iterations_since_restore: 9
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.094285714285714
    gpu_util_percent0: 0.38142857142857145
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7771428571428576
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15346149790902208
    mean_env_wait_ms: 1.1757427571199577
    mean_inference_ms: 4.790542357135236
    mean_raw_obs_processing_ms: 0.40320785193427666
  time_since_restore: 267.5187027454376
  time_this_iter_s: 29.63637638092041
  time_total_s: 267.5187027454376
  timers:
    learn_throughput: 7244.772
    learn_time_ms: 22332.243
    sample_throughput: 22199.608
    sample_time_ms: 7288.057
    update_time_ms: 46.32
  timestamp: 1602441967
  timesteps_since_restore: 0
  timesteps_total: 1456128
  training_iteration: 9
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |      9 |          267.519 | 1456128 |  223.505 |               276.02 |              109.051 |            852.164 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3569.9983905579397
    time_step_min: 3234
  date: 2020-10-11_18-46-37
  done: false
  episode_len_mean: 844.4392177589853
  episode_reward_max: 276.92929292929244
  episode_reward_mean: 225.801978559378
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 311
  episodes_total: 1892
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9910648465156555
        entropy_coeff: 0.0001
        kl: 0.007140809126819174
        model: {}
        policy_loss: -0.012344766136569282
        total_loss: 26.632726192474365
        vf_explained_var: 0.9699724316596985
        vf_loss: 26.64445622762044
    num_steps_sampled: 1617920
    num_steps_trained: 1617920
  iterations_since_restore: 10
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.5
    gpu_util_percent0: 0.3064705882352941
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7764705882352945
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15266093049721688
    mean_env_wait_ms: 1.1789334693723512
    mean_inference_ms: 4.737094007637805
    mean_raw_obs_processing_ms: 0.40057401456281355
  time_since_restore: 297.12480759620667
  time_this_iter_s: 29.606104850769043
  time_total_s: 297.12480759620667
  timers:
    learn_throughput: 7245.222
    learn_time_ms: 22330.853
    sample_throughput: 22227.943
    sample_time_ms: 7278.766
    update_time_ms: 45.543
  timestamp: 1602441997
  timesteps_since_restore: 0
  timesteps_total: 1617920
  training_iteration: 10
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     10 |          297.125 | 1617920 |  225.802 |              276.929 |              109.051 |            844.439 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3561.7685093780847
    time_step_min: 3234
  date: 2020-10-11_18-47-07
  done: false
  episode_len_mean: 840.8851022395327
  episode_reward_max: 276.929292929293
  episode_reward_mean: 227.08811090456646
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 162
  episodes_total: 2054
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9775462100903193
        entropy_coeff: 0.0001
        kl: 0.007571308485542734
        model: {}
        policy_loss: -0.017289329320192337
        total_loss: 13.902438004811605
        vf_explained_var: 0.9763643145561218
        vf_loss: 13.919067939122518
    num_steps_sampled: 1779712
    num_steps_trained: 1779712
  iterations_since_restore: 11
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.3
    gpu_util_percent0: 0.32457142857142857
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7800000000000002
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15233319563018138
    mean_env_wait_ms: 1.1804247496481288
    mean_inference_ms: 4.714597910354211
    mean_raw_obs_processing_ms: 0.3994846155797721
  time_since_restore: 326.62196373939514
  time_this_iter_s: 29.497156143188477
  time_total_s: 326.62196373939514
  timers:
    learn_throughput: 7242.298
    learn_time_ms: 22339.871
    sample_throughput: 22819.515
    sample_time_ms: 7090.072
    update_time_ms: 51.297
  timestamp: 1602442027
  timesteps_since_restore: 0
  timesteps_total: 1779712
  training_iteration: 11
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     11 |          326.622 | 1779712 |  227.088 |              276.929 |              109.051 |            840.885 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3553.1080586080584
    time_step_min: 3198
  date: 2020-10-11_18-47-36
  done: false
  episode_len_mean: 837.8318264014466
  episode_reward_max: 281.4747474747478
  episode_reward_mean: 228.27409264434567
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 158
  episodes_total: 2212
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9627491434415182
        entropy_coeff: 0.0001
        kl: 0.007169151833901803
        model: {}
        policy_loss: -0.014201596456890305
        total_loss: 13.748513221740723
        vf_explained_var: 0.9749014377593994
        vf_loss: 13.762094418207804
    num_steps_sampled: 1941504
    num_steps_trained: 1941504
  iterations_since_restore: 12
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.379411764705882
    gpu_util_percent0: 0.3508823529411766
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7823529411764705
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15204209242210945
    mean_env_wait_ms: 1.1817791367892942
    mean_inference_ms: 4.694686369077963
    mean_raw_obs_processing_ms: 0.39848884694159153
  time_since_restore: 356.3015396595001
  time_this_iter_s: 29.67957592010498
  time_total_s: 356.3015396595001
  timers:
    learn_throughput: 7242.591
    learn_time_ms: 22338.968
    sample_throughput: 23000.722
    sample_time_ms: 7034.214
    update_time_ms: 50.766
  timestamp: 1602442056
  timesteps_since_restore: 0
  timesteps_total: 1941504
  training_iteration: 12
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     12 |          356.302 | 1941504 |  228.274 |              281.475 |              109.051 |            837.832 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3540.2184145334436
    time_step_min: 3159
  date: 2020-10-11_18-48-06
  done: false
  episode_len_mean: 833.4930612244898
  episode_reward_max: 287.383838383838
  episode_reward_mean: 230.2338693052978
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 238
  episodes_total: 2450
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9222608854373296
        entropy_coeff: 0.0001
        kl: 0.007358024129644036
        model: {}
        policy_loss: -0.012704586464678869
        total_loss: 16.081321795781452
        vf_explained_var: 0.9782974123954773
        vf_loss: 16.093383073806763
    num_steps_sampled: 2103296
    num_steps_trained: 2103296
  iterations_since_restore: 13
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.888571428571428
    gpu_util_percent0: 0.3311428571428571
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7685714285714287
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1516506202059422
    mean_env_wait_ms: 1.1839309717328825
    mean_inference_ms: 4.6679607018693785
    mean_raw_obs_processing_ms: 0.39717454723639095
  time_since_restore: 386.0821316242218
  time_this_iter_s: 29.78059196472168
  time_total_s: 386.0821316242218
  timers:
    learn_throughput: 7235.333
    learn_time_ms: 22361.376
    sample_throughput: 22950.631
    sample_time_ms: 7049.567
    update_time_ms: 50.193
  timestamp: 1602442086
  timesteps_since_restore: 0
  timesteps_total: 2103296
  training_iteration: 13
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     13 |          386.082 | 2103296 |  230.234 |              287.384 |              109.051 |            833.493 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3529.1568848758466
    time_step_min: 3159
  date: 2020-10-11_18-48-36
  done: false
  episode_len_mean: 829.5833953834698
  episode_reward_max: 287.383838383838
  episode_reward_mean: 231.85658145114576
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 236
  episodes_total: 2686
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9192133545875549
        entropy_coeff: 0.0001
        kl: 0.006822101616611083
        model: {}
        policy_loss: -0.016151844184302416
        total_loss: 12.465920289357504
        vf_explained_var: 0.9800860285758972
        vf_loss: 12.481482028961182
    num_steps_sampled: 2265088
    num_steps_trained: 2265088
  iterations_since_restore: 14
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.697058823529414
    gpu_util_percent0: 0.41117647058823525
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7764705882352945
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15132621544972552
    mean_env_wait_ms: 1.1857662892562821
    mean_inference_ms: 4.646169029276983
    mean_raw_obs_processing_ms: 0.39609355732450824
  time_since_restore: 415.6353807449341
  time_this_iter_s: 29.55324912071228
  time_total_s: 415.6353807449341
  timers:
    learn_throughput: 7232.213
    learn_time_ms: 22371.023
    sample_throughput: 22921.722
    sample_time_ms: 7058.458
    update_time_ms: 49.22
  timestamp: 1602442116
  timesteps_since_restore: 0
  timesteps_total: 2265088
  training_iteration: 14
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     14 |          415.635 | 2265088 |  231.857 |              287.384 |              109.051 |            829.583 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3521.4108664772725
    time_step_min: 3159
  date: 2020-10-11_18-49-06
  done: false
  episode_len_mean: 827.1944444444445
  episode_reward_max: 287.383838383838
  episode_reward_mean: 233.05580062225619
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 158
  episodes_total: 2844
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.9072132706642151
        entropy_coeff: 0.0001
        kl: 0.007571491063572466
        model: {}
        policy_loss: -0.014775883017743277
        total_loss: 10.67902167638143
        vf_explained_var: 0.9788177013397217
        vf_loss: 10.693131049474081
    num_steps_sampled: 2426880
    num_steps_trained: 2426880
  iterations_since_restore: 15
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.10285714285714
    gpu_util_percent0: 0.34942857142857137
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7771428571428576
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15112948913164986
    mean_env_wait_ms: 1.1868970376303971
    mean_inference_ms: 4.632835251262484
    mean_raw_obs_processing_ms: 0.3954262660926718
  time_since_restore: 445.269362449646
  time_this_iter_s: 29.633981704711914
  time_total_s: 445.269362449646
  timers:
    learn_throughput: 7226.28
    learn_time_ms: 22389.39
    sample_throughput: 22932.425
    sample_time_ms: 7055.163
    update_time_ms: 49.153
  timestamp: 1602442146
  timesteps_since_restore: 0
  timesteps_total: 2426880
  training_iteration: 15
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     15 |          445.269 | 2426880 |  233.056 |              287.384 |              109.051 |            827.194 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3513.1579826319307
    time_step_min: 3159
  date: 2020-10-11_18-49-34
  done: false
  episode_len_mean: 824.6796823295831
  episode_reward_max: 287.383838383838
  episode_reward_mean: 234.19615412898
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 178
  episodes_total: 3022
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8721793442964554
        entropy_coeff: 0.0001
        kl: 0.006953845770719151
        model: {}
        policy_loss: -0.013813901699904818
        total_loss: 13.542894045511881
        vf_explained_var: 0.977494478225708
        vf_loss: 13.55609941482544
    num_steps_sampled: 2588672
    num_steps_trained: 2588672
  iterations_since_restore: 16
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 23.26666666666667
    gpu_util_percent0: 0.3987878787878788
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.775757575757576
    vram_util_percent0: 0.10437848474909807
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15092395103660428
    mean_env_wait_ms: 1.188198584811227
    mean_inference_ms: 4.61901893326744
    mean_raw_obs_processing_ms: 0.39473631972234147
  time_since_restore: 473.98322224617004
  time_this_iter_s: 28.713859796524048
  time_total_s: 473.98322224617004
  timers:
    learn_throughput: 7254.621
    learn_time_ms: 22301.923
    sample_throughput: 22897.046
    sample_time_ms: 7066.064
    update_time_ms: 48.614
  timestamp: 1602442174
  timesteps_since_restore: 0
  timesteps_total: 2588672
  training_iteration: 16
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     16 |          473.983 | 2588672 |  234.196 |              287.384 |              109.051 |             824.68 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3499.5743387047737
    time_step_min: 3159
  date: 2020-10-11_18-50-04
  done: false
  episode_len_mean: 820.9131745553211
  episode_reward_max: 287.383838383838
  episode_reward_mean: 236.24466552775257
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 295
  episodes_total: 3317
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8561922212441763
        entropy_coeff: 0.0001
        kl: 0.006693084452611704
        model: {}
        policy_loss: -0.013814363735339915
        total_loss: 11.206264972686768
        vf_explained_var: 0.9836785793304443
        vf_loss: 11.219495217005411
    num_steps_sampled: 2750464
    num_steps_trained: 2750464
  iterations_since_restore: 17
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 21.880000000000003
    gpu_util_percent0: 0.35114285714285715
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7714285714285714
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15061936876920487
    mean_env_wait_ms: 1.1901631288795955
    mean_inference_ms: 4.598500888052732
    mean_raw_obs_processing_ms: 0.39372602811990576
  time_since_restore: 503.6025516986847
  time_this_iter_s: 29.61932945251465
  time_total_s: 503.6025516986847
  timers:
    learn_throughput: 7246.068
    learn_time_ms: 22328.247
    sample_throughput: 22897.556
    sample_time_ms: 7065.907
    update_time_ms: 48.141
  timestamp: 1602442204
  timesteps_since_restore: 0
  timesteps_total: 2750464
  training_iteration: 17
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     17 |          503.603 | 2750464 |  236.245 |              287.384 |              109.051 |            820.913 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3494.1809744779584
    time_step_min: 3159
  date: 2020-10-11_18-50-34
  done: false
  episode_len_mean: 819.0002876869966
  episode_reward_max: 287.383838383838
  episode_reward_mean: 237.06808011065772
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 159
  episodes_total: 3476
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8518367956082026
        entropy_coeff: 0.0001
        kl: 0.006655753046895067
        model: {}
        policy_loss: -0.012292408112746974
        total_loss: 10.00081737836202
        vf_explained_var: 0.9804852604866028
        vf_loss: 10.012529214223227
    num_steps_sampled: 2912256
    num_steps_trained: 2912256
  iterations_since_restore: 18
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.032352941176473
    gpu_util_percent0: 0.33558823529411763
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7882352941176474
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15047190150620915
    mean_env_wait_ms: 1.1911365354578527
    mean_inference_ms: 4.588669913035827
    mean_raw_obs_processing_ms: 0.3932458295993541
  time_since_restore: 533.2093524932861
  time_this_iter_s: 29.60680079460144
  time_total_s: 533.2093524932861
  timers:
    learn_throughput: 7237.951
    learn_time_ms: 22353.289
    sample_throughput: 22871.328
    sample_time_ms: 7074.01
    update_time_ms: 48.434
  timestamp: 1602442234
  timesteps_since_restore: 0
  timesteps_total: 2912256
  training_iteration: 18
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     18 |          533.209 | 2912256 |  237.068 |              287.384 |              109.051 |                819 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3487.922949002217
    time_step_min: 3159
  date: 2020-10-11_18-51-04
  done: false
  episode_len_mean: 817.3264576457645
  episode_reward_max: 288.89898989898944
  episode_reward_mean: 238.01173450678394
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 160
  episodes_total: 3636
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8309680372476578
        entropy_coeff: 0.0001
        kl: 0.0074748302189012366
        model: {}
        policy_loss: -0.012168505093238005
        total_loss: 9.40784764289856
        vf_explained_var: 0.9818739295005798
        vf_loss: 9.41935165723165
    num_steps_sampled: 3074048
    num_steps_trained: 3074048
  iterations_since_restore: 19
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.12857142857143
    gpu_util_percent0: 0.3897142857142858
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7771428571428576
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15033201392986675
    mean_env_wait_ms: 1.1920887634656414
    mean_inference_ms: 4.579379054344755
    mean_raw_obs_processing_ms: 0.39278211048098977
  time_since_restore: 562.6794378757477
  time_this_iter_s: 29.470085382461548
  time_total_s: 562.6794378757477
  timers:
    learn_throughput: 7240.935
    learn_time_ms: 22344.075
    sample_throughput: 22873.71
    sample_time_ms: 7073.273
    update_time_ms: 40.962
  timestamp: 1602442264
  timesteps_since_restore: 0
  timesteps_total: 3074048
  training_iteration: 19
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.3/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     19 |          562.679 | 3074048 |  238.012 |              288.899 |              109.051 |            817.326 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3477.6596016343206
    time_step_min: 3086
  date: 2020-10-11_18-51-34
  done: false
  episode_len_mean: 814.6970081135903
  episode_reward_max: 298.4444444444441
  episode_reward_mean: 239.52641014608554
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 308
  episodes_total: 3944
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.8081399450699488
        entropy_coeff: 0.0001
        kl: 0.006784297137831648
        model: {}
        policy_loss: -0.01104643041617237
        total_loss: 11.86494255065918
        vf_explained_var: 0.9839428067207336
        vf_loss: 11.875391324361166
    num_steps_sampled: 3235840
    num_steps_trained: 3235840
  iterations_since_restore: 20
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.40857142857143
    gpu_util_percent0: 0.38542857142857145
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7685714285714287
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.15009020130346112
    mean_env_wait_ms: 1.1938104149330007
    mean_inference_ms: 4.562973108959315
    mean_raw_obs_processing_ms: 0.3919911921606026
  time_since_restore: 592.5581364631653
  time_this_iter_s: 29.878698587417603
  time_total_s: 592.5581364631653
  timers:
    learn_throughput: 7229.921
    learn_time_ms: 22378.116
    sample_throughput: 22897.882
    sample_time_ms: 7065.806
    update_time_ms: 40.76
  timestamp: 1602442294
  timesteps_since_restore: 0
  timesteps_total: 3235840
  training_iteration: 20
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 80/80 CPUs, 1/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 RUNNING)
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status   | loc              |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | RUNNING  | 172.17.0.4:70886 |     20 |          592.558 | 3235840 |  239.526 |              298.444 |              109.051 |            814.697 |
+-------------------------+----------+------------------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


Result for PPO_jss_env_622ba_00000:
  custom_metrics:
    time_step_max: 4336
    time_step_mean: 3472.3325980392156
    time_step_min: 3086
  date: 2020-10-11_18-52-03
  done: true
  episode_len_mean: 813.5421129503408
  episode_reward_max: 298.4444444444441
  episode_reward_mean: 240.3138197948324
  episode_reward_min: 109.05050505050495
  episodes_this_iter: 164
  episodes_total: 4108
  experiment_id: d2c99ef838e14e779777318f4b8fa462
  experiment_tag: '0'
  hostname: f85e62b52919
  info:
    learner:
      default_policy:
        allreduce_latency: 0.0
        cur_kl_coeff: 0.10000000000000002
        cur_lr: 5.0e-05
        entropy: 0.791996826728185
        entropy_coeff: 0.0001
        kl: 0.007390244165435433
        model: {}
        policy_loss: -0.014146684533140311
        total_loss: 7.371609568595886
        vf_explained_var: 0.985375702381134
        vf_loss: 7.385096549987793
    num_steps_sampled: 3397632
    num_steps_trained: 3397632
  iterations_since_restore: 21
  node_ip: 172.17.0.4
  num_healthy_workers: 79
  off_policy_estimator: {}
  perf:
    cpu_util_percent: 22.429411764705883
    gpu_util_percent0: 0.3902941176470588
    gpu_util_percent1: 0.0
    gpu_util_percent2: 0.0
    ram_util_percent: 3.7882352941176474
    vram_util_percent0: 0.10437848474909808
    vram_util_percent1: 0.0
    vram_util_percent2: 0.0
  pid: 70886
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.14997558762050514
    mean_env_wait_ms: 1.1946256935079285
    mean_inference_ms: 4.55510855465102
    mean_raw_obs_processing_ms: 0.39161722872220556
  time_since_restore: 622.1245801448822
  time_this_iter_s: 29.56644368171692
  time_total_s: 622.1245801448822
  timers:
    learn_throughput: 7231.765
    learn_time_ms: 22372.408
    sample_throughput: 22841.194
    sample_time_ms: 7083.343
    update_time_ms: 34.885
  timestamp: 1602442323
  timesteps_since_restore: 0
  timesteps_total: 3397632
  training_iteration: 21
  trial_id: 622ba_00000
  
== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | TERMINATED |       |     21 |          622.125 | 3397632 |  240.314 |              298.444 |              109.051 |            813.542 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


== Status ==
Memory usage on this node: 28.2/754.6 GiB
Using FIFO scheduling algorithm.
Resources requested: 0/80 CPUs, 0/3 GPUs, 0.0/558.25 GiB heap, 0.0/128.52 GiB objects (0/1.0 accelerator_type:RTX)
Result logdir: /root/ray_results/ppo-jss
Number of trials: 1 (1 TERMINATED)
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+
| Trial name              | status     | loc   |   iter |   total time (s) |      ts |   reward |   episode_reward_max |   episode_reward_min |   episode_len_mean |
|-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------|
| PPO_jss_env_622ba_00000 | TERMINATED |       |     21 |          622.125 | 3397632 |  240.314 |              298.444 |              109.051 |            813.542 |
+-------------------------+------------+-------+--------+------------------+---------+----------+----------------------+----------------------+--------------------+


